{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reasoning in Large Language Models"
      ],
      "id": "57b6c32e-a12e-41fd-9022-1a9b1355c40b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "a977e8eb-0d0d-4413-9cec-10415de2477a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"http://projects.rajivshah.com/images/hf/r-title.png\"\n",
        "alt=\"Reasoning\" />"
      ],
      "id": "e677d36e-c3b0-4b81-a204-dd55199d8875"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figcaption aria-hidden=\"true\">"
      ],
      "id": "74b9a34a-e1fd-43e1-8a91-f59359245c63"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reasoning"
      ],
      "id": "5440d24f-04bb-4bfa-82d8-5b77a3838e4a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figcaption>"
      ],
      "id": "a15c5de1-e259-4d14-a912-6ae22887ee19"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "fcd22a01-a664-457b-a7b2-7218ca11b775"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction\n",
        "\n",
        "I was wowed by ChatGPT. While I understood tasks like text generation\n",
        "and summarization, something was different with ChatGPT. When I looked\n",
        "at the literature, I saw this work exploring reasoning. Models\n",
        "reasoning, c’mon. As a very skeptical data scientist, that seemed\n",
        "far-fetched to me. But I had to explore.\n",
        "\n",
        "I came upon the [Big Bench\n",
        "Benchmark](https://github.com/google/BIG-bench), composed of more than\n",
        "200 reasoning tasks. The tasks include playing chess, describing code,\n",
        "guessing the perpetrator of a crime in a short story, identifying\n",
        "sarcasm, and even recognizing self-awareness. A common benchmark to test\n",
        "models is the Big Bench Hard (BBH), a subset of 23 tasks from Big Bench.\n",
        "Early models like OpenAI’s text-ada-00 struggle to reach a random score\n",
        "of 25. However, several newer models reach and surpass the average human\n",
        "rater score of 67.7. You can see results for these models in these\n",
        "publications: [1](https://arxiv.org/pdf/2301.13688.pdf),\n",
        "[2](https://arxiv.org/pdf/2210.09261.pdf), and\n",
        "[3](https://arxiv.org/pdf/2210.11416.pdf)."
      ],
      "id": "41376aaa-fa7c-4f0a-b930-a4c4b2c409e9"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "6092d779-20a0-4108-9846-4eab6aa6105c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"http://projects.rajivshah.com/images/hf/BBH.png\"\n",
        "alt=\"Big Bench Hard (23 Tasks) (1).png\" />"
      ],
      "id": "61fce54b-5b20-419b-8d1f-7b895d759bb9"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figcaption aria-hidden=\"true\">"
      ],
      "id": "95123057-b550-40ab-a335-d83ba0a94547"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Big Bench Hard (23 Tasks) (1).png"
      ],
      "id": "2fa81d35-87d4-41da-b5ba-beef2d2a62b7"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figcaption>"
      ],
      "id": "eec707c8-6a41-4cc5-932c-e669d7493765"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "930a2a3d-51f2-451b-968c-aa71fc556d7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A [survey of the\n",
        "research](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec09.pdf)\n",
        "pointed out some common starting points for evaluating reasoning in\n",
        "models, including Arithmetic Reasoning, Symbolic Reasoning, and\n",
        "Commonsense Reasoning. This blog post provides examples of reasoning,\n",
        "but you should try out all these examples yourself. Hugging Face has a\n",
        "[space where you can\n",
        "try](https://huggingface.co/spaces/osanseviero/i-like-flan) to test a\n",
        "Flan T5 model yourself.\n",
        "\n",
        "### Arithmetic **Reasoning**\n",
        "\n",
        "Let’s start with the following problem.\n",
        "\n",
        "    Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "    A: The answer is 5\n",
        "\n",
        "If you ask an older text generation model like GPT-2 to complete this,\n",
        "it doesn’t understand the question and instead continues to write a\n",
        "story like this.\n",
        "\n",
        "<img src=\"http://projects.rajivshah.com/images/hf/R-Cars-GPT2.png\" alt=\"R-Cars-GPT2.png\" style=\"zoom:50%;\" />\n",
        "\n",
        "While I don’t have access to PalM - 540B parameter model in the Big\n",
        "Bench, I was able to work with the Flan-T5 XXL using this publicly\n",
        "available space. I entered the problem and got this answer!"
      ],
      "id": "07cab3b6-13f0-439d-882e-1450aa28a61a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "03ee2d88-aabe-4ea3-89c7-7aa66780c6e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"http://projects.rajivshah.com/images/hf/R-Cars-Flan.png\"\n",
        "alt=\"R-Cars-Flan.png\" />"
      ],
      "id": "6f2531d6-f9eb-4b79-a6ad-67a727ee5079"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figcaption aria-hidden=\"true\">"
      ],
      "id": "3bfe5892-0708-49a5-8324-5d7ed03fe598"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "R-Cars-Flan.png"
      ],
      "id": "c70670ef-9fff-434c-b526-6832c4250f06"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figcaption>"
      ],
      "id": "c93ced68-09b0-48b4-94e1-0805f457e7ba"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "befc4297-47d7-42a6-a6d2-c4ff888f4535"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It solved it! I tried messing with it and changing the words, but it\n",
        "still answered correctly. To my untrained eye, it is trying to take the\n",
        "numbers and perform a calculation using the surrounding information.\n",
        "This is an elementary problem, but this is more sophisticated than the\n",
        "GPT-2 response. I next wanted to do a more challenging problem like\n",
        "this:\n",
        "\n",
        "    Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n",
        "\n",
        "The model gave an answer of 8, which isn’t correct. Recent research has\n",
        "found using chain-of-thought prompting can improve the ability of\n",
        "models. This involves providing intermediate reasoning to help the model\n",
        "determine the answer.\n",
        "\n",
        "    Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
        "    A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is \n",
        "\n",
        "The model correctly answers 11. To solve the juggling problem, I used\n",
        "this chain-of-thought prompt as an example. Giving the model some\n",
        "examples is known as few-shot learning. The new combined prompt using\n",
        "chain-of-thought and few-shot learning is:\n",
        "\n",
        "    Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
        "    A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
        "    Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n",
        "    A:\n",
        "\n",
        "Try it, it works! Giving it an example and making it think everything\n",
        "through step by step was beneficial. This was fascinating for me. We\n",
        "don’t train the model in the sense of updating it’s weights. Instead, we\n",
        "are guiding it purely by the inference process.\n",
        "\n",
        "### **Symbolic Reasoning**\n",
        "\n",
        "The first symbolic reasoning was doing a reversal and the Flan-T5 worked\n",
        "very well on this type of problem.\n",
        "\n",
        "    Reverse the sequence \"glasses, pen, alarm, license\".\n",
        "\n",
        "A more complex problem on coin flipping was more interesting for me.\n",
        "\n",
        "    Q: A coin is heads up. Tom does not flip the coin. Mike does not flip the coin. Is the coin still heads up?\n",
        "    A:\n",
        "\n",
        "For this one, I played around with different combinations of people\n",
        "flipping and showing the coin and the model, and it answered correctly.\n",
        "It was following the logic that was going through.\n",
        "\n",
        "### **Common sense reasoning**\n",
        "\n",
        "The last category was common sense reasoning and much less obvious to me\n",
        "how models know how to solve these problems correctly.\n",
        "\n",
        "    Q: What home entertainment equipment requires cable?\n",
        "    Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet\n",
        "    A: The answer is\n",
        "\n",
        "I was amazed at how well the model did, even when I changed the order."
      ],
      "id": "b85a030d-340b-4039-84bd-ac81a7b43c95"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure>"
      ],
      "id": "bbfdd4e4-04ec-4486-9ad8-4735da1226c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"http://projects.rajivshah.com/images/hf/Rcommon2.gif\"\n",
        "alt=\"Reasongif\" />"
      ],
      "id": "228aad4d-5ae1-4b53-bda5-3a0499bd3bf0"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figcaption aria-hidden=\"true\">"
      ],
      "id": "5ccc4c90-df43-43cc-8337-c9b2599127a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reasongif"
      ],
      "id": "9d36c9fa-a47e-47bc-a685-471faf6420cf"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figcaption>"
      ],
      "id": "4666eae3-d212-427a-a74f-4711466ffbb4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "510d81ff-673b-4cc8-90e4-ade821d39224"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another common reasoning example goes like this:\n",
        "\n",
        "    Q: Can Barack Obama have a conversation with George Washington? Give the rationale before answering.\n",
        "\n",
        "I changed around people to someone currently living, and it still works\n",
        "well.\n",
        "\n",
        "### **Thoughts**\n",
        "\n",
        "As the first step, please, go try out these models for yourself.\n",
        "[Google’s Flan-T5 is\n",
        "available](https://huggingface.co/google/flan-t5-xxl) with an Apache 2.0\n",
        "license. Hugging Face has a [space where you can\n",
        "try](https://huggingface.co/spaces/osanseviero/i-like-flan) all these\n",
        "reasoning examples yourself. You can also replicate this using OpenAI’s\n",
        "GPT or other language models. I have a [short video on the\n",
        "reasoning](https://youtu.be/teRu-ZT9XJs) that also shows several\n",
        "examples.\n",
        "\n",
        "The current language models have many known limitations. The next\n",
        "generation of models will likely be able to retrieve relevant\n",
        "information before answering. Additionally, language models will likely\n",
        "be able to delegate tasks to other services. You can see a demo of this\n",
        "integrating [ChatGPT with Wolfram’s scientific\n",
        "API](https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain). By\n",
        "letting language models offload other tasks, the role of language models\n",
        "will emphasize communication and reasoning.\n",
        "\n",
        "The current generation of models is starting to solve some reasoning\n",
        "tasks and match average human raters. It also appears that performance\n",
        "can still keep increasing. What happens when there are a set of\n",
        "reasoning tasks that computers are better than humans? While plenty of\n",
        "academic literature highlights the limitations, the overall trajectory\n",
        "is clear and has extraordinary implications."
      ],
      "id": "b8d987b6-a387-4a62-bdbe-7c5b64158974"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  }
}