{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Vectors to Agents: Managing RAG in an Agentic World\n",
        "\n",
        "## Video\n",
        "\n",
        "<https://youtu.be/AS_HlJbJjH8>\n",
        "\n",
        "Watch the [full video](https://youtu.be/AS_HlJbJjH8) \\|\n",
        "[Slides](https://youtu.be/AS_HlJbJjH8)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Annotated Presentation\n",
        "\n",
        "Below is an annotated version of the presentation, with timestamped\n",
        "links to the relevant parts of the video for each slide.\n",
        "\n",
        "Here is the slide-by-slide annotated presentation based on the video\n",
        "“From Vectors to Agents: Managing RAG in an Agentic World” by Rajiv\n",
        "Shah.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### 1. Title Slide\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_1.png\"\n",
        "alt=\"Slide 1\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 1</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:00](https://youtu.be/AS_HlJbJjH8&t=0s))\n",
        "\n",
        "The presentation begins with the title slide, introducing the core\n",
        "theme: **“From Vectors to Agents: Managing RAG in an Agentic World.”**\n",
        "The speaker, Rajiv Shah from Contextual, sets the stage for a technical\n",
        "deep dive into Retrieval-Augmented Generation (RAG).\n",
        "\n",
        "He outlines the agenda, promising to move beyond basic RAG concepts to\n",
        "focus specifically on **retrieval approaches**. The talk is designed to\n",
        "cover the spectrum from traditional methods like BM25 and Language\n",
        "Models to the emerging field of Agentic Search.\n",
        "\n",
        "### 2. ACME GPT\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_2.png\"\n",
        "alt=\"Slide 2\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 2</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:40](https://youtu.be/AS_HlJbJjH8&t=40s))\n",
        "\n",
        "This slide displays a stylized logo for “ACME GPT,” representing the\n",
        "typical enterprise aspiration. Companies see tools like ChatGPT and\n",
        "immediately want to apply that capability to their internal data, asking\n",
        "questions like, “Can I get the list of board of directors?”\n",
        "\n",
        "However, the speaker notes a common hurdle: generic models don’t know\n",
        "enterprise-specific knowledge. This sets up the necessity for\n",
        "RAG—injecting private data into the model—rather than relying solely on\n",
        "the model’s pre-trained knowledge.\n",
        "\n",
        "### 3. Building RAG is Easy\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_3.png\"\n",
        "alt=\"Slide 3\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 3</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:10](https://youtu.be/AS_HlJbJjH8&t=70s))\n",
        "\n",
        "The speaker illustrates the deceptively simple workflow of a basic RAG\n",
        "demo. The diagram shows the standard path: a user query is converted to\n",
        "vectors, matched against a database, and sent to an LLM.\n",
        "\n",
        "Shah acknowledges that building a “hello world” version of this is\n",
        "trivial. He notes, “You can build a very easy RAG demo out of the box by\n",
        "just grabbing some data, using an embedding model, creating vectors,\n",
        "doing the similarity.”\n",
        "\n",
        "### 4. Building RAG is Easy (Code Example)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_4.png\"\n",
        "alt=\"Slide 4\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 4</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:22](https://youtu.be/AS_HlJbJjH8&t=82s))\n",
        "\n",
        "A Python code snippet using **LangChain** is displayed to reinforce how\n",
        "accessible basic RAG has become. The code demonstrates loading a\n",
        "document, chunking it, and setting up a retrieval chain in just a few\n",
        "lines.\n",
        "\n",
        "This slide serves as a foil for the upcoming reality check. While the\n",
        "code works for a demo, it hides the immense complexity required to make\n",
        "such a system robust, accurate, and scalable in a real-world production\n",
        "environment.\n",
        "\n",
        "### 5. RAG Reality Check\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_5.png\"\n",
        "alt=\"Slide 5\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 5</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:35](https://youtu.be/AS_HlJbJjH8&t=95s))\n",
        "\n",
        "The tone shifts to the challenges of production. The slide highlights a\n",
        "sobering statistic: **95% of Gen AI projects fail to reach production**.\n",
        "The speaker details the specific reasons why demos fail when scaled:\n",
        "poor accuracy, unbearable latency, scaling issues with millions of\n",
        "documents, and ballooning costs.\n",
        "\n",
        "He emphasizes a critical, often overlooked factor: **Compliance**.\n",
        "“Inside an enterprise, not everybody gets to read every document.” A\n",
        "demo ignores entitlements, but a production system cannot.\n",
        "\n",
        "### 6. Maybe try a different RAG?\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_6.png\"\n",
        "alt=\"Slide 6\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 6</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:00](https://youtu.be/AS_HlJbJjH8&t=180s))\n",
        "\n",
        "This slide lists a dizzying array of RAG variants (GraphRAG, RAPTOR,\n",
        "CRAG, etc.) and retrieval techniques. It represents the “analysis\n",
        "paralysis” developers face when scouring arXiv papers for a solution to\n",
        "their accuracy problems.\n",
        "\n",
        "Shah warns against blindly chasing the latest academic paper to fix\n",
        "fundamental system issues. “The answer is not in here of pulling\n",
        "together like a bunch of archive papers.” Instead, he advocates for a\n",
        "structured framework to make decisions.\n",
        "\n",
        "### 7. Ultimate RAG Solution\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_7.png\"\n",
        "alt=\"Slide 7\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 7</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:30](https://youtu.be/AS_HlJbJjH8&t=210s))\n",
        "\n",
        "A humorous cartoon depicts a “Rube Goldberg” machine, representing the\n",
        "**“Ultimate RAG Solution.”** It mocks the tendency to over-engineer\n",
        "systems with too many interconnected, fragile components in the pursuit\n",
        "of performance.\n",
        "\n",
        "The speaker uses this visual to argue for simplicity and deliberate\n",
        "design. The goal is to avoid building a monstrosity that is impossible\n",
        "to maintain, urging the audience to think about trade-offs before\n",
        "complexity.\n",
        "\n",
        "### 8. RAG as a system\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_8.png\"\n",
        "alt=\"Slide 8\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 8</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:35](https://youtu.be/AS_HlJbJjH8&t=215s))\n",
        "\n",
        "The speaker introduces a clean system architecture for RAG, broken into\n",
        "four distinct stages: **Parsing, Querying, Retrieving, and Generation**.\n",
        "This framework serves as the mental map for the rest of the\n",
        "presentation.\n",
        "\n",
        "He highlights that “Parsing” is vastly overlooked—getting information\n",
        "out of complex documents cleanly is a prerequisite for success. Today’s\n",
        "talk, however, will zoom in specifically on the **Retrieving** and\n",
        "**Querying** components.\n",
        "\n",
        "### 9. Designing a RAG Solution\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_9.png\"\n",
        "alt=\"Slide 9\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 9</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 04:10](https://youtu.be/AS_HlJbJjH8&t=250s))\n",
        "\n",
        "This slide presents a “Tradeoff Triangle” for RAG, balancing **Problem\n",
        "Complexity, Latency, and Cost**. The speaker advises having a serious\n",
        "conversation with stakeholders about these constraints before writing\n",
        "code.\n",
        "\n",
        "A key concept introduced here is the **“Cost of a mistake.”** In coding\n",
        "assistants, a mistake is low-cost (the developer fixes it). In medical\n",
        "RAG systems, the cost of a mistake is high (life or death), which\n",
        "dictates a completely different architectural approach.\n",
        "\n",
        "### 10. RAG Considerations\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_10.png\"\n",
        "alt=\"Slide 10\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 10</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 05:30](https://youtu.be/AS_HlJbJjH8&t=330s))\n",
        "\n",
        "A detailed table breaks down specific considerations that influence RAG\n",
        "design, such as domain difficulty, multilingual requirements, and data\n",
        "quality. This slide was originally created for sales teams to help scope\n",
        "customer problems.\n",
        "\n",
        "Shah emphasizes that understanding the **nuances** of the use case\n",
        "upfront saves heartache later. For instance, knowing if users will ask\n",
        "simple questions or require complex reasoning changes the retrieval\n",
        "strategy entirely.\n",
        "\n",
        "### 11. Consider Query Complexity\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_11.png\"\n",
        "alt=\"Slide 11\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 11</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 06:15](https://youtu.be/AS_HlJbJjH8&t=375s))\n",
        "\n",
        "The speaker categorizes queries by complexity, ranging from simple\n",
        "**Keywords** (“Total Revenue”) to **Semantic** variations (“How much\n",
        "bank?”), to **Multi-hop** reasoning, and finally **Agentic** scenarios.\n",
        "\n",
        "He points out a common failure mode: “The answers aren’t in the\n",
        "documents… all of a sudden they’re asking for knowledge that’s outside.”\n",
        "Recognizing the query complexity determines whether you need a simple\n",
        "search engine or a complex agentic workflow.\n",
        "\n",
        "### 12. Retrieval (Highlighted)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_12.png\"\n",
        "alt=\"Slide 12\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 12</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 07:32](https://youtu.be/AS_HlJbJjH8&t=452s))\n",
        "\n",
        "The presentation zooms back into the system diagram, highlighting the\n",
        "**“Retrieving”** box. This signals the start of the deep technical dive\n",
        "into retrieval algorithms.\n",
        "\n",
        "Shah notes that this area causes the most confusion due to the sheer\n",
        "number of model choices and architectures available. He aims to provide\n",
        "a practical guide to selecting the right retrieval tool.\n",
        "\n",
        "### 13. Retrieval Approaches\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_13.png\"\n",
        "alt=\"Slide 13\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 13</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 08:16](https://youtu.be/AS_HlJbJjH8&t=496s))\n",
        "\n",
        "Three primary retrieval pillars are introduced: 1. **BM25:** The\n",
        "lexical, keyword-based standard. 2. **Language Models:** Semantic\n",
        "embeddings and vector search. 3. **Agentic Search:** The new frontier of\n",
        "iterative reasoning.\n",
        "\n",
        "The speaker emphasizes that documents must be broken into pieces\n",
        "(**chunking**) because no single model context window is efficient\n",
        "enough to hold all enterprise data for every query.\n",
        "\n",
        "### 14. Building RAG is Easy (Code Highlight)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_14.png\"\n",
        "alt=\"Slide 14\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 14</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 08:50](https://youtu.be/AS_HlJbJjH8&t=530s))\n",
        "\n",
        "Returning to the initial code snippet, the speaker highlights the\n",
        "`vectorstore` and `retriever` initialization lines. This pinpoints\n",
        "exactly where the upcoming concepts fit into the implementation.\n",
        "\n",
        "This visual anchor helps developers map the theoretical concepts of BM25\n",
        "and Embeddings back to the actual lines of code they write in libraries\n",
        "like LangChain or LlamaIndex.\n",
        "\n",
        "### 15. BM25\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_15.png\"\n",
        "alt=\"Slide 15\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 15</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:18](https://youtu.be/AS_HlJbJjH8&t=558s))\n",
        "\n",
        "**BM25 (Best Match 25)** is explained as a probabilistic lexical ranking\n",
        "function. The slide visualizes an **inverted index**, mapping words\n",
        "(like “butterfly”) to the specific documents containing them.\n",
        "\n",
        "Shah explains that this is the 25th iteration of the formula, designed\n",
        "to score documents based on word frequency and saturation. It remains a\n",
        "powerful, fast baseline for retrieval.\n",
        "\n",
        "### 16. BM25 Performance\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_16.png\"\n",
        "alt=\"Slide 16\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 16</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:55](https://youtu.be/AS_HlJbJjH8&t=595s))\n",
        "\n",
        "A table compares the speed of a **Linear Scan** (Ctrl+F style) versus an\n",
        "**Inverted Index (BM25)** as the document count grows from 1,000 to\n",
        "9,000.\n",
        "\n",
        "The data shows that linear search becomes exponentially slower (taking\n",
        "3,000 seconds for 1k documents in this synthetic test), while BM25\n",
        "remains orders of magnitude faster. This efficiency is why lexical\n",
        "search is still widely used in production.\n",
        "\n",
        "### 17. BM25 Failure Cases\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_17.png\"\n",
        "alt=\"Slide 17\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 17</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 11:08](https://youtu.be/AS_HlJbJjH8&t=668s))\n",
        "\n",
        "The limitations of BM25 are exposed. Because it relies on exact word\n",
        "matches, it fails when users use synonyms. If a user searches for\n",
        "**“Physician”** but the documents only contain **“Doctor,”** BM25 will\n",
        "return zero results.\n",
        "\n",
        "Similarly, it struggles with acronyms like **“IBM”** vs **“International\n",
        "Business Machines.”** Despite this, Shah argues BM25 is a “very strong\n",
        "baseline” that often beats complex neural models on specific\n",
        "keyword-heavy datasets.\n",
        "\n",
        "### 18. Hands on: BM25s\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_18.png\"\n",
        "alt=\"Slide 18\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 18</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 12:14](https://youtu.be/AS_HlJbJjH8&t=734s))\n",
        "\n",
        "For developers wanting to implement this, the slide points to a library\n",
        "called `bm25s`, a high-performance Python implementation available on\n",
        "Hugging Face.\n",
        "\n",
        "This reinforces the practical nature of the talk—BM25 isn’t just a\n",
        "legacy concept; it is an active, installable tool that developers should\n",
        "consider using alongside vector search.\n",
        "\n",
        "### 19. Enter Language Models\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_19.png\"\n",
        "alt=\"Slide 19\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 19</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 12:24](https://youtu.be/AS_HlJbJjH8&t=744s))\n",
        "\n",
        "The talk transitions to **Language Models (Embeddings)**. The slide\n",
        "explains how an encoder model turns text into a dense vector (a list of\n",
        "numbers) that captures semantic meaning.\n",
        "\n",
        "Because these models are trained on vast amounts of data, they “have an\n",
        "idea of these similar concepts.” This solves the synonym problem that\n",
        "plagues BM25.\n",
        "\n",
        "### 20. Embeddings Visualized\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_20.png\"\n",
        "alt=\"Slide 20\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 20</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 12:50](https://youtu.be/AS_HlJbJjH8&t=770s))\n",
        "\n",
        "A 2D visualization demonstrates how embeddings group related concepts in\n",
        "**latent space**. The word “Doctor” and “Physician” would be located\n",
        "very close to each other mathematically.\n",
        "\n",
        "This spatial proximity allows for **Semantic Search**: finding documents\n",
        "that mean the same thing as the query, even if they don’t share a single\n",
        "word.\n",
        "\n",
        "### 21. Semantic search is widely used\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_21.png\"\n",
        "alt=\"Slide 21\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 21</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 13:15](https://youtu.be/AS_HlJbJjH8&t=795s))\n",
        "\n",
        "The speaker validates the importance of semantic search by showing a\n",
        "tweet from Google’s SearchLiaison regarding BERT, and a screenshot of\n",
        "Hugging Face’s model repository.\n",
        "\n",
        "This confirms that semantic search is the industry standard for modern\n",
        "information retrieval, having been deployed at massive scale by tech\n",
        "giants to improve result relevance.\n",
        "\n",
        "### 22. Which language model?\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_22.png\"\n",
        "alt=\"Slide 22\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 22</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 13:30](https://youtu.be/AS_HlJbJjH8&t=810s))\n",
        "\n",
        "A scatter plot compares various models based on **Inference Speed**\n",
        "(X-axis) and **NDCG@10** (Y-axis, a measure of retrieval quality).\n",
        "\n",
        "Shah places **BM25** on the right (fast but lower accuracy) to orient\n",
        "the audience. He points out that there is a massive variety of models\n",
        "with different trade-offs between compute cost and retrieval quality.\n",
        "\n",
        "### 23. Static Embeddings\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_23.png\"\n",
        "alt=\"Slide 23\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 23</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 14:43](https://youtu.be/AS_HlJbJjH8&t=883s))\n",
        "\n",
        "The speaker introduces **Static Embeddings** (like Word2Vec or GloVe)\n",
        "which are located on the far right of the previous scatter\n",
        "plot—extremely fast, even on CPUs.\n",
        "\n",
        "These models assign a fixed vector to every word. While efficient, they\n",
        "lack context. The word “bank” has the same vector whether referring to a\n",
        "river bank or a financial bank, which limits their accuracy.\n",
        "\n",
        "### 24. Why Context Matters\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_24.png\"\n",
        "alt=\"Slide 24\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 24</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 15:16](https://youtu.be/AS_HlJbJjH8&t=916s))\n",
        "\n",
        "A cartoon illustrates the difference between Static Embeddings and\n",
        "Transformers. The Transformer can distinguish between “Model” in a data\n",
        "science context versus “Model” in a fashion context.\n",
        "\n",
        "This contextual awareness is why modern Transformer-based embeddings\n",
        "(like BERT) generally outperform static embeddings and BM25 in complex\n",
        "retrieval tasks, despite being slower.\n",
        "\n",
        "### 25. Many more models!\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_25.png\"\n",
        "alt=\"Slide 25\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 25</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 15:55](https://youtu.be/AS_HlJbJjH8&t=955s))\n",
        "\n",
        "Returning to the scatter plot, a red arrow points toward the top-left\n",
        "quadrant—models that are slower but achieve higher accuracy.\n",
        "\n",
        "The speaker notes that the field is constantly evolving, with “newer\n",
        "generations of models” pushing the boundary of what is possible in terms\n",
        "of retrieval quality.\n",
        "\n",
        "### 26. MTEB/RTEB\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_26.png\"\n",
        "alt=\"Slide 26\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 26</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:35](https://youtu.be/AS_HlJbJjH8&t=995s))\n",
        "\n",
        "To help developers choose, Shah introduces the **MTEB (Massive Text\n",
        "Embedding Benchmark)** and **RTEB (Retrieval Text Embedding\n",
        "Benchmark)**. These are leaderboards hosted on Hugging Face.\n",
        "\n",
        "He highlights a key distinction: MTEB uses public datasets, while RTEB\n",
        "uses **private, held-out datasets**. This is crucial for avoiding “data\n",
        "contamination,” where models perform well simply because they were\n",
        "trained on the test data.\n",
        "\n",
        "### 27. Selecting an embedding model\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_27.png\"\n",
        "alt=\"Slide 27\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 27</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:48](https://youtu.be/AS_HlJbJjH8&t=1008s))\n",
        "\n",
        "The speaker switches to a live browser view (captured in the slide) of\n",
        "the leaderboard. He discusses the bubble chart visualization where size\n",
        "often correlates with parameter count.\n",
        "\n",
        "He points out an interesting trend: “You’ll see that there’s a bunch of\n",
        "models here that are all the same size… but the performance differs.”\n",
        "This indicates improvements in training strategies and architecture\n",
        "rather than just throwing more compute at the problem.\n",
        "\n",
        "### 28. Selecting an embedding model (Other Considerations)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_28.png\"\n",
        "alt=\"Slide 28\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 28</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 19:07](https://youtu.be/AS_HlJbJjH8&t=1147s))\n",
        "\n",
        "Beyond the leaderboard score, Shah lists practical selection criteria:\n",
        "**Model Size** (can it fit in memory?), **Architecture** (CPU vs GPU),\n",
        "**Embedding Dimension** (storage costs), and **Training Data**\n",
        "(multilingual support).\n",
        "\n",
        "He advises checking if a model is open source and quantizable, as this\n",
        "can significantly reduce latency without a major hit to accuracy.\n",
        "\n",
        "### 29. Matryoshka Embedding Models\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_29.png\"\n",
        "alt=\"Slide 29\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 29</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 20:53](https://youtu.be/AS_HlJbJjH8&t=1253s))\n",
        "\n",
        "A specific innovation is highlighted: **Matryoshka Embeddings**. These\n",
        "models allow developers to truncate vectors (e.g., from 768 dimensions\n",
        "down to 64) while retaining most of the performance.\n",
        "\n",
        "This is a “neat kind of innovation” for optimizing storage and search\n",
        "speed. OpenAI’s newer models also support this feature, offering\n",
        "flexibility between cost and accuracy.\n",
        "\n",
        "### 30. Sentence Transformer\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_30.png\"\n",
        "alt=\"Slide 30\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 30</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:42](https://youtu.be/AS_HlJbJjH8&t=1302s))\n",
        "\n",
        "The **Sentence Transformer** architecture is described as the dominant\n",
        "approach for RAG. Unlike standard BERT which works on tokens, these are\n",
        "fine-tuned to understand full sentences and paragraphs.\n",
        "\n",
        "This architecture uses Siamese networks to ensure that semantically\n",
        "similar sentences are close in vector space, making them ideal for the\n",
        "“chunk-level” retrieval required in RAG.\n",
        "\n",
        "### 31. Cross Encoder / Reranker\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_31.png\"\n",
        "alt=\"Slide 31\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 31</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 22:16](https://youtu.be/AS_HlJbJjH8&t=1336s))\n",
        "\n",
        "The concept of a **Cross Encoder (or Reranker)** is introduced. Unlike\n",
        "the bi-encoder (retriever) which processes query and document\n",
        "separately, the cross-encoder processes them *together*.\n",
        "\n",
        "This allows for a much deeper calculation of relevance. It is typically\n",
        "used as a second stage: retrieve 50 documents quickly with vectors, then\n",
        "use the slow but accurate Cross Encoder to rank the top 5.\n",
        "\n",
        "### 32. Cross Encoder / Reranker (Duplicate)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_32.png\"\n",
        "alt=\"Slide 32\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 32</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 22:16](https://youtu.be/AS_HlJbJjH8&t=1336s))\n",
        "\n",
        "(This slide reinforces the previous diagram, emphasizing the “crossing”\n",
        "of the query and document in the model architecture.)\n",
        "\n",
        "### 33. Cross Encoder / Reranker (Accuracy Boost)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_33.png\"\n",
        "alt=\"Slide 33\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 33</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:07](https://youtu.be/AS_HlJbJjH8&t=1387s))\n",
        "\n",
        "A bar chart quantifies the value of reranking. It shows a significant\n",
        "boost in **NDCG (accuracy)** when a reranker is added to the pipeline.\n",
        "\n",
        "The speaker notes that while you get a “bump” in quality, it “doesn’t\n",
        "come for free.” The trade-off is increased latency, as the cross-encoder\n",
        "is computationally expensive.\n",
        "\n",
        "### 34. Cross Encoder / Reranker (Execution Flow)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_34.png\"\n",
        "alt=\"Slide 34\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 34</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:15](https://youtu.be/AS_HlJbJjH8&t=1395s))\n",
        "\n",
        "The execution flow diagram highlights the reranker’s position in the\n",
        "pipeline. It sits between the Vector Store retrieval and the LLM\n",
        "generation.\n",
        "\n",
        "This visual reinforces the latency implication: the user has to wait for\n",
        "both the initial search *and* the reranking pass before the LLM even\n",
        "starts generating an answer.\n",
        "\n",
        "### 35. Hands On: Retriever & Reranker\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_35.png\"\n",
        "alt=\"Slide 35\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 35</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:30](https://youtu.be/AS_HlJbJjH8&t=1410s))\n",
        "\n",
        "A screenshot of a Google Colab notebook is shown, demonstrating a\n",
        "practical implementation of the Retrieve and Re-rank strategy using the\n",
        "`SentenceTransformer` and `CrossEncoder` libraries.\n",
        "\n",
        "This provides a concrete resource for the audience to test the accuracy\n",
        "vs. speed trade-offs themselves on simple datasets like Wikipedia.\n",
        "\n",
        "### 36. Instruction Following Reranker\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_36.png\"\n",
        "alt=\"Slide 36\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 36</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:48](https://youtu.be/AS_HlJbJjH8&t=1428s))\n",
        "\n",
        "Shah mentions a specific advancement: **Instruction Following\n",
        "Rerankers** (developed by his company, Contextual). These allow\n",
        "developers to pass a prompt to the reranker, such as “Prioritize safety\n",
        "notices.”\n",
        "\n",
        "This adds a “knob” for developers to tune retrieval based on business\n",
        "logic without retraining the model.\n",
        "\n",
        "### 37. Combine Multiple Retrievers\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_37.png\"\n",
        "alt=\"Slide 37\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 37</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:19](https://youtu.be/AS_HlJbJjH8&t=1459s))\n",
        "\n",
        "The presentation suggests that you don’t have to pick just one method.\n",
        "You can combine BM25, various embedding models (E5, BGE), and rerankers.\n",
        "\n",
        "While combining them (Ensemble Retrieval) often yields better recall,\n",
        "Shah warns that “you got to engineer this.” Managing multiple indexes\n",
        "and fusion logic increases operational complexity and compute costs.\n",
        "\n",
        "### 38. Cascading Rerankers in Kaggle\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_38.png\"\n",
        "alt=\"Slide 38\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 38</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:56](https://youtu.be/AS_HlJbJjH8&t=1496s))\n",
        "\n",
        "A complex diagram from a Kaggle competition winner illustrates a\n",
        "**Cascade Strategy**. The solution used three different rerankers,\n",
        "filtering from 64 documents down to 8, and then to 5.\n",
        "\n",
        "This shows the extreme end of retrieval engineering, where multiple\n",
        "models are chained to squeeze out every percentage point of accuracy.\n",
        "\n",
        "### 39. Best practices\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_39.png\"\n",
        "alt=\"Slide 39\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 39</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 25:16](https://youtu.be/AS_HlJbJjH8&t=1516s))\n",
        "\n",
        "Shah distills the complexity into a recommended **Best Practice**: 1.\n",
        "**Hybrid Search:** Combine Semantic Search (Vectors) and Lexical Search\n",
        "(BM25). 2. **Reciprocal Rank Fusion:** Merge the results. 3.\n",
        "**Reranker:** Pass the top results through a cross-encoder.\n",
        "\n",
        "This setup provides a “pretty good standard performance out of the box”\n",
        "and should be the default baseline before trying exotic methods.\n",
        "\n",
        "### 40. Families of Embedding Models\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_40.png\"\n",
        "alt=\"Slide 40\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 40</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 25:42](https://youtu.be/AS_HlJbJjH8&t=1542s))\n",
        "\n",
        "A taxonomy slide categorizes the models discussed: **Static**\n",
        "(Fastest/Low Accuracy), **Bi-Encoders** (Fast/Good Accuracy), and\n",
        "**Cross-Encoders** (Slow/Best Accuracy).\n",
        "\n",
        "This summary helps the audience mentally organize the tools available in\n",
        "their toolbox.\n",
        "\n",
        "### 41. Lots of New Models\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_41.png\"\n",
        "alt=\"Slide 41\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 41</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 25:50](https://youtu.be/AS_HlJbJjH8&t=1550s))\n",
        "\n",
        "Logos for IBM Granite, Google EmbeddingGemma, and others appear. The\n",
        "speaker notes that while new models from major players appear weekly,\n",
        "the improvements are often “incremental.”\n",
        "\n",
        "He advises against “ripping up” a working system just to switch to a\n",
        "model that is 1% better on a leaderboard.\n",
        "\n",
        "### 42. Other retrieval methods\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_42.png\"\n",
        "alt=\"Slide 42\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 42</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:18](https://youtu.be/AS_HlJbJjH8&t=1578s))\n",
        "\n",
        "Alternative methods are briefly listed: **SPLADE** (Sparse retrieval),\n",
        "**ColBERT** (Late interaction), and **GraphRAG**.\n",
        "\n",
        "Shah acknowledges these exist and may fit specific niches, but warns\n",
        "against chasing the “flavor of the week” before establishing a solid\n",
        "baseline with hybrid search.\n",
        "\n",
        "### 43. Operational Concerns\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_43.png\"\n",
        "alt=\"Slide 43\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 43</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:30](https://youtu.be/AS_HlJbJjH8&t=1650s))\n",
        "\n",
        "The talk shifts to operations. Libraries like **FAISS** are mentioned\n",
        "for efficient vector similarity search.\n",
        "\n",
        "A key point is that for many use cases, you can simply store embeddings\n",
        "**in memory**. You don’t always need a complex vector database if your\n",
        "dataset fits in RAM.\n",
        "\n",
        "### 44. Vector Database Options\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_44.png\"\n",
        "alt=\"Slide 44\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 44</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:55](https://youtu.be/AS_HlJbJjH8&t=1675s))\n",
        "\n",
        "A diagram categorizes storage into **Hot (In-Memory)**, **Warm\n",
        "(SSD/Disk)**, and **Cold** tiers.\n",
        "\n",
        "Shah notes there are “tons of vector database options” (Snowflake,\n",
        "Pinecone, etc.). The choice should be governed by **latency\n",
        "requirements**. If you need sub-millisecond retrieval, you need\n",
        "in-memory storage.\n",
        "\n",
        "### 45. Operational Concerns (Datastore Size)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_45.png\"\n",
        "alt=\"Slide 45\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 45</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:40](https://youtu.be/AS_HlJbJjH8&t=1720s))\n",
        "\n",
        "A graph shows that as **Datastore Size** increases (X-axis), retrieval\n",
        "performance naturally degrades (Y-axis).\n",
        "\n",
        "To combat this, the speaker strongly recommends using **Metadata\n",
        "Filtering**. “If you’re not using something like metadata… it’s going to\n",
        "be very tough.” Narrowing the search scope is essential for scaling to\n",
        "millions of documents.\n",
        "\n",
        "### 46. Search Strategy Comparison\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_46.png\"\n",
        "alt=\"Slide 46\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 46</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:22](https://youtu.be/AS_HlJbJjH8&t=1762s))\n",
        "\n",
        "The presentation pivots to the “exciting part”: **Agentic RAG**. A\n",
        "visual compares “Traditional RAG” (a linear path) with “Agentic RAG” (a\n",
        "winding, exploratory path).\n",
        "\n",
        "This represents the shift from a “one-shot” retrieval attempt to an\n",
        "iterative system that can explore, backtrack, and reason.\n",
        "\n",
        "### 47. Tools use / Reasoning\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_47.png\"\n",
        "alt=\"Slide 47\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 47</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:40](https://youtu.be/AS_HlJbJjH8&t=1780s))\n",
        "\n",
        "Reasoning models (like o1 or DeepSeek R1) enable LLMs to use tools\n",
        "effectively. A code snippet shows an agent loop: query -\\> generate -\\>\n",
        "**“Did it answer the question?”**\n",
        "\n",
        "If the answer is no, the model can “rewrite the query… try to find that\n",
        "missing information, feed that back into the loop.” This self-correction\n",
        "is the core of Agentic RAG.\n",
        "\n",
        "### 48. Agentic RAG (Workflow)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_48.png\"\n",
        "alt=\"Slide 48\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 48</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:32](https://youtu.be/AS_HlJbJjH8&t=1832s))\n",
        "\n",
        "A flowchart details the Agentic RAG lifecycle. The model thinks through\n",
        "steps: “Oh, this is the query I need to make… based on those results…\n",
        "maybe we should do it a different way.”\n",
        "\n",
        "This workflow allows the system to synthesize answers from multiple\n",
        "sources or clarify ambiguous queries automatically.\n",
        "\n",
        "### 49. Tools use / Reasoning (Detailed Example)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_49.png\"\n",
        "alt=\"Slide 49\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 49</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:35](https://youtu.be/AS_HlJbJjH8&t=1835s))\n",
        "\n",
        "A specific example of a complex query is shown. The agent breaks the\n",
        "problem down, calls tools, and iterates.\n",
        "\n",
        "This demonstrates that the “Thinking” time is where the value is\n",
        "generated, allowing for a depth of research that a single retrieval pass\n",
        "cannot match.\n",
        "\n",
        "### 50. Open Deep Research\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_50.png\"\n",
        "alt=\"Slide 50\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 50</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:02](https://youtu.be/AS_HlJbJjH8&t=1862s))\n",
        "\n",
        "Shah references **“Open Deep Research”** by LangChain, an open-source\n",
        "framework where sub-agents go out, perform research, and report back.\n",
        "\n",
        "This is a specific category of Agentic RAG focused on generating\n",
        "comprehensive reports rather than quick answers.\n",
        "\n",
        "### 51. DeepResearch Bench\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_51.png\"\n",
        "alt=\"Slide 51\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 51</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:30](https://youtu.be/AS_HlJbJjH8&t=1890s))\n",
        "\n",
        "A leaderboard for **DeepResearch Bench** is shown, testing models on\n",
        "“100 PhD level research tasks.”\n",
        "\n",
        "The speaker warns that this approach “can get very expensive.” Solving a\n",
        "single complex query might cost significant money due to the number of\n",
        "tokens and iterative steps required.\n",
        "\n",
        "### 52. Westlaw AI Deep Research\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_52.png\"\n",
        "alt=\"Slide 52\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 52</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:55](https://youtu.be/AS_HlJbJjH8&t=1915s))\n",
        "\n",
        "A real-world application is highlighted: **Westlaw AI**. In the legal\n",
        "field, thoroughness is worth the latency and cost.\n",
        "\n",
        "This proves that Agentic RAG isn’t just a toy; it is being\n",
        "commercialized in high-value verticals where accuracy is paramount.\n",
        "\n",
        "### 53. Agentic RAG (Self-RAG)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_53.png\"\n",
        "alt=\"Slide 53\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 53</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:11](https://youtu.be/AS_HlJbJjH8&t=1931s))\n",
        "\n",
        "The concept of **Self-RAG** is introduced, emphasizing the “Reflection”\n",
        "step. The model critiques its own retrieved documents and generation\n",
        "quality.\n",
        "\n",
        "Shah notes that this isn’t brand new, but has become practical due to\n",
        "better reasoning models.\n",
        "\n",
        "### 54. Agentic RAG (LangChain Reddit)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_54.png\"\n",
        "alt=\"Slide 54\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 54</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:04](https://youtu.be/AS_HlJbJjH8&t=2044s))\n",
        "\n",
        "A Reddit post is shown where a developer discusses building a\n",
        "self-reflection RAG system. This highlights the community’s active\n",
        "experimentation with these loops.\n",
        "\n",
        "### 55. Agentic RAG (Efficiency Concerns)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_55.png\"\n",
        "alt=\"Slide 55\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 55</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:15](https://youtu.be/AS_HlJbJjH8&t=2055s))\n",
        "\n",
        "The discussion turns to the “Rub”: **Inefficiency**. Agentic loops can\n",
        "be slow and wasteful, re-retrieving data unnecessarily.\n",
        "\n",
        "This sets up the trade-off conversation again: Is the extra time and\n",
        "compute worth the accuracy gain?\n",
        "\n",
        "### 56. Research: BRIGHT\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_56.png\"\n",
        "alt=\"Slide 56\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 56</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:11](https://youtu.be/AS_HlJbJjH8&t=1931s))\n",
        "\n",
        "*Note: The speaker introduces the BRIGHT benchmark around 32:11,\n",
        "slightly out of slide order in the transcript flow, but connects it\n",
        "here.*\n",
        "\n",
        "**BRIGHT** is a benchmark specifically designed for **Retrieval\n",
        "Reasoning**. Unlike standard benchmarks that test keyword matching,\n",
        "BRIGHT tests questions that require thinking, logic, and multi-step\n",
        "deduction to find the correct document.\n",
        "\n",
        "### 57. BRIGHT #1: DIVER\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_57.png\"\n",
        "alt=\"Slide 57\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 57</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:48](https://youtu.be/AS_HlJbJjH8&t=1968s))\n",
        "\n",
        "The top-performing system on BRIGHT is **DIVER**. The diagram shows it\n",
        "uses the exact components discussed earlier: Chunking, Retrieving, and\n",
        "Reranking, but wrapped in an iterative loop.\n",
        "\n",
        "Shah points out, “It probably doesn’t look that crazy to you if you’re\n",
        "used to RAG.” The innovation is in the process, not necessarily a\n",
        "magical new model architecture.\n",
        "\n",
        "### 58. BRIGHT #1: DIVER (LLM Instructions)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_58.png\"\n",
        "alt=\"Slide 58\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 58</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 33:31](https://youtu.be/AS_HlJbJjH8&t=2011s))\n",
        "\n",
        "The specific prompts used in DIVER are shown. The system asks the LLM:\n",
        "“Given a query… what do you think would be possibly helpful to do?”\n",
        "\n",
        "This **Query Expansion** allows the system to generate new search terms\n",
        "that the user didn’t think of, bridging the semantic gap through\n",
        "reasoning.\n",
        "\n",
        "### 59. Agentic RAG on WixQA\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_59.png\"\n",
        "alt=\"Slide 59\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 59</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:36](https://youtu.be/AS_HlJbJjH8&t=2076s))\n",
        "\n",
        "Shah shares his own experiment results on the **WixQA** dataset\n",
        "(technical support). \\* **One Shot RAG:** 5 seconds latency, **76%**\n",
        "Factuality. \\* **Agentic RAG:** Slower latency, **93%** Factuality.\n",
        "\n",
        "This massive jump in accuracy (0.76 to 0.93) is the key takeaway. “That\n",
        "has a ton of implications.” It suggests that the limitation of RAG often\n",
        "isn’t the data, but the lack of reasoning applied to the retrieval\n",
        "process.\n",
        "\n",
        "### 60. Rethink your Assumptions\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_60.png\"\n",
        "alt=\"Slide 60\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 60</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 37:10](https://youtu.be/AS_HlJbJjH8&t=2230s))\n",
        "\n",
        "**This is the climax of the technical argument.** A graph from the\n",
        "BRIGHT paper shows that **BM25 (lexical search)** combined with an\n",
        "Agentic loop (GPT-4) outperforms advanced embedding models (Qwen).\n",
        "\n",
        "“This is crazy,” Shah exclaims. Because the LLM can rewrite queries into\n",
        "many variations, it mitigates BM25’s weakness (synonyms). This implies\n",
        "you might not need complex vector databases if you have a smart agent.\n",
        "\n",
        "### 61. Agentic RAG with BM25\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_61.png\"\n",
        "alt=\"Slide 61\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 61</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 38:20](https://youtu.be/AS_HlJbJjH8&t=2300s))\n",
        "\n",
        "Shah validates the paper’s finding with his own internal data (Financial\n",
        "10Ks). **Agentic RAG with BM25** performed nearly as well as Agentic RAG\n",
        "with Embeddings.\n",
        "\n",
        "He suggests a radical possibility: “I could throw all that away \\[vector\n",
        "DBs\\]… just stick this in a text-only database and use BM25.”\n",
        "\n",
        "### 62. Agentic RAG for Code Search\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_62.png\"\n",
        "alt=\"Slide 62\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 62</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 39:46](https://youtu.be/AS_HlJbJjH8&t=2386s))\n",
        "\n",
        "He connects this finding to **Claude Code**, which uses a lexical\n",
        "approach (like `grep`) rather than vectors for code search.\n",
        "\n",
        "Since code doesn’t have the same semantic ambiguity as natural language,\n",
        "and agents can iterate rapidly, lexical search is proving to be superior\n",
        "for coding assistants.\n",
        "\n",
        "### 63. Combine Retrieval Approaches\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_63.png\"\n",
        "alt=\"Slide 63\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 63</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 40:15](https://youtu.be/AS_HlJbJjH8&t=2415s))\n",
        "\n",
        "A **DoorDash** case study illustrates a two-tier guardrail system. They\n",
        "use simple text similarity first (fast/cheap). If that fails or is\n",
        "uncertain, they kick it to an LLM (slow/expensive).\n",
        "\n",
        "This “Tiered” approach optimizes the trade-off between cost and accuracy\n",
        "in production.\n",
        "\n",
        "### 64. Hands on: Agentic RAG (Smolagents)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_64.png\"\n",
        "alt=\"Slide 64\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 64</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 41:07](https://youtu.be/AS_HlJbJjH8&t=2467s))\n",
        "\n",
        "The speaker points to **Smolagents**, a Hugging Face library, as a way\n",
        "to get hands-on with these concepts. A Colab notebook is provided for\n",
        "the audience to build their own agentic retrieval loops.\n",
        "\n",
        "### 65. Solutions for a RAG Solution\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_65.png\"\n",
        "alt=\"Slide 65\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 65</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 41:18](https://youtu.be/AS_HlJbJjH8&t=2478s))\n",
        "\n",
        "Shah updates the “Problem Complexity” framework from the beginning of\n",
        "the talk with specific recommendations: \\* **Low Latency (\\<5s):** Use\n",
        "BM25 or Static Embeddings. \\* **High Cost of Mistake:** Add a Reranker.\n",
        "\\* **Complex Multi-hop:** Use Agentic RAG.\n",
        "\n",
        "### 66. Retriever Checklist\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_66.png\"\n",
        "alt=\"Slide 66\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 66</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 41:52](https://youtu.be/AS_HlJbJjH8&t=2512s))\n",
        "\n",
        "A final checklist summarizes the retrieval hierarchy: 1.\n",
        "**Keyword/BM25** (The baseline). 2. **Semantic Search** (The standard).\n",
        "3. **Agentic/Reasoning** (The problem solver).\n",
        "\n",
        "This provides the audience with a mental menu to choose from based on\n",
        "their specific constraints.\n",
        "\n",
        "### 67. RAG as a system (Retrieval with Instruction Following Reranker)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_67.png\"\n",
        "alt=\"Slide 67\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 67</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:00](https://youtu.be/AS_HlJbJjH8&t=2520s))\n",
        "\n",
        "The system diagram is shown one last time, updated to include the\n",
        "**Instruction Following Reranker** in the retrieval box, solidifying the\n",
        "modern RAG architecture.\n",
        "\n",
        "### 68. RAG - Generation\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_68.png\"\n",
        "alt=\"Slide 68\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 68</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:10](https://youtu.be/AS_HlJbJjH8&t=2530s))\n",
        "\n",
        "*Note: The speaker concludes the talk at 42:10, stating “I’m going to\n",
        "end it here.” Slides 68-70 regarding the Generation stage were included\n",
        "in the deck but skipped in the video recording due to time constraints.*\n",
        "\n",
        "This slide would have covered the final stage of RAG: generating the\n",
        "answer. The focus here is typically on reducing hallucinations and\n",
        "ensuring the tone matches the user’s needs.\n",
        "\n",
        "### 69. RAG - Generation (Model Selection)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_69.png\"\n",
        "alt=\"Slide 69\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 69</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:10](https://youtu.be/AS_HlJbJjH8&t=2530s))\n",
        "\n",
        "*Skipped in video.* This slide illustrates the choice of LLM for\n",
        "generation (e.g., GPT-4 vs Llama 3 vs Claude). The choice depends on the\n",
        "“Cost/Latency budget” and specific domain requirements.\n",
        "\n",
        "### 70. Chunking approaches\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_70.png\"\n",
        "alt=\"Slide 70\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 70</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:10](https://youtu.be/AS_HlJbJjH8&t=2530s))\n",
        "\n",
        "*Skipped in video.* This slide compares **Original Chunking** (cutting\n",
        "text at fixed intervals) with **Contextual Chunking** (adding a summary\n",
        "prefix to every chunk). Contextual chunking significantly improves\n",
        "retrieval because every chunk carries the context of the parent\n",
        "document.\n",
        "\n",
        "### 71. Title Slide (Duplicate)\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://projects.rajivshah.com/images/rag-talk/slide_71.png\"\n",
        "alt=\"Slide 71\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 71</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:10](https://youtu.be/AS_HlJbJjH8&t=2530s))\n",
        "\n",
        "The presentation concludes with the title slide. Rajiv Shah thanks the\n",
        "audience, encouraging them to think about trade-offs rather than just\n",
        "chasing the latest models. “Hopefully I’ve given you a sense of thinking\n",
        "about these trade-offs… thank you all.”\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "*This annotated presentation was generated from the talk using\n",
        "AI-assisted tools. Each slide includes timestamps and detailed\n",
        "explanations.*"
      ],
      "id": "31364b9b-e210-4344-b59b-0a2cf799be2e"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}