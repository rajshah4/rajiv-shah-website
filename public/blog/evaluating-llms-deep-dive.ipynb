{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation for Large Language Models (LLMs) and Generative AI - A Deep\n",
        "\n",
        "Dive\n",
        "\n",
        "## Video\n",
        "\n",
        "<https://youtu.be/iQl03pQlYWY>\n",
        "\n",
        "Watch the [full video](https://youtu.be/iQl03pQlYWY)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Annotated Presentation\n",
        "\n",
        "Below is an annotated version of the presentation, with timestamped\n",
        "links to the relevant parts of the video for each slide.\n",
        "\n",
        "Here is the annotated presentation for “Evaluating LLMs” by Rajiv Shah.\n",
        "\n",
        "### 1. Title Slide: Evaluating LLMs\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_1.png\"\n",
        "alt=\"Slide 1\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 1</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:00](https://youtu.be/iQl03pQlYWY&t=0s))\n",
        "\n",
        "The presentation begins with the title slide, introducing the speaker,\n",
        "Rajiv Shah, and the topic of **Evaluating Large Language Models\n",
        "(LLMs)**. The slide includes a link to a GitHub repository\n",
        "(`LLM-Evaluation`), which serves as a companion resource containing\n",
        "notebooks and code examples referenced throughout the talk.\n",
        "\n",
        "Rajiv sets the stage by explaining his motivation: he sees many\n",
        "enterprises treating Generative AI as “science experiments” that fail to\n",
        "reach production. He argues that a major reason for this failure is a\n",
        "lack of proper evaluation strategies.\n",
        "\n",
        "The goal of this talk is to move beyond experimentation and discuss how\n",
        "to rigorously evaluate models to get them into production and keep them\n",
        "there, covering technical, business, and operational perspectives.\n",
        "\n",
        "### 2. No Impact!\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_2.png\"\n",
        "alt=\"Slide 2\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 2</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:05](https://youtu.be/iQl03pQlYWY&t=5s))\n",
        "\n",
        "This slide humorously illustrates the current state of many LLM\n",
        "projects. It depicts a chaotic lab scene and a cartoon character in a\n",
        "strange vehicle, captioned “No impact!” This visualizes the frustration\n",
        "of data scientists building cool things that never deliver real-world\n",
        "value.\n",
        "\n",
        "Rajiv uses this to highlight the “science experiment” nature of current\n",
        "GenAI work. Without proper evaluation, teams cannot prove the\n",
        "reliability or value of their models, preventing deployment.\n",
        "\n",
        "The slide emphasizes the necessity of shifting from “playing around”\n",
        "with models to applying rigorous engineering discipline, starting with\n",
        "evaluation.\n",
        "\n",
        "### 3. Three Pillars of Evaluation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_3.png\"\n",
        "alt=\"Slide 3\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 3</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:41](https://youtu.be/iQl03pQlYWY&t=41s))\n",
        "\n",
        "This slide breaks down Generative AI evaluation into three critical\n",
        "dimensions: **Technical (F1)**, **Business (\\$\\$)**, and **Operational\n",
        "(TCO)**. While the talk focuses heavily on technical metrics, Rajiv\n",
        "stresses that the other two are equally vital for production success.\n",
        "\n",
        "The **Business** dimension asks about the return on investment and the\n",
        "cost of errors, while the **Operational** dimension considers the Total\n",
        "Cost of Ownership (TCO), latency, and maintenance.\n",
        "\n",
        "Understanding all three pillars is what distinguishes a successful\n",
        "production deployment from a mere prototype.\n",
        "\n",
        "### 4. Generative AI Evaluation Methods\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_4.png\"\n",
        "alt=\"Slide 4\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 4</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:03](https://youtu.be/iQl03pQlYWY&t=63s))\n",
        "\n",
        "This chart is the central framework of the presentation. It categorizes\n",
        "technical evaluation methods based on **Cost** (y-axis) and\n",
        "**Flexibility** (x-axis). The methods range from rigid, low-cost\n",
        "approaches like **Exact Matching** to flexible, high-cost approaches\n",
        "like **Red Teaming**.\n",
        "\n",
        "The slide lists specific methodologies: Exact matching, Similarity\n",
        "(BLEU/ROUGE), Functional correctness (Unit tests), Benchmarks (MMLU),\n",
        "Human evaluation, Model-based approaches (LLM-as-a-Judge), and Red\n",
        "teaming.\n",
        "\n",
        "Rajiv notes that these categories overlap and are not mutually\n",
        "exclusive. This visual guide helps practitioners choose the right tool\n",
        "for their specific stage of development and resource constraints.\n",
        "\n",
        "### 5. Application to RAG\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_5.png\"\n",
        "alt=\"Slide 5\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 5</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:16](https://youtu.be/iQl03pQlYWY&t=76s))\n",
        "\n",
        "This slide previews the case study at the end of the talk: **Retrieval\n",
        "Augmented Generation (RAG)**. It shows a diagram splitting the RAG\n",
        "process into two distinct components: **Retrieval** (finding the data)\n",
        "and **Augmented Generation** (synthesizing the answer).\n",
        "\n",
        "Rajiv introduces this here to promise a practical application of the\n",
        "concepts. He explains that after covering the evaluation methods, he\n",
        "will demonstrate how to apply them specifically to a RAG system.\n",
        "\n",
        "This foreshadows the importance of **component-wise\n",
        "evaluation**—evaluating the retriever and the generator separately\n",
        "rather than just the system as a whole.\n",
        "\n",
        "### 6. Evaluating LLMs (Title Repeat)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_6.png\"\n",
        "alt=\"Slide 6\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 6</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:31](https://youtu.be/iQl03pQlYWY&t=91s))\n",
        "\n",
        "This slide serves as a transition point, reiterating the talk’s title\n",
        "and contact information. It signals the end of the introduction and the\n",
        "beginning of the deep dive into the current state of LLMs.\n",
        "\n",
        "Rajiv notes that this will be a long, detailed talk, encouraging viewers\n",
        "to use the video timeline to skip around. He sets expectations for the\n",
        "pace and depth of the technical content to follow.\n",
        "\n",
        "### 7. Many Ways to Use LLMs\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_7.png\"\n",
        "alt=\"Slide 7\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 7</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:45](https://youtu.be/iQl03pQlYWY&t=105s))\n",
        "\n",
        "This slide illustrates the versatility of LLMs, showing examples of\n",
        "**Question Answering** and **Code Generation**. It highlights that LLMs\n",
        "are not limited to a single task like classification; they can\n",
        "summarize, chat, write code, and reason.\n",
        "\n",
        "Rajiv explains that this versatility makes evaluation difficult. Unlike\n",
        "traditional ML where a simple confusion matrix might suffice, LLMs\n",
        "produce varied, open-ended outputs that require more complex assessment\n",
        "strategies.\n",
        "\n",
        "The slide sets up the problem statement: because LLMs can do so much, we\n",
        "need a diverse set of evaluation tools to measure their performance\n",
        "across different modalities.\n",
        "\n",
        "### 8. Open Source LLM Leaderboard\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_8.png\"\n",
        "alt=\"Slide 8\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 8</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 02:18](https://youtu.be/iQl03pQlYWY&t=138s))\n",
        "\n",
        "This slide shows a screenshot of the **Hugging Face Open LLM\n",
        "Leaderboard**. It notes that over 2,000 LLMs have been evaluated,\n",
        "visualizing the sheer volume of models available to practitioners.\n",
        "\n",
        "Rajiv describes the experience of looking for a model as “overwhelming.”\n",
        "With new models releasing weekly, relying solely on public leaderboards\n",
        "to pick a model is daunting and potentially misleading.\n",
        "\n",
        "This introduces the concept of “Leaderboard Fatigue” and questions\n",
        "whether these general-purpose rankings are useful for specific\n",
        "enterprise use cases.\n",
        "\n",
        "### 9. HELM Framework\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_9.png\"\n",
        "alt=\"Slide 9\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 9</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 02:52](https://youtu.be/iQl03pQlYWY&t=172s))\n",
        "\n",
        "This slide introduces **HELM (Holistic Evaluation of Language Models)**\n",
        "from Stanford. It displays the framework’s structure, which evaluates\n",
        "models across various scenarios (datasets) and metrics (accuracy, bias,\n",
        "toxicity).\n",
        "\n",
        "Rajiv presents HELM as the academic approach to the evaluation problem.\n",
        "It attempts to be comprehensive by measuring everything across many\n",
        "dimensions, offering a more rigorous alternative to simple leaderboards.\n",
        "\n",
        "However, he points out that even this comprehensive approach has its\n",
        "downsides, primarily the sheer volume of data it produces.\n",
        "\n",
        "### 10. Overwhelming Information\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_10.png\"\n",
        "alt=\"Slide 10\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 10</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:25](https://youtu.be/iQl03pQlYWY&t=205s))\n",
        "\n",
        "This slide displays a screenshot of the HELM research paper, emphasizing\n",
        "its length (163 pages). The caption “Overwhelming!” reflects the\n",
        "difficulty a data scientist faces when trying to digest this amount of\n",
        "information.\n",
        "\n",
        "Rajiv humorously compares the paper’s size to a “Harry Potter book,”\n",
        "illustrating that while the academic rigor is high, the practical\n",
        "barrier to entry is also significant.\n",
        "\n",
        "The key takeaway is that while comprehensive benchmarks exist, they are\n",
        "often too dense for quick, practical decision-making in an enterprise\n",
        "setting.\n",
        "\n",
        "### 11. Feeling Overwhelmed\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_11.png\"\n",
        "alt=\"Slide 11\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 11</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:46](https://youtu.be/iQl03pQlYWY&t=226s))\n",
        "\n",
        "This visual slide features a person looking frustrated and burying their\n",
        "face in their hands. It represents the emotional state of a data\n",
        "scientist trying to navigate the complex, rapidly changing landscape of\n",
        "LLM evaluation.\n",
        "\n",
        "Rajiv uses this to empathize with the audience. Between the thousands of\n",
        "models on Hugging Face and the hundreds of pages of academic papers, it\n",
        "is easy to feel lost.\n",
        "\n",
        "This sets the stage for the need for simpler, more fundamental\n",
        "principles to guide evaluation.\n",
        "\n",
        "### 12. Reliability of HELM\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_12.png\"\n",
        "alt=\"Slide 12\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 12</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:57](https://youtu.be/iQl03pQlYWY&t=237s))\n",
        "\n",
        "This slide questions the reliability of benchmarks like HELM. It\n",
        "presents data showing that minor changes in dataset selection can lead\n",
        "to different scoring and winners **22% of the time**. A correlation\n",
        "matrix visualizes the relationships between different metrics.\n",
        "\n",
        "Rajiv points out that benchmarks are fragile. If you change the specific\n",
        "datasets used to represent a “scenario,” the ranking of the models\n",
        "changes.\n",
        "\n",
        "This implies that “winners” on leaderboards are often dependent on the\n",
        "specific composition of the benchmark rather than inherent superiority\n",
        "across all tasks.\n",
        "\n",
        "### 13. Davinci-002 vs Davinci-003\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_13.png\"\n",
        "alt=\"Slide 13\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 13</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 04:15](https://youtu.be/iQl03pQlYWY&t=255s))\n",
        "\n",
        "This slide highlights a specific anomaly in HELM results where an older\n",
        "model (`text-davinci-002`) appears to outperform a newer, better model\n",
        "(`text-davinci-003`) in accuracy.\n",
        "\n",
        "Rajiv expresses skepticism, noting that OpenAI is unlikely to release a\n",
        "newer model that is objectively worse. This discrepancy suggests that\n",
        "the benchmark might not be capturing the improvements in the newer\n",
        "model, such as better instruction following or safety.\n",
        "\n",
        "The slide serves as a warning: **Do not blindly trust benchmark\n",
        "rankings**, as they may not reflect the actual capabilities or “quality”\n",
        "of a model for your specific needs.\n",
        "\n",
        "### 14. Leaderboard Reliability\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_14.png\"\n",
        "alt=\"Slide 14\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 14</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 04:53](https://youtu.be/iQl03pQlYWY&t=293s))\n",
        "\n",
        "This slide examines the **Open LLM Leaderboard** again, pointing out\n",
        "that rankings are heavily influenced by specific datasets like\n",
        "**TruthfulQA**. It asks, “Is this impactful?”\n",
        "\n",
        "Rajiv argues that if a model’s high ranking is driven primarily by its\n",
        "performance on a dataset like TruthfulQA, it might not be relevant to a\n",
        "user whose use case (e.g., summarizing financial documents) has nothing\n",
        "to do with that specific benchmark.\n",
        "\n",
        "This reinforces the idea that general-purpose leaderboards may not align\n",
        "with specific business goals.\n",
        "\n",
        "### 15. Model Evals vs System Evals\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_15.png\"\n",
        "alt=\"Slide 15\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 15</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 05:33](https://youtu.be/iQl03pQlYWY&t=333s))\n",
        "\n",
        "This slide distinguishes between **Model Evals** (selecting the best\n",
        "model from *n* options) and **System Evals** (optimizing a single model\n",
        "for a specific task).\n",
        "\n",
        "Rajiv explains that most public benchmarks focus on the former—comparing\n",
        "thousands of models. However, in enterprise settings, the goal is\n",
        "usually the latter: you pick a model (like GPT-4 or Llama 2) and need to\n",
        "evaluate how to optimize it for your specific application.\n",
        "\n",
        "The talk focuses on bridging this gap, helping practitioners evaluate\n",
        "their specific implementation rather than just comparing base models.\n",
        "\n",
        "### 16. Lost in the Maze\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_16.png\"\n",
        "alt=\"Slide 16\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 16</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 06:33](https://youtu.be/iQl03pQlYWY&t=393s))\n",
        "\n",
        "This slide features an image of a hedge maze with the word “Lost,”\n",
        "symbolizing the confusion in the current evaluation landscape.\n",
        "\n",
        "Rajiv uses this to pivot back to **fundamentals**. When lost in complex\n",
        "new technology, the best approach is to return to first principles of\n",
        "data science evaluation.\n",
        "\n",
        "He prepares the audience to look at a classic machine learning problem\n",
        "to ground the upcoming LLM concepts.\n",
        "\n",
        "### 17. Evaluating Customer Churn\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_17.png\"\n",
        "alt=\"Slide 17\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 17</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 06:49](https://youtu.be/iQl03pQlYWY&t=409s))\n",
        "\n",
        "This slide introduces a classic “Data Science 101” problem: **Customer\n",
        "Churn**. It depicts an exit door and a pie chart, setting up a scenario\n",
        "where a data scientist must evaluate a model designed to predict which\n",
        "customers will leave.\n",
        "\n",
        "Rajiv uses this familiar example to contrast different levels of\n",
        "evaluation maturity, which he will then map onto GenAI.\n",
        "\n",
        "### 18. Junior Data Scientist Approach\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_18.png\"\n",
        "alt=\"Slide 18\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 18</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 07:07](https://youtu.be/iQl03pQlYWY&t=427s))\n",
        "\n",
        "This slide shows standard classification metrics: **ROC curve**,\n",
        "**Confusion Matrix**, **F1 Score**, and **True Positive Rate**. Rajiv\n",
        "labels this as the “Junior Data Scientist” approach.\n",
        "\n",
        "While these metrics are technically correct, they are abstract. A junior\n",
        "data scientist presents these to a boss and says, “Look, I improved the\n",
        "AUC,” which often fails to communicate business value.\n",
        "\n",
        "This represents the **Technical** pillar of evaluation—necessary, but\n",
        "insufficient for business stakeholders.\n",
        "\n",
        "### 19. Senior Data Scientist Approach\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_19.png\"\n",
        "alt=\"Slide 19\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 19</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 07:40](https://youtu.be/iQl03pQlYWY&t=460s))\n",
        "\n",
        "This slide introduces **Profit Curves**. It translates the confusion\n",
        "matrix into dollar values (cost of false positives vs. value of true\n",
        "positives). Rajiv calls this the “Senior Data Scientist” approach.\n",
        "\n",
        "Here, the evaluation focuses on **Business Value**: “How much profit\n",
        "will this model generate compared to the baseline?” This aligns the\n",
        "technical model with business goals (\\$\\$).\n",
        "\n",
        "The lesson is that LLM evaluation must eventually map to business\n",
        "outcomes, not just technical benchmarks.\n",
        "\n",
        "### 20. Data Science Leader Approach\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_20.png\"\n",
        "alt=\"Slide 20\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 20</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 08:27](https://youtu.be/iQl03pQlYWY&t=507s))\n",
        "\n",
        "This slide discusses the **Total Cost of Ownership (TCO)** and\n",
        "**Monitoring**. It reflects the “Data Science Leader” perspective, which\n",
        "looks at the system holistically.\n",
        "\n",
        "A leader asks: “Is it worth spending 5 more weeks to get 3% more\n",
        "accuracy?” and “How will we monitor this when customer behavior\n",
        "changes?”\n",
        "\n",
        "This corresponds to the **Operational** pillar. It emphasizes that\n",
        "evaluation includes considering the cost of building, maintaining, and\n",
        "running the model over time.\n",
        "\n",
        "### 21. Evaluate Generative AI Tasks?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_21.png\"\n",
        "alt=\"Slide 21\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 21</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:23](https://youtu.be/iQl03pQlYWY&t=563s))\n",
        "\n",
        "This slide transitions back to Generative AI, showing examples of code\n",
        "generation and summarization. It asks how to apply the principles just\n",
        "discussed (Technical, Business, Operational) to these new, complex\n",
        "tasks.\n",
        "\n",
        "Rajiv acknowledges that while the outputs (text, code) are different\n",
        "from simple classification labels, the fundamental need to evaluate\n",
        "across three dimensions remains.\n",
        "\n",
        "### 22. Three Pillars (GenAI Context)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_22.png\"\n",
        "alt=\"Slide 22\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 22</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:36](https://youtu.be/iQl03pQlYWY&t=576s))\n",
        "\n",
        "This slide repeats the **Technical, Business, Operational** framework,\n",
        "asserting “Still the same principles!”\n",
        "\n",
        "Rajiv reinforces that despite the hype and novelty of LLMs, we must not\n",
        "abandon standard engineering practices. We still need to measure\n",
        "technical accuracy (F1 equivalent), business impact (\\$\\$), and\n",
        "operational costs (TCO).\n",
        "\n",
        "### 23. Evaluation in the ML Lifecycle\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_23.png\"\n",
        "alt=\"Slide 23\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 23</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:45](https://youtu.be/iQl03pQlYWY&t=585s))\n",
        "\n",
        "This slide displays a “multi-headed llama” graphic representing the ML\n",
        "lifecycle: **Development**, **Training**, and **Deployment**.\n",
        "\n",
        "Rajiv explains that evaluation is not a one-time step. It happens: 1.\n",
        "**Before:** To decide if a project is viable. 2. **During:** To train\n",
        "and tune the model. 3. **After:** To monitor the model in production\n",
        "(Monitoring is the “sibling” of Evaluation).\n",
        "\n",
        "### 24. Faster, Better, Cheaper\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_24.png\"\n",
        "alt=\"Slide 24\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 24</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 10:33](https://youtu.be/iQl03pQlYWY&t=633s))\n",
        "\n",
        "This slide features a tweet by Eugene Yan, stating that automated\n",
        "evaluations lead to **“faster, better, cheaper”** LLMs. It mentions that\n",
        "good eval pipelines allow for safer deployments and faster experiments.\n",
        "\n",
        "Rajiv cites the example of Hugging Face’s **Zephyr** model. The team\n",
        "built it in just a few days because they had spent months building a\n",
        "robust evaluation pipeline.\n",
        "\n",
        "The key insight is that investing in evaluation infrastructure upfront\n",
        "accelerates actual model development and iteration.\n",
        "\n",
        "### 25. Traditional NLP Tasks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_25.png\"\n",
        "alt=\"Slide 25\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 25</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 11:51](https://youtu.be/iQl03pQlYWY&t=711s))\n",
        "\n",
        "This slide advises that if you are using GenAI for a traditional NLP\n",
        "task (like sentiment analysis), you should **“start with traditional\n",
        "metrics/datasets.”**\n",
        "\n",
        "However, Rajiv warns about **Data Leakage**. Because LLMs are trained on\n",
        "the internet, they may have already seen the test sets of standard\n",
        "benchmarks.\n",
        "\n",
        "The takeaway: Use standard metrics if applicable, but be skeptical of\n",
        "results that seem too good, as the model might be memorizing the test\n",
        "data.\n",
        "\n",
        "### 26. Breaking Existing Evaluations\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_26.png\"\n",
        "alt=\"Slide 26\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 26</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 12:38](https://youtu.be/iQl03pQlYWY&t=758s))\n",
        "\n",
        "This slide explains that LLMs can **“break existing evaluations.”** It\n",
        "cites research where LLMs scored poorly on automated metrics but were\n",
        "rated highly by humans.\n",
        "\n",
        "Rajiv explains that LLMs have such a fluid and rich understanding of\n",
        "language that they often produce correct answers that old, rigid metrics\n",
        "fail to recognize.\n",
        "\n",
        "This highlights the limitation of using pre-LLM automated metrics for\n",
        "modern models; the models have outpaced the measurement tools.\n",
        "\n",
        "### 27. Beating Human Baselines\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_27.png\"\n",
        "alt=\"Slide 27\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 27</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 13:29](https://youtu.be/iQl03pQlYWY&t=809s))\n",
        "\n",
        "This slide presents data showing LLMs (GPT-3/4) beating **human\n",
        "baselines** in tasks like summarization. The charts show LLMs scoring\n",
        "higher in faithfulness, coherence, and relevance.\n",
        "\n",
        "Rajiv mentions recent research where GPT-4 wrote medical reports that\n",
        "doctors preferred over those written by other humans.\n",
        "\n",
        "This poses a challenge: How do you evaluate a model when it is better\n",
        "than the human annotators you would typically use as a gold standard?\n",
        "\n",
        "### 28. Methods Chart (Recap)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_28.png\"\n",
        "alt=\"Slide 28\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 28</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 14:03](https://youtu.be/iQl03pQlYWY&t=843s))\n",
        "\n",
        "This slide brings back the **Generative AI Evaluation Methods** chart\n",
        "(Cost vs. Flexibility). An arrow points to “Raj guess,” indicating that\n",
        "the placement of these methods is an estimation.\n",
        "\n",
        "Rajiv uses this to reorient the audience before diving into the specific\n",
        "methods one by one, starting from the bottom left (least\n",
        "flexible/cheapest).\n",
        "\n",
        "### 29. Progression of Evaluation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_29.png\"\n",
        "alt=\"Slide 29\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 29</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 14:27](https://youtu.be/iQl03pQlYWY&t=867s))\n",
        "\n",
        "This slide shows a directional arrow moving “up” the chart, from **Exact\n",
        "Matching** toward **Red Teaming**.\n",
        "\n",
        "Rajiv explains the flow of the presentation: we will start with rigid,\n",
        "simple metrics and move toward more complex, flexible, and human-centric\n",
        "evaluation methods.\n",
        "\n",
        "### 30. Exact Matching Approach\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_30.png\"\n",
        "alt=\"Slide 30\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 30</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 15:22](https://youtu.be/iQl03pQlYWY&t=922s))\n",
        "\n",
        "This slide highlights the **“Exact matching approach”** box on the\n",
        "chart.\n",
        "\n",
        "This is the starting point: the simplest form of evaluation where the\n",
        "model’s output must be identical to a reference answer.\n",
        "\n",
        "### 31. How Hard Could It Be?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_31.png\"\n",
        "alt=\"Slide 31\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 31</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:02](https://youtu.be/iQl03pQlYWY&t=962s))\n",
        "\n",
        "This slide asks, **“How hard could evaluation be?”** It shows simple\n",
        "outputs (Yes/No, A/B/C/D) and suggests that checking if string A equals\n",
        "string B should be trivial.\n",
        "\n",
        "Rajiv uses this to set up a contrast. While it *looks* simple like a\n",
        "basic Python script, the reality of LLMs makes even this basic task\n",
        "complicated due to formatting and non-determinism.\n",
        "\n",
        "### 32. Consistent Prediction Workflow\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_32.png\"\n",
        "alt=\"Slide 32\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 32</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:20](https://youtu.be/iQl03pQlYWY&t=980s))\n",
        "\n",
        "This slide outlines a workflow: **Inputs** (Tokenization, Prompts) -\\>\n",
        "**Model** (Hyperparameters) -\\> **Outputs** (Evaluation).\n",
        "\n",
        "Rajiv emphasizes that to get exact matching to work, you need extreme\n",
        "consistency across this entire pipeline. He warns that you must plan for\n",
        "multiple iterations because things will go wrong at every step.\n",
        "\n",
        "### 33. Story Time: MMLU Leaderboards\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_33.png\"\n",
        "alt=\"Slide 33\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 33</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:40](https://youtu.be/iQl03pQlYWY&t=1000s))\n",
        "\n",
        "This slide shows a tweet announcing a new LLM topping the leaderboard,\n",
        "but points out a discrepancy: **“Why did we have two different MMLU\n",
        "scores?”**\n",
        "\n",
        "Rajiv tells the story of how a model claimed a high score on Twitter,\n",
        "but the actual paper showed a lower score. This discrepancy triggered an\n",
        "investigation into why the same model on the same benchmark produced\n",
        "different results.\n",
        "\n",
        "### 34. What is MMLU?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_34.png\"\n",
        "alt=\"Slide 34\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 34</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 17:37](https://youtu.be/iQl03pQlYWY&t=1057s))\n",
        "\n",
        "This slide defines **MMLU (Massive Multitask Language Understanding)**.\n",
        "It is a benchmark covering 57 tasks (Math, History, CS), designed to\n",
        "measure the “knowledge” of a model.\n",
        "\n",
        "Rajiv shows examples of questions (Microeconomics, Physics) to\n",
        "illustrate that these are multiple-choice questions used to gauge\n",
        "general intelligence.\n",
        "\n",
        "### 35. Why MMLU Evaluation Differed\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_35.png\"\n",
        "alt=\"Slide 35\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 35</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 18:16](https://youtu.be/iQl03pQlYWY&t=1096s))\n",
        "\n",
        "This slide reveals the culprit behind the score discrepancy: **Prompt\n",
        "Formatting**. It shows three different prompt styles (HELM, Eleuther,\n",
        "Original) used by different evaluation harnesses.\n",
        "\n",
        "Rajiv challenges the audience to spot the differences. They are subtle:\n",
        "an extra space, a different bracket style around the letter `(A)` vs\n",
        "`A.`, or the inclusion of a subject line.\n",
        "\n",
        "### 36. Style Changes Accuracy\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_36.png\"\n",
        "alt=\"Slide 36\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 36</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 19:13](https://youtu.be/iQl03pQlYWY&t=1153s))\n",
        "\n",
        "This slide states that these simple style changes resulted in a **~5%\n",
        "change in accuracy**.\n",
        "\n",
        "Rajiv underscores the significance: a 5% swing is massive on a\n",
        "leaderboard. This proves that LLMs are incredibly sensitive to prompt\n",
        "syntax. It also serves as a warning to be skeptical of reported\n",
        "benchmark scores, as they can be “massaged” simply by tweaking the\n",
        "prompt format.\n",
        "\n",
        "### 37. Story: Falcon Model Bias\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_37.png\"\n",
        "alt=\"Slide 37\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 37</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 20:35](https://youtu.be/iQl03pQlYWY&t=1235s))\n",
        "\n",
        "This slide introduces the **Falcon** model story. Users noticed that\n",
        "when asked for a “technologically advanced city,” Falcon would almost\n",
        "always suggest **Abu Dhabi**.\n",
        "\n",
        "Rajiv sets up the mystery: Was the model biased because it was trained\n",
        "in the Middle East? Why was it so fixated on this specific city?\n",
        "\n",
        "### 38. Biased Model (Human Rights)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_38.png\"\n",
        "alt=\"Slide 38\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 38</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:59](https://youtu.be/iQl03pQlYWY&t=1319s))\n",
        "\n",
        "This slide shows that the Falcon model also refused to discuss **human\n",
        "rights abuses** in Abu Dhabi.\n",
        "\n",
        "This fueled speculation that the model had been censored or biased\n",
        "during training to avoid sensitive topics regarding its region of\n",
        "origin.\n",
        "\n",
        "### 39. Demo Placeholder\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_39.png\"\n",
        "alt=\"Slide 39\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 39</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:16](https://youtu.be/iQl03pQlYWY&t=1276s))\n",
        "\n",
        "This slide simply says “Let’s try to demo this.” In the video, Rajiv\n",
        "switches to a live recording of him interacting with the model to\n",
        "demonstrate the bias firsthand.\n",
        "\n",
        "### 40. Check the System Prompt\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_40.png\"\n",
        "alt=\"Slide 40\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 40</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 22:26](https://youtu.be/iQl03pQlYWY&t=1346s))\n",
        "\n",
        "This slide reveals the answer to the Falcon mystery: **The System\n",
        "Prompt**.\n",
        "\n",
        "It turns out the model had a hidden system instruction explicitly\n",
        "stating it was built in Abu Dhabi. When researchers changed this prompt\n",
        "(e.g., to “Mexico”), the model’s behavior changed, and it stopped\n",
        "forcing Abu Dhabi into answers.\n",
        "\n",
        "The lesson: **System prompts heavily influence evaluation results**.\n",
        "Small changes in hidden instructions can radically alter model behavior.\n",
        "\n",
        "### 41. Prompt Engineering\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_41.png\"\n",
        "alt=\"Slide 41\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 41</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:26](https://youtu.be/iQl03pQlYWY&t=1406s))\n",
        "\n",
        "This slide discusses **Prompt Engineering** techniques like\n",
        "**Chain-of-Thought (COT)**. It shows how asking a model to “think step\n",
        "by step” improves reasoning on math problems.\n",
        "\n",
        "Rajiv emphasizes that identifying the *best* prompt is a crucial part of\n",
        "the evaluation workflow. You aren’t just evaluating the model; you are\n",
        "evaluating the model *plus* the prompt.\n",
        "\n",
        "### 42. Hands on: Prompting\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_42.png\"\n",
        "alt=\"Slide 42\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 42</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:05](https://youtu.be/iQl03pQlYWY&t=1445s))\n",
        "\n",
        "This slide introduces a hands-on exercise. It encourages users to use\n",
        "OpenAI’s playground to experiment with different prompts, specifically\n",
        "COT and system prompt variations.\n",
        "\n",
        "### 43. Hands on: GLaDOS\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_43.png\"\n",
        "alt=\"Slide 43\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 43</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:14](https://youtu.be/iQl03pQlYWY&t=1454s))\n",
        "\n",
        "This slide shows a fun example where the system prompt turns ChatGPT\n",
        "into **GLaDOS** (from the game Portal).\n",
        "\n",
        "Rajiv uses this to demonstrate the power of the system prompt to change\n",
        "the persona and tone of the model completely.\n",
        "\n",
        "### 44. Workflow: Inputs Recap\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_44.png\"\n",
        "alt=\"Slide 44\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 44</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:26](https://youtu.be/iQl03pQlYWY&t=1466s))\n",
        "\n",
        "This slide updates the **Consistent Prediction Workflow**. Under\n",
        "“Inputs,” it now explicitly lists **System Prompt**, Tokenization,\n",
        "Prompt Styles, and Prompt Engineering.\n",
        "\n",
        "This summarizes the section: to get consistent evaluation, you must\n",
        "control all these input variables.\n",
        "\n",
        "### 45. Variability of LLM Models\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_45.png\"\n",
        "alt=\"Slide 45\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 45</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:39](https://youtu.be/iQl03pQlYWY&t=1479s))\n",
        "\n",
        "This slide shifts focus to the **Model** component. It notes that model\n",
        "size affects scores (Llama-2 example) and introduces the concept of\n",
        "**Non-deterministic inference**.\n",
        "\n",
        "Rajiv points out that GPU calculations introduce slight randomness,\n",
        "meaning you might not get bit-wise reproducibility even with the same\n",
        "settings.\n",
        "\n",
        "### 46. GPT-4 vs GPT-3.5\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_46.png\"\n",
        "alt=\"Slide 46\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 46</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:46](https://youtu.be/iQl03pQlYWY&t=1486s))\n",
        "\n",
        "This slide compares **GPT-4 vs GPT-3.5**. It shows that even models from\n",
        "the same “family” give very different answers to political opinion\n",
        "questions.\n",
        "\n",
        "Rajiv uses this to show that you cannot swap models (e.g., using a\n",
        "cheaper model for dev and a larger one for prod) without re-evaluating,\n",
        "as their behaviors diverge significantly.\n",
        "\n",
        "### 47. Non-deterministic Inference\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_47.png\"\n",
        "alt=\"Slide 47\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 47</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 25:28](https://youtu.be/iQl03pQlYWY&t=1528s))\n",
        "\n",
        "This slide dives deeper into **Non-deterministic inference**. It\n",
        "explains that floating-point calculations on GPUs can have tiny\n",
        "variances that ripple out to affect token selection.\n",
        "\n",
        "For data scientists coming from deterministic systems (like logistic\n",
        "regression), this lack of 100% reproducibility can be a shock and\n",
        "complicates “exact match” testing.\n",
        "\n",
        "### 48. Reliability of Commercial APIs\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_48.png\"\n",
        "alt=\"Slide 48\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 48</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:03](https://youtu.be/iQl03pQlYWY&t=1563s))\n",
        "\n",
        "This slide addresses **Model Drift** in commercial APIs. It shows graphs\n",
        "of GPT-3.5 and GPT-4 performance changing over time on tasks like\n",
        "identifying prime numbers.\n",
        "\n",
        "Rajiv warns that if you don’t own the model (i.e., you use an API), the\n",
        "vendor might update it behind the scenes, breaking your evaluation\n",
        "baselines.\n",
        "\n",
        "### 49. Hyperparameters\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_49.png\"\n",
        "alt=\"Slide 49\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 49</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:27](https://youtu.be/iQl03pQlYWY&t=1587s))\n",
        "\n",
        "This slide shows the UI for **Hyperparameters** (Temperature, Max\n",
        "Length, Top P).\n",
        "\n",
        "Rajiv reminds the audience that these settings drastically influence\n",
        "predictions. Evaluation must be done with the exact same hyperparameters\n",
        "intended for production.\n",
        "\n",
        "### 50. Output Evaluation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_50.png\"\n",
        "alt=\"Slide 50\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 50</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:40](https://youtu.be/iQl03pQlYWY&t=1600s))\n",
        "\n",
        "This slide highlights the **“Output evaluation”** step in the workflow.\n",
        "\n",
        "Now that we’ve covered inputs and models, Rajiv moves to the challenge\n",
        "of parsing and judging the text the model actually produces.\n",
        "\n",
        "### 51. Generating Multiple Choice Output\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_51.png\"\n",
        "alt=\"Slide 51\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 51</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:46](https://youtu.be/iQl03pQlYWY&t=1606s))\n",
        "\n",
        "This slide discusses the difficulty of evaluating **Multiple Choice**\n",
        "answers. \\* **First Letter Approach:** Just look for “A” or “B”. Fails\n",
        "if the model says “The answer is A”. \\* **Entire Answer:** Look for the\n",
        "full text. Fails if the model phrases it slightly differently.\n",
        "\n",
        "Rajiv illustrates that even “simple” multiple-choice evaluation requires\n",
        "complex parsing logic because LLMs love to “chat” and add extra text.\n",
        "\n",
        "### 52. Evaluating MMLU: Different Outputs\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_52.png\"\n",
        "alt=\"Slide 52\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 52</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:28](https://youtu.be/iQl03pQlYWY&t=1648s))\n",
        "\n",
        "This slide compares how **HELM**, **AI Harness**, and the **Original**\n",
        "MMLU implementation parsed outputs.\n",
        "\n",
        "It reveals that the discrepancy in MMLU scores wasn’t just about\n",
        "prompts; it was also about *how* the evaluation code extracted the\n",
        "answer from the model’s response.\n",
        "\n",
        "### 53. Consistency is Hard!\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_53.png\"\n",
        "alt=\"Slide 53\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 53</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:40](https://youtu.be/iQl03pQlYWY&t=1660s))\n",
        "\n",
        "This slide summarizes the MMLU saga: **“Consistency is hard!”** It shows\n",
        "the table of scores again.\n",
        "\n",
        "The takeaway is that “Exact Match” is a misnomer. It requires rigorous\n",
        "standardization of inputs, models, and output parsing to be reliable.\n",
        "\n",
        "### 54. Hands on: Evaluating Outputs\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_54.png\"\n",
        "alt=\"Slide 54\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 54</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:59](https://youtu.be/iQl03pQlYWY&t=1679s))\n",
        "\n",
        "This slide introduces a hands-on exercise evaluating sentiment analysis.\n",
        "It shows a spreadsheet where different models output sentiment in\n",
        "different formats (some verbose, some concise).\n",
        "\n",
        "Rajiv uses this to show the messy reality of parsing LLM outputs.\n",
        "\n",
        "### 55. Solutions: Standardizing Outputs\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_55.png\"\n",
        "alt=\"Slide 55\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 55</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:34](https://youtu.be/iQl03pQlYWY&t=1714s))\n",
        "\n",
        "This slide presents solutions for the output problem: 1. **OpenAI\n",
        "Function Calling:** Forces the model to output structured JSON. 2.\n",
        "**Guardrails AI:** A library for validating outputs against a schema.\n",
        "\n",
        "Rajiv suggests that using these tools to tame the model into structured\n",
        "output makes “Exact Match” evaluation much more feasible.\n",
        "\n",
        "### 56. Workflow: Types of Prompts\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_56.png\"\n",
        "alt=\"Slide 56\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 56</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:59](https://youtu.be/iQl03pQlYWY&t=1739s))\n",
        "\n",
        "This slide adds **“Types of Prompts”** to the Input section of the\n",
        "workflow diagram.\n",
        "\n",
        "Rajiv reiterates the need to plan for **multiple iterations**. You will\n",
        "likely need to tweak your prompts and parsing logic many times to get a\n",
        "stable evaluation pipeline.\n",
        "\n",
        "### 57. Resources: Prompting\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_57.png\"\n",
        "alt=\"Slide 57\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 57</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:10](https://youtu.be/iQl03pQlYWY&t=1750s))\n",
        "\n",
        "This slide lists resources for learning prompting, including the OpenAI\n",
        "Cookbook and the DAIR.AI Prompt Engineering Guide.\n",
        "\n",
        "### 58. Similarity Approach\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_58.png\"\n",
        "alt=\"Slide 58\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 58</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:13](https://youtu.be/iQl03pQlYWY&t=1753s))\n",
        "\n",
        "This slide moves up the chart to the **Similarity approach**.\n",
        "\n",
        "Rajiv introduces this as the next level of flexibility. If exact\n",
        "matching is too rigid, we check if the output is “similar enough” to the\n",
        "reference.\n",
        "\n",
        "### 59. Story: Translation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_59.png\"\n",
        "alt=\"Slide 59\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 59</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:29](https://youtu.be/iQl03pQlYWY&t=1769s))\n",
        "\n",
        "This slide presents a translation challenge. It shows three human\n",
        "references for a Chinese-to-English translation and two computer\n",
        "candidates.\n",
        "\n",
        "Rajiv asks the audience to guess which candidate is better. This\n",
        "exercise builds intuition for how similarity metrics work: we look for\n",
        "overlapping words and phrases between the candidate and the references.\n",
        "\n",
        "### 60. BLEU Metric\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_60.png\"\n",
        "alt=\"Slide 60\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 60</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:47](https://youtu.be/iQl03pQlYWY&t=1847s))\n",
        "\n",
        "This slide introduces **BLEU (Bilingual Evaluation Understudy)**. It\n",
        "explains that BLEU calculates scores based on n-gram overlap (1-gram to\n",
        "4-gram) between the generated text and reference text.\n",
        "\n",
        "This is the mathematical formalization of the intuition from the\n",
        "previous slide. It’s a standard metric for translation.\n",
        "\n",
        "### 61. Many Similarity Methods\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_61.png\"\n",
        "alt=\"Slide 61\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 61</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:15](https://youtu.be/iQl03pQlYWY&t=1875s))\n",
        "\n",
        "This slide lists various similarity metrics: **Exact match, Edit\n",
        "distance, ROUGE, WER, METEOR, Cosine similarity**.\n",
        "\n",
        "Rajiv notes the pros and cons: They are fast and easy to calculate, but\n",
        "they **don’t consider meaning** (semantics) and are biased toward\n",
        "shorter text. They measure lexical overlap, not understanding.\n",
        "\n",
        "### 62. Taxonomy of Similarity Methods\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_62.png\"\n",
        "alt=\"Slide 62\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 62</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:06](https://youtu.be/iQl03pQlYWY&t=1926s))\n",
        "\n",
        "This slide shows a complex flow chart categorizing similarity methods\n",
        "into **Untrained** (lexical, character-based) and **Trained**\n",
        "(embedding-based).\n",
        "\n",
        "It illustrates the depth of research in this field, showing that there\n",
        "are dozens of ways to calculate “similarity.”\n",
        "\n",
        "### 63. Similarity Methods for Code\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_63.png\"\n",
        "alt=\"Slide 63\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 63</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:15](https://youtu.be/iQl03pQlYWY&t=1935s))\n",
        "\n",
        "This slide asks if similarity works for **Code**. It shows a Python\n",
        "function `incr_list`.\n",
        "\n",
        "Rajiv argues that similarity **“Doesn’t work for code.”** In code,\n",
        "variable names can change, and logic can be refactored, resulting in\n",
        "zero string similarity even if the code functions identically.\n",
        "Conversely, a single missing character (syntax error) can break code\n",
        "that is 99% similar textually.\n",
        "\n",
        "### 64. Functional Correctness\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_64.png\"\n",
        "alt=\"Slide 64\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 64</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:37](https://youtu.be/iQl03pQlYWY&t=1957s))\n",
        "\n",
        "This slide highlights **Functional Correctness** on the chart.\n",
        "\n",
        "This is the solution to the code evaluation problem. Instead of checking\n",
        "if the text looks right, we execute it to see if it *works*.\n",
        "\n",
        "### 65. Problem: Evaluating Code\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_65.png\"\n",
        "alt=\"Slide 65\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 65</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:54](https://youtu.be/iQl03pQlYWY&t=1974s))\n",
        "\n",
        "This slide reinforces the failure of BLEU for code. It shows that a\n",
        "correct solution might have a low BLEU score because it uses different\n",
        "variable names than the reference.\n",
        "\n",
        "### 66. Evaluating Code with Unit Tests\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_66.png\"\n",
        "alt=\"Slide 66\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 66</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 33:17](https://youtu.be/iQl03pQlYWY&t=1997s))\n",
        "\n",
        "This slide introduces the **Unit Test** approach. We take the generated\n",
        "code, run it against a set of test cases (inputs and expected outputs),\n",
        "and check for a pass/fail result.\n",
        "\n",
        "Rajiv advocates for this approach because it is unambiguous. The code\n",
        "either runs and produces the right result, or it doesn’t.\n",
        "\n",
        "### 67. HumanEval Benchmark\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_67.png\"\n",
        "alt=\"Slide 67\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 67</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:16](https://youtu.be/iQl03pQlYWY&t=2056s))\n",
        "\n",
        "This slide presents **HumanEval**, a famous benchmark for code LLMs that\n",
        "uses functional correctness (pass@1). It lists models like GPT-4 and\n",
        "WizardCoder and their scores.\n",
        "\n",
        "This validates that functional correctness is the industry standard for\n",
        "evaluating coding capabilities.\n",
        "\n",
        "### 68. Hands on: Building Functional Tests (Email)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_68.png\"\n",
        "alt=\"Slide 68\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 68</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:28](https://youtu.be/iQl03pQlYWY&t=2068s))\n",
        "\n",
        "This slide asks how to apply functional correctness to **Text** (e.g.,\n",
        "drafting emails).\n",
        "\n",
        "Rajiv suggests defining “functional” properties for text: Is it concise?\n",
        "Does it include a call to action? Is the tone polite? These are testable\n",
        "assertions we can make about text output.\n",
        "\n",
        "### 69. Hands on: Python Test for Text\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_69.png\"\n",
        "alt=\"Slide 69\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 69</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 35:19](https://youtu.be/iQl03pQlYWY&t=2119s))\n",
        "\n",
        "This slide shows a Python snippet that tests if an email uses “informal\n",
        "language.”\n",
        "\n",
        "It demonstrates that we can write code to evaluate text properties,\n",
        "effectively treating text generation as a “functional” problem with\n",
        "pass/fail criteria.\n",
        "\n",
        "### 70. Evaluation Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_70.png\"\n",
        "alt=\"Slide 70\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 70</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 35:54](https://youtu.be/iQl03pQlYWY&t=2154s))\n",
        "\n",
        "This slide highlights **Evaluation Benchmarks** on the chart.\n",
        "\n",
        "Rajiv moves to this category, explaining that benchmarks are essentially\n",
        "collections of the previous methods (exact match, functional tests)\n",
        "aggregated into large suites.\n",
        "\n",
        "### 71. Story: GLUE Benchmark\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_71.png\"\n",
        "alt=\"Slide 71\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 71</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 36:05](https://youtu.be/iQl03pQlYWY&t=2165s))\n",
        "\n",
        "This slide tells the history of **GLUE (2018)**. Before GLUE, models\n",
        "were specialized for single tasks. GLUE introduced the idea of a\n",
        "**General Language Understanding Evaluation**, pushing the field toward\n",
        "models that could handle many different tasks well.\n",
        "\n",
        "Rajiv credits GLUE with driving the progress that led to modern LLMs by\n",
        "giving researchers a unified target.\n",
        "\n",
        "### 72. So Many Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_72.png\"\n",
        "alt=\"Slide 72\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 72</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 37:47](https://youtu.be/iQl03pQlYWY&t=2267s))\n",
        "\n",
        "This slide introduces successors to GLUE: **HellaSwag** (commonsense)\n",
        "and **Big Bench** (reasoning).\n",
        "\n",
        "Rajiv notes that Big Bench Hard compares models to average and max human\n",
        "performance, providing a measuring stick for how close AI is getting to\n",
        "human-level reasoning.\n",
        "\n",
        "### 73. Even More Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_73.png\"\n",
        "alt=\"Slide 73\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 73</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 38:40](https://youtu.be/iQl03pQlYWY&t=2320s))\n",
        "\n",
        "This slide scrolls through a massive list of over 80 benchmarks.\n",
        "\n",
        "Rajiv uses this to illustrate the explosion of evaluation datasets.\n",
        "There is a benchmark for almost everything, but this abundance can be\n",
        "paralyzed.\n",
        "\n",
        "### 74. Multi-task Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_74.png\"\n",
        "alt=\"Slide 74\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 74</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 39:02](https://youtu.be/iQl03pQlYWY&t=2342s))\n",
        "\n",
        "This slide explains that **Multi-task benchmarks** aggregate many\n",
        "specific tasks (stories, code, legal) into a single score.\n",
        "\n",
        "This allows for a robust, high-level view of a model’s general\n",
        "capability, though it risks hiding specific weaknesses.\n",
        "\n",
        "### 75. Gaming Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_75.png\"\n",
        "alt=\"Slide 75\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 75</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 39:36](https://youtu.be/iQl03pQlYWY&t=2376s))\n",
        "\n",
        "This slide discusses **Gaming** and **Data Contamination**. It mentions\n",
        "**AlpacaEval** and how models might cheat by training on the test data.\n",
        "\n",
        "Rajiv warns that high benchmark scores might just mean the model has\n",
        "memorized the answers, making the benchmark useless for measuring true\n",
        "generalization.\n",
        "\n",
        "### 76. Hands on: Langtest\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_76.png\"\n",
        "alt=\"Slide 76\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 76</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 40:21](https://youtu.be/iQl03pQlYWY&t=2421s))\n",
        "\n",
        "This slide introduces **Langtest** by John Snow Labs. It is a library\n",
        "with 50+ test types for accuracy, bias, and robustness.\n",
        "\n",
        "Rajiv recommends it as a tool for running standard benchmarks on your\n",
        "own models.\n",
        "\n",
        "### 77. Hands on: Eleuther Harness\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_77.png\"\n",
        "alt=\"Slide 77\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 77</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 40:40](https://youtu.be/iQl03pQlYWY&t=2440s))\n",
        "\n",
        "This slide introduces the **Eleuther AI Evaluation Harness**. Rajiv\n",
        "calls this the “OG” (original gangster) framework. It supports over 200\n",
        "tasks.\n",
        "\n",
        "He provides a code snippet showing how easy it is to run a benchmark\n",
        "like MMLU on a Hugging Face model using this harness.\n",
        "\n",
        "### 78. OpenAI Evals\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_78.png\"\n",
        "alt=\"Slide 78\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 78</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 41:20](https://youtu.be/iQl03pQlYWY&t=2480s))\n",
        "\n",
        "This slide mentions **OpenAI Evals**, another framework for evaluating\n",
        "LLMs.\n",
        "\n",
        "Rajiv notes it is useful but emphasizes that standardized templates work\n",
        "best when content variation is low.\n",
        "\n",
        "### 79. Benchmarking Test Suites Summary\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_79.png\"\n",
        "alt=\"Slide 79\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 79</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 41:29](https://youtu.be/iQl03pQlYWY&t=2489s))\n",
        "\n",
        "This slide summarizes the Pros and Cons of benchmarks. \\* **Pros:** Wide\n",
        "coverage, cheap, automated. \\* **Cons:** Limited to easily measured\n",
        "tasks (often multiple choice), risk of leakage.\n",
        "\n",
        "Rajiv reminds us that benchmarks are proxies for quality, not definitive\n",
        "proof of utility for a specific business case.\n",
        "\n",
        "### 80. So Many Leaderboards\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_80.png\"\n",
        "alt=\"Slide 80\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 80</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:07](https://youtu.be/iQl03pQlYWY&t=2527s))\n",
        "\n",
        "This slide visualizes the ecosystem of leaderboards: Open LLM, Mosaic\n",
        "Eval Gauntlet, HELM.\n",
        "\n",
        "### 81. Pro Tip: Build Your Own Benchmark\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_81.png\"\n",
        "alt=\"Slide 81\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 81</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:18](https://youtu.be/iQl03pQlYWY&t=2538s))\n",
        "\n",
        "This is a key takeaway: **“Build your own benchmark / leaderboards.”**\n",
        "\n",
        "Rajiv argues that for an enterprise, public leaderboards are\n",
        "insufficient. You should curate a set of tasks that reflect *your*\n",
        "specific domain (e.g., legal, IT ops) and evaluate models against that.\n",
        "\n",
        "### 82. Custom Leaderboard Example\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_82.png\"\n",
        "alt=\"Slide 82\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 82</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 43:31](https://youtu.be/iQl03pQlYWY&t=2611s))\n",
        "\n",
        "This slide shows an example of a custom internal leaderboard\n",
        "(“AtmosBank”). It tracks how different models perform on the specific\n",
        "datasets that matter to that organization.\n",
        "\n",
        "This allows a company to quickly vet new models (like a new Llama\n",
        "release) against their specific needs.\n",
        "\n",
        "### 83. Benchmark Dataset: OWL\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_83.png\"\n",
        "alt=\"Slide 83\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 83</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 43:42](https://youtu.be/iQl03pQlYWY&t=2622s))\n",
        "\n",
        "This slide details **OWL**, a benchmark for IT Operations. It highlights\n",
        "the effort required to build it: manual review of hundreds of questions.\n",
        "\n",
        "Rajiv uses this to be realistic: building a custom benchmark has a\n",
        "**cost**. You need to invest human time to create the “Gold Standard”\n",
        "questions and answers.\n",
        "\n",
        "### 84. Averaging Can Mask Issues\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_84.png\"\n",
        "alt=\"Slide 84\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 84</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 44:39](https://youtu.be/iQl03pQlYWY&t=2679s))\n",
        "\n",
        "This slide warns that **“Averaging can mask issues.”** If Model 2 is\n",
        "amazing at your specific task but terrible at 9 others, an average score\n",
        "will hide its value.\n",
        "\n",
        "Rajiv advises looking at individual task scores rather than just the\n",
        "aggregate number on a leaderboard.\n",
        "\n",
        "### 85. Human Evaluation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_85.png\"\n",
        "alt=\"Slide 85\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 85</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:13](https://youtu.be/iQl03pQlYWY&t=2713s))\n",
        "\n",
        "This slide highlights **Human Evaluation** on the chart.\n",
        "\n",
        "Rajiv moves to the high-cost, high-flexibility zone. Humans are the\n",
        "ultimate judges of quality, capturing nuance that automated metrics\n",
        "miss.\n",
        "\n",
        "### 86. Human Evaluation - Best Practices\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_86.png\"\n",
        "alt=\"Slide 86\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 86</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:38](https://youtu.be/iQl03pQlYWY&t=2738s))\n",
        "\n",
        "This slide lists best practices: **Inter-annotator agreement**, clear\n",
        "guidelines, and training.\n",
        "\n",
        "Rajiv notes that we know how to do this from traditional data labeling.\n",
        "If humans can’t agree on the quality of an output (e.g., only 80%\n",
        "agreement), you can’t expect the model to do better.\n",
        "\n",
        "### 87. Human Evaluation - Limitations\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_87.png\"\n",
        "alt=\"Slide 87\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 87</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 46:25](https://youtu.be/iQl03pQlYWY&t=2785s))\n",
        "\n",
        "This slide discusses limitations. Humans are bad at checking\n",
        "**factuality** (it takes effort to Google facts) and are easily swayed\n",
        "by **assertiveness**.\n",
        "\n",
        "If an LLM sounds confident, humans tend to rate it highly even if it is\n",
        "wrong.\n",
        "\n",
        "### 88. Sycophancy Bias\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_88.png\"\n",
        "alt=\"Slide 88\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 88</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 46:43](https://youtu.be/iQl03pQlYWY&t=2803s))\n",
        "\n",
        "This slide defines **Sycophancy**: LLMs tend to generate responses that\n",
        "please the user rather than telling the truth.\n",
        "\n",
        "Rajiv shows an example where a model reinforces a user’s misconception\n",
        "because it wants to be “helpful.” Humans often rate these pleasing\n",
        "answers higher, reinforcing the bias.\n",
        "\n",
        "### 89. Human Evaluation Summary\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_89.png\"\n",
        "alt=\"Slide 89\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 89</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 47:03](https://youtu.be/iQl03pQlYWY&t=2823s))\n",
        "\n",
        "This slide summarizes Human Eval. \\* **Strengths:** Gold standard,\n",
        "handles variety. \\* **Weaknesses:** Expensive, slow, high variance,\n",
        "subject to bias.\n",
        "\n",
        "### 90. Hands on: Argilla\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_90.png\"\n",
        "alt=\"Slide 90\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 90</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 47:57](https://youtu.be/iQl03pQlYWY&t=2877s))\n",
        "\n",
        "This slide showcases **Argilla**, an open-source tool for data\n",
        "annotation.\n",
        "\n",
        "Rajiv encourages teams to set up tools like this to make it easy for\n",
        "domain experts (doctors, lawyers) to provide feedback on model outputs.\n",
        "\n",
        "### 91. Annotation Tools\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_91.png\"\n",
        "alt=\"Slide 91\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 91</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 48:20](https://youtu.be/iQl03pQlYWY&t=2900s))\n",
        "\n",
        "This slide lists other tools: **LabelStudio** and **Prodigy**. The\n",
        "message is: don’t reinvent the wheel, use existing tooling to gather\n",
        "human feedback.\n",
        "\n",
        "### 92. LongEval\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_92.png\"\n",
        "alt=\"Slide 92\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 92</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 48:39](https://youtu.be/iQl03pQlYWY&t=2919s))\n",
        "\n",
        "This slide references **LongEval**, a study on evaluating long\n",
        "summaries. It emphasizes that guidelines for humans need to be specific\n",
        "(coarse vs fine-grained) to get reliable results.\n",
        "\n",
        "### 93. Human Comparison/Arena\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_93.png\"\n",
        "alt=\"Slide 93\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 93</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 49:04](https://youtu.be/iQl03pQlYWY&t=2944s))\n",
        "\n",
        "This slide highlights **Human Comparison/Arena** on the chart.\n",
        "\n",
        "This is a specific subset of human evaluation focused on *preferences*\n",
        "rather than absolute scoring.\n",
        "\n",
        "### 94. Story: Dating (Preferences)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_94.png\"\n",
        "alt=\"Slide 94\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 94</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 49:26](https://youtu.be/iQl03pQlYWY&t=2966s))\n",
        "\n",
        "This slide uses a dating analogy. Old dating sites used long forms\n",
        "(detailed evaluation), but modern apps use swiping (binary preference).\n",
        "\n",
        "Rajiv argues that it is much easier and faster for humans to say “I\n",
        "prefer A over B” (swiping) than to fill out a detailed scorecard. This\n",
        "is the logic behind Arena evaluations.\n",
        "\n",
        "### 95. Head to Head Preferences\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_95.png\"\n",
        "alt=\"Slide 95\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 95</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:16](https://youtu.be/iQl03pQlYWY&t=3016s))\n",
        "\n",
        "This slide shows a “Head to Head” interface. The user sees two model\n",
        "outputs and clicks the one they like better.\n",
        "\n",
        "This method is widely used (e.g., in RLHF) because it scales well and\n",
        "reduces cognitive load on annotators.\n",
        "\n",
        "### 96. Head to Head Leaderboards\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_96.png\"\n",
        "alt=\"Slide 96\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 96</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:50](https://youtu.be/iQl03pQlYWY&t=3050s))\n",
        "\n",
        "This slide introduces the **LM-SYS Arena**. It uses an **Elo rating\n",
        "system** (like in Chess) based on thousands of anonymous battles between\n",
        "models.\n",
        "\n",
        "Rajiv notes this is a very effective way to rank models based on general\n",
        "human preference.\n",
        "\n",
        "### 97. Arena Solutions\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_97.png\"\n",
        "alt=\"Slide 97\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 97</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:44](https://youtu.be/iQl03pQlYWY&t=3104s))\n",
        "\n",
        "This slide provides links to the code for the LM-SYS arena. Rajiv\n",
        "suggests that enterprises can set up their own internal arenas to gamify\n",
        "evaluation for their employees.\n",
        "\n",
        "### 98. Model Based Approaches\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_98.png\"\n",
        "alt=\"Slide 98\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 98</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:55](https://youtu.be/iQl03pQlYWY&t=3115s))\n",
        "\n",
        "This slide highlights **Model based Approaches** on the chart.\n",
        "\n",
        "This is the most rapidly evolving area: using **LLMs to evaluate other\n",
        "LLMs** (LLM-as-a-Judge).\n",
        "\n",
        "### 99. Evaluating Factuality\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_99.png\"\n",
        "alt=\"Slide 99\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 99</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 52:24](https://youtu.be/iQl03pQlYWY&t=3144s))\n",
        "\n",
        "This slide discusses the limitation of reference-based factuality\n",
        "(comparing to a known ground truth). It notes that this is “Pretty\n",
        "limited utility” because we often don’t have ground truth for every new\n",
        "query.\n",
        "\n",
        "### 100. Model Based Evaluation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_100.png\"\n",
        "alt=\"Slide 100\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 100</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 52:54](https://youtu.be/iQl03pQlYWY&t=3174s))\n",
        "\n",
        "This slide illustrates the core concept: Instead of a human checking if\n",
        "the story is grammatical, we ask GPT-3 (or GPT-4) to do it.\n",
        "\n",
        "Rajiv explains that models are now good enough to act as proxy\n",
        "evaluators.\n",
        "\n",
        "### 101. Assertions\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_101.png\"\n",
        "alt=\"Slide 101\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 101</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 53:12](https://youtu.be/iQl03pQlYWY&t=3192s))\n",
        "\n",
        "This slide lists simple model-based checks called **Assertions**:\n",
        "Language Match, Sentiment, Toxicity, Length.\n",
        "\n",
        "These act like unit tests but use the LLM to classify the output (e.g.,\n",
        "“Is this text toxic? Yes/No”).\n",
        "\n",
        "### 102. G-Eval\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_102.png\"\n",
        "alt=\"Slide 102\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 102</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 55:07](https://youtu.be/iQl03pQlYWY&t=3307s))\n",
        "\n",
        "This slide introduces **G-Eval**, a framework that uses Chain-of-Thought\n",
        "(CoT) to generate a score. It provides the model with evaluation\n",
        "criteria and steps, asking it to reason before assigning a grade.\n",
        "\n",
        "### 103. SelfCheckGPT\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_103.png\"\n",
        "alt=\"Slide 103\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 103</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 55:26](https://youtu.be/iQl03pQlYWY&t=3326s))\n",
        "\n",
        "This slide describes **SelfCheckGPT**. This method detects\n",
        "hallucinations by sampling the model multiple times. If the model tells\n",
        "the same story consistently, it’s likely true. If the details change\n",
        "every time, it’s likely hallucinating.\n",
        "\n",
        "### 104. Which Model for Evaluation?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_104.png\"\n",
        "alt=\"Slide 104\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 104</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 55:50](https://youtu.be/iQl03pQlYWY&t=3350s))\n",
        "\n",
        "This slide asks which model to use as the judge. \\* **GPT-4:** Strongest\n",
        "evaluator, best for reasoning. \\* **GPT-3.5:** Cheaper, good for simple\n",
        "tasks. \\* **JudgeLM:** Fine-tuned specifically for evaluation.\n",
        "\n",
        "### 105. Human Alignment\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_105.png\"\n",
        "alt=\"Slide 105\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 105</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 56:44](https://youtu.be/iQl03pQlYWY&t=3404s))\n",
        "\n",
        "This slide presents data showing high **Human Alignment**. GPT-4 judges\n",
        "agree with human judges 80-95% of the time.\n",
        "\n",
        "This validates the approach: LLM judges are a scalable, cheap proxy for\n",
        "human evaluation.\n",
        "\n",
        "### 106. Model Evaluation Biases\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_106.png\"\n",
        "alt=\"Slide 106\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 106</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 57:32](https://youtu.be/iQl03pQlYWY&t=3452s))\n",
        "\n",
        "This slide warns about biases in LLM judges: \\* **Position Bias:**\n",
        "Preferring the first answer. \\* **Verbosity Bias:** Preferring longer\n",
        "answers. \\* **Self-Enhancement:** Preferring its own outputs.\n",
        "\n",
        "Rajiv suggests mitigations like swapping order and using different\n",
        "models for judging.\n",
        "\n",
        "### 107. Summary: Model Based Evaluation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_107.png\"\n",
        "alt=\"Slide 107\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 107</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 58:31](https://youtu.be/iQl03pQlYWY&t=3511s))\n",
        "\n",
        "This slide categorizes model-based methods: **Assertions** (simple),\n",
        "**Concept based** (G-Eval), **Sampling based** (SelfCheck), and\n",
        "**Preference based** (RLHF).\n",
        "\n",
        "### 108. Pros and Cons\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_108.png\"\n",
        "alt=\"Slide 108\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 108</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 59:17](https://youtu.be/iQl03pQlYWY&t=3557s))\n",
        "\n",
        "This slide summarizes the trade-offs. \\* **Pros:** Cheaper/faster than\n",
        "humans, good alignment. \\* **Cons:** Sensitive to prompts, known biases.\n",
        "\n",
        "### 109. Ragas\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_109.png\"\n",
        "alt=\"Slide 109\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 109</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 59:49](https://youtu.be/iQl03pQlYWY&t=3589s))\n",
        "\n",
        "This slide introduces **Ragas**, a framework specifically for evaluating\n",
        "RAG pipelines. It calculates a score based on **Faithfulness** and\n",
        "**Relevancy**.\n",
        "\n",
        "### 110. DeepEval\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_110.png\"\n",
        "alt=\"Slide 110\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 110</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:00:10](https://youtu.be/iQl03pQlYWY&t=3610s))\n",
        "\n",
        "This slide mentions **DeepEval**, another tool that treats evaluation\n",
        "like unit tests for LLMs, checking for bias, toxicity, etc.\n",
        "\n",
        "### 111. Hands on: Using Ragas\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_111.png\"\n",
        "alt=\"Slide 111\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 111</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:00:19](https://youtu.be/iQl03pQlYWY&t=3619s))\n",
        "\n",
        "This slide shows code for using Ragas. It demonstrates how to pass a\n",
        "dataset to the `evaluate` function and get metrics like\n",
        "`context_precision` and `answer_relevancy`.\n",
        "\n",
        "### 112. Hands on: Prompts (SALMONN)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_112.png\"\n",
        "alt=\"Slide 112\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 112</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:01:00](https://youtu.be/iQl03pQlYWY&t=3660s))\n",
        "\n",
        "This slide shows prompts from the **SALMONN** paper. Rajiv includes\n",
        "these to show real-world examples of how researchers craft prompts to\n",
        "evaluate specific qualities like coherence.\n",
        "\n",
        "### 113. Quality Prompt\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_113.png\"\n",
        "alt=\"Slide 113\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 113</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:01:24](https://youtu.be/iQl03pQlYWY&t=3684s))\n",
        "\n",
        "This slide shows a prompt for evaluating **Data Quality**. It asks the\n",
        "model to rate the helpfulness and relevance of text on a scale.\n",
        "\n",
        "### 114. RAG Relevancy Prompt\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_114.png\"\n",
        "alt=\"Slide 114\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 114</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:01:35](https://youtu.be/iQl03pQlYWY&t=3695s))\n",
        "\n",
        "This slide details a **“RAG RELEVANCY PROMPT TEMPLATE.”** It instructs\n",
        "the model to compare a question and a reference text to determine if the\n",
        "reference contains the answer.\n",
        "\n",
        "### 115. Impartial Judge Prompt\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_115.png\"\n",
        "alt=\"Slide 115\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 115</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:01:49](https://youtu.be/iQl03pQlYWY&t=3709s))\n",
        "\n",
        "This slide shows a prompt for an **“Impartial Judge.”** It asks the\n",
        "model to be an assistant that evaluates the quality of a response,\n",
        "ensuring it is helpful, accurate, and detailed.\n",
        "\n",
        "### 116. Resources: Model Based Eval\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_116.png\"\n",
        "alt=\"Slide 116\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 116</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:02:08](https://youtu.be/iQl03pQlYWY&t=3728s))\n",
        "\n",
        "This slide lists libraries: **Ragas, Microsoft llm-eval, TrueLens,\n",
        "Guardrails**.\n",
        "\n",
        "Rajiv notes that while libraries are great, many people end up writing\n",
        "their own hand-crafted prompts to fit their specific needs.\n",
        "\n",
        "### 117. Red Teaming\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_117.png\"\n",
        "alt=\"Slide 117\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 117</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:02:23](https://youtu.be/iQl03pQlYWY&t=3743s))\n",
        "\n",
        "This slide highlights **Red Teaming** on the chart.\n",
        "\n",
        "This is the final, most flexible, and rigorous technical evaluation\n",
        "method.\n",
        "\n",
        "### 118. Story: Microsoft Tay\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_118.png\"\n",
        "alt=\"Slide 118\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 118</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:02:37](https://youtu.be/iQl03pQlYWY&t=3757s))\n",
        "\n",
        "This slide tells the cautionary tale of **Microsoft Tay (2016)**. The\n",
        "chatbot learned from Twitter users and became racist/genocidal in less\n",
        "than 24 hours.\n",
        "\n",
        "Rajiv cites this as the “Origin of Red Teaming in AI”—the realization\n",
        "that we must proactively attack our models to find vulnerabilities\n",
        "before the public does.\n",
        "\n",
        "### 119. Why Red Teaming?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_119.png\"\n",
        "alt=\"Slide 119\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 119</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:04:11](https://youtu.be/iQl03pQlYWY&t=3851s))\n",
        "\n",
        "This slide defines Red Teaming: **Eliciting model vulnerabilities to\n",
        "prevent undesirable behaviors.**\n",
        "\n",
        "It is about adversarial testing—trying to trick the model into doing\n",
        "something bad.\n",
        "\n",
        "### 120. Every Use Case Should Be Red Teamed\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_120.png\"\n",
        "alt=\"Slide 120\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 120</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:04:23](https://youtu.be/iQl03pQlYWY&t=3863s))\n",
        "\n",
        "This slide argues that **“Every use case should be Red Teamed.”**\n",
        "\n",
        "Rajiv explains that fine-tuning a model (even slightly) can destroy the\n",
        "safety alignment (RLHF) provided by the base model creator. You cannot\n",
        "assume a model is safe just because it was safe before you fine-tuned\n",
        "it.\n",
        "\n",
        "### 121. How to: Red Teaming with a Model\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_121.png\"\n",
        "alt=\"Slide 121\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 121</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:05:07](https://youtu.be/iQl03pQlYWY&t=3907s))\n",
        "\n",
        "This slide suggests a technique: Use a separate “Risk Assessment” model\n",
        "(like Llama-2) to monitor the inputs and outputs of your main model,\n",
        "logging any risky queries.\n",
        "\n",
        "### 122. How to: Red Teaming from Meta\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_122.png\"\n",
        "alt=\"Slide 122\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 122</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:05:22](https://youtu.be/iQl03pQlYWY&t=3922s))\n",
        "\n",
        "This slide describes **Meta’s approach** to Llama 2. They hired diverse\n",
        "teams to attack the model regarding specific risks (criminal planning,\n",
        "trafficking).\n",
        "\n",
        "Rajiv notes that Meta actually held back a specific model (33b) because\n",
        "it failed these red team tests.\n",
        "\n",
        "### 123. Red Teaming Process\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_123.png\"\n",
        "alt=\"Slide 123\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 123</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:05:54](https://youtu.be/iQl03pQlYWY&t=3954s))\n",
        "\n",
        "This slide outlines the workflow: Generate prompts (multilingual),\n",
        "Annotate risk (Likert scale), and use data for safety training.\n",
        "\n",
        "### 124. Technical Methods Recap\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_124.png\"\n",
        "alt=\"Slide 124\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 124</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:06:04](https://youtu.be/iQl03pQlYWY&t=3964s))\n",
        "\n",
        "This slide shows the full **Generative AI Evaluation Methods** chart\n",
        "again.\n",
        "\n",
        "Rajiv concludes the technical section, having covered the spectrum from\n",
        "Exact Match to Red Teaming.\n",
        "\n",
        "### 125. Operational (TCO)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_125.png\"\n",
        "alt=\"Slide 125\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 125</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:06:16](https://youtu.be/iQl03pQlYWY&t=3976s))\n",
        "\n",
        "This slide highlights the **Operational (TCO)** pillar.\n",
        "\n",
        "Rajiv shifts gears to discuss the cost and maintenance of running these\n",
        "models.\n",
        "\n",
        "### 126. Story: GitHub Copilot Costs\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_126.png\"\n",
        "alt=\"Slide 126\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 126</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:06:33](https://youtu.be/iQl03pQlYWY&t=3993s))\n",
        "\n",
        "This slide references a story that **GitHub Copilot** was losing money\n",
        "per user (costing \\$20-\\$80/month while charging \\$10).\n",
        "\n",
        "Rajiv uses this to warn about the “Epidemic of cloud laundering.” You\n",
        "must calculate the inference costs upfront, or your successful product\n",
        "might bankrupt you.\n",
        "\n",
        "### 127. Monitoring\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_127.png\"\n",
        "alt=\"Slide 127\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 127</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:07:28](https://youtu.be/iQl03pQlYWY&t=4048s))\n",
        "\n",
        "This slide introduces **Monitoring** as the “Sibling of Evaluate.”\n",
        "\n",
        "It lists things to watch: Functional metrics (latency, errors), Prompt\n",
        "Drift, and Response Monitoring.\n",
        "\n",
        "### 128. Monitoring Metrics (GPU/Responsible AI)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_128.png\"\n",
        "alt=\"Slide 128\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 128</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:07:41](https://youtu.be/iQl03pQlYWY&t=4061s))\n",
        "\n",
        "This slide lists specific metrics. \\* **GPU:** Error rates (429), token\n",
        "counts. \\* **Responsible AI:** How often is the content filter\n",
        "triggering?\n",
        "\n",
        "### 129. Performance Metrics\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_129.png\"\n",
        "alt=\"Slide 129\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 129</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:07:53](https://youtu.be/iQl03pQlYWY&t=4073s))\n",
        "\n",
        "This slide lists **Performance Metrics**: \\* **Time to first token\n",
        "(TTFT):** Critical for user experience. \\* **Requests Per Second\n",
        "(RPS).** \\* **Token render rate.**\n",
        "\n",
        "### 130. User Engagement Funnel\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_130.png\"\n",
        "alt=\"Slide 130\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 130</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:08:01](https://youtu.be/iQl03pQlYWY&t=4081s))\n",
        "\n",
        "This slide suggests monitoring **User Engagement**. \\* Funnel: Trigger\n",
        "-\\> Response -\\> User Keeps/Accepts Response.\n",
        "\n",
        "Rajiv notes that OpenAI monitors the **KV Cache** utilization to\n",
        "understand real usage patterns better than simple GPU utilization.\n",
        "\n",
        "### 131. Application to RAG\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_131.png\"\n",
        "alt=\"Slide 131\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 131</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:09:14](https://youtu.be/iQl03pQlYWY&t=4154s))\n",
        "\n",
        "This slide acts as a section header: **APPLICATION TO RAG**.\n",
        "\n",
        "Rajiv will now apply all the previous concepts to a specific use case:\n",
        "Retrieval Augmented Generation.\n",
        "\n",
        "### 132. Bring Your Own Facts\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_132.png\"\n",
        "alt=\"Slide 132\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 132</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:09:26](https://youtu.be/iQl03pQlYWY&t=4166s))\n",
        "\n",
        "This slide explains the core philosophy of RAG: **“If you need facts -\n",
        "bring them yourself.”** Don’t rely on the LLM’s training data; provide\n",
        "the context.\n",
        "\n",
        "### 133. What is RAG?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_133.png\"\n",
        "alt=\"Slide 133\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 133</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:09:33](https://youtu.be/iQl03pQlYWY&t=4173s))\n",
        "\n",
        "This slide defines RAG: Improving responses by grounding the model on\n",
        "external knowledge sources.\n",
        "\n",
        "### 134. Evaluating RAG (The Wrong Way)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_134.png\"\n",
        "alt=\"Slide 134\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 134</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:09:51](https://youtu.be/iQl03pQlYWY&t=4191s))\n",
        "\n",
        "This slide shows a “recipe” for RAG evaluation focusing solely on\n",
        "factuality precision (95%).\n",
        "\n",
        "Rajiv presents this as a **trap**. He asks, “What’s wrong with this?”\n",
        "\n",
        "### 135. Missing the Point\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_135.png\"\n",
        "alt=\"Slide 135\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 135</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:10:07](https://youtu.be/iQl03pQlYWY&t=4207s))\n",
        "\n",
        "This slide explicitly states that focusing only on technical details\n",
        "misses the larger point of view.\n",
        "\n",
        "Rajiv is baiting the audience to remember the **Three Pillars**.\n",
        "\n",
        "### 136. Three Pillars (RAG Context)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_136.png\"\n",
        "alt=\"Slide 136\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 136</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:10:20](https://youtu.be/iQl03pQlYWY&t=4220s))\n",
        "\n",
        "This slide brings back the **Technical, Business, Operational** pillars.\n",
        "\n",
        "Rajiv insists we must start with the Business metrics before jumping\n",
        "into technical precision.\n",
        "\n",
        "### 137. Business Metric for RAG\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_137.png\"\n",
        "alt=\"Slide 137\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 137</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:10:28](https://youtu.be/iQl03pQlYWY&t=4228s))\n",
        "\n",
        "This slide outlines the **Business questions**: \\* What is the value of\n",
        "a correct answer? \\* What is the **cost/consequence** of a wrong answer?\n",
        "\n",
        "Rajiv warns against building a “science experiment” without knowing the\n",
        "ROI.\n",
        "\n",
        "### 138. Operational Metrics for RAG\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_138.png\"\n",
        "alt=\"Slide 138\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 138</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:10:58](https://youtu.be/iQl03pQlYWY&t=4258s))\n",
        "\n",
        "This slide lists **Operational questions**: \\* Labeling effort? \\*\n",
        "Running costs? \\* Is IT ready to support this?\n",
        "\n",
        "### 139. Three Pillars (Transition)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_139.png\"\n",
        "alt=\"Slide 139\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 139</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:11:43](https://youtu.be/iQl03pQlYWY&t=4303s))\n",
        "\n",
        "This slide shows the three pillars again, preparing to zoom in on the\n",
        "Technical side.\n",
        "\n",
        "### 140. Technical Pillar\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_140.png\"\n",
        "alt=\"Slide 140\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 140</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:11:45](https://youtu.be/iQl03pQlYWY&t=4305s))\n",
        "\n",
        "This slide highlights **Technical (F1)**. Now that we’ve justified the\n",
        "business case, how do we technically evaluate RAG?\n",
        "\n",
        "### 141. Current Approaches (Eyeballing)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_141.png\"\n",
        "alt=\"Slide 141\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 141</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:11:51](https://youtu.be/iQl03pQlYWY&t=4311s))\n",
        "\n",
        "This slide critiques the current state: **“Eyeballing a few examples.”**\n",
        "\n",
        "Rajiv notes that most developers just look at a few chats and say “looks\n",
        "good.” This is insufficient for production.\n",
        "\n",
        "### 142. Evaluate LLM System\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_142.png\"\n",
        "alt=\"Slide 142\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 142</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:12:14](https://youtu.be/iQl03pQlYWY&t=4334s))\n",
        "\n",
        "This slide lists system-level questions: Accuracy, references,\n",
        "understandability, query time.\n",
        "\n",
        "### 143. Decomposing RAG\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_143.png\"\n",
        "alt=\"Slide 143\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 143</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:12:37](https://youtu.be/iQl03pQlYWY&t=4357s))\n",
        "\n",
        "This slide brings back the RAG diagram, emphasizing\n",
        "**decomposition**. 1. Retrieval 2. Augmented Generation\n",
        "\n",
        "Rajiv argues we must evaluate these independently to find the\n",
        "bottleneck. Often, the problem is the **Retriever**, not the LLM.\n",
        "\n",
        "### 144. Component Metrics\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_144.png\"\n",
        "alt=\"Slide 144\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 144</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:12:53](https://youtu.be/iQl03pQlYWY&t=4373s))\n",
        "\n",
        "This slide details metrics for each component: \\* **Retrieval:**\n",
        "Precision, Recall, Order. \\* **Augmentation:** Correctness, Toxicity,\n",
        "Hallucination.\n",
        "\n",
        "### 145. Analyze Retrieval\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_145.png\"\n",
        "alt=\"Slide 145\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 145</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:13:53](https://youtu.be/iQl03pQlYWY&t=4433s))\n",
        "\n",
        "This slide explains how to evaluate retrieval. You need a dataset of\n",
        "**(Query, Relevant Documents)**.\n",
        "\n",
        "You run your retriever and check if it found the documents in your\n",
        "ground truth set.\n",
        "\n",
        "### 146. Methods for Retrieval\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_146.png\"\n",
        "alt=\"Slide 146\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 146</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:14:13](https://youtu.be/iQl03pQlYWY&t=4453s))\n",
        "\n",
        "This slide highlights **Exact Matching** on the chart.\n",
        "\n",
        "For retrieval, we can use exact matching (or set intersection) because\n",
        "we know exactly which document IDs should be returned.\n",
        "\n",
        "### 147. Retrieval Metrics\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_147.png\"\n",
        "alt=\"Slide 147\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 147</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:14:16](https://youtu.be/iQl03pQlYWY&t=4456s))\n",
        "\n",
        "This slide lists retrieval metrics: **Success rate (Hit-rate)** and\n",
        "**Mean Reciprocal Rank (MRR)**.\n",
        "\n",
        "### 148. Analyze Augmentation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_148.png\"\n",
        "alt=\"Slide 148\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 148</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:14:48](https://youtu.be/iQl03pQlYWY&t=4488s))\n",
        "\n",
        "This slide explains how to evaluate the generation step. You need\n",
        "**(Context, Generated Response, Ground Truth)**.\n",
        "\n",
        "### 149. Methods for Augmentation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_149.png\"\n",
        "alt=\"Slide 149\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 149</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:15:02](https://youtu.be/iQl03pQlYWY&t=4502s))\n",
        "\n",
        "This slide highlights **Human** and **Model-based** approaches on the\n",
        "chart.\n",
        "\n",
        "For generation, exact match doesn’t work. We need flexible evaluators\n",
        "(Humans or LLMs) to judge faithfulness and relevancy.\n",
        "\n",
        "### 150. Augmentation Modules\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_150.png\"\n",
        "alt=\"Slide 150\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 150</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:15:05](https://youtu.be/iQl03pQlYWY&t=4505s))\n",
        "\n",
        "This slide lists modules to test: \\* **Label-free:** Faithfulness (did\n",
        "it stick to context?), Relevancy. \\* **With-labels:** Correctness\n",
        "(compared to ground truth).\n",
        "\n",
        "### 151. Pro Tip: Imbalance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_151.png\"\n",
        "alt=\"Slide 151\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 151</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:15:28](https://youtu.be/iQl03pQlYWY&t=4528s))\n",
        "\n",
        "This slide warns about **Imbalanced Data**. If most retrieved documents\n",
        "are irrelevant, accuracy is a bad metric. Use **Precision and Recall**.\n",
        "\n",
        "### 152. Pro Tip: Synthetic Data\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_152.png\"\n",
        "alt=\"Slide 152\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 152</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:15:35](https://youtu.be/iQl03pQlYWY&t=4535s))\n",
        "\n",
        "This slide suggests generating **Synthetic Evaluation Datasets**.\n",
        "\n",
        "You can use an LLM to read your documents and generate Question/Answer\n",
        "pairs. This creates a “Gold Standard” dataset for retrieval evaluation\n",
        "without manual labeling.\n",
        "\n",
        "### 153. Notebooks Used\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_153.png\"\n",
        "alt=\"Slide 153\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 153</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:16:14](https://youtu.be/iQl03pQlYWY&t=4574s))\n",
        "\n",
        "This slide lists the notebooks available in the GitHub repo: Prompting,\n",
        "Guidance, Eleuther Harness, Langtest, Ragas.\n",
        "\n",
        "### 154. Final Slide\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/evaluating-llms-deep-dive/slide_154.png\"\n",
        "alt=\"Slide 154\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 154</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 1:16:32](https://youtu.be/iQl03pQlYWY&t=4592s))\n",
        "\n",
        "The presentation concludes with the title slide again, providing the\n",
        "speaker’s contact info and the GitHub link one last time. Rajiv thanks\n",
        "the audience and promises updates as the field evolves.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "*This annotated presentation was generated from the talk using\n",
        "AI-assisted tools. Each slide includes timestamps and detailed\n",
        "explanations.*"
      ],
      "id": "1c59367e-26ad-49b4-b495-cccb6a4e1eaf"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}