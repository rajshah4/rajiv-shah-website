[
  {
    "objectID": "xgbfi.html",
    "href": "xgbfi.html",
    "title": "Using xgbfi for revealing feature interactions",
    "section": "",
    "text": "Introduction\nTree based methods excel in using feature or variable interactions. As a tree is built, it picks up on the interaction of features. For example, buying ice cream may not be affected by having extra money unless the weather is hot. It is the interaction of both of these features that can affect whether ice cream will be consumed.\nThe traditional manner for examining interactions is relying on measures of variable importance. However, these measures don‚Äôt provide insights into second or third order interactions. Identifying these interactions are important in building better models, especially when finding features to use within linear models.\nIn this post, I show how to find higher order interactions using XGBoost Feature Interactions & Importance. This tool has been available for a while, but outside of kagglers, it has received relatively little attention.\nAs a starting point, I used the Ice Cream dataset to illustrate using xgbfi. This walkthrough is in R, but python instructions are also available at the repo. I am going to break the code into three sections, the initial build of the model, exporting the files necessary for xgbfi, and running xgbi.\n\nBuilding the model\nLets start by loading the data:\nlibrary(xgboost)\nlibrary(Ecdat)\ndata(Icecream)\ntrain.data &lt;- data.matrix(Icecream[,-1])\nThe next step is running xgboost:\nbst &lt;- xgboost(data = train.data, label = Icecream$cons, max.depth = 3, eta = 1, nthread = 2, nround = 2, objective = \"reg:linear\")\nTo better understand how the model is working, lets go ahead and look at the trees:\nxgb.plot.tree(feature_names = names((Icecream[,-1])), model = bst)\n\n\n\nxg tree plot\n\n\nThe results here line up with our intution. Hot days seems to be the biggest variable by just eyeing the plot. This lines up with the results of a variable importance calculation:\n&gt; xgb.importance(colnames(train.data, do.NULL = TRUE, prefix = \"col\"), model = bst)\n   Feature       Gain      Cover Frequency\n1:    temp 0.75047187 0.66896552 0.4444444\n2:  income 0.18846270 0.27586207 0.4444444\n3:   price 0.06106542 0.05517241 0.1111111\nAll of this should be very familiar to anyone who has used decision trees for modeling. But what are the second order interactions? Third order interactions? Can you rank them?\n\n\nExporting the tree\nThe next step involves saving the tree and moving it outside of R so xgbfi can parse the tree. The code below will help to create two files that are needed:xgb.dump and fmap.text.\nfeatureList &lt;- names(Icecream[,-1])\nfeatureVector &lt;- c() \nfor (i in 1:length(featureList)) { \n  featureVector[i] &lt;- paste(i-1, featureList[i], \"q\", sep=\"\\t\") \n}\nwrite.table(featureVector, \"fmap.txt\", row.names=FALSE, quote = FALSE, col.names = FALSE)\nxgb.dump(model = bst, fname = 'xgb.dump', fmap = \"fmap.txt\", with.stats = TRUE)\n\n\nRunning xgbfi\nThe first step is to clone the xgbfi repository onto your computer. Then copy the files xgb.dump and fmap.text to the bin directory.\nGo to your terminal or command line and run: XgbFeatureInteractions.exe application. On a mac, download mono and then run the command: mono XgbFeatureInteractions.exe. There is also a XgbFeatureInteractions.exe.config file that contains configuration settings in the bin directory.\nAfter the application runs, it will write out an excel spreadsheet titled: XgbFeatureInteractions.xlsx. This spreadsheet has the good stuff! Open up the spreadsheet and you should see:\n\n\n\ninteraction depth 0\n\n\nThis tab of the spreadsheet shows the first order interactions. These results are similar to what variable importance showed. The good stuff is when you click on the tab for Interaction Depth 1 or Interaction Depth 2.\n\n\n\ninteraction depth 1\n\n\n\n\n\ninteraction depth 2\n\n\nIt is now possible to rank the higher order interactions. With the simple dataset, you can see that the results out of xgbfi match what is happening in the tree. The real value of this tool is for much larger datasets, where its difficult to examine the trees for the interactions."
  },
  {
    "objectID": "standup.html",
    "href": "standup.html",
    "title": "Stand Up for Best Practices",
    "section": "",
    "text": "Source: Yuriy Guts selection from Shutterstock\n\nStand Up for Best Practices:\n\n\nMisuse of Deep Learning in Nature‚Äôs Earthquake Aftershock Paper\n\n\nThe Dangers of Machine Learning Hype\nPractitioners of AI, machine learning, predictive modeling, and data science have grown enormously over the last few years. What was once a niche field defined by its blend of knowledge is becoming a rapidly growing profession. As the excitement around AI continues to grow, the new wave of ML augmentation, automation, and GUI tools will lead to even more growth in the number of people trying to build predictive models.\nBut here‚Äôs the rub: While it becomes easier to use the tools of predictive modeling, predictive modeling knowledge is not yet a widespread commodity. Errors can be counterintuitive and subtle, and they can easily lead you to the wrong conclusions if you‚Äôre not careful.\nI‚Äôm a data scientist who works with dozens of expert data science teams for a living. In my day job, I see these teams striving to build high-quality models. The best teams work together to review their models to detect problems. There are many hard-to-detect-ways that lead to problematic models (say, by allowing target leakage into their training data).\nIdentifying issues is not fun. This requires admitting that exciting results are ‚Äútoo good to be true‚Äù or that their methods were not the right approach. In other words, it‚Äôs less about the sexy data science hype that gets headlines and more about a rigorous scientific discipline.\n\n\nBad Methods Create Bad Results\nAlmost a year ago, I read an article in Nature that claimed unprecedented accuracy in predicting earthquake aftershocks by using deep learning. Reading the article, my internal radar became deeply suspicious of their results. Their methods simply didn‚Äôt carry many of the hallmarks of careful predicting modeling.\nI started to dig deeper. In the meantime, this article blew up and became widely recognized! It was even included in the release notes for Tensorflow as an example of what deep learning could do. However, in my digging, I found major flaws in the paper. Namely, data leakage which leads to unrealistic accuracy scores and a lack of attention to model selection (you don‚Äôt build a 6 layer neural network when a simpler model provides the same level of accuracy).\nThe testing dataset had a much higher AUC than the training set . . . this is not normal\nTo my earlier point: these are subtle, but incredibly basic predictive modeling errors that can invalidate the entire results of an experiment. Data scientists are trained to recognize and avoid these issues in their work. I assumed that this was simply overlooked by the author, so I contacted her and let her know so that she could improve her analysis. Although we had previously communicated, she did not respond to my email over concerns with the paper.\n\n\nFalling On Deaf Ears\nSo, what was I to do? My coworkers told me to just tweet it and let it go, but I wanted to stand up for good modeling practices. I thought reason and best practices would prevail, so I started a 6-month process of writing up my results and shared them with Nature.\nUpon sharing my results, I received a note from Nature in January 2019 that despite serious concerns about data leakage and model selection that invalidate their experiment, they saw no need to correct the errors, because ‚ÄúDevries et al.¬†are concerned primarily with using machine learning as [a] tool to extract insight into the natural world, and not with details of the algorithm design‚Äù. The authors provided a much harsher response.\nYou can read the entire exchange on my github.\nIt‚Äôs not enough to say that I was disappointed. This was a major paper (it‚Äôs Nature!) that bought into AI hype and published a paper despite it using flawed methods.\nThen, just this week, I ran across articles by Arnaud Mignan and Marco Broccardo on shortcomings that they found in the aftershocks article. Here are two more data scientists with expertise in earthquake analysis who also noticed flaws in the paper. I also have placed my analysis and reproducible code on github.\nGo run the analysis yourself and see the issue\n\n\nStanding Up For Predictive Modeling Methods\nI want to make it clear: my goal is not to villainize the authors of the aftershocks paper. I don‚Äôt believe that they were malicious, and I think that they would argue their goal was to just show how machine learning could be applied to aftershocks. Devries is an accomplished earthquake scientist who wanted to use the latest methods for her field of study and found exciting results from it.\nBut here‚Äôs the problem: their insights and results were based on fundamentally flawed methods. It‚Äôs not enough to say, ‚ÄúThis isn‚Äôt a machine learning paper, it‚Äôs an earthquake paper.‚Äù If you use predictive modeling, then the quality of your results are determined by the quality of your modeling. Your work becomes data science work, and you are on the hook for your scientific rigor.\nThere is a huge appetite for papers that use the latest technologies and approaches. It becomes very difficult to push back on these papers.\nBut if we allow papers or projects with fundamental issues to advance, it hurts all of us. It undermines the field of predictive modeling.\nPlease push back on bad data science. Report bad findings to papers. And if they don‚Äôt take action, go to twitter, post about it, share your results and make noise. This type of collective action worked to raise awareness of p-values and combat the epidemic of p-hacking. We need good machine learning practices if we want our field to continue to grow and maintain credibility.\nAcknowledgments: I want to thank all the great data scientists at DataRobot that collaborated and supported me this past year, a few of these include: Lukas Innig, Amanda Schierz, Jett Oristaglio, Thomas Stearns, and Taylor Larkin.\nThis article was orignally posted on Medium and featured on Reddit"
  },
  {
    "objectID": "setfit.html",
    "href": "setfit.html",
    "title": "Few shot text classification with SetFit",
    "section": "",
    "text": "SetFit\n\n\n\nIntroduction\nData scientists often do not have large amounts of labeled data. This issue is even graver when dealing with problems with tens or hundreds of classes. The reality is very few text classification problems get to the point where adding more labeled data isn‚Äôt improving performance.\nSetFit offers a few-shot learning approach for text classification. The paper‚Äôs results show across many datasets, it‚Äôs possible to get better performance with less labeled data. This technique uses contrastive learning to build a larger dataset for fine-tuning a text classification model. This approach was new to me and was why I did a video explaining how contrastive learning helps with text classification.\nI have created a Colab üìì companion notebook at https://bit.ly/raj_setfit, and the Youtube üé• video that provides a detailed explanation. I walk through a simple churn example to give the intuition behind SetFit. The notebook trains the CR (customer review dataset) highlighted in the SetFit paper.\nThe SetFit github contains the code, and a great deep dive for text classification is found on Philipp‚Äôs blog. For those looking to productionize a SetFit model, Philipp has also documented how to create the Hugging Face endpoint for a SetFit model.\nSo grab your favorite text classification dataset and give it a try!"
  },
  {
    "objectID": "running-code-failing-models.html",
    "href": "running-code-failing-models.html",
    "title": "Running Code and Failing Models",
    "section": "",
    "text": "Source: Yuriy Guts selection from Shutterstock\nMachine learning is a glass cannon. When used correctly, it can be a truly transformative technology, but just a small oversight can cause it to become misleading and even actively harmful. Even if all the code runs and the model seems to be spitting out reasonable answers, it‚Äôs possible for a model to encode fundamental data science mistakes that invalidate its results. These errors might seem small, but the effects can be disastrous when the model is used to make decisions in the real world.\nThe promise and power of AI lead many researchers to gloss over the ways in which things can go wrong when building and operationalizing machine learning models. As a data scientist, one of my passions is to reproduce research papers as a learning exercise. Along the way, I have uncovered cases where the research was published with faulty methodologies. My hope is that this analysis can increase awareness about data science mistakes and raise the standards for machine learning in research. For example, last year I shared an analysis of a project by Harvard and Google researchers that contained fundamental errors. The researchers refused to fix their mistake even when confronted with it directly.\nOver the holidays, I used DataRobot to reproduce a few machine learning benchmarks. I found many examples of machine learning code that ran without errors but that were built using flawed data science practices. The examples I share in this post come from the world‚Äôs best data scientists and affect hundreds of peer-reviewed research publications. As these examples show, errors in machine learning can be subtle. The key to finding these errors is to work with a tool that offers guardrails and insights along the way."
  },
  {
    "objectID": "running-code-failing-models.html#target-leakage-in-a-fast.ai-example",
    "href": "running-code-failing-models.html#target-leakage-in-a-fast.ai-example",
    "title": "Running Code and Failing Models",
    "section": "Target Leakage in a fast.ai Example",
    "text": "Target Leakage in a fast.ai Example\nDeep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD by Jeremy Howard and Sylvain Gugger is a hands-on guide that helps people with little math background understand and use deep learning quickly. In the section about tabular datasets, the authors use the Blue Book for Bulldozers problem, the goal of which is to predict the sale price for heavy equipment at auction. I tried to replicate their machine learning model and wasn‚Äôt able to beat their model‚Äôs predictive performance, which piqued my interest.\nAfter carefully inspecting their code, I found a mistake in their validation dataset. Their code attempted to create a validation test set based on a prediction point of November 1, 2011. The goal was to split the data at this point so that you could train on the data known at prediction time. The performance of the model is then analyzed on a test set, which is located after the prediction point. Unfortunately, the code was not written correctly; there was contamination from the future in the training data.\n\n\n\nLeakage.png\n\n\nThe code below might at first look like it separates data before and after November 1, 2011, but there‚Äôs a subtle mistake that includes future dates. The use of information in the model training process that would not be expected at prediction time is known as target leakage, and it led to an over-optimistic accuracy. Because I used DataRobot, which requires and validates a date when creating a validation dataset based on time, I was able to find the mistake in the fast.ai book.\nAfter the target leakage was fixed, the fast.ai scores dropped, and I was able to reproduce the results outside of fast.ai. This simple coding mistake led to a notebook and model that appeared valid. If this model were put into production, the results would have been much worse on new data. After I identified this issue, Jeremy Howard agreed to add a note in the course materials.\n\n\n\nfastai2.png"
  },
  {
    "objectID": "running-code-failing-models.html#sarcos-dataset-failure",
    "href": "running-code-failing-models.html#sarcos-dataset-failure",
    "title": "Running Code and Failing Models",
    "section": "SARCOS Dataset Failure",
    "text": "SARCOS Dataset Failure\nThe SARCOS dataset is a widely used benchmark dataset in machine learning. Based on predicting the movement of a robotic arm, SARCOS appears in more than one hundred academic papers. I tested this dataset because it appears in various benchmarks by Google and fast.ai.\nThe SARCOS dataset is broken into two parts: a training dataset (sarcos_inv) and a test dataset (sarcos_inv_test). Following common data science practices, DataRobot broke the SARCOS training set into a training partition and a validation partition. I treated the SARCOS test set (sarcos_inv_test) as a holdout. When I looked at the results, I immediately noticed something suspicious. Do you see it?\n\n\n\nsarcos3.png\n\n\nThe large drop between the validation score and the holdout score indicates that something is very different between the validation and holdout datasets. When I examined the holdout dataset (the SARCOS test set), I found that every row in the test set was in the training data too. After some investigation, I discovered that the holdout dataset was built out of the training dataset. Of the 4,449 examples in the test set, 4,445 examples are present in the training set, too. The target leakage here is significant. By overfitting or memorizing the training dataset, it‚Äôs possible to get perfect results on the test set. Overfitting, a well-known issue in machine learning, is illustrated in the following figure. The test dataset should have used out-of-sample testing to prevent overfitting.\n\n\n\noverfit4.png\n\n\nTarget leakage helped to explain the very low scores of the deep learning models. For comparison, a random forest model achieves 2.38 mean squared error (MSE), while a deep learning model overfits and produces 0.038 MSE. Judging from the suspiciously large difference between the models, it appears that the deep learning model just memorized the training data, which is why it had such low error.\nThe consequences of this target leakage are far-reaching. More than one hundred journal articles relied on this dataset. Thousands of data scientists have used it to benchmark their machine learning code. Researcher Kai Arulkumaran has already acknowledged this issue and now the research community is dealing with the ramifications of the target leakage.\nWhy wasn‚Äôt this error discovered earlier? When I reproduced the SARCOS benchmarks, I used a tool that includes technical safeguards for proper validation splits and provides transparency in the display of the results of each split. DataRobot‚Äôs AutoML was designed by data scientists to prevent these sorts of issues. In contrast, working within code, it was quite easy to overlook this fundamental issue. After all, thousands of data scientists have rerun their code and published their results without a second thought."
  },
  {
    "objectID": "running-code-failing-models.html#poker-hand-dataset",
    "href": "running-code-failing-models.html#poker-hand-dataset",
    "title": "Running Code and Failing Models",
    "section": "Poker Hand Dataset",
    "text": "Poker Hand Dataset\nThe Poker Hand dataset is another widely used benchmark dataset in machine learning. It‚Äôs used to predict poker hands (for example, a full house from five cards). The fast.ai and Google benchmarks for this model use the accuracy metric. Accuracy is a measurement for assessing the predictive performance of a model (basically, the percentage of predictions that are correct). Although it‚Äôs easy to get running code with the accuracy metric, it‚Äôs not good data science practice for this problem.\nWhen DataRobot builds a model with the Poker Hand dataset, by default, it uses log loss as an optimization metric. Log loss is a measure of error for a model. At DataRobot, we believe that it isn‚Äôt good practice to use accuracy as your metric on a classification project with imbalanced classes. With imbalanced data, you can easily build a highly accurate model that‚Äôs useless.\nTo understand why accuracy isn‚Äôt the best metric when classifying unbalanced data, consider the following figure. Minesweeper is a popular game where the goal is to identify a few mines that are scattered across a board. Because there are a lot of squares with no mines, you could generate a very accurate model just by predicting that every square is safe. Although a 99% accurate model for Minesweeper sounds impressive, it‚Äôs not very useful.\n\n\n\nminesweeper5.png\n\n\nAutomated feature selection in DataRobot provides a more parsimonious featurelist. In the Poker Hand dataset, DataRobot created a DR Reduced Features list with only six features. The starting feature list for this dataset, Cat+Cont, contained 15 features. The leaderboard below shows that the simpler DR Reduced Features list performs better than the full Cat+Cont feature list. The model below was optimized on log loss, but I am viewing the accuracy metrics for comparison to the existing benchmarks.\n\n\n\nDRreduce6.png"
  },
  {
    "objectID": "running-code-failing-models.html#conclusion",
    "href": "running-code-failing-models.html#conclusion",
    "title": "Running Code and Failing Models",
    "section": "Conclusion",
    "text": "Conclusion\nI have shared simple examples of how data scientists can have running code, but failed models. After spending a week going through a half dozen datasets, I am even more convinced that automation with technical safeguards is a required part of building trusted AI. The mistakes I‚Äôve shared here are not isolated incidents.\nThe issues go beyond the reproducibility crisis for machine learning research. It‚Äôs a great first step for researchers to publish their code and make the data available, but as these examples show, sharing code isn‚Äôt enough to validate models. So, what should you do about this?\nIn regulated industries, there are processes in place to validate running code (for example, building a challenger model using a different technical framework). For its safeguards and transparency, many organizations use DataRobot to validate models. Just rereading or rerunning a project isn‚Äôt enough to identify errors."
  },
  {
    "objectID": "running-code-failing-models.html#links",
    "href": "running-code-failing-models.html#links",
    "title": "Running Code and Failing Models",
    "section": "Links",
    "text": "Links\n\nStand Up for Best Practices (Harvard Leakage)\nFast.AI Issue\nSARCOS"
  },
  {
    "objectID": "outlier_app.html",
    "href": "outlier_app.html",
    "title": "Outlier App",
    "section": "",
    "text": "algorithms\n\n\n\nIntroduction\nI was recently trying various outlier detection algorithms. For me, the best way to understand an algorithm is to tinker with it. I wanted to share my recent work on a shiny app that allows you to play around with various outlier algorithms.\nThe shiny app is available on my site, but even better, the code is on github for you to run locally or improve! I also posted a video that provides background on the app. Let me give you a quick tour of the app:\n\n\nAlgorithms\nThe available algorithms include:\n\nHierarchical Clustering (DMwR)\nKmeans (distance metrics from proxy)\n\nKmeans Euclidean Distance\nKmeans Mahalanobis\nKmeans Manhattan\n\nFuzzy kmeans (all from fclust)\n\nFuzzy kmeans - Gustafson and Kessel\nFuzzy k-medoids\nFuzzy k-means with polynomial fuzzifier\n\nLocal Outlier Factor (dbscan)\nRandomForest (proximity from randomForest)\n\nIsolation Forest (IsolationForest)\n\nAutoencoder (Autoencoder)\nFBOD and SOD (HighDimOut)\n\n\n\n\nalgorithms\n\n\n\n\nDatasets\nThere are also a wide range of datasets to try as well:\n\n\n\ndatasets\n\n\nOnce the data is loaded, you can start exploring. One thing you can do is look at the effect scaling can have. In this example, you can see how outliers differ when scaling is used. The values on the far right no longer dominate the distance measurements, and there are now outliers from other areas:\n\n\n\nscaling\n\n\nBy trying different algorithms, you can see how different algorithms will select outliers. In this case, you see a difference between the outliers selected using an autoencoder versus isolation forest.\nAnother example here is the difference between kmeans and fuzzy kmeans as show below:\n\n\n\nfuzzy\n\n\nA density based algorithm can also select different outliers versus a distance based algorithm. This example nicely shows the difference between kmeans and lof (local outlier factor from dbscan)\n\n\n\ndensity\n\n\nAn important part of using this visualization is studying the distance numbers that are calculated. Are these numbers meshing with your intuition? How big of a quantitative difference is there between outliers and other points?\n\n\n\noutlier_table\n\n\nSo that is the 2D app. Please send me bug fixes, additional algorithms, or tighter code!\n3D+ App?\nThe next thing is whether to expand this to larger datasets. This is something that you would run locally (large datasets take too long to run for my shiny server). The downside of larger datasets is that it gets tricker to visualize them. For now, I am using a TSNE plot. I am open to suggestions, but the intent here is a way to evaluate outlier algorithms on a variety of datasets.\n\n\n\ndatasets"
  },
  {
    "objectID": "openai_mod.html",
    "href": "openai_mod.html",
    "title": "Building Worlds for Reinforcement Learning",
    "section": "",
    "text": "Introduction\nOpenAI‚Äôs Gym places reinforcement learning into the masses. It comes with a wealth of environments from the classic cart pole, board games, Atari, and now the new Universe which adds flash games and PC Games like GTA or Portal. This is great news, but for someone starting out, working on some of these games is overkill. You can learn a lot more in a shorter time, by playing around with some smaller toy environments.\nOne area I like within the gym environments are the classic control problems (besides the fun of eating melon and poop). These are great problems for understanding the basics of reinforcement learning because we intutiively understand the rewards and they run really fast. Its not like pong that can take several days to train, instead, you can train these environments within minutes!\nIf you aren‚Äôt happy with the current environments, it is possible to modify and even add more environments. In this post, I will highlight other environments and share how I modified an Acrobot-v1 environment.\n\n\nRLPy\nTo begin, grab the repo for the OpenAI gym. Inside the repo, navigate to gym/envs/classic_controlwhere you will see the scripts that define the class control environments. If you open one of the scripts, you will see a heading on the top that says:\n__copyright__ = \"Copyright 2013, RLPy http://acl.mit.edu/RLPy\"\nAhh! In the spirit of open source, OpenAI stands on the shoulders of another reinforcement library, RLPy. You can learn a lot more about them at the RLPy site or take a look at their github. If you browse here, you can find the original script that was used in OpenAI under rlpy /rlpy/Domains. The interesting thing here is that there are a ton more interesting reinforcement problems!\n\n\n\nRLlisting\n\n\nYou can run these using RLPy or you can try and hack this into OpenAI.\n\n\nModifying OpenAI Environments\nI decided to modify the Acrobot environment. Acrobot is a 2-link pendulum with only the second joint actuated (it has three states, left, right, and no movement). The goal is to swing the end to a height of at least one link above the base. If you look at the leaderboard on OpenAIs site, they meet that criterion, but its not very impressive. Here is the current highest scoring entry:\n\n\n\ntraining\n\n\nThis is way boring compared to what Hardmaru shows in his demo, where a pendulum is capable of balancing for a short time.\nSo I decided to try and modify the Acrobot demo to make this task a little more interesting, Acrobot gist here. The main change was to the reward system. I added a variable steps_beyond_done that would keep track of successes when the end was swung high. I also changed the reward structure, so it would gradually be rewarded as it swung higher. I also changed g to 0, this removes gravity‚Äôs effect.\nself.rewardx = (-np.cos(s[0]) - np.cos(s[1] + s[0])) ##Swung height is calculated \nif self.rewardx &lt; .5:\n    reward = -1.\n    self.steps_beyond_done = 0\nif (self.rewardx &gt; .5 and self.rewardx &lt; .8):\n    reward = -0.8\n    self.steps_beyond_done = 0  \nif self.rewardx &gt; .8:\n    reward = -0.6 \nif self.rewardx &gt; 1:\n    reward = -0.4\n    self.steps_beyond_done += 1 \nif self.steps_beyond_done &gt; 4:\n    reward = -0.2\nif self.steps_beyond_done &gt; 8:\n    reward = -0.1\nif self.steps_beyond_done &gt; 12:\n    reward = 0.\nAnother important file to be aware of is where the benchmarks are kept for each environment. You can navigate to this at gym/gym/benchmarks/__init__.pyWithin this file, you will see the following:\n{'env_id': 'Acrobot-v1',\n         'trials': 3,\n         'max_timesteps': 100000,\n         'reward_floor': -500.0,\n         'reward_ceiling': 0.0,\n        },\nI then ran an implementation of Asynchronous Advantage Actor Critic A3C) by Arno Moonens. After running for a half hour, you can see the improvement in the algorithm:\n\n\n\ntraining1\n\n\nNow a half hour later:\n\n\n\ntraining2\n\n\nThe result is teaching the pendulum to stay up for an extended time! This is much more interesting and what I was looking for. I hope this will inspire others to build new and interesting environments."
  },
  {
    "objectID": "deeplearningR.html",
    "href": "deeplearningR.html",
    "title": "Deep Learning with R",
    "section": "",
    "text": "For R users, there hasn‚Äôt been a production grade solution for deep learning (sorry MXNET). This post introduces the Keras interface for R and how it can be used to perform image classification. The post ends by providing some code snippets that show Keras is intuitive and powerful üí™üèΩ.\n\nTensorflow\nLast January, Tensorflow for R was released, which provided access to the Tensorflow API from R. This was signficant, as Tensorflow is the most popular library for deep learning. However, for most R users, the Tensorflow for R interface was not very R like. ü§¢ Take a look at this code chunk for training a model:\ncross_entropy &lt;- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))\ntrain_step &lt;- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)\ncorrect_prediction &lt;- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))\naccuracy &lt;- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))\nsess$run(tf$global_variables_initializer())\n\nfor (i in 1:20000) {\n  batch &lt;- mnist$train$next_batch(50L)\n  if (i %% 100 == 0) {\n    train_accuracy &lt;- accuracy$eval(feed_dict = dict(\n        x = batch[[1]], y_ = batch[[2]], keep_prob = 1.0))\n    cat(sprintf(\"step %d, training accuracy %g\\n\", i, train_accuracy))\n  }\n  train_step$run(feed_dict = dict(\n    x = batch[[1]], y_ = batch[[2]], keep_prob = 0.5))\n}\n\ntest_accuracy &lt;- accuracy$eval(feed_dict = dict(\n     x = mnist$test$images, y_ = mnist$test$labels, keep_prob = 1.0))\ncat(sprintf(\"test accuracy %g\", test_accuracy))\nYikes!\nUnless you are familiar with tensorflow, it‚Äôs not readily apparent what is going on. A quick search on Github finds less than a 100 code results using tensorflow for R. üòî\n\n\nKeras\nAll this is going to change with Keras and R! ‚ò∫Ô∏è\nFor background, Keras is a high-level neural network API that is designed for experimentation and can run on top of Tensorflow. Keras is what data scientists like to use. ü§ì Keras has grown in popularity and supported on a wide set of platforms including Tensorflow, CNTK, Apple‚Äôs CoreML, and Theano. It is becoming the de factor language for deep learning.\nAs a simple example, here is the code to train a model in Keras:\nmodel_top %&gt;% fit(\n        x = train_x, y = train_y,\n        epochs=epochs, \n        batch_size=batch_size,\n        validation_data=valid)\n\n\nImage Classification with Keras\nSo if you are still with me, let me show you how to build deep learning models using R, Keras, and Tensorflow together. You will find a Github repo at https://github.com/rajshah4/image_keras/ that contains the code and data you will need. Included is an R notebook (and Python notebooks) that walks through building an image classifier (telling üê± from üê∂), but can easily be generalized to other images. The walk through includes advanced methods that are commonly used for production deep learning work including:\n\naugmenting data\nusing the bottleneck features of a pre-trained network\nfine-tuning the top layers of a pre-trained network\nsaving weights of models\n\n\n\nCode Snippets of Keras\nThe R interface to Keras truly makes it easy to build deep learning models in R. Here are some code snippets based on my example of building an image classifier to illustrate how intuitive and useful Keras for R is:\nTo load üñº from a folder:\ntrain_generator &lt;- flow_images_from_directory(train_directory, generator = image_data_generator(), target_size = c(img_width, img_height), color_mode = \"rgb\",\n  class_mode = \"binary\", batch_size = batch_size, shuffle = TRUE,\n  seed = 123)\nTo define a simple convolutional neural network:\nmodel &lt;- keras_model_sequential()\n\nmodel %&gt;%\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, 3)) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% \n  \n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n  \n  layer_conv_2d(filter = 64, kernel_size = c(3,3)) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n  \n  layer_flatten() %&gt;%\n  layer_dense(64) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1) %&gt;%\n  layer_activation(\"sigmoid\")\nTo augment data:\naugment &lt;- image_data_generator(rescale=1./255,\n                               shear_range=0.2,\n                               zoom_range=0.2,\n                               horizontal_flip=TRUE)\nTo load a pretrained network:\nmodel_vgg &lt;- application_vgg16(include_top = FALSE, weights = \"imagenet\")\nTo save model weights:\nsave_model_weights_hdf5(model_ft, 'finetuning_30epochs_vggR.h5', overwrite = TRUE)\nThe Keras for R interface makes it much easier for R users to build and refine deep learning models. Its no longer necessary to force everyone to use Python to build, refine, and test deep learning models. I really think this will open up deep learning to a wider audience that was a bit apprehensive on using python.\nTo start with, you can grab my repo, fire up RStudio (or your IDE of choice), and go build a simple classifier using Keras. There are also a wealth of other examples such as generating text from Nietzsche‚Äôs writings, deep dreaming, or creating a variational encoder.\nSo for now, give it a spin!\nAn earlier version of this post was posted at Datascience+."
  },
  {
    "objectID": "QuickDraw.html",
    "href": "QuickDraw.html",
    "title": "Using Google‚Äôs Quickdraw to create an MNIST style dataset!",
    "section": "",
    "text": "https://www.tensorflow.org/images/MNIST.png\n\n\n\nIntroduction\nFor those running deep learning models, MNIST is ubiquotuous. This dataset of handwritten digits serves many purposes from benchmarking numerous algorithms (its referenced in thousands of papers) and as a visualization, its even more prevelant than Napoleon‚Äôs 1812 March. The digits look like this:\nThere are many reasons for its enduring use, but much of it is the lack of an alternative. In this post, I want to introduce an alternative, the Google QuickDraw dataset. The quickdraw dataset was captured in 2017 by Google‚Äôs drawing game, Quick, Draw!. The dataset consists of 50 million drawings across 345 categories. The drawings look like this:\n\n\n\nhttps://github.com/googlecreativelab/quickdraw-dataset/blob/master/preview.jpg\n\n\n\n\nBuild your own Quickdraw dataset\nI want to walk through how you can use this drawings and create your own MNIST like dataset. Google has made available 28x28 grayscale bitmap files of each drawing. These can serve as drop in replacements for the MNIST 28x28 grayscale bitmap images.\nAs a starting point, Google has graciously made the dataset publicly available with documentation on the dataset. All the data is sitting in Google‚Äôs Cloud Console, but for the images, you want this link of the numpy_bitmaps.\n\n\n\nGCP2\n\n\nYou should arrive on a page that allows you to download all the images for any category. So this is when you have fun! Go ahead and pick your own categories. I started with eyeglasses, face, pencil, and television. As I learned from the face, the drawings that have fine points can be more difficult to learn. But you should play around and pick fun categories.\n\n\n\nshortqd\n\n\nThe next challenge is taking these .npy files and using them. Here is a short python gist that I used to read the .npy files and combine them to create a 80,000 images dataset that I could use in place of MNIST. They are saved in a hdf5 format that is cross platform and often used in deep learning.\n\n\n\ngist\n\n\n\n\nUsing Quickdraw instead of MNIST\nThe next thing is to go have fun with it. I used this dataset in place of MNIST for some work playing around with autoencoders in Python from the Keras tutorials. The below picture represents the original images at the top and reconstructed ones at the bottom, using an autoencoder.\n\n\n\nvae10\n\n\nI next used this dataset with a variational autoencoder in R. Here is the code snippet to import the data:\nlibrary(rhdf5)\nx_test &lt;- t(h5read(\"x_test.h5\", \"name-of-dataset\"))\nx_train &lt;- t(h5read(\"x_train.h5\", \"name-of-dataset\"))\ny_test &lt;- (h5read(\"y_test.h5\", \"name-of-dataset\"))\ny_train &lt;- (h5read(\"y_train.h5\", \"name-of-dataset\"))\nHere is a visualization of its latent space using my custom quickdraw dataset. For me, this was a nice fresh alternative to always staring at the MNIST dataset. So next time you see MNIST listed . . . go build your own!\n\n\n\nVAE15"
  },
  {
    "objectID": "HF-Endpoint.html",
    "href": "HF-Endpoint.html",
    "title": "Text style transfer in a spreadsheet using Hugging Face Inference Endpoints",
    "section": "",
    "text": "SetFit\n\n\n\nIntroduction\nWe change our conversational style from informal to formal speech. We often do this without thinking when talking to our friends compared to addressing a judge. Computers now have this capability! I use textual style transfer in this post to convert informal text to formal text. To make this easy to use, we do it in a spreadsheet.\n\n\nStep 1\nThe first step is identifying an informal to formal text style model. Next, we deploy the model using Hugging Face Inference endpoints. Inference endpoints is a production-grade solution for model deployment.\n\n\n\nStep 2\nLet‚Äôs incorporate the endpoint into Google Sheets custom function to make the model easy to use.\n\nI added the code to Google Sheets through the Apps Script extension. Grab it here as a gist. Once that is saved, you can use the new function as a formula. Now, I can use one simple command if I want to do textual style transfer!\n\n\n\nAlt Text\n\n\n\n\nResources\nI created a Youtube üé• video for a more detailed walkthrough.\nGo try this out with your favorite model! For another example, check out the positive style textual model in a Tik Tok video."
  },
  {
    "objectID": "DADC.html",
    "href": "DADC.html",
    "title": "Dynamic Adversarial Data Collection",
    "section": "",
    "text": "img\n\n\nAre you looking for better training data for your models? Let me tell you about dynamic adversarial data collection!\nI had a large enterprise customer asking me to incorporate this workflow into a Hugging Face private hub demo. Here are some resources I found useful: Chris Emezue put together a blog post: ‚ÄúHow to train your model dynamically using adversarial data‚Äù and a real-life example using MNIST using Spaces.\nIf you want an academic paper that details this process, check out: Analyzing Dynamic Adversarial Training Data in the Limit. By using this approach, this paper found models made 26% fewer errors on the expert-curated test set.\nAnd if you prefer a video‚Ää‚Äî‚Äächeck out my Tik Tok:\nhttps://www.tiktok.com/@rajistics/video/7123667796453592366?is_from_webapp=1&sender_device=pc&web_id=7106277315414181422"
  },
  {
    "objectID": "rag-agentic-world.html",
    "href": "rag-agentic-world.html",
    "title": "From Vectors to Agents: Managing RAG in an Agentic World",
    "section": "",
    "text": "Watch the full video | Slides"
  },
  {
    "objectID": "rag-agentic-world.html#video",
    "href": "rag-agentic-world.html#video",
    "title": "From Vectors to Agents: Managing RAG in an Agentic World",
    "section": "",
    "text": "Watch the full video | Slides"
  },
  {
    "objectID": "rag-agentic-world.html#annotated-presentation",
    "href": "rag-agentic-world.html#annotated-presentation",
    "title": "From Vectors to Agents: Managing RAG in an Agentic World",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the slide-by-slide annotated presentation based on the video ‚ÄúFrom Vectors to Agents: Managing RAG in an Agentic World‚Äù by Rajiv Shah.\n\n\n1. Title Slide\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00)\nThe presentation begins with the title slide, introducing the core theme: ‚ÄúFrom Vectors to Agents: Managing RAG in an Agentic World.‚Äù The speaker, Rajiv Shah from Contextual, sets the stage for a technical deep dive into Retrieval-Augmented Generation (RAG).\nHe outlines the agenda, promising to move beyond basic RAG concepts to focus specifically on retrieval approaches. The talk is designed to cover the spectrum from traditional methods like BM25 and Language Models to the emerging field of Agentic Search.\n\n\n2. ACME GPT\n\n\n\nSlide 2\n\n\n(Timestamp: 00:40)\nThis slide displays a stylized logo for ‚ÄúACME GPT,‚Äù representing the typical enterprise aspiration. Companies see tools like ChatGPT and immediately want to apply that capability to their internal data, asking questions like, ‚ÄúCan I get the list of board of directors?‚Äù\nHowever, the speaker notes a common hurdle: generic models don‚Äôt know enterprise-specific knowledge. This sets up the necessity for RAG‚Äîinjecting private data into the model‚Äîrather than relying solely on the model‚Äôs pre-trained knowledge.\n\n\n3. Building RAG is Easy\n\n\n\nSlide 3\n\n\n(Timestamp: 01:10)\nThe speaker illustrates the deceptively simple workflow of a basic RAG demo. The diagram shows the standard path: a user query is converted to vectors, matched against a database, and sent to an LLM.\nShah acknowledges that building a ‚Äúhello world‚Äù version of this is trivial. He notes, ‚ÄúYou can build a very easy RAG demo out of the box by just grabbing some data, using an embedding model, creating vectors, doing the similarity.‚Äù\n\n\n4. Building RAG is Easy (Code Example)\n\n\n\nSlide 4\n\n\n(Timestamp: 01:22)\nA Python code snippet using LangChain is displayed to reinforce how accessible basic RAG has become. The code demonstrates loading a document, chunking it, and setting up a retrieval chain in just a few lines.\nThis slide serves as a foil for the upcoming reality check. While the code works for a demo, it hides the immense complexity required to make such a system robust, accurate, and scalable in a real-world production environment.\n\n\n5. RAG Reality Check\n\n\n\nSlide 5\n\n\n(Timestamp: 01:35)\nThe tone shifts to the challenges of production. The slide highlights a sobering statistic: 95% of Gen AI projects fail to reach production. The speaker details the specific reasons why demos fail when scaled: poor accuracy, unbearable latency, scaling issues with millions of documents, and ballooning costs.\nHe emphasizes a critical, often overlooked factor: Compliance. ‚ÄúInside an enterprise, not everybody gets to read every document.‚Äù A demo ignores entitlements, but a production system cannot.\n\n\n6. Maybe try a different RAG?\n\n\n\nSlide 6\n\n\n(Timestamp: 03:00)\nThis slide lists a dizzying array of RAG variants (GraphRAG, RAPTOR, CRAG, etc.) and retrieval techniques. It represents the ‚Äúanalysis paralysis‚Äù developers face when scouring arXiv papers for a solution to their accuracy problems.\nShah warns against blindly chasing the latest academic paper to fix fundamental system issues. ‚ÄúThe answer is not in here of pulling together like a bunch of archive papers.‚Äù Instead, he advocates for a structured framework to make decisions.\n\n\n7. Ultimate RAG Solution\n\n\n\nSlide 7\n\n\n(Timestamp: 03:30)\nA humorous cartoon depicts a ‚ÄúRube Goldberg‚Äù machine, representing the ‚ÄúUltimate RAG Solution.‚Äù It mocks the tendency to over-engineer systems with too many interconnected, fragile components in the pursuit of performance.\nThe speaker uses this visual to argue for simplicity and deliberate design. The goal is to avoid building a monstrosity that is impossible to maintain, urging the audience to think about trade-offs before complexity.\n\n\n8. RAG as a system\n\n\n\nSlide 8\n\n\n(Timestamp: 03:35)\nThe speaker introduces a clean system architecture for RAG, broken into four distinct stages: Parsing, Querying, Retrieving, and Generation. This framework serves as the mental map for the rest of the presentation.\nHe highlights that ‚ÄúParsing‚Äù is vastly overlooked‚Äîgetting information out of complex documents cleanly is a prerequisite for success. Today‚Äôs talk, however, will zoom in specifically on the Retrieving and Querying components.\n\n\n9. Designing a RAG Solution\n\n\n\nSlide 9\n\n\n(Timestamp: 04:10)\nThis slide presents a ‚ÄúTradeoff Triangle‚Äù for RAG, balancing Problem Complexity, Latency, and Cost. The speaker advises having a serious conversation with stakeholders about these constraints before writing code.\nA key concept introduced here is the ‚ÄúCost of a mistake.‚Äù In coding assistants, a mistake is low-cost (the developer fixes it). In medical RAG systems, the cost of a mistake is high (life or death), which dictates a completely different architectural approach.\n\n\n10. RAG Considerations\n\n\n\nSlide 10\n\n\n(Timestamp: 05:30)\nA detailed table breaks down specific considerations that influence RAG design, such as domain difficulty, multilingual requirements, and data quality. This slide was originally created for sales teams to help scope customer problems.\nShah emphasizes that understanding the nuances of the use case upfront saves heartache later. For instance, knowing if users will ask simple questions or require complex reasoning changes the retrieval strategy entirely.\n\n\n11. Consider Query Complexity\n\n\n\nSlide 11\n\n\n(Timestamp: 06:15)\nThe speaker categorizes queries by complexity, ranging from simple Keywords (‚ÄúTotal Revenue‚Äù) to Semantic variations (‚ÄúHow much bank?‚Äù), to Multi-hop reasoning, and finally Agentic scenarios.\nHe points out a common failure mode: ‚ÄúThe answers aren‚Äôt in the documents‚Ä¶ all of a sudden they‚Äôre asking for knowledge that‚Äôs outside.‚Äù Recognizing the query complexity determines whether you need a simple search engine or a complex agentic workflow.\n\n\n12. Retrieval (Highlighted)\n\n\n\nSlide 12\n\n\n(Timestamp: 07:32)\nThe presentation zooms back into the system diagram, highlighting the ‚ÄúRetrieving‚Äù box. This signals the start of the deep technical dive into retrieval algorithms.\nShah notes that this area causes the most confusion due to the sheer number of model choices and architectures available. He aims to provide a practical guide to selecting the right retrieval tool.\n\n\n13. Retrieval Approaches\n\n\n\nSlide 13\n\n\n(Timestamp: 08:16)\nThree primary retrieval pillars are introduced: 1. BM25: The lexical, keyword-based standard. 2. Language Models: Semantic embeddings and vector search. 3. Agentic Search: The new frontier of iterative reasoning.\nThe speaker emphasizes that documents must be broken into pieces (chunking) because no single model context window is efficient enough to hold all enterprise data for every query.\n\n\n14. Building RAG is Easy (Code Highlight)\n\n\n\nSlide 14\n\n\n(Timestamp: 08:50)\nReturning to the initial code snippet, the speaker highlights the vectorstore and retriever initialization lines. This pinpoints exactly where the upcoming concepts fit into the implementation.\nThis visual anchor helps developers map the theoretical concepts of BM25 and Embeddings back to the actual lines of code they write in libraries like LangChain or LlamaIndex.\n\n\n15. BM25\n\n\n\nSlide 15\n\n\n(Timestamp: 09:18)\nBM25 (Best Match 25) is explained as a probabilistic lexical ranking function. The slide visualizes an inverted index, mapping words (like ‚Äúbutterfly‚Äù) to the specific documents containing them.\nShah explains that this is the 25th iteration of the formula, designed to score documents based on word frequency and saturation. It remains a powerful, fast baseline for retrieval.\n\n\n16. BM25 Performance\n\n\n\nSlide 16\n\n\n(Timestamp: 09:55)\nA table compares the speed of a Linear Scan (Ctrl+F style) versus an Inverted Index (BM25) as the document count grows from 1,000 to 9,000.\nThe data shows that linear search becomes exponentially slower (taking 3,000 seconds for 1k documents in this synthetic test), while BM25 remains orders of magnitude faster. This efficiency is why lexical search is still widely used in production.\n\n\n17. BM25 Failure Cases\n\n\n\nSlide 17\n\n\n(Timestamp: 11:08)\nThe limitations of BM25 are exposed. Because it relies on exact word matches, it fails when users use synonyms. If a user searches for ‚ÄúPhysician‚Äù but the documents only contain ‚ÄúDoctor,‚Äù BM25 will return zero results.\nSimilarly, it struggles with acronyms like ‚ÄúIBM‚Äù vs ‚ÄúInternational Business Machines.‚Äù Despite this, Shah argues BM25 is a ‚Äúvery strong baseline‚Äù that often beats complex neural models on specific keyword-heavy datasets.\n\n\n18. Hands on: BM25s\n\n\n\nSlide 18\n\n\n(Timestamp: 12:14)\nFor developers wanting to implement this, the slide points to a library called bm25s, a high-performance Python implementation available on Hugging Face.\nThis reinforces the practical nature of the talk‚ÄîBM25 isn‚Äôt just a legacy concept; it is an active, installable tool that developers should consider using alongside vector search.\n\n\n19. Enter Language Models\n\n\n\nSlide 19\n\n\n(Timestamp: 12:24)\nThe talk transitions to Language Models (Embeddings). The slide explains how an encoder model turns text into a dense vector (a list of numbers) that captures semantic meaning.\nBecause these models are trained on vast amounts of data, they ‚Äúhave an idea of these similar concepts.‚Äù This solves the synonym problem that plagues BM25.\n\n\n20. Embeddings Visualized\n\n\n\nSlide 20\n\n\n(Timestamp: 12:50)\nA 2D visualization demonstrates how embeddings group related concepts in latent space. The word ‚ÄúDoctor‚Äù and ‚ÄúPhysician‚Äù would be located very close to each other mathematically.\nThis spatial proximity allows for Semantic Search: finding documents that mean the same thing as the query, even if they don‚Äôt share a single word.\n\n\n21. Semantic search is widely used\n\n\n\nSlide 21\n\n\n(Timestamp: 13:15)\nThe speaker validates the importance of semantic search by showing a tweet from Google‚Äôs SearchLiaison regarding BERT, and a screenshot of Hugging Face‚Äôs model repository.\nThis confirms that semantic search is the industry standard for modern information retrieval, having been deployed at massive scale by tech giants to improve result relevance.\n\n\n22. Which language model?\n\n\n\nSlide 22\n\n\n(Timestamp: 13:30)\nA scatter plot compares various models based on Inference Speed (X-axis) and NDCG@10 (Y-axis, a measure of retrieval quality).\nShah places BM25 on the right (fast but lower accuracy) to orient the audience. He points out that there is a massive variety of models with different trade-offs between compute cost and retrieval quality.\n\n\n23. Static Embeddings\n\n\n\nSlide 23\n\n\n(Timestamp: 14:43)\nThe speaker introduces Static Embeddings (like Word2Vec or GloVe) which are located on the far right of the previous scatter plot‚Äîextremely fast, even on CPUs.\nThese models assign a fixed vector to every word. While efficient, they lack context. The word ‚Äúbank‚Äù has the same vector whether referring to a river bank or a financial bank, which limits their accuracy.\n\n\n24. Why Context Matters\n\n\n\nSlide 24\n\n\n(Timestamp: 15:16)\nA cartoon illustrates the difference between Static Embeddings and Transformers. The Transformer can distinguish between ‚ÄúModel‚Äù in a data science context versus ‚ÄúModel‚Äù in a fashion context.\nThis contextual awareness is why modern Transformer-based embeddings (like BERT) generally outperform static embeddings and BM25 in complex retrieval tasks, despite being slower.\n\n\n25. Many more models!\n\n\n\nSlide 25\n\n\n(Timestamp: 15:55)\nReturning to the scatter plot, a red arrow points toward the top-left quadrant‚Äîmodels that are slower but achieve higher accuracy.\nThe speaker notes that the field is constantly evolving, with ‚Äúnewer generations of models‚Äù pushing the boundary of what is possible in terms of retrieval quality.\n\n\n26. MTEB/RTEB\n\n\n\nSlide 26\n\n\n(Timestamp: 16:35)\nTo help developers choose, Shah introduces the MTEB (Massive Text Embedding Benchmark) and RTEB (Retrieval Text Embedding Benchmark). These are leaderboards hosted on Hugging Face.\nHe highlights a key distinction: MTEB uses public datasets, while RTEB uses private, held-out datasets. This is crucial for avoiding ‚Äúdata contamination,‚Äù where models perform well simply because they were trained on the test data.\n\n\n27. Selecting an embedding model\n\n\n\nSlide 27\n\n\n(Timestamp: 16:48)\nThe speaker switches to a live browser view (captured in the slide) of the leaderboard. He discusses the bubble chart visualization where size often correlates with parameter count.\nHe points out an interesting trend: ‚ÄúYou‚Äôll see that there‚Äôs a bunch of models here that are all the same size‚Ä¶ but the performance differs.‚Äù This indicates improvements in training strategies and architecture rather than just throwing more compute at the problem.\n\n\n28. Selecting an embedding model (Other Considerations)\n\n\n\nSlide 28\n\n\n(Timestamp: 19:07)\nBeyond the leaderboard score, Shah lists practical selection criteria: Model Size (can it fit in memory?), Architecture (CPU vs GPU), Embedding Dimension (storage costs), and Training Data (multilingual support).\nHe advises checking if a model is open source and quantizable, as this can significantly reduce latency without a major hit to accuracy.\n\n\n29. Matryoshka Embedding Models\n\n\n\nSlide 29\n\n\n(Timestamp: 20:53)\nA specific innovation is highlighted: Matryoshka Embeddings. These models allow developers to truncate vectors (e.g., from 768 dimensions down to 64) while retaining most of the performance.\nThis is a ‚Äúneat kind of innovation‚Äù for optimizing storage and search speed. OpenAI‚Äôs newer models also support this feature, offering flexibility between cost and accuracy.\n\n\n30. Sentence Transformer\n\n\n\nSlide 30\n\n\n(Timestamp: 21:42)\nThe Sentence Transformer architecture is described as the dominant approach for RAG. Unlike standard BERT which works on tokens, these are fine-tuned to understand full sentences and paragraphs.\nThis architecture uses Siamese networks to ensure that semantically similar sentences are close in vector space, making them ideal for the ‚Äúchunk-level‚Äù retrieval required in RAG.\n\n\n31. Cross Encoder / Reranker\n\n\n\nSlide 31\n\n\n(Timestamp: 22:16)\nThe concept of a Cross Encoder (or Reranker) is introduced. Unlike the bi-encoder (retriever) which processes query and document separately, the cross-encoder processes them together.\nThis allows for a much deeper calculation of relevance. It is typically used as a second stage: retrieve 50 documents quickly with vectors, then use the slow but accurate Cross Encoder to rank the top 5.\n\n\n32. Cross Encoder / Reranker (Duplicate)\n\n\n\nSlide 32\n\n\n(Timestamp: 22:16)\n(This slide reinforces the previous diagram, emphasizing the ‚Äúcrossing‚Äù of the query and document in the model architecture.)\n\n\n33. Cross Encoder / Reranker (Accuracy Boost)\n\n\n\nSlide 33\n\n\n(Timestamp: 23:07)\nA bar chart quantifies the value of reranking. It shows a significant boost in NDCG (accuracy) when a reranker is added to the pipeline.\nThe speaker notes that while you get a ‚Äúbump‚Äù in quality, it ‚Äúdoesn‚Äôt come for free.‚Äù The trade-off is increased latency, as the cross-encoder is computationally expensive.\n\n\n34. Cross Encoder / Reranker (Execution Flow)\n\n\n\nSlide 34\n\n\n(Timestamp: 23:15)\nThe execution flow diagram highlights the reranker‚Äôs position in the pipeline. It sits between the Vector Store retrieval and the LLM generation.\nThis visual reinforces the latency implication: the user has to wait for both the initial search and the reranking pass before the LLM even starts generating an answer.\n\n\n35. Hands On: Retriever & Reranker\n\n\n\nSlide 35\n\n\n(Timestamp: 23:30)\nA screenshot of a Google Colab notebook is shown, demonstrating a practical implementation of the Retrieve and Re-rank strategy using the SentenceTransformer and CrossEncoder libraries.\nThis provides a concrete resource for the audience to test the accuracy vs.¬†speed trade-offs themselves on simple datasets like Wikipedia.\n\n\n36. Instruction Following Reranker\n\n\n\nSlide 36\n\n\n(Timestamp: 23:48)\nShah mentions a specific advancement: Instruction Following Rerankers (developed by his company, Contextual). These allow developers to pass a prompt to the reranker, such as ‚ÄúPrioritize safety notices.‚Äù\nThis adds a ‚Äúknob‚Äù for developers to tune retrieval based on business logic without retraining the model.\n\n\n37. Combine Multiple Retrievers\n\n\n\nSlide 37\n\n\n(Timestamp: 24:19)\nThe presentation suggests that you don‚Äôt have to pick just one method. You can combine BM25, various embedding models (E5, BGE), and rerankers.\nWhile combining them (Ensemble Retrieval) often yields better recall, Shah warns that ‚Äúyou got to engineer this.‚Äù Managing multiple indexes and fusion logic increases operational complexity and compute costs.\n\n\n38. Cascading Rerankers in Kaggle\n\n\n\nSlide 38\n\n\n(Timestamp: 24:56)\nA complex diagram from a Kaggle competition winner illustrates a Cascade Strategy. The solution used three different rerankers, filtering from 64 documents down to 8, and then to 5.\nThis shows the extreme end of retrieval engineering, where multiple models are chained to squeeze out every percentage point of accuracy.\n\n\n39. Best practices\n\n\n\nSlide 39\n\n\n(Timestamp: 25:16)\nShah distills the complexity into a recommended Best Practice: 1. Hybrid Search: Combine Semantic Search (Vectors) and Lexical Search (BM25). 2. Reciprocal Rank Fusion: Merge the results. 3. Reranker: Pass the top results through a cross-encoder.\nThis setup provides a ‚Äúpretty good standard performance out of the box‚Äù and should be the default baseline before trying exotic methods.\n\n\n40. Families of Embedding Models\n\n\n\nSlide 40\n\n\n(Timestamp: 25:42)\nA taxonomy slide categorizes the models discussed: Static (Fastest/Low Accuracy), Bi-Encoders (Fast/Good Accuracy), and Cross-Encoders (Slow/Best Accuracy).\nThis summary helps the audience mentally organize the tools available in their toolbox.\n\n\n41. Lots of New Models\n\n\n\nSlide 41\n\n\n(Timestamp: 25:50)\nLogos for IBM Granite, Google EmbeddingGemma, and others appear. The speaker notes that while new models from major players appear weekly, the improvements are often ‚Äúincremental.‚Äù\nHe advises against ‚Äúripping up‚Äù a working system just to switch to a model that is 1% better on a leaderboard.\n\n\n42. Other retrieval methods\n\n\n\nSlide 42\n\n\n(Timestamp: 26:18)\nAlternative methods are briefly listed: SPLADE (Sparse retrieval), ColBERT (Late interaction), and GraphRAG.\nShah acknowledges these exist and may fit specific niches, but warns against chasing the ‚Äúflavor of the week‚Äù before establishing a solid baseline with hybrid search.\n\n\n43. Operational Concerns\n\n\n\nSlide 43\n\n\n(Timestamp: 27:30)\nThe talk shifts to operations. Libraries like FAISS are mentioned for efficient vector similarity search.\nA key point is that for many use cases, you can simply store embeddings in memory. You don‚Äôt always need a complex vector database if your dataset fits in RAM.\n\n\n44. Vector Database Options\n\n\n\nSlide 44\n\n\n(Timestamp: 27:55)\nA diagram categorizes storage into Hot (In-Memory), Warm (SSD/Disk), and Cold tiers.\nShah notes there are ‚Äútons of vector database options‚Äù (Snowflake, Pinecone, etc.). The choice should be governed by latency requirements. If you need sub-millisecond retrieval, you need in-memory storage.\n\n\n45. Operational Concerns (Datastore Size)\n\n\n\nSlide 45\n\n\n(Timestamp: 28:40)\nA graph shows that as Datastore Size increases (X-axis), retrieval performance naturally degrades (Y-axis).\nTo combat this, the speaker strongly recommends using Metadata Filtering. ‚ÄúIf you‚Äôre not using something like metadata‚Ä¶ it‚Äôs going to be very tough.‚Äù Narrowing the search scope is essential for scaling to millions of documents.\n\n\n46. Search Strategy Comparison\n\n\n\nSlide 46\n\n\n(Timestamp: 29:22)\nThe presentation pivots to the ‚Äúexciting part‚Äù: Agentic RAG. A visual compares ‚ÄúTraditional RAG‚Äù (a linear path) with ‚ÄúAgentic RAG‚Äù (a winding, exploratory path).\nThis represents the shift from a ‚Äúone-shot‚Äù retrieval attempt to an iterative system that can explore, backtrack, and reason.\n\n\n47. Tools use / Reasoning\n\n\n\nSlide 47\n\n\n(Timestamp: 29:40)\nReasoning models (like o1 or DeepSeek R1) enable LLMs to use tools effectively. A code snippet shows an agent loop: query -&gt; generate -&gt; ‚ÄúDid it answer the question?‚Äù\nIf the answer is no, the model can ‚Äúrewrite the query‚Ä¶ try to find that missing information, feed that back into the loop.‚Äù This self-correction is the core of Agentic RAG.\n\n\n48. Agentic RAG (Workflow)\n\n\n\nSlide 48\n\n\n(Timestamp: 30:32)\nA flowchart details the Agentic RAG lifecycle. The model thinks through steps: ‚ÄúOh, this is the query I need to make‚Ä¶ based on those results‚Ä¶ maybe we should do it a different way.‚Äù\nThis workflow allows the system to synthesize answers from multiple sources or clarify ambiguous queries automatically.\n\n\n49. Tools use / Reasoning (Detailed Example)\n\n\n\nSlide 49\n\n\n(Timestamp: 30:35)\nA specific example of a complex query is shown. The agent breaks the problem down, calls tools, and iterates.\nThis demonstrates that the ‚ÄúThinking‚Äù time is where the value is generated, allowing for a depth of research that a single retrieval pass cannot match.\n\n\n50. Open Deep Research\n\n\n\nSlide 50\n\n\n(Timestamp: 31:02)\nShah references ‚ÄúOpen Deep Research‚Äù by LangChain, an open-source framework where sub-agents go out, perform research, and report back.\nThis is a specific category of Agentic RAG focused on generating comprehensive reports rather than quick answers.\n\n\n51. DeepResearch Bench\n\n\n\nSlide 51\n\n\n(Timestamp: 31:30)\nA leaderboard for DeepResearch Bench is shown, testing models on ‚Äú100 PhD level research tasks.‚Äù\nThe speaker warns that this approach ‚Äúcan get very expensive.‚Äù Solving a single complex query might cost significant money due to the number of tokens and iterative steps required.\n\n\n52. Westlaw AI Deep Research\n\n\n\nSlide 52\n\n\n(Timestamp: 31:55)\nA real-world application is highlighted: Westlaw AI. In the legal field, thoroughness is worth the latency and cost.\nThis proves that Agentic RAG isn‚Äôt just a toy; it is being commercialized in high-value verticals where accuracy is paramount.\n\n\n53. Agentic RAG (Self-RAG)\n\n\n\nSlide 53\n\n\n(Timestamp: 32:11)\nThe concept of Self-RAG is introduced, emphasizing the ‚ÄúReflection‚Äù step. The model critiques its own retrieved documents and generation quality.\nShah notes that this isn‚Äôt brand new, but has become practical due to better reasoning models.\n\n\n54. Agentic RAG (LangChain Reddit)\n\n\n\nSlide 54\n\n\n(Timestamp: 34:04)\nA Reddit post is shown where a developer discusses building a self-reflection RAG system. This highlights the community‚Äôs active experimentation with these loops.\n\n\n55. Agentic RAG (Efficiency Concerns)\n\n\n\nSlide 55\n\n\n(Timestamp: 34:15)\nThe discussion turns to the ‚ÄúRub‚Äù: Inefficiency. Agentic loops can be slow and wasteful, re-retrieving data unnecessarily.\nThis sets up the trade-off conversation again: Is the extra time and compute worth the accuracy gain?\n\n\n56. Research: BRIGHT\n\n\n\nSlide 56\n\n\n(Timestamp: 32:11)\nNote: The speaker introduces the BRIGHT benchmark around 32:11, slightly out of slide order in the transcript flow, but connects it here.\nBRIGHT is a benchmark specifically designed for Retrieval Reasoning. Unlike standard benchmarks that test keyword matching, BRIGHT tests questions that require thinking, logic, and multi-step deduction to find the correct document.\n\n\n57. BRIGHT #1: DIVER\n\n\n\nSlide 57\n\n\n(Timestamp: 32:48)\nThe top-performing system on BRIGHT is DIVER. The diagram shows it uses the exact components discussed earlier: Chunking, Retrieving, and Reranking, but wrapped in an iterative loop.\nShah points out, ‚ÄúIt probably doesn‚Äôt look that crazy to you if you‚Äôre used to RAG.‚Äù The innovation is in the process, not necessarily a magical new model architecture.\n\n\n58. BRIGHT #1: DIVER (LLM Instructions)\n\n\n\nSlide 58\n\n\n(Timestamp: 33:31)\nThe specific prompts used in DIVER are shown. The system asks the LLM: ‚ÄúGiven a query‚Ä¶ what do you think would be possibly helpful to do?‚Äù\nThis Query Expansion allows the system to generate new search terms that the user didn‚Äôt think of, bridging the semantic gap through reasoning.\n\n\n59. Agentic RAG on WixQA\n\n\n\nSlide 59\n\n\n(Timestamp: 34:36)\nShah shares his own experiment results on the WixQA dataset (technical support). * One Shot RAG: 5 seconds latency, 76% Factuality. * Agentic RAG: Slower latency, 93% Factuality.\nThis massive jump in accuracy (0.76 to 0.93) is the key takeaway. ‚ÄúThat has a ton of implications.‚Äù It suggests that the limitation of RAG often isn‚Äôt the data, but the lack of reasoning applied to the retrieval process.\n\n\n60. Rethink your Assumptions\n\n\n\nSlide 60\n\n\n(Timestamp: 37:10)\nThis is the climax of the technical argument. A graph from the BRIGHT paper shows that BM25 (lexical search) combined with an Agentic loop (GPT-4) outperforms advanced embedding models (Qwen).\n‚ÄúThis is crazy,‚Äù Shah exclaims. Because the LLM can rewrite queries into many variations, it mitigates BM25‚Äôs weakness (synonyms). This implies you might not need complex vector databases if you have a smart agent.\n\n\n61. Agentic RAG with BM25\n\n\n\nSlide 61\n\n\n(Timestamp: 38:20)\nShah validates the paper‚Äôs finding with his own internal data (Financial 10Ks). Agentic RAG with BM25 performed nearly as well as Agentic RAG with Embeddings.\nHe suggests a radical possibility: ‚ÄúI could throw all that away [vector DBs]‚Ä¶ just stick this in a text-only database and use BM25.‚Äù\n\n\n62. Agentic RAG for Code Search\n\n\n\nSlide 62\n\n\n(Timestamp: 39:46)\nHe connects this finding to Claude Code, which uses a lexical approach (like grep) rather than vectors for code search.\nSince code doesn‚Äôt have the same semantic ambiguity as natural language, and agents can iterate rapidly, lexical search is proving to be superior for coding assistants.\n\n\n63. Combine Retrieval Approaches\n\n\n\nSlide 63\n\n\n(Timestamp: 40:15)\nA DoorDash case study illustrates a two-tier guardrail system. They use simple text similarity first (fast/cheap). If that fails or is uncertain, they kick it to an LLM (slow/expensive).\nThis ‚ÄúTiered‚Äù approach optimizes the trade-off between cost and accuracy in production.\n\n\n64. Hands on: Agentic RAG (Smolagents)\n\n\n\nSlide 64\n\n\n(Timestamp: 41:07)\nThe speaker points to Smolagents, a Hugging Face library, as a way to get hands-on with these concepts. A Colab notebook is provided for the audience to build their own agentic retrieval loops.\n\n\n65. Solutions for a RAG Solution\n\n\n\nSlide 65\n\n\n(Timestamp: 41:18)\nShah updates the ‚ÄúProblem Complexity‚Äù framework from the beginning of the talk with specific recommendations: * Low Latency (&lt;5s): Use BM25 or Static Embeddings. * High Cost of Mistake: Add a Reranker. * Complex Multi-hop: Use Agentic RAG.\n\n\n66. Retriever Checklist\n\n\n\nSlide 66\n\n\n(Timestamp: 41:52)\nA final checklist summarizes the retrieval hierarchy: 1. Keyword/BM25 (The baseline). 2. Semantic Search (The standard). 3. Agentic/Reasoning (The problem solver).\nThis provides the audience with a mental menu to choose from based on their specific constraints.\n\n\n67. RAG as a system (Retrieval with Instruction Following Reranker)\n\n\n\nSlide 67\n\n\n(Timestamp: 42:00)\nThe system diagram is shown one last time, updated to include the Instruction Following Reranker in the retrieval box, solidifying the modern RAG architecture.\n\n\n68. RAG - Generation\n\n\n\nSlide 68\n\n\n(Timestamp: 42:10)\nNote: The speaker concludes the talk at 42:10, stating ‚ÄúI‚Äôm going to end it here.‚Äù Slides 68-70 regarding the Generation stage were included in the deck but skipped in the video recording due to time constraints.\nThis slide would have covered the final stage of RAG: generating the answer. The focus here is typically on reducing hallucinations and ensuring the tone matches the user‚Äôs needs.\n\n\n69. RAG - Generation (Model Selection)\n\n\n\nSlide 69\n\n\n(Timestamp: 42:10)\nSkipped in video. This slide illustrates the choice of LLM for generation (e.g., GPT-4 vs Llama 3 vs Claude). The choice depends on the ‚ÄúCost/Latency budget‚Äù and specific domain requirements.\n\n\n70. Chunking approaches\n\n\n\nSlide 70\n\n\n(Timestamp: 42:10)\nSkipped in video. This slide compares Original Chunking (cutting text at fixed intervals) with Contextual Chunking (adding a summary prefix to every chunk). Contextual chunking significantly improves retrieval because every chunk carries the context of the parent document.\n\n\n71. Title Slide (Duplicate)\n\n\n\nSlide 71\n\n\n(Timestamp: 42:10)\nThe presentation concludes with the title slide. Rajiv Shah thanks the audience, encouraging them to think about trade-offs rather than just chasing the latest models. ‚ÄúHopefully I‚Äôve given you a sense of thinking about these trade-offs‚Ä¶ thank you all.‚Äù\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "genai-evaluation-guide.html",
    "href": "genai-evaluation-guide.html",
    "title": "A Practical Guide to Evaluating Generative AI Applications",
    "section": "",
    "text": "Watch the full video | Slides"
  },
  {
    "objectID": "genai-evaluation-guide.html#video",
    "href": "genai-evaluation-guide.html#video",
    "title": "A Practical Guide to Evaluating Generative AI Applications",
    "section": "",
    "text": "Watch the full video | Slides"
  },
  {
    "objectID": "genai-evaluation-guide.html#annotated-presentation",
    "href": "genai-evaluation-guide.html#annotated-presentation",
    "title": "A Practical Guide to Evaluating Generative AI Applications",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the annotated presentation for Rajiv Shah‚Äôs workshop on ‚ÄúHill Climbing: Best Practices for Evaluating LLMs.‚Äù\n\n1. Title Slide\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00)\nThis slide introduces the workshop titled ‚ÄúHill Climbing: Best Practices for Evaluating LLMs,‚Äù presented by Rajiv Shah, PhD, at the Open Data Science Conference (ODSC). The presentation focuses on the technical nuances of Generative AI and how to build effective evaluation workflows.\nRajiv sets the stage by outlining his three main goals for the session: understanding the technical differences in GenAI evaluation, learning a basic introductory workflow for building evaluation datasets, and inspiring practitioners to start ‚Äúlearning by doing‚Äù rather than just reading papers.\nThe concept of ‚ÄúHill Climbing‚Äù refers to the iterative process of improving LLM applications‚Äîstarting with a baseline and continuously optimizing performance through rigorous testing and error analysis.\n\n\n2. Evaluating for Gen AI Resources\n\n\n\nSlide 2\n\n\n(Timestamp: 00:06)\nThis slide provides a QR code and a GitHub URL, directing the audience to the code and resources associated with the talk. It emphasizes that the workshop is practical, with code examples available for attendees to replicate the evaluation techniques discussed.\nRajiv encourages the audience to access these resources to follow along with the technical implementations of the concepts, such as building LLM judges and creating unit tests, which will be covered later in the presentation.\n\n\n3. Customer Support Use Case\n\n\n\nSlide 3\n\n\n(Timestamp: 00:48)\nTo motivate the need for evaluation, the presentation introduces a common real-world use case: Customer Support. Generative AI is frequently deployed to help agents compose emails or chat responses based on user inquiries.\nThis scenario serves as the baseline example throughout the talk. It represents a high-volume task where automation is desirable, but accuracy and tone are critical for maintaining customer satisfaction and brand reputation.\n\n\n4. Vibe Coding\n\n\n\nSlide 4\n\n\n(Timestamp: 00:59)\nThis slide introduces the concept of ‚ÄúVibe Coding‚Äù‚Äîthe initial phase where developers grab a simple prompt, feed it to a model, and get a result that feels right. It highlights the misconception that GenAI is easy because it works ‚Äúout of the box‚Äù for simple demos.\nRajiv notes that while ‚Äúvibe coding‚Äù might work for a quick demo app, it is insufficient for production systems. Relying on a ‚Äúvibe‚Äù that the model is working prevents teams from catching subtle failures that occur at scale.\n\n\n5. Good Response: Delayed Order\n\n\n\nSlide 5\n\n\n(Timestamp: 01:10)\nHere, we see a successful output generated by the LLM. The customer inquired about a delayed order, and the AI generated a polite, relevant response acknowledging the delay and apologizing.\nThis example reinforces the ‚ÄúVibe Coding‚Äù trap: because the model often produces high-quality, human-sounding text like this, developers can be lulled into a false sense of security regarding the system‚Äôs reliability.\n\n\n6. Good Response: Damaged Product\n\n\n\nSlide 6\n\n\n(Timestamp: 01:12)\nThis slide provides another example of a ‚Äúgood‚Äù response. The AI correctly identifies that the customer received a damaged product and initiates a replacement protocol.\nThese positive examples establish a baseline of expected behavior. The challenge in evaluation is not just confirming that the model can work, but ensuring it works consistently across all edge cases.\n\n\n7. Bad Response: Irrelevance\n\n\n\nSlide 7\n\n\n(Timestamp: 01:26)\nThe presentation shifts to failure modes. In this example, the user asks about an ‚ÄúOrder Delay,‚Äù but the AI responds with information about a ‚ÄúNew Product Launch.‚Äù\nThis illustrates a complete context mismatch. The model failed to attend to the user‚Äôs intent, generating a coherent but completely irrelevant response. This type of failure frustrates users and degrades trust in the automated system.\n\n\n8. Bad Response: Hallucination\n\n\n\nSlide 8\n\n\n(Timestamp: 01:36)\nThis slide shows a more dangerous failure: Hallucination. The AI apologizes for a defective ‚Äúespresso machine,‚Äù but as the speaker notes, ‚ÄúWe don‚Äôt actually sell espresso machines.‚Äù\nThis highlights the risk of the model fabricating facts to be helpful. Such errors can lead to logistical nightmares, such as customers expecting replacements for products that do not exist or that the company never sold.\n\n\n9. Risks of LLM Mistakes\n\n\n\nSlide 9\n\n\n(Timestamp: 01:51)\nRajiv categorizes the risks associated with LLM failures into three buckets: Reputational, Legal, and Financial. He cites the example of Cursor, an IDE company, where a support bot hallucinated a policy restricting users to one device, causing customers to cancel subscriptions.\nThe slide emphasizes that courts may view AI agents as employees; if a bot makes a promise (like a refund or policy change), the company might be legally bound to honor it. This escalates evaluation from a technical nice-to-have to a business necessity.\n\n\n10. The Despair of Gen AI\n\n\n\nSlide 10\n\n\n(Timestamp: 02:38)\nThis visual represents the frustration developers feel when moving from a successful demo to a failing production system. The ‚Äúdespair‚Äù comes from the realization that the stochastic nature of LLMs makes them difficult to control.\nIt serves as an emotional anchor for the audience, acknowledging that while GenAI is exciting, the unpredictability of its failures causes significant stress for engineering teams responsible for deployment.\n\n\n11. High Failure Rates\n\n\n\nSlide 11\n\n\n(Timestamp: 02:48)\nThe slide cites an MIT report stating that ‚Äú95% of GenAI pilots are failing.‚Äù While Rajiv notes this number might be overstated, it reflects a trend where executives are demanding ROI and seeing lackluster results.\nThis shift in 2025 means that evaluation is no longer just for debugging; it is required to prove business value and justify the high costs of running Generative AI infrastructure.\n\n\n12. Evaluation Improves Applications\n\n\n\nSlide 12\n\n\n(Timestamp: 03:14)\nThis slide asserts the core thesis: Evaluation helps you build better GenAI applications. It references a previous viral video by the speaker on the same topic, positioning this talk as an updated, condensed version with fresh content.\nRajiv explains that you cannot improve what you cannot measure. Without a robust evaluation framework, developers are essentially guessing whether changes to prompts or models are actually improving performance.\n\n\n13. Why Evaluation is Necessary\n\n\n\nSlide 13\n\n\n(Timestamp: 03:40)\nThis concentric diagram illustrates the stakeholders involved in evaluation. It starts with ‚ÄúThings Go Wrong‚Äù (technical reality), moves to ‚ÄúBuy-in‚Äù (convincing managers/teams), and ends with ‚ÄúRegulators‚Äù (external compliance).\nEvaluation serves multiple audiences: it helps the developer debug, it provides the metrics needed to convince management that the app is production-ready, and it creates the audit trails required by third-party auditors or regulators.\n\n\n14. Evaluation Dimensions\n\n\n\nSlide 14\n\n\n(Timestamp: 04:18)\nEvaluation must cover three dimensions: Technical (F1 scores, accuracy), Business (ROI, value generated), and Operational (Total Cost of Ownership, latency).\nRajiv highlights that data scientists often focus solely on the technical, but ignoring operational costs (like the expense of hosting GPUs vs.¬†using APIs) can kill a project. A comprehensive evaluation strategy considers the cost-to-quality ratio.\n\n\n15. Public Benchmarks\n\n\n\nSlide 15\n\n\n(Timestamp: 05:06)\nThe slide discusses Public Benchmarks (like MMLU, GSM8K). While useful for a general idea of a model‚Äôs capabilities (e.g., ‚ÄúIs Llama 3 better than Llama 2?‚Äù), they are insufficient for specific applications.\nRajiv warns against using these benchmarks to determine if a model fits your specific use case. Companies promote these numbers for marketing, but they rarely reflect performance on proprietary business data.\n\n\n16. Custom Benchmarks\n\n\n\nSlide 16\n\n\n(Timestamp: 05:22)\nThe solution to the limitations of public benchmarks is Custom Benchmarks. This slide defines a benchmark as a combination of a Task, a Dataset, and an Evaluation Metric.\nThis is a critical definition for the workshop. To ‚Äútame‚Äù GenAI, you must build a dataset that reflects your specific customer queries and define success metrics that matter to your business logic, rather than relying on generic academic tests.\n\n\n17. Taming Gen AI\n\n\n\nSlide 17\n\n\n(Timestamp: 05:28)\nThis title slide signals a transition into the technical ‚Äúhow-to‚Äù section of the talk. ‚ÄúTaming‚Äù implies that the default state of GenAI is wild and unpredictable.\nThe goal of the following sections is to bring structure and control to this chaos through rigorous engineering practices and evaluation workflows.\n\n\n18. Workshop Roadmap\n\n\n\nSlide 18\n\n\n(Timestamp: 05:31)\nThe roadmap outlines the four main sections of the talk: 1. Basics of Gen AI: Understanding variability and technical nuances. 2. Evaluation Workflow: Building the dataset and running the first tests. 3. More Complexity: Adding unit tests and conducting error analysis. 4. Agents: Evaluating complex, multi-step workflows.\n\n\n19. Variability in Responses\n\n\n\nSlide 19\n\n\n(Timestamp: 06:00)\nThis slide visually demonstrates the Non-Determinism of LLMs. It shows two responses to the same prompt generated just minutes apart. While substantively similar, the wording and structure differ slightly.\nThis variability makes exact string matching (a common software testing technique) impossible for LLMs. It necessitates semantic evaluation techniques, which complicates the testing pipeline.\n\n\n20. Input-Model-Output Diagram\n\n\n\nSlide 20\n\n\n(Timestamp: 06:24)\nA simple diagram illustrates the flow: Prompt -&gt; Model -&gt; Output. Rajiv uses this to structure the analysis of where variability comes from.\nHe explains that ‚Äúchaos‚Äù can enter the system at any of these three stages: the input (prompt sensitivity), the model (inference non-determinism), or the output (formatting and evaluation).\n\n\n21. Inconsistent Benchmark Scores\n\n\n\nSlide 21\n\n\n(Timestamp: 06:44)\nThe slide presents a discrepancy between benchmark scores tweeted by Hugging Face and those in the official Llama paper. Both used the same dataset (MMLU), but reported different accuracy numbers.\nThis introduces the problem of Evaluation Harness Sensitivity. Even with standard benchmarks, how you ask the model to take the test changes the score, proving that evaluation is fragile and implementation-dependent.\n\n\n22. MMLU Overview\n\n\n\nSlide 22\n\n\n(Timestamp: 07:25)\nMMLU (Massive Multitask Language Understanding) is explained here. It is a multiple-choice test covering 57 tasks across STEM, the humanities, and more.\nIt is currently the standard for measuring general ‚Äúintelligence‚Äù in models. However, because it is a multiple-choice format, it is susceptible to prompt formatting nuances, as the next slides demonstrate.\n\n\n23. Prompt Sensitivity\n\n\n\nSlide 23\n\n\n(Timestamp: 07:44)\nThis slide reveals why the scores in Slide 21 differed. The three evaluation harnesses used slightly different prompt structures (e.g., using the word ‚ÄúQuestion‚Äù vs.¬†just listing the text).\nThese minor changes resulted in significant accuracy shifts. This proves that LLMs are highly sensitive to syntax, meaning a ‚Äúbetter‚Äù model might just be one that was prompted more effectively for the test, not one that is actually smarter.\n\n\n24. Formatting Changes\n\n\n\nSlide 24\n\n\n(Timestamp: 08:22)\nExpanding on sensitivity, this slide references Anthropic‚Äôs research showing that changing answer choices from (A) to [A] or (1) affects the output.\nThis level of fragility is a key takeaway: seemingly cosmetic changes in how inputs are formatted can alter the model‚Äôs reasoning capabilities or its ability to output the correct token.\n\n\n25. GPT-4o Performance Drop\n\n\n\nSlide 25\n\n\n(Timestamp: 08:38)\nA bar chart demonstrates that this issue persists even in state-of-the-art models like GPT-4o. Subtle changes in wording can lead to a 5-10% drop in performance.\nThis counters the assumption that newer, larger models have ‚Äúsolved‚Äù prompt sensitivity. It remains a persistent variable that evaluators must control for.\n\n\n26. Tone Sensitivity\n\n\n\nSlide 26\n\n\n(Timestamp: 08:46)\nThis slide shows that the tone of a prompt (e.g., being polite vs.¬†direct) affects accuracy. Rajiv jokes, ‚ÄúI guess this is why mom always said to be polite.‚Äù\nThe graph indicates that prompt engineering strategies, like adding emotional weight or politeness, can statistically alter model performance, adding another layer of complexity to evaluation.\n\n\n27. Persistent Sensitivity\n\n\n\nSlide 27\n\n\n(Timestamp: 09:00)\nThe slide reiterates that despite years of progress, models are still sensitive to specific phrases. It shows a ‚ÄúPrompt Engineering‚Äù guide suggesting specific words to use.\nThe takeaway is that developers cannot treat the prompt as a static instruction; it is a hyperparameter that requires optimization and constant testing.\n\n\n28. Falcon LLM Bias\n\n\n\nSlide 28\n\n\n(Timestamp: 09:18)\nThis slide introduces a case study with the Falcon LLM. A user tweet shows the model recommending Abu Dhabi as a technological city with glowing sentiment, which raised suspicions about bias given the model‚Äôs origin in the Middle East.\nThis serves as a detective story: users wondered if the model weights were altered or if specific training data was injected to force this positive association.\n\n\n29. Potential Cover-up?\n\n\n\nSlide 29\n\n\n(Timestamp: 09:50)\nAnother tweet speculates if the model is ‚Äúcovering up human rights abuses‚Äù because it provides different answers for Abu Dhabi compared to other cities.\nThis highlights how model behavior can be misinterpreted as malicious bias or censorship, when the root cause might be something much simpler in the input stack.\n\n\n30. Inspecting the System Prompt\n\n\n\nSlide 30\n\n\n(Timestamp: 10:00)\nThe reveal: The bias wasn‚Äôt in the weights, but in the System Prompt. The slide suggests looking at the hidden instructions given to the model.\nIn Falcon‚Äôs case, the system prompt explicitly told the model, ‚ÄúYou are a model built in Abu Dhabi.‚Äù This context influenced its generation probabilities, causing it to favor Abu Dhabi in its responses.\n\n\n31. Claude System Prompt\n\n\n\nSlide 31\n\n\n(Timestamp: 10:33)\nRajiv points out that most developers never read the system prompts of the models they use. He highlights the Claude System Prompt, which is 1700 words long and takes nearly 10 minutes to read.\nThese extensive instructions define the model‚Äôs personality and safety guardrails. Ignoring them means you don‚Äôt fully understand the inputs driving your application‚Äôs behavior.\n\n\n32. Complexity of a Single Response\n\n\n\nSlide 32\n\n\n(Timestamp: 11:00)\nThe diagram is updated to show that a ‚Äúsingle response‚Äù is actually the result of complex interactions: Tokenization -&gt; Prompt Styles -&gt; Prompt Engineering -&gt; System Prompt.\nThis visual summarizes the ‚ÄúInput‚Äù section of the talk, reinforcing that before the model even processes data, multiple layers of text transformation occur that can alter the result.\n\n\n33. Inter-text Similarity\n\n\n\nSlide 33\n\n\n(Timestamp: 11:15)\nThis heatmap compares Inter-text similarity between models. It highlights Llama 70B and Llama 8B. Even though they are from the same family and likely trained on similar data, they are not identical.\nThis means you cannot swap a smaller model for a larger one (or vice versa) and expect the exact same behavior. Any model change requires a full re-evaluation.\n\n\n34. Sycophantic Models\n\n\n\nSlide 34\n\n\n(Timestamp: 12:16)\nThe slide discusses Sycophancy‚Äîthe tendency of models to agree with the user even when the user is wrong. It mentions how early versions of GPT-4 were sometimes ‚Äúoverly nice.‚Äù\nThis behavior is a specific type of model bias that evaluators must watch for. If a user asks a leading question containing false premises, a sycophantic model might validate the falsehood rather than correct it.\n\n\n35. Model Drift\n\n\n\nSlide 35\n\n\n(Timestamp: 12:37)\n‚ÄúModel Drift‚Äù refers to the phenomenon where commercial APIs (like OpenAI or Anthropic) change their model behavior over time without warning.\nBecause developers do not control the weights of API-based models, the ‚Äúground underneath them‚Äù can shift. A prompt that worked yesterday might fail today because the provider updated the backend or the inference infrastructure.\n\n\n36. Degraded Responses Timeline\n\n\n\nSlide 36\n\n\n(Timestamp: 12:55)\nThis slide shows a timeline of Degraded Responses from an Anthropic incident. Technical issues like context window routing errors led to corrupted outputs for a period of days.\nThis illustrates that drift isn‚Äôt always about model updates; it can be infrastructure failures. Continuous monitoring is required to detect when an external dependency degrades your application‚Äôs performance.\n\n\n37. Hyperparameters\n\n\n\nSlide 37\n\n\n(Timestamp: 13:33)\nThe slide lists Hyperparameters like Temperature, Top-P, and Max Length. Rajiv explains that users can control these ‚Äúknobs‚Äù to influence creativity versus determinism.\nSetting temperature to 0 makes the model less random, but as the next slides show, it does not guarantee perfect determinism due to hardware nuances.\n\n\n38. Non-Deterministic Inference\n\n\n\nSlide 38\n\n\n(Timestamp: 14:03)\nThis slide tackles Non-Deterministic Inference. Unlike traditional ML models (e.g., XGBoost) where a fixed seed guarantees identical output, LLMs on GPUs often produce different results for identical inputs.\nCauses include floating-point accumulation errors and the behavior of Mixture of Experts (MoE) models where different batches might activate different experts.\n\n\n39. Addressing Non-Determinism\n\n\n\nSlide 39\n\n\n(Timestamp: 15:11)\nRajiv references recent work by Thinking Machines and updates to vLLM that attempt to solve the non-determinism problem through correct batching.\nWhile solutions are emerging, the takeaway is that most current setups are non-deterministic by default. Evaluators must design their tests to tolerate this variance rather than expecting bit-wise reproducibility.\n\n\n40. Updated Model Diagram\n\n\n\nSlide 40\n\n\n(Timestamp: 15:43)\nThe diagram expands again. The ‚ÄúModel‚Äù box now includes Model Selection, Hyperparameters, Non-deterministic Inference, and Forced Updates.\nThis visual summarizes the ‚ÄúModel‚Äù section, showing that the ‚Äúblack box‚Äù is actually a dynamic system with internal variables (weights/architecture) and external variables (infrastructure/updates) that all add noise to the output.\n\n\n41. Output Format Issues\n\n\n\nSlide 41\n\n\n(Timestamp: 16:01)\nMoving to the ‚ÄúOutput‚Äù stage, this slide uses MMLU again to show how Output Formatting affects evaluation. How do you ask the model to answer a multiple-choice question?\nDo you ask it to output just the letter ‚ÄúA‚Äù? Or the full text? Or the probability of the token ‚ÄúA‚Äù? Different evaluation harnesses use different methods, leading to the score discrepancies seen earlier.\n\n\n42. Evaluation Harness Variations\n\n\n\nSlide 42\n\n\n(Timestamp: 16:35)\nThis table details the specific differences in implementation between harnesses (e.g., original MMLU vs.¬†HELM vs.¬†EleutherAI).\nIt reinforces that there is no standard ‚Äúruler‚Äù for measuring LLMs. The tool you use to measure the model introduces its own bias and variance into the final score.\n\n\n43. Score Comparison Table\n\n\n\nSlide 43\n\n\n(Timestamp: 16:56)\nA spreadsheet shows the same models scoring differently across different evaluation implementations. The variance is not trivial; it can be large enough to change the ranking of which model is ‚Äúbest.‚Äù\nThis data drives home the point: You must control your own evaluation pipeline. Relying on reported numbers is risky because you don‚Äôt know the implementation details behind them.\n\n\n44. Sentiment Analysis Variance\n\n\n\nSlide 44\n\n\n(Timestamp: 17:09)\nThis slide shows varying Sentiment Analysis outputs. Different models (or the same model with different prompts) might classify a review as ‚ÄúPositive‚Äù while another says ‚ÄúNeutral.‚Äù\nThis introduces the concept that even ‚Äúsimple‚Äù classification tasks in GenAI are subject to interpretation and variance, unlike traditional classifiers that have a fixed decision boundary.\n\n\n45. Tool Use Variance\n\n\n\nSlide 45\n\n\n(Timestamp: 17:23)\nRadar charts illustrate variance in Tool Use. Models might be good at using an ‚ÄúEmail‚Äù tool but fail at ‚ÄúCalendar‚Äù or ‚ÄúTerminal‚Äù tools.\nFurthermore, models exhibit non-determinism in decision making‚Äîsometimes they choose to use a tool, and sometimes they try to answer from memory. This adds a layer of logic errors on top of text generation errors.\n\n\n46. Summary: Why Responses Differ\n\n\n\nSlide 46\n\n\n(Timestamp: 17:49)\nThis comprehensive slide aggregates all the factors discussed: Inputs (prompts, system prompts), Model (drift, hyperparams), Outputs (formatting), and Infrastructure.\nIt serves as a checklist for the audience. If your application is behaving inconsistently, investigate these specific layers of the stack to find the source of the noise.\n\n\n47. Chaos is Okay\n\n\n\nSlide 47\n\n\n(Timestamp: 18:17)\nRajiv reassures the audience that ‚ÄúChaos is Okay.‚Äù The slide presents a chart of evaluation methods ranging from flexible/expensive (human eval) to rigid/cheap (code assertions).\nThe message is that while the technology is chaotic, there is a spectrum of tools available to manage it. We don‚Äôt need to solve every source of variance; we just need a robust process to measure it.\n\n\n48. From Chaos to Control\n\n\n\nSlide 48\n\n\n(Timestamp: 18:27)\nThis transition slide marks the beginning of the Evaluation Workflow section. The presentation shifts from describing the problem to prescribing the solution.\nThe goal here is to move from ‚ÄúVibe Coding‚Äù to a structured engineering discipline where changes are measured against a stable baseline.\n\n\n49. Build the Evaluation Dataset\n\n\n\nSlide 49\n\n\n(Timestamp: 18:37)\nThe first step in the workflow is to Build the Evaluation Dataset. The slide lists examples of prompts for tasks like summarization, extraction, and translation.\nRajiv emphasizes that this dataset should reflect your actual use case. It is the foundation of the ‚ÄúCustom Benchmark‚Äù concept introduced earlier.\n\n\n50. Get Labeled Outputs (Gold)\n\n\n\nSlide 50\n\n\n(Timestamp: 18:46)\nStep two is to get Labeled Outputs, also known as Gold Outputs, Reference, or Ground Truth. The slide adds a column showing the ideal answer for each prompt.\nThis is the standard against which the model will be judged. While obtaining these labels can be expensive (requiring human effort), they are essential for calculating accuracy.\n\n\n51. Compare to Model Output\n\n\n\nSlide 51\n\n\n(Timestamp: 19:00)\nStep three is to generate responses from your system and place them alongside the Gold Outputs. The slide adds a ‚ÄúModel Output‚Äù column.\nThis visual comparison allows developers (and automated judges) to see the delta between what was expected and what was produced.\n\n\n52. Measure Equivalence\n\n\n\nSlide 52\n\n\n(Timestamp: 19:10)\nStep four is to Measure Equivalence. Since LLMs rarely produce exact string matches, we use an LLM Judge (another model) to determine if the Model Output means the same thing as the Gold Output.\nThe slide shows a prompt for the judge: ‚ÄúAre these two responses semantically equivalent?‚Äù This converts a fuzzy text comparison problem into a binary (Pass/Fail) metric.\n\n\n53. Optimize Using Equivalence\n\n\n\nSlide 53\n\n\n(Timestamp: 19:57)\nOnce you have an equivalence metric, you can Optimize. The slide shows Config A vs.¬†Config B. By changing prompts or models, you can track if your ‚ÄúEquivalence Score‚Äù goes up or down.\nThis treats GenAI engineering like traditional hyperparameter tuning. The goal is to maximize the equivalence score on your custom dataset.\n\n\n54. Why Global Metrics Aren‚Äôt Enough\n\n\n\nSlide 54\n\n\n(Timestamp: 20:28)\nThe slide discusses the limitations of the ‚ÄúEquivalence‚Äù approach. While good for a general sense of quality, Global Metrics miss nuances.\nSometimes it‚Äôs hard to get a Gold Answer for open-ended creative tasks. Furthermore, a simple ‚ÄúPass/Fail‚Äù doesn‚Äôt tell you why the model failed (e.g., was it tone, length, or factuality?).\n\n\n55. From Global to Targeted Evaluation\n\n\n\nSlide 55\n\n\n(Timestamp: 20:55)\nThis slide argues for Targeted Evaluation. To maximize performance, you need to dig deeper into the data and identify specific error modes.\nThis transitions the talk from ‚ÄúBasic Workflow‚Äù to ‚ÄúAdvanced Testing,‚Äù where we break down ‚ÄúQuality‚Äù into specific, testable components like tone, length, and safety.\n\n\n56. Building Tests\n\n\n\nSlide 56\n\n\n(Timestamp: 21:14)\nThe section title ‚ÄúBuilding Tests‚Äù appears. This is where the presentation moves into the ‚ÄúUnit Testing‚Äù philosophy for GenAI.\nJust as software engineering relies on unit tests to verify specific functions, GenAI engineering should use targeted tests to verify specific attributes of the generated text.\n\n\n57. Good vs.¬†Bad Examples\n\n\n\nSlide 57\n\n\n(Timestamp: 21:20)\nThe slide displays a Good Example and a Bad Example of a response. The bad example is visibly shorter and less polite.\nRajiv asks the audience to identify why it is bad. This exercise is crucial: you cannot build a test until you can articulate exactly what makes a response a failure.\n\n\n58. Develop an Evaluation Mindset\n\n\n\nSlide 58\n\n\n(Timestamp: 21:46)\nTo define ‚ÄúBad,‚Äù developers need an Evaluation Mindset. This involves observing real-world user interactions and problems.\nData scientists often want to stay in their ‚Äúchair‚Äù and optimize algorithms, but Rajiv argues that effective evaluation requires understanding the user‚Äôs pain points.\n\n\n59. Collaborate with Experts\n\n\n\nSlide 59\n\n\n(Timestamp: 21:58)\nThe slide stresses Collaboration. You must talk to domain experts (e.g., the customer support team) to define what a ‚Äúgood‚Äù answer looks like.\nNaive bootstrapping‚Äîpretending to be a user‚Äîis a good start, but long-term success requires input from the people who actually know the business domain.\n\n\n60. Identify and Categorize Failures\n\n\n\nSlide 60\n\n\n(Timestamp: 22:52)\nOnce you understand the domain, you can Categorize Failure Types. The slide shows a chart grouping errors into categories like ‚ÄúHarmful Content,‚Äù ‚ÄúBias,‚Äù or ‚ÄúIncorrect Info.‚Äù\nThis clustering allows you to see patterns. Instead of just knowing ‚Äúthe model failed 20% of the time,‚Äù you know ‚Äúthe model has a specific problem with tone.‚Äù\n\n\n61. Define What Good Looks Like\n\n\n\nSlide 61\n\n\n(Timestamp: 23:11)\nUsing the categorization, you can explicitly Define What Good Looks Like. The slide contrasts the good/bad examples again, but now with labels: ‚ÄúToo short,‚Äù ‚ÄúLacks professional tone.‚Äù\nThis transforms a subjective feeling (‚Äúthis response sucks‚Äù) into objective criteria (‚Äúresponse must be &gt;50 words and use polite honorifics‚Äù).\n\n\n62. Document Every Issue\n\n\n\nSlide 62\n\n\n(Timestamp: 23:32)\nThe slide shows a spreadsheet where humans evaluate responses and Document Every Issue. Columns track specific attributes like ‚ÄúIs it helpful?‚Äù or ‚ÄúIs the tone right?‚Äù\nThis manual annotation is the training data for your automated tests. You need humans to establish the ground truth before you can automate the checking.\n\n\n63. Evaluation Tooling\n\n\n\nSlide 63\n\n\n(Timestamp: 23:53)\nRajiv mentions that Tooling Can Help. The slide shows a custom chat viewer designed to make human review easier.\nHowever, he warns against getting sidetracked by building fancy tools. Simple spreadsheets often suffice for the early stages. The goal is the data, not the interface.\n\n\n64. Test 1: Length Check\n\n\n\nSlide 64\n\n\n(Timestamp: 24:05)\nNow we build the automated tests. Test 1 is a Length Check. The slide shows Python code asserting that the word count is between 8 and 200.\nThis is a deterministic test. You don‚Äôt need an LLM to count words. Rajiv encourages using simple Python assertions wherever possible because they are fast, cheap, and reliable.\n\n\n65. Test 2: Tone and Style\n\n\n\nSlide 65\n\n\n(Timestamp: 24:22)\nTest 2 checks Tone and Style. Since ‚Äútone‚Äù is subjective, we use an LLM Judge (OpenAI model) to classify the response.\nThe prompt asks the judge to identify the style. This allows us to automate the ‚Äúvibe check‚Äù that humans were previously doing manually.\n\n\n66. Adding Metrics to Documentation\n\n\n\nSlide 66\n\n\n(Timestamp: 24:41)\nThe spreadsheet is updated with new columns: Length_OK and Tone_OK. These are the results of the automated tests.\nNow, for every row in the dataset, we have granular pass/fail metrics. This helps pinpoint exactly why a specific response failed, rather than just a generic failure.\n\n\n67. Check Judges Against Humans\n\n\n\nSlide 67\n\n\n(Timestamp: 25:12)\nA critical step: Check LLM Judges Against Humans. You must verify that your automated ‚ÄúTone Judge‚Äù agrees with your human experts.\nIf the human says the tone is rude, but the LLM Judge says it‚Äôs polite, your metric is useless. You must iterate on the judge‚Äôs prompt until alignment is high.\n\n\n68. Self-Evaluation Bias\n\n\n\nSlide 68\n\n\n(Timestamp: 26:06)\nThe slide illustrates Self-Evaluation Bias. LLMs tend to rate their own outputs higher than outputs from other models. GPT-4 prefers GPT-4 text.\nTo mitigate this, Rajiv suggests mixing models‚Äîuse Claude to judge GPT-4, or Gemini to judge Claude. This helps ensure a more neutral evaluation.\n\n\n69. Alignment Checks\n\n\n\nSlide 69\n\n\n(Timestamp: 26:46)\nThis slide reinforces the need for Continuous Alignment. Just because your judge aligned with humans last month doesn‚Äôt mean it still does (due to model drift).\nHuman spot-checks should be a permanent part of the pipeline to ensure the automated judges haven‚Äôt drifted.\n\n\n70. Biases in LLM Judges\n\n\n\nSlide 70\n\n\n(Timestamp: 27:02)\nThe slide lists known Biases in LLM Judges, such as Position Bias (favoring the first answer presented) or Verbosity Bias (favoring longer answers).\nEvaluators must be aware of these. For example, you should shuffle the order of answers when asking a judge to compare two options to cancel out position bias.\n\n\n71. Best Practices for LLM Judges\n\n\n\nSlide 71\n\n\n(Timestamp: 27:11)\nA summary of Best Practices: Calibrate with human data, use ensembles (multiple judges), avoid asking for ‚Äúrelevance‚Äù (too vague), and use discrete rating scales (1-5) rather than continuous numbers.\nThese tips help stabilize the inherently noisy process of using AI to evaluate AI.\n\n\n72. Error Analysis Chart\n\n\n\nSlide 72\n\n\n(Timestamp: 27:46)\nWith tests in place, we move to Error Analysis. The bar chart shows the number of failed cases categorized by error type (Length, Tone, Professional, Context).\nThis visualization tells you where to focus your efforts. If ‚ÄúTone‚Äù is the biggest bar, you work on the system prompt‚Äôs tone instructions. If ‚ÄúContext‚Äù is the issue, you might need better Retrieval Augmented Generation (RAG).\n\n\n73. Comparing Prompts\n\n\n\nSlide 73\n\n\n(Timestamp: 27:58)\nThe chart can compare Prompt A vs.¬†Prompt B. This allows for A/B testing of prompt engineering strategies.\nYou can see if a new prompt improves ‚ÄúTone‚Äù but accidentally degrades ‚ÄúContext.‚Äù This tradeoff analysis is impossible with a single global score.\n\n\n74. Explanations Guide Improvement\n\n\n\nSlide 74\n\n\n(Timestamp: 28:14)\nRajiv suggests asking the LLM Judge for Explanations. Don‚Äôt just ask for a score; ask for ‚Äúone sentence explaining why.‚Äù\nThese explanations act as metadata that helps developers understand the judge‚Äôs reasoning, making it easier to debug discrepancies between human and AI judgments.\n\n\n75. Limits to Explanations\n\n\n\nSlide 75\n\n\n(Timestamp: 28:35)\nA warning: Explanations are not causal. When an LLM explains why it did something, it is generating a plausible justification, not a trace of its actual neural activations.\nTreat explanations as a heuristic or a helpful hint, not as absolute truth about the model‚Äôs internal state.\n\n\n76. The Evaluation Flywheel\n\n\n\nSlide 76\n\n\n(Timestamp: 28:46)\nThe Evaluation Flywheel describes the iterative cycle: Build Eval -&gt; Analyze -&gt; Improve -&gt; Repeat.\nThis concept, credited to Hamill, emphasizes that evaluation is not a one-time event but a continuous loop that spins faster as you gather more data and build better tests.\n\n\n77. Financial Analyst Agent Example\n\n\n\nSlide 77\n\n\n(Timestamp: 29:20)\nTo demonstrate advanced unit testing, Rajiv introduces a Financial Analyst Agent. The goal is to assess the specific ‚Äústyle‚Äù of a financial report.\nThis is a complex domain where ‚Äúgood‚Äù is highly specific (regulated, precise, risk-aware), making it a perfect candidate for granular unit tests.\n\n\n78. Use a Global Test?\n\n\n\nSlide 78\n\n\n(Timestamp: 29:43)\nYou could use a Global Test: ‚ÄúWas this explained as a financial analyst would?‚Äù\nWhile simple, this test is opaque. If it fails, you don‚Äôt know if it was because of compliance issues, lack of clarity, or poor formatting.\n\n\n79. Global vs.¬†Unit Tests\n\n\n\nSlide 79\n\n\n(Timestamp: 29:54)\nThe slide contrasts the Global approach with Unit Tests. Instead of one question, we ask six: Context, Clarity, Precision, Compliance, Actionability, and Risks.\nThis breakdown allows for targeted debugging. You might find the model is great at ‚ÄúClarity‚Äù but terrible at ‚ÄúCompliance.‚Äù\n\n\n80. Scoring Radar Chart\n\n\n\nSlide 80\n\n\n(Timestamp: 30:16)\nA Radar Chart visualizes the unit test scores. This allows for a quick visual assessment of the model‚Äôs profile.\nIt facilitates comparison: you can overlay the profiles of two different models to see which one has the better balance of attributes for your specific needs.\n\n\n81. Analyzing Failures with Clusters\n\n\n\nSlide 81\n\n\n(Timestamp: 30:37)\nWith enough unit test data, you can use Clustering (e.g., K-Means) to group failures. The slide shows clusters like ‚ÄúSynthesis,‚Äù ‚ÄúContext,‚Äù and ‚ÄúHallucination.‚Äù\nThis moves error analysis from reading individual logs to analyzing aggregate trends, helping you prioritize which class of errors to fix first.\n\n\n82. Designing Good Unit Tests\n\n\n\nSlide 82\n\n\n(Timestamp: 30:52)\nAdvice on Designing Unit Tests: Keep them focused (one concept per test), use unambiguous language, and use small rating ranges.\nGood unit tests are the building blocks of a reliable evaluation pipeline. If the tests themselves are noisy or vague, the entire system collapses.\n\n\n83. Examples of Unit Tests\n\n\n\nSlide 83\n\n\n(Timestamp: 30:55)\nThe slide lists specific examples of tests for Legal (Compliance, Terminology), Retrieval (Relevance, Completeness), and Bias/Fairness.\nThis serves as a menu of options for the audience, showing that unit tests can cover almost any dimension of quality required by the business.\n\n\n84. Evaluating New Prompts\n\n\n\nSlide 84\n\n\n(Timestamp: 30:58)\nA bar chart shows how unit tests are used to Evaluate New Prompts. By running the full suite of unit tests on a new prompt, you get a ‚Äúscorecard‚Äù of its performance.\nThis data-driven approach removes the guesswork from prompt engineering.\n\n\n85. Tools - No Silver Bullet\n\n\n\nSlide 85\n\n\n(Timestamp: 31:02)\nRajiv reminds the audience that Tools are No Silver Bullet. You must master the basics (datasets, metrics) first.\nHe advises logging traces and experiments and practicing Dataset Versioning. Tools facilitate these practices, but they cannot replace the fundamental engineering discipline.\n\n\n86. Forest and Trees\n\n\n\nSlide 86\n\n\n(Timestamp: 31:04)\nAn analogy helps structure the analysis: Forest (Global/Integration) vs.¬†Trees (Test Case/Unit Tests).\nYou need to look at both. The forest tells you the overall health of the app, while the trees tell you specifically what needs pruning or fixing.\n\n\n87. Change One Thing at a Time\n\n\n\nSlide 87\n\n\n(Timestamp: 31:17)\nA crucial scientific principle: Change One Thing at a Time. With so many knobs (prompt, temp, model, RAG settings), changing multiple variables simultaneously makes it impossible to know what caused the improvement (or regression).\nIsolate your variables to conduct valid experiments.\n\n\n88. Error Analysis Tips\n\n\n\nSlide 88\n\n\n(Timestamp: 31:32)\nA summary of Error Analysis Tips: Use ablation studies (removing parts to see impact), categorize failures, save interesting examples, and leverage logs/traces.\nThese are the daily habits of successful GenAI engineers.\n\n\n89. The Evaluation Story\n\n\n\nSlide 89\n\n\n(Timestamp: 32:08)\nThe slide shows the ‚ÄúStory We Tell‚Äù‚Äîa linear graph of improvement over time. This is the idealized version of progress often presented in case studies.\nIt suggests a smooth journey from ‚ÄúOut of the box‚Äù to ‚ÄúSpecialized‚Äù to ‚ÄúUser Feedback.‚Äù\n\n\n90. The Reality of Progress\n\n\n\nSlide 90\n\n\n(Timestamp: 32:24)\nThe Reality is a messy, non-linear graph. You take two steps forward, one step back. Sometimes an ‚Äúimprovement‚Äù breaks the model.\nRajiv encourages resilience. Experienced practitioners know that this messy graph is normal and that sticking to the process eventually yields results.\n\n\n91. Continual Process\n\n\n\nSlide 91\n\n\n(Timestamp: 33:01)\nEvaluation is a Continual Process. It involves Problem ID, Data Collection, Optimization, User Acceptance Testing (UAT), and Updates.\nCrucially, UAT is your holdout set. Since you don‚Äôt have a traditional test set in GenAI, your real users act as the final validation layer.\n\n\n92. Eating the Elephant\n\n\n\nSlide 92\n\n\n(Timestamp: 34:03)\nThe metaphor ‚ÄúHow do you eat an elephant?‚Äù addresses the overwhelming nature of building a comprehensive evaluation suite.\nThe answer, of course, is ‚Äúone bite at a time.‚Äù You don‚Äôt need 100 tests on day one.\n\n\n93. Adding Tests Over Time\n\n\n\nSlide 93\n\n\n(Timestamp: 34:10)\nThe slide visualizes the ‚Äúelephant‚Äù being broken down into bites. You start with a few critical tests. As the app matures and you discover new failure modes, you add more tests.\nSix months in, you might have 100 tests, but you built them incrementally. This makes the task manageable.\n\n\n94. Doing Evaluation the Right Way\n\n\n\nSlide 94\n\n\n(Timestamp: 34:39)\nA summary slide listing best practices: Annotated Examples, Systematic Documentation, Continuous Error Analysis, Collaboration, and awareness of Generalization.\nThis concludes the core methodology section of the talk.\n\n\n95. Agentic Use Cases\n\n\n\nSlide 95\n\n\n(Timestamp: 34:50)\nThe final section covers Agentic Use Cases, symbolized by a dragon. Agents add a layer of complexity because the model is now making decisions (routing, tool use) rather than just generating text.\nThis ‚Äúagency‚Äù makes the system harder to track and evaluate.\n\n\n96. Crossing the River\n\n\n\nSlide 96\n\n\n(Timestamp: 35:06)\nA conceptual slide asking, ‚ÄúHow should it cross the river?‚Äù (Fly, Swim, Bridge?). This represents the decision-making step in an agent.\nEvaluating an agent requires evaluating how it made the decision (the router) separately from how well it executed the action.\n\n\n97. Chat-to-Purchase Router\n\n\n\nSlide 97\n\n\n(Timestamp: 35:22)\nA complex flowchart shows a Chat-to-Purchase Router. The agent must decide if the user wants to search for a product, get support, or track a package.\nRajiv suggests breaking this down: evaluate the Router component first (did it pick the right path?), then evaluate the specific workflow (did it track the package correctly?).\n\n\n98. Text to SQL Agent\n\n\n\nSlide 98\n\n\n(Timestamp: 36:17)\nAnother example: Text to SQL Agent. This workflow involves classification, feature extraction, and SQL generation.\nYou can isolate the ‚ÄúClassification‚Äù step (is this a valid SQL question?) and build a test just for that, before testing the actual SQL generation.\n\n\n99. Evaluating Office-Style Agents\n\n\n\nSlide 99\n\n\n(Timestamp: 36:46)\nThe slide discusses OdysseyBench, a benchmark for office tasks. It highlights failure modes like ‚ÄúFailed to create folder‚Äù or ‚ÄúFailed to use tool.‚Äù\nEvaluating agents involves checking if they successfully manipulated the environment (files, APIs), which is a functional test rather than a text similarity test.\n\n\n100. Error Analysis for Agents\n\n\n\nSlide 100\n\n\n(Timestamp: 37:00)\nError Analysis for Agentic Workflows requires assessing the overall performance, the routing decisions, and the individual steps.\nIt is the same ‚Äúaction error analysis‚Äù process but applied recursively to every node in the agent‚Äôs decision tree.\n\n\n101. Evaluating Workflow vs.¬†Response\n\n\n\nSlide 101\n\n\n(Timestamp: 37:19)\nThis slide distinguishes between evaluating a Response (text) and a Workflow (process). The flowchart shows a conversational flow.\nEvaluating a workflow might mean checking if the agent successfully moved the user from ‚ÄúGreeting‚Äù to ‚ÄúResolution,‚Äù regardless of the exact words used.\n\n\n102. Agentic Frameworks\n\n\n\nSlide 102\n\n\n(Timestamp: 37:48)\nRajiv warns that ‚ÄúAgentic Frameworks Help ‚Äì Until They Don‚Äôt.‚Äù Frameworks (like LangChain or AutoGen) are great for demos because they abstract complexity.\nHowever, in production, these abstractions can break or become outdated. He often recommends using straight Python for production agents to maintain control and reliability.\n\n\n103. Abstraction for Workflows\n\n\n\nSlide 103\n\n\n(Timestamp: 38:32)\nThe slide illustrates the trade-off in Abstraction. You can build rigid workflows (orchestration) where you control every step, or use general agents where the LLM decides.\nOrchestration is more reliable but rigid. General agents are flexible but prone to non-deterministic errors.\n\n\n104. When Abstractions Break\n\n\n\nSlide 104\n\n\n(Timestamp: 38:53)\nModel providers are training models to handle workflows internally (removing the need for external orchestration).\nHowever, until models are perfect, developers often need to break tasks down into specific pieces to ensure reliability. The choice between ‚Äúletting the model do it‚Äù and ‚Äúscripting the flow‚Äù depends on the application‚Äôs risk tolerance.\n\n\n105. Lessons from Agent Benchmarks\n\n\n\nSlide 105\n\n\n(Timestamp: 39:15)\nThe slide lists Lessons from Reproducing Agent Benchmarks: Standardize evaluation, measure efficiency, detect shortcuts, and log real behavior.\nThese are advanced tips for those pushing the boundaries of what agents can do.\n\n\n106. Conclusion\n\n\n\nSlide 106\n\n\n(Timestamp: 39:27)\nThe final slide, ‚ÄúWe did it!‚Äù, concludes the presentation. Rajiv thanks the audience and provides the QR code again.\nHis final message is one of empowerment: he hopes the audience now has the confidence to go out, build their own evaluation datasets, and start ‚Äúhill climbing‚Äù their own applications.\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rajiv Shah - rajistics blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Created - Oldest\n      \n      \n        Created - Newest\n      \n      \n        Title\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nRunning Code and Failing Models\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Practical Guide to Evaluating Generative AI Applications\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Vectors to Agents: Managing RAG in an Agentic World\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Sparse Matrices through Interactive Visualizations\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReasoning in Large Language Models\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nText style transfer in a spreadsheet using Hugging Face Inference Endpoints\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFew shot text classification with SetFit\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGetting predictions intervals with conformal inference\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining predictions from ü§ó transformer models\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Adversarial Data Collection\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStand Up for Best Practices\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization Strategies\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Unlabeled Data to Label Data\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Google‚Äôs Quickdraw to create an MNIST style dataset!\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning with R\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Worlds for Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nTaking an H2O Model to Production\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nUsing xgbfi for revealing feature interactions\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nOutlier App\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nRNN Addition (1st Grade)\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSportVu Analysis\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nShiny front end for Tensorflow demo\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2016\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sparsedataframe.html",
    "href": "sparsedataframe.html",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "",
    "text": "When working with machine learning models, preparing data properly is essential. One common preprocessing technique is one-hot encoding, which transforms categorical data into a format algorithms can understand. However, this transformation often creates sparse matrices - dataframes where most values are zero."
  },
  {
    "objectID": "sparsedataframe.html#basic-one-hot-encoding",
    "href": "sparsedataframe.html#basic-one-hot-encoding",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "Basic One-Hot Encoding",
    "text": "Basic One-Hot Encoding\nThe first animation illustrates the fundamental concept of one-hot encoding. This transformation converts a single categorical column (like ‚Äúcity‚Äù) into multiple binary columns, where each column represents one possible category value.\nView the basic one-hot encoding animation\nThis visualization walks through the transformation step-by-step:\n\nStarting with the original dataset containing categorical values\nAdding binary indicator columns for each category\nShowing how the dataset becomes wider but sparse (mostly filled with zeros)\nDemonstrating how the original categorical column becomes redundant\n\nIn traditional tabular data processing, we often don‚Äôt see this sparsity visually. The animation makes it clear how one-hot encoding dramatically changes the structure of our data."
  },
  {
    "objectID": "sparsedataframe.html#the-curse-of-dimensionality",
    "href": "sparsedataframe.html#the-curse-of-dimensionality",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "The Curse of Dimensionality",
    "text": "The Curse of Dimensionality\nThe second animation takes the concept further by demonstrating what happens with high-cardinality categorical features - those with many possible values.\nView the curse of dimensionality animation\nThis more advanced visualization shows how one-hot encoding can lead to the ‚Äúcurse of dimensionality‚Äù:\n\nStarting with a modest 4-column dataset\nExpanding to over 150 columns when encoding a categorical feature with many values\nCreating an extremely sparse matrix where 99% of values are zeros\nIllustrating the practical challenges this presents for machine learning"
  },
  {
    "objectID": "sparsedataframe.html#why-it-matters",
    "href": "sparsedataframe.html#why-it-matters",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "Why It Matters",
    "text": "Why It Matters\nUnderstanding the sparsity that results from one-hot encoding is crucial for several reasons:\n\nMemory usage: Sparse matrices can consume excessive memory if not properly handled\nComputational efficiency: Processing mostly-zero matrices is inefficient\nModel performance: Many algorithms struggle with extremely sparse data\nFeature selection: With hundreds of binary columns, feature selection becomes critical\n\nFor high-cardinality features, consider alternatives like feature hashing, target encoding, or embeddings to avoid the dimensionality explosion shown in the second animation.\nThese visualizations help build intuition about what‚Äôs happening ‚Äúunder the hood‚Äù when we preprocess data - something that‚Äôs often hidden when we use high-level libraries that handle these transformations automatically.\nRelated videos: Sparsity in AI or Curse of Dimensionality or Reality of Models"
  },
  {
    "objectID": "H2O_prod.html",
    "href": "H2O_prod.html",
    "title": "Taking an H2O Model to Production",
    "section": "",
    "text": "Introduction\nOne of the best feelings as a Data Scientist is when the model you have poured your heart and soul into, moves into production. Your model is now grown-up and you get to watch it mature.\nThis post shows how to take a H2O model and move it into a production environment. In this post, we will develop a simple H2O based predictive model, convert it into a Plain Old Java Object (POJO), compile it along with other Java packages, and package the compiled class files into a deployable JAR file so that it can readily be deployed onto any Java based application servers. This model will accept the input data set in the form of CSV file and return the predicted output in CSV format.\nH2O is one of my favorite tools for building models because it is well designed from an algorithm perspective, easy to use, and can scale to larger datasets. However, H2O‚Äôs documentation, though voluminous, doesn‚Äôt have clear instructions for moving a POJO model into production. This post will discuss this approach in greater detail besides providing code for how to do this. (H2O does have a post on doing real time predictions with storm). Special thanks to Socrates Krishnamurthy who co-wrote this post with me.\n\n\nBuilding H2O Model\nAs a starting point, lets use our favorite ice cream dataset to create a toy model in H2O:\n  library(h2o)  \n  library(Ecdat)  \n  data(Icecream)  \n  h2o.init()  \n  train.h2o &lt;- as.h2o(Icecream)  \n  rf &lt;- h2o.randomForest(x=2:4, y=1, ntrees=2, training_frame=train.h2o)   \nOnce you have developed your model in H2O, then the next step is downloading the POJO:\nh2o.download_pojo(rf, getjar=TRUE, path=\"~/Code/h2o-3.9.1.3459/test/\")\n# you must give a path to download a file\nThis will save two files, a H2O jar file about the model and an actual model file (that begins with DRF and ends with .java). Go ahead and open the model file in a text editor if you want to have a look at it.\n\n\nCompiling the H2O Model\nThe next step is to compile and run the model (say, the downloaded model name is DRF_model_R_1470416938086_15.java), then type:\n&gt; javac -cp h2o-genmodel.jar -J-Xmx2g DRF_model_R_1470416938086_15.java  \nThis creates a bunch of java class files.\n\n\nScoring the Input Data\nThe final step is scoring some input data. Prior to running the model, it is necessary to have files created for the input and output. For the input, the default setting is to read the first row as a header. The assumption is that the csv is well formed (this approach is not using the H2O parser). Once that is done, run:\n&gt; java -cp .:h2o-genmodel.jar hex.genmodel.tools.PredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv\nIf you open the output.csv file, it can be noticed that the predicted values are in Hexadecimal and not in Numeric format. For example, the output will be something like this:\n0x1.a24dd2p-2\n\n\nFixing the Hexadecimal Issue\nThe model is now predicting, but the predictions are in the wrong format. Yikes! To fix this issue requires some hacking of the java code. The rest of this post will show you how to hack the java code in PredictCsv, which can fix this issue and other unexpected issues with PredictCsv (for example, if your input comes tab separated).\nIf we take a deeper look at the PredictCsv java file located in the h2o github, the myDoubleToString method returns Hexadecimal string. But the challenge is this method being static in nature, cannot be overridden in a subclass or cannot be updated directly since it was provided by H2O jar file, to return regular numeric value in String format.\nThis can be fixed by creating a new java file (say, NewPredictCsv.java) by copying the entire content of PredictCsv.java from the above location and saving it locally. You then need to:\n\ncomment out the first line, so it should be //package hex.genmodel.tools;\n\nchange the name of the class name (~line 20) to read: public class NewPredictCsv {\n\ncorrect the hexadecimal issue by changing the return statement of myDoubleToString method to .toString() in lieu of .toHexString() (~line 131).\n\nAfter creating NewPredictCsv.java, compile it using the following command:\n&gt; javac -cp h2o-genmodel.jar -J-Xmx2g NewPredictCsv.java DRF_model_R_1470416938086_15.java\nRun the compiled file by providing input and output CSV files using the following command (Ensure that the input.csv file is in the current folder where you will run this):\n&gt; java -cp .:h2o-genmodel.jar NewPredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv\nIf you open the output.csv file now, it will be in the proper numeric format as follows:\n0.40849998593330383\n\n\nDeploying the Solution into Production:\nAt this point, we have a workable flow for using our model to score new data. But we can clean up the code to make it a little friendlier for our data engineers. First, create a jar file out of the class files created in previous steps. To do that, issue the following command:\n&gt; jar cf my-RF-model.jar *.class\nThis will place all the class files and our NewPredictCsv inside the jar. This is helpful when we have a model with say 500 trees. Now all we need is three files to run our scorer. So copy the above two jar files along with input.csv file in any folder/directory from where the program has to be executed. After copying, the folder should contain following files:\n&gt; my-RF-model.jar  \n&gt; h2o-genmodel.jar  \n&gt; input.csv  \nThe above input.csv file contains the dataset for which the dependent variable has to be predicted. To compute/ predict the values, run the java command as below:\n&gt; java -cp .:my-RF-model.jar:h2o-genmodel.jar NewPredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv\n\n\nNote:\nReplace : with ; in above commands if you are working in Windows (yuck)."
  },
  {
    "objectID": "HF-Reasoning.html",
    "href": "HF-Reasoning.html",
    "title": "Reasoning in Large Language Models",
    "section": "",
    "text": "Reasoning\n\n\n\nIntroduction\nI was wowed by ChatGPT. While I understood tasks like text generation and summarization, something was different with ChatGPT. When I looked at the literature, I saw this work exploring reasoning. Models reasoning, c‚Äômon. As a very skeptical data scientist, that seemed far-fetched to me. But I had to explore.\nI came upon the Big Bench Benchmark, composed of more than 200 reasoning tasks. The tasks include playing chess, describing code, guessing the perpetrator of a crime in a short story, identifying sarcasm, and even recognizing self-awareness. A common benchmark to test models is the Big Bench Hard (BBH), a subset of 23 tasks from Big Bench. Early models like OpenAI‚Äôs text-ada-00 struggle to reach a random score of 25. However, several newer models reach and surpass the average human rater score of 67.7. You can see results for these models in these publications: 1, 2, and 3.\n\n\n\nBig Bench Hard (23 Tasks) (1).png\n\n\nA survey of the research pointed out some common starting points for evaluating reasoning in models, including Arithmetic Reasoning, Symbolic Reasoning, and Commonsense Reasoning. This blog post provides examples of reasoning, but you should try out all these examples yourself. Hugging Face has a space where you can try to test a Flan T5 model yourself.\n\n\nArithmetic Reasoning\nLet‚Äôs start with the following problem.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: The answer is 5\nIf you ask an older text generation model like GPT-2 to complete this, it doesn‚Äôt understand the question and instead continues to write a story like this.\n\nWhile I don‚Äôt have access to PalM - 540B parameter model in the Big Bench, I was able to work with the Flan-T5 XXL using this publicly available space. I entered the problem and got this answer!\n\n\n\nR-Cars-Flan.png\n\n\nIt solved it! I tried messing with it and changing the words, but it still answered correctly. To my untrained eye, it is trying to take the numbers and perform a calculation using the surrounding information. This is an elementary problem, but this is more sophisticated than the GPT-2 response. I next wanted to do a more challenging problem like this:\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\nThe model gave an answer of 8, which isn‚Äôt correct. Recent research has found using chain-of-thought prompting can improve the ability of models. This involves providing intermediate reasoning to help the model determine the answer.\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is \nThe model correctly answers 11. To solve the juggling problem, I used this chain-of-thought prompt as an example. Giving the model some examples is known as few-shot learning. The new combined prompt using chain-of-thought and few-shot learning is:\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\nA:\nTry it, it works! Giving it an example and making it think everything through step by step was beneficial. This was fascinating for me. We don‚Äôt train the model in the sense of updating it‚Äôs weights. Instead, we are guiding it purely by the inference process.\n\n\nSymbolic Reasoning\nThe first symbolic reasoning was doing a reversal and the Flan-T5 worked very well on this type of problem.\nReverse the sequence \"glasses, pen, alarm, license\".\nA more complex problem on coin flipping was more interesting for me.\nQ: A coin is heads up. Tom does not flip the coin. Mike does not flip the coin. Is the coin still heads up?\nA:\nFor this one, I played around with different combinations of people flipping and showing the coin and the model, and it answered correctly. It was following the logic that was going through.\n\n\nCommon sense reasoning\nThe last category was common sense reasoning and much less obvious to me how models know how to solve these problems correctly.\nQ: What home entertainment equipment requires cable?\nAnswer Choices: (a) radio shack (b) substation (c) television (d) cabinet\nA: The answer is\nI was amazed at how well the model did, even when I changed the order.\n\n\n\nReasongif\n\n\nAnother common reasoning example goes like this:\nQ: Can Barack Obama have a conversation with George Washington? Give the rationale before answering.\nI changed around people to someone currently living, and it still works well.\n\n\nThoughts\nAs the first step, please, go try out these models for yourself. Google‚Äôs Flan-T5 is available with an Apache 2.0 license. Hugging Face has a space where you can try all these reasoning examples yourself. You can also replicate this using OpenAI‚Äôs GPT or other language models. I have a short video on the reasoning that also shows several examples.\nThe current language models have many known limitations. The next generation of models will likely be able to retrieve relevant information before answering. Additionally, language models will likely be able to delegate tasks to other services. You can see a demo of this integrating ChatGPT with Wolfram‚Äôs scientific API. By letting language models offload other tasks, the role of language models will emphasize communication and reasoning.\nThe current generation of models is starting to solve some reasoning tasks and match average human raters. It also appears that performance can still keep increasing. What happens when there are a set of reasoning tasks that computers are better than humans? While plenty of academic literature highlights the limitations, the overall trajectory is clear and has extraordinary implications."
  },
  {
    "objectID": "conformal_predictions.html",
    "href": "conformal_predictions.html",
    "title": "Getting predictions intervals with conformal inference",
    "section": "",
    "text": "Conformal\n\n\n\nIntroduction\nData scientists often overstate the certainty of their predictions. I have had engineers laugh at my point predictions and point out several types of errors in my model that create uncertainty. Prediction intervals are an excellent counterbalance for communicating the uncertainty of predictions.\nConformal inference offers a model agnostic technique for prediction intervals. It‚Äôs well known within statistics but not as well established in machine learning. This post focuses on a straightforward conformal inference technique, but there are more sophisticated techniques that provide more adaptable prediction intervals.\nI have created a Colab üìì companion notebook at https://bit.ly/raj_conf, and the Youtube üé• video that provides a detailed explanation. This explanation is a toy example to learn how conformal inference works. Typical applications will use a more sophisticated methodology along with implementations found within the resources below.\nFor python folks, a great package to start using conformal inference is MAPIE - Model Agnostic Prediction Interval Estimator. It works for tabular and time series problems.\n\n\nFurther Resources:\nQuick intro to conformal prediction using MAPIE in medium\nA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification, paper link\nAwesome Conformal Prediction (lots of resources)"
  },
  {
    "objectID": "explaining_transformers.html",
    "href": "explaining_transformers.html",
    "title": "Explaining predictions from ü§ó transformer models",
    "section": "",
    "text": "Banner\n\n\n\nIntroduction\nThis post covers 3 easy-to-use üì¶ packages to get started. You can also check out the Colab üìì companion notebook at https://bit.ly/raj_explain and the Youtube üé• video for a deeper treatment.\nExplanations are useful for explaining predictions. In the case of text, they highlight how the text influenced the prediction. They are helpful for ü©∫ diagnosing model issues, üëÄ showing stakeholders understand how a model is working, and üßë‚Äç‚öñÔ∏è meeting regulatory requirements. Here is an explanation üëá using shap. For more on explanations, check out the explanations in machine learning video.\n\n\n\nScreen Shot 2022-08-12 at 9.25.07 AM\n\n\nLet‚Äôs review 3 packages you can use to get explanations. All of these work with transformers, provide visualizations, and only require a few lines of code.\n\n\n\nRed and Purple Real Estate Soft Gradients Twitter Ad (1)\n\n\n\n\nShap\n\nSHAP is a well-known, well-regarded, and robust package for explanations. In working with text, SHAP typically defers to using a Partition Shap explainer. This method makes the shap computation tractable by using hierarchical clustering and Owens values. The image here shows the clustering for a simple phrase. If you want to learn more about Shapley values, I have a video on shapley values and a deep dive on Partition Shap explainer is here.\n\n\n\n\nScreen Shot 2022-08-12 at 9.35.34 AM\n\n\n\n\nTransformers Interpret\n\nTransformers Interpret uses Integrated Gradients from Captum to calculate the explanations. This approach is üêá quicker than shap! Check out this space to see a demo.\n\n\n\n\nScreen Shot 2022-08-12 at 9.27.04 AM\n\n\n\n\nFerret\n\nFerret is built for benchmarking interpretability techniques and includes multiple explanation methodologies (including Partition Shap and Integrated Gradients). A spaces demo for ferret is here along with a paper that explains the various metrics incorporated in ferret.\nYou can see below how explanations can differ when using different explanation methods. A great reminder that explanations for text are complicated and need to be appropriately caveated.\n\n\n\nScreen Shot 2022-08-11 at 1.19.05 PM\n\n\nReady to dive in? üü¢\nFor a longer walkthrough of all the üì¶ packages with code snippets, web-based demos, and links to documentation/papers, check out:\nüëâ Colab notebook: https://bit.ly/raj_explain\nüé• https://youtu.be/j6WbCS0GLuY"
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Optimization Strategies",
    "section": "",
    "text": "fanduel\n\n\n\nIntroduction\nAs a data scientist, you spend a lot of your time helping to make better decisions. You build predictive models to provide improved insights. You might be predicting whether an image is a cat or dog, store sales for the next month, or the likelihood if a part will fail. In this post, I won‚Äôt help you with making better predictions, but instead how to make the best decision.\nThe post strives to give you some background on optimization. It starts with a simply toy example show you the math behind an optimization calculation. After that, this post tackles a more sophisticated optimization problem, trying to pick the best team for fantasy football. The FanDuel image below is a very common sort of game that is widely played (ask your inlaws). The optimization strategies in this post were shown to consistently win! Along the way, I will show a few code snippets and provide links to working code in R, Python, and Julia. And if you do win money, feel free to share it :)\n\n\nSimple Optimization Example\nA simple example, which I found online, starts with a carpenter making bookcases in two sizes, large and small. It takes 6 hours to make a large bookcase and 2 hours to make a small one. The profit on a large bookcase is $50, and the profit on a small bookcase is $20. The carpenter can spend only 24 hours per week making bookcases and must make at least 2 of each size per week. Your job as a data scientist is to help your carpenter maximize her revenue.\nYour initial inclination could be that since the large bookcase is the most profitable, why not focus on them. In that case, you would profit (2*$20) + (3*$50) which is $190. That is a pretty good baseline, but not the best possible answer. It is time to get the algebra out and create equations that define the problem. First, we start with the constraints:\nx&gt;=2    ## large bookcases\n\ny&gt;=2    ## small bookcases\n\n6x + 2y &lt;= 24  (labor constraint)\nOur objective function which we are trying to maximize is:\nP = 50x + 20y\nIf we do the algebra by hand, we can convert out constraints to y &lt;= 12 - 3x. Then we graph all the constraints and find the feasible area for the portion of making small and large bookcases:\n\n\n\ngraph\n\n\nThe next step is figuring out the optimal point. Using the corner-point principle of linear programming, the maximum and minimum values of the objective function each occur at one of the vertices of the feasible region. Looking here, the maximum values (2,6) is when we make 2 large bookcases and 6 small bookcases, which results in an income of $220.\nThis is a very simple toy problem, typically there are many more constraints and the objective functions can get complicated. There are lots of classic problems in optimization such as routing algorithms to find the best path, scheduling algorithms to optimize staffing, or trying to find the best way to allocate a group of people to set of tasks. As a data scientist, you need to dissect what you are trying to maximize and identify the constraints in the form of equations. Once you can do this, we can hand this over to a computer to solve. So lets next walk through a bit more complicated example.\n\n\nFantasy Football\nOver the last few years, fantasy sports have increasingly grown in popularity. One game is to pick a set of football players to make the best possible team. Each football player has a price and there is a salary cap limit. The challenge is to optimize your team to produce the highest total points while staying within a salary cap limit. This type of optimization problem is known as the knapsack problem or an assignment problem.\n\n\nSimple Linear Optimization\nSo for this problem, let‚Äôs start by loading a dataset and taking a look at the raw data. You need to know both the salary as well as the expected points. Most football fans spend a lot of time trying to predict how many points a player will score. If you want to build a model for predicting the expected performance of a player, take a look at Ben‚Äôs blog post.\n\n\n\nQB points\n\n\nThe goal here is to build the best possible team for a salary cap, let‚Äôs say $50,000. A team consists of a quarterback, running backs, wide receivers, tight ends, and a defense. We can use the lpSolve package in R to set up the problem. Here is a code snippet for setting up the constraints.\n\n\n\nconstraints\n\n\nIf you parse through this, you can see we have set a minimum and maximum for QB of 1 player. However, for the RB, we have allowed a maximum of 3 and a minimum of 2. This is not unusual in fantasy football, be because there is a role called a flex player, which anyone can choose and they can either be a RB, WR, or TE. Now let‚Äôs look at the code for the objective:\n\n\n\nobjective\n\n\nThe code shows that we have set up the problem to maximize the objective of the most points and include our constraints. Once the code is run, it outputs an optimal team! I forked an existing repo and have made the R code and dataset are available here. A more sophisticated python optimization repo is also available.\n\n\n\nfinalteam\n\n\n\n\nAdvanced steps\nSo far, we have built a very simple optimization to solve the problem. There are several other strategies to further improve the optimizer. First, the variance of our teams can be increased by using a strategy called stacking, where you make sure your QB and WR are on the same team. A simple optimization is a constraint for selecting a QB and WR from the same team. Another strategy is using an overlap constraint for selecting multiple lineups. An overlap constraint ensures a diversity of players and not the same set of players for each optimized team. This strategy is particularly effective when submitting multiple lineups. You can read more about these strategies here and run the code in Julia here. An code snippet of the stacking constraint (this is for a hockey optimization):\n.\nLast year, at Sloan sports conference, Haugh and Sighal , presented a paper with additional optimization constraints. They include what an opponents team is likely to look like. After all, there are some players that are much more popular. Using this knowledge, you can predict the likely teams that will oppose your team. The approach here used Dirichlet regressions for modeling players. The result was a much-improved optimizer that was capable of consistently winning!\nI hope this post has shown you how optimization strategies can help you find the best possible solution.\n‚Äã"
  },
  {
    "objectID": "rnn_addition.html",
    "href": "rnn_addition.html",
    "title": "RNN Addition (1st Grade)",
    "section": "",
    "text": "Introduction\nEver since I ran across RNNs, they have intrigued me with their ability to learn. The best background is Denny Britz‚Äôs tutorial, Karpathy‚Äôs totally accessible and fun post on character-level language models, and Colah‚Äôs detailed descriptions of LSTMs. Besides all the fun examples of generating content with RNNs, other people have been applying them and winning Kaggle competitions and the ECML/PKDD challenge.\nI am still blown away by how RNN‚Äôs can learn to add. RNNs are trained through thousands of examples and can learn how to sum numbers. For example, the Keras addition example show how to add two sets of numbers up to 5 digital long each (e.g., 54678 + 78967). It achieves 99% train/test accuracy in 30 epochs with a one layer LSTM (128 HN) and 550k training examples.\nMy eventual goal is to use RNNs to study various sequenced data (such as the NBA SportVu), so I thought I should start simple. I wanted to teach a RNN to add a series of numbers. For example: 5+7+9. The rest of the post discusses this journey.\n\n\n1st Grade Model\nMy first model was teaching an RNN to add between 5 to 15 single digit numbers. This would be at the level of a first grader in the US. For example, using a 2 layer LSTM network with 100 hidden units, a batch of 50 training examples, and 5000 epochs, the RNN summed up:\n8+6+4+4+0+9+1+1+7+3+9+2+8 as 66.2154007\nThis isn‚Äôt too far from the actual answer of 62. The Keras addition example show that with even more examples/training, the RNN can get much better. The code for this RNN is available as a gist using tensorflow. I made this in a notebook format so its easy to play with.\nThere are lots of parameters to tweak with RNN models, such as the number of hidden units, epochs, batch size, dropout, and training rate. Each of these has different sorts of effects on the model. For example, increasing the number of hidden units will provide more space for learning, but consequently take longer to learn/train. The chart below shows the effect of different choices. Please take the time to really study/investigate the role of hidden units. Its a dynamic plot so you can zoom in and examine each series individually by clicking on the legend."
  },
  {
    "objectID": "semi_sup.html",
    "href": "semi_sup.html",
    "title": "Using Unlabeled Data to Label Data",
    "section": "",
    "text": "https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg\n\n\nYour boss hands you a pile of a 100,000 unlabeled images and asks you to categorize whether they are sandals, pants, boots, etc.\nSo now you have a massive set of unlabeled data and you need labels. What should you do?\nThis problem is commonplace. Lots of companies are swimming with data, whether its transactional, IoT sensors, security logs, images, voice, or more, and its all unlabeled. With so little labeled data, it is a tedious and slow process for data scientists to build machine learning models in most all enterprises.\nTake Google‚Äôs street view data. Gebru had to figure out how to label cars in 50 million images with very little labeled data. Over at Facebook, they used algorithms to label half a million videos, a task that would have otherwise taken 16 years.\nThis post shows you how to label hundreds of thousands of images in an afternoon. You can use the same approach whether you are labeling images or labeling traditional tabular data (e.g, identifying cyber security atacks or potential part failures.)\n\nThe Manual Method\nFor most data scientists when asked to do something, the first step is to calculate who else should do this.\n\n\n\nhttp://vni.s3.amazonaws.com/130905163633604.jpg\n\n\nBut 100,000 images could cost you at least $30,000 on Mechanical Turk or some other competitor. Your boss expects this done cheaply, since after all, they hired you because you use free software. Now, she doesn‚Äôt budget for anything other than your salary (if you don‚Äôt believe me, ask to go to pydata).\nYou take a deep breath and figure you can probably label 200 images in an hour. So that means in three weeks of non stop work, you can get this done!! Yikes!\n\n\nJust Build a Model\nThe first idea is to label a handful of the images, train a machine learning algorithm, and then predict the remaining set of labels. For this exercise, I am using the Fashion-MNIST dataset (you could also make your own using quickdraw). There are ten classes of images to identify and here is a sample of what they look like:\n\n\n\nhttps://pbs.twimg.com/media/DJNuE7BWAAAo3eC.jpg\n\n\nI like this dataset, because each image is 28 by 28 pixels, which means it contains 784 unique features/variables. For a blog post this works great, but its also not like any datasets you see in the real world, which are often either much narrower (traditional tabluar business problem datasets) or much wider (real images are much bigger and include color).\nI built models using the most common data science algorithms: logistic regression, support vector machines (SVM), random forest and gradient boosted machines (GBM).\nI evaluated the performance based on labeling 100, 200, 500, 1000, and 2000 images.\n\n\n\nAll\n\n\nAt this point in the post, if you are still with me, slow down and mull this graph over. There is a lot of good stuff here. Which algorithm does the best? (If you a data scientist, you shouldn‚Äôt fall for that question.) It really depends on the context.\nYou want something quick and dependable out of the box, you could go for the logistic regression. While the random forest starts way ahead, the SVM is coming on fast. If we had more labeled data the SVM would pass the random forest. And the GBM works great, but can take a bit of work to perform their best. The scores here are using out of the box implementations in R (e1071, randomForest, gbm, nnet).\nIf our benchmark is 80% accuracy for ten classes of images, we could get there by building a Random Forest model with 1000 images. But 1000 images is still a lot of data to label, 5 hours by my estimate. Lets think about ways we can improve.\n\n\nLet‚Äôs Think About Data\nAfter a little reflection, you remember what you often tell others ‚Äî that data isn‚Äôt random, but has patterns. By taking advantage of these patterns we can get insight in our data.\nLets start with an autoencoder (AE). An autoencoder squeezes and compresses your data, kind of like turning soup into a bouillon cube. Autoencoders are the hipster‚Äôs Principle Component Analysis (PCA) , since they support nonlinear transformations.\nEffectively this means we are taking our wide data (784 features/variables) reducing it down to 128 features. We then take this new compressed data and train our machine learning algorithm (SVM in this case). The graph below shows the difference in performance between an SVM fed with an autoencoder (AE_SVM) versus the SVM on the raw data.\n\n\n\nAE\n\n\nBy squeezing the information down to 128 features, we were able to actually improve the performance of the SVM algorithm at the low end. At the 100 labels mark, accuracy went from 44% to 59%. At the 1000 labels mark, the autoencoder was still helping, we see an improvement from 74% to 78%. So we are on to something here. We just need think a bit more about the distribution and patterns in our data that we can take advantage of.\n\n\nThinking Deeper About Your Data\nWe know that our data are images and since 2012, the hammer for images is a convolutional neural network (CNN). There are a couple of ways we could use a CNN, from a pretrained network or as a simple model to pre-process the images. For this post, I am going to use a Convolutional Variational Autoencoder as a path towards the technique by Kingma for semi-supervised learning.\nSo lets build a Convolutional Variational Autoencoder (CVAE). The leap here is twofold. First, ‚Äúvariational‚Äù means the autoenconder compress the information down into a probability distribution. Second is the addition of using a convolutional neural networks as an encoder. This is a bit of deep learning, but the emphasis here is on how we are solving the problem, not the latest shiny toy.\nFor coding my CVAE, I used the example CVAE from the list of examples over at RStudio‚Äôs Keras page. Like the previous autoencoder, we design the latent space to reduce the data to 128 features. We then use this new data to train an SVM model. Below is a plot of the performance of the CVAE as compared to the SVM and RandomForest on the raw data.\n\n\n\nCVAE\n\n\nWow! The new model is much more accurate. We can get well past 80% accuracy with just 500 labels. By using these techniques we get better performance and require less labelled images! At the top end, we can also do much better than the RandomForest or SVM model.\n\n\nNext Steps\nBy using some very simple semi-supervised techniques with autoencoders, its possible to quickly and accurately label data. But the takeaway is not to use deep learning auto encoders! Instead, I hope you understand the methodology here of starting very simple and then trying gradually more complex solutions. Don‚Äôt fall for the latest shiny toy ‚Äî pratical data science is not about using the latest approaches found in arxiv.\nIf this idea of semi-supervised learning inspires you, this post is the logistic regression of semi-supervised learning. If you want to dig further into Semi-Supervised Learning and Domain Adaptation, check out Brian Keng‚Äôs great walkthrough of using variational autoencoders (which goes beyond what we have done here) or the work of Curious AI, which has been advancing semi-supervised learning using deep learning and sharing their code. But at the very least, don‚Äôt reflexively think all your data has to be hand labeled."
  },
  {
    "objectID": "sportvu_analysis.html",
    "href": "sportvu_analysis.html",
    "title": "SportVu Analysis",
    "section": "",
    "text": "Introduction\nThis post shares some of the code that I have created for analyzing NBA SportVu data.\nFor background, the NBA SportVu data is motion data for the basketball and players taken 25 times a second. For a typical NBA game, this means about 2 million rows of data. The data for over 600 NBA games (first half of the 2015-2016 season) is available. This is over a billion rows of telematics (iOT) type data. This is a gold mine and here are some early pieces from studying that data.\n\n\nEDA\nThe first is basic EDA on the movement data. This code allows you to start analyzing the ball and player movement. \n\n\nPBP\nThe next markdown, PBP, shows how to merge play by play data with the SportVu movement data. This allows using the annotated data which contains information on the type of play, score, and home/visitor info.\n\n\nChull\n The next set of documents start analyzing the data. The first measures player spacing using convex hulls. The next shows how to calculate player velocity, acceleration, and jerk. (I really wanted to do a post on the biggest jerk in the NBA, but unfortunately the jerk data is way too noisy.)  The third document offers a few different ways for analyzing player and ball trajectories.\nYou can find all these files at my SportVu Github repo."
  },
  {
    "objectID": "tensorflow_shiny.html",
    "href": "tensorflow_shiny.html",
    "title": "Shiny front end for Tensorflow demo",
    "section": "",
    "text": "shiny_tensor\n\n\n\nIntroduction\nI built a GUI front end for tensorflow from shiny, the code is available at Github. The shiny app allows trying different inputs, RNN cell types, and even optimizers. The results are shown with plots as well as a link to tensorboard. The app allows anyone to try out these models with a variety of modelling options.\nThe code for the shiny web app was based around work by Sachin Jogleka. Sachin focused on RNNs that had two numeric inputs. (This is slightly different than most RNN examples which focus on language models.)\nSachin‚Äôs code was modified to allow different cell types and reworked so it could be called from rPython. The shiny web app relies on rPython to run the tensorflow models. There is also an iPython notebook in the repository if you would like to test this outside of shiny.\n\n\n\nshiny_tensor\n\n\n\n\nLive Demo:\nI have a live demo of this app, but it‚Äôs flaky. Building RNN models is computationally intensive and the shiny front end is intended to be used on development boxes with tensorflow. My live demo app is limited in several ways. First, the server lacks the horsepower to build models quickly. Second, if the instructions below are not carefully followed the app will crash. Third, its not designed for multiple people building different types of models at the same time. Finally, tensorboard application sometimes stops running, so the link to tensorboard within the live demo app may not work. Again, to really use this app, please install it locally.\nThe requirements for the app include tensorflow and numpy on the Python side. Shiny, Metrics, plotly, and rPython on the R side. rPython can be difficult to install/configure, so please verify that rPython is working correctly if you are having problems running the code.\n\n\nUsing the App:\nTo use the app, select your model options. For the inputs, there are three options of increasing complexity. Steps for prediction window refers to how far ahead is the model suppose to predict. For this data, 20s seemed a reasonable window. For Cell Type, select one of the cell types and press Initialize Model. Then select iterations (max of 10,000) and press Train. After a few seconds, you will see the output.\nTake advantage of the plots to zoom in and out and see the shape of the actual and predicted outputs. To further improve the model, you can add iterations by pressing the train button. The plots show how the RNN model is learning and getting better at predicting the output.\nTo try a new model, select a new cell type and press initialize model. Then select the number of iterations and press train.\nIf the app crashes, no worries, it happens. I have not accounted for everything that could go wrong."
  }
]