[
  {
    "objectID": "Snowpark_Forecasting_Bus.html",
    "href": "Snowpark_Forecasting_Bus.html",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "",
    "text": "‚Äî title: Snowflake ML Intro Notebook - ML Forecasting date: ‚Äú2024-05-20‚Äùcategories: [snowflake, MLOps]image: fe_forecasting.png‚Äî\nThis notebook introduces several key features of Snowflake ML in the process of training a machine learning model for forecasting Chicago bus ridership.\nThis notebook is intended to highlight Snowflake functionality and should not be taken as a best practice for time series forecasting.\nGet Notebook\nGo to folder with dataset\nSee more snowflake notebooks from raj"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#setup-environment",
    "href": "Snowpark_Forecasting_Bus.html#setup-environment",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "1. Setup Environment",
    "text": "1. Setup Environment\n\n# Snowflake connector\nfrom snowflake import connector\n#from snowflake.ml.utils import connection_params\n\n# Snowpark for Python\nfrom snowflake.snowpark.session import Session\nfrom snowflake.snowpark.types import Variant\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark.types import *\n\n# Snowpark ML\nfrom snowflake.ml.modeling.compose import ColumnTransformer\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.preprocessing import StandardScaler, OrdinalEncoder\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml import version\nmlversion = version.VERSION\n\n\n# Misc\nimport pandas as pd\nimport json\nimport logging \nlogger = logging.getLogger(\"snowflake.snowpark.session\")\nlogger.setLevel(logging.ERROR)"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#establish-secure-connection-to-snowflake",
    "href": "Snowpark_Forecasting_Bus.html#establish-secure-connection-to-snowflake",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "Establish Secure Connection to Snowflake",
    "text": "Establish Secure Connection to Snowflake\nUsing the Snowflake ML Python API, it‚Äôs quick and easy to establish a secure connection between Snowflake and Notebook. I prefer using a toml configuration file as documented here. Note: Other connection options include Username/Password, MFA, OAuth, Okta, SSO\nThe creds.json should look like this:\n{\n    \"account\": \"awb99999\",\n    \"user\": \"your_user_name\",\n    \"password\": \"your_password\",\n    \"warehouse\": \"your_warehouse\"\n  }\n\n::: {#953f6906 .cell execution_count=2}\n``` {.python .cell-code}\nwith open('../../creds.json') as f:\n    data = json.load(f)\n    USERNAME = data['user']\n    PASSWORD = data['password']\n    SF_ACCOUNT = data['account']\n    SF_WH = data['warehouse']\n\nCONNECTION_PARAMETERS = {\n   \"account\": SF_ACCOUNT,\n   \"user\": USERNAME,\n   \"password\": PASSWORD,\n}\n\nsession = Session.builder.configs(CONNECTION_PARAMETERS).create()\n:::\nVerify everything is connected. I like to do this to remind people to make sure they are using the latest versions.\n\nsnowflake_environment = session.sql('select current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\nprint('Snowflake ML version        : {}.{}.{}'.format(mlversion[0],mlversion[2],mlversion[4]))\n\nUser                        : RSHAH\nRole                        : \"RAJIV\"\nDatabase                    : \"RAJIV\"\nSchema                      : \"DOCAI\"\nWarehouse                   : \"RAJIV\"\nSnowflake version           : 8.19.2\nSnowpark for Python version : 1.15.0a1\nSnowflake ML version        : 1.5.0\n\n\nThroughout this notebook, I will change warehouse sizes. For this notebook warehouse size really doesn‚Äôt matter much, but I want people to understand how easily and quickly you can change the warehouse size. This is one of my favorite features of Snowflake, just how its always ready for me.\n\nsession.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'SMALL'\").collect()\nsession.sql(\"USE SCHEMA PUBLIC\").collect()\n\n[Row(status='Statement executed successfully.')]"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#load-data-in-snowflake",
    "href": "Snowpark_Forecasting_Bus.html#load-data-in-snowflake",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "2. Load Data in Snowflake",
    "text": "2. Load Data in Snowflake\nLet‚Äôs get the data (900k rows) and also make the column names all upper cases. It‚Äôs easier to work with columns names that aren‚Äôt case sensitive.\n\ndf_clean = pd.read_csv('CTA_Daily_Totals_by_Route.csv')\ndf_clean.columns = df_clean.columns.str.upper()\nprint (df_clean.shape)\nprint (df_clean.dtypes)\ndf_clean.head()\n\n(893603, 4)\nROUTE      object\nDATE       object\nDAYTYPE    object\nRIDES       int64\ndtype: object\n\n\n\n\n\n\n\n\n\nROUTE\nDATE\nDAYTYPE\nRIDES\n\n\n\n\n0\n3\n01/01/2001\nU\n7354\n\n\n1\n4\n01/01/2001\nU\n9288\n\n\n2\n6\n01/01/2001\nU\n6048\n\n\n3\n8\n01/01/2001\nU\n6309\n\n\n4\n9\n01/01/2001\nU\n11207\n\n\n\n\n\n\n\nLet‚Äôs create a Snowpark dataframe and split the data for test/train. This operation is done inside Snowflake and not in your local environment. We will also save this as a table so we don‚Äôt ever have to manually upload this dataset again.\nPRO TIP ‚Äì Snowpark will inherit the schema of a pandas dataframe into Snowflake. Either change your schema before importing or after it has landed in snowflake. People that put models into production are very careful about data types.\n\ninput_df = session.create_dataframe(df_clean)\nschema = input_df.schema\nprint(schema)\n\nStructType([StructField('ROUTE', StringType(16777216), nullable=True), StructField('DATE', StringType(16777216), nullable=True), StructField('DAYTYPE', StringType(16777216), nullable=True), StructField('RIDES', LongType(), nullable=True)])\n\n\n\ninput_df.write.mode('overwrite').save_as_table('CHICAGO_BUS_RIDES')\n\nLet‚Äôs read from the table, since that is generally what you will be doing in production. We have 893,000 rows of ridership data.\n\ndf = session.read.table('CHICAGO_BUS_RIDES')\nprint (df.count())\ndf.show()\n\n893603\n----------------------------------------------\n|\"ROUTE\"  |\"DATE\"      |\"DAYTYPE\"  |\"RIDES\"  |\n----------------------------------------------\n|3        |01/01/2001  |U          |7354     |\n|4        |01/01/2001  |U          |9288     |\n|6        |01/01/2001  |U          |6048     |\n|8        |01/01/2001  |U          |6309     |\n|9        |01/01/2001  |U          |11207    |\n|10       |01/01/2001  |U          |385      |\n|11       |01/01/2001  |U          |610      |\n|12       |01/01/2001  |U          |3678     |\n|18       |01/01/2001  |U          |375      |\n|20       |01/01/2001  |U          |7096     |\n----------------------------------------------"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#distributed-feature-engineering",
    "href": "Snowpark_Forecasting_Bus.html#distributed-feature-engineering",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "3. Distributed Feature Engineering",
    "text": "3. Distributed Feature Engineering\nLet‚Äôs add the Day of the week and then Aggregate the data by day. Let‚Äôs join in weather data\nThese operations are done inside the Snowpark warehouse which provides improved performance and scalability with distributed execution for these scikit-learn preprocessing functions. This dataset uses SMALL, but you can always move up to larger ones including Snowpark Optimized warehouses (16x memory per node than a standard warehouse), e.g., session.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\n\nsession.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\n\n[Row(status='Warehouse SNOWPARK_OPT_WH successfully created.')]\n\n\nSimple feature engineering\n\nfrom snowflake.snowpark.functions import col, to_timestamp, dayofweek, month,sum, listagg, lag\nfrom snowflake.snowpark import Window\n\ndf = df.with_column('DATE', to_timestamp(col('DATE'), 'MM/DD/YYYY'))\n\n# Add a new column for the day of the week\n# The day of week is represented as an integer, with 0 = Sunday, 1 = Monday, ..., 6 = Saturday\ndf = df.with_column('DAY_OF_WEEK', dayofweek(col('DATE')))\n\n# Show the resulting dataframe\ndf.show()\n\n-----------------------------------------------------------------------\n|\"ROUTE\"  |\"DAYTYPE\"  |\"RIDES\"  |\"DATE\"               |\"DAY_OF_WEEK\"  |\n-----------------------------------------------------------------------\n|3        |U          |7354     |2001-01-01 00:00:00  |1              |\n|4        |U          |9288     |2001-01-01 00:00:00  |1              |\n|6        |U          |6048     |2001-01-01 00:00:00  |1              |\n|8        |U          |6309     |2001-01-01 00:00:00  |1              |\n|9        |U          |11207    |2001-01-01 00:00:00  |1              |\n|10       |U          |385      |2001-01-01 00:00:00  |1              |\n|11       |U          |610      |2001-01-01 00:00:00  |1              |\n|12       |U          |3678     |2001-01-01 00:00:00  |1              |\n|18       |U          |375      |2001-01-01 00:00:00  |1              |\n|20       |U          |7096     |2001-01-01 00:00:00  |1              |\n-----------------------------------------------------------------------\n\n\n\nA bit more feature engineering, but again, this is very familiar syntax.\n\n# Add a new column for the month\ndf = df.with_column('MONTH', month(col('DATE')))\n\n# Group by DATE, DAY_OF_WEEK, and MONTH, then aggregate\ntotal_riders = df.group_by('DATE','DAY_OF_WEEK','MONTH').agg(\n    F.listagg('DAYTYPE', is_distinct=True).alias('DAYTYPE'),\n    F.sum('RIDES').alias('TOTAL_RIDERS')\n).order_by('DATE')\n\n#Define a window specification\nwindow_spec = Window.order_by('DATE')\n\n# Add a lagged column for total ridership of the previous day\ntotal_riders = total_riders.with_column('PREV_DAY_RIDERS', lag(col('TOTAL_RIDERS'), 1).over(window_spec))\n\n# Show the resulting dataframe\nprint (total_riders.count())\nprint (total_riders.show())\n\n7364\n--------------------------------------------------------------------------------------------------\n|\"DATE\"               |\"DAY_OF_WEEK\"  |\"MONTH\"  |\"DAYTYPE\"  |\"TOTAL_RIDERS\"  |\"PREV_DAY_RIDERS\"  |\n--------------------------------------------------------------------------------------------------\n|2001-01-01 00:00:00  |1              |1        |U          |295439          |NULL               |\n|2001-01-02 00:00:00  |2              |1        |W          |776862          |295439             |\n|2001-01-03 00:00:00  |3              |1        |W          |820048          |776862             |\n|2001-01-04 00:00:00  |4              |1        |W          |867675          |820048             |\n|2001-01-05 00:00:00  |5              |1        |W          |887519          |867675             |\n|2001-01-06 00:00:00  |6              |1        |A          |575407          |887519             |\n|2001-01-07 00:00:00  |0              |1        |U          |374435          |575407             |\n|2001-01-08 00:00:00  |1              |1        |W          |980660          |374435             |\n|2001-01-09 00:00:00  |2              |1        |W          |974858          |980660             |\n|2001-01-10 00:00:00  |3              |1        |W          |980656          |974858             |\n--------------------------------------------------------------------------------------------------\n\nNone\n\n\n\nAlso, you can use ChatGPT to generate the code for you."
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#join-in-the-weather-data-from-the-snowflake-marketplace",
    "href": "Snowpark_Forecasting_Bus.html#join-in-the-weather-data-from-the-snowflake-marketplace",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "Join in the Weather Data from the Snowflake Marketplace",
    "text": "Join in the Weather Data from the Snowflake Marketplace\nInstead of downloading data and building pipelines, Snowflake has a lot of useful data, including weather data in it‚Äôs Marketplace. This means the data is only a SQL query away.\nCybersyn Weather\nSQL QUERY:\nSELECT\n  ts.noaa_weather_station_id,\n  ts.DATE,\n  COALESCE(MAX(CASE WHEN ts.variable = 'minimum_temperature' THEN ts.Value ELSE NULL END), 0) AS minimum_temperature,\n  COALESCE(MAX(CASE WHEN ts.variable = 'precipitation' THEN ts.Value ELSE NULL END), 0) AS precipitation,\n  COALESCE(MAX(CASE WHEN ts.variable = 'maximum_temperature' THEN ts.Value ELSE NULL END), 0) AS maximum_temperature\nFROM\n  cybersyn.noaa_weather_metrics_timeseries AS ts\nJOIN\n  cybersyn.noaa_weather_station_index AS idx\nON\n  (ts.noaa_weather_station_id = idx.noaa_weather_station_id)\nWHERE\n  idx.NOAA_WEATHER_STATION_ID = 'USW00014819'\n  AND (ts.VARIABLE = 'minimum_temperature' OR ts.VARIABLE = 'precipitation' OR ts.VARIABLE = 'maximum_temperature')\nGROUP BY\n  ts.noaa_weather_station_id,\n  ts.DATE\nLIMIT 1000;\n\nweather = session.read.table('CHICAGO_WEATHER')\n\nfrom snowflake.snowpark.types import DoubleType\nweather = weather.withColumn('MINIMUM_TEMPERATURE', weather['MINIMUM_TEMPERATURE'].cast(DoubleType()))\nweather = weather.withColumn('MAXIMUM_TEMPERATURE', weather['MAXIMUM_TEMPERATURE'].cast(DoubleType()))\nweather = weather.withColumn('PRECIPITATION', weather['PRECIPITATION'].cast(DoubleType()))\n\nweather.show()\n\n------------------------------------------------------------------------------------------------------------\n|\"NOAA_WEATHER_STATION_ID\"  |\"DATE\"      |\"MINIMUM_TEMPERATURE\"  |\"MAXIMUM_TEMPERATURE\"  |\"PRECIPITATION\"  |\n------------------------------------------------------------------------------------------------------------\n|USW00014819                |2019-07-16  |22.2                   |28.9                   |3.8              |\n|USW00014819                |2002-01-06  |-3.9                   |3.3                    |0.0              |\n|USW00014819                |2008-03-17  |-0.5                   |4.4                    |2.0              |\n|USW00014819                |2000-01-29  |-6.7                   |-2.2                   |0.0              |\n|USW00014819                |2004-06-12  |16.7                   |26.7                   |6.6              |\n|USW00014819                |2017-07-15  |16.1                   |28.3                   |0.0              |\n|USW00014819                |2001-10-22  |12.2                   |18.9                   |2.3              |\n|USW00014819                |2021-05-01  |6.1                    |28.3                   |0.0              |\n|USW00014819                |2016-11-29  |7.2                    |14.4                   |0.0              |\n|USW00014819                |2020-08-01  |18.3                   |26.1                   |5.1              |\n------------------------------------------------------------------------------------------------------------\n\n\n\n\n# Perform the join operation\njoined_df = weather.join(\n    total_riders,\n    weather[\"DATE\"] == total_riders[\"DATE\"],\n    \"inner\",  # This is the type of join: inner, outer, left, right,\n    lsuffix=\"w\"\n)\n# Show the result of the join\njoined_df.show()\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"NOAA_WEATHER_STATION_ID\"  |\"DATEW\"     |\"MINIMUM_TEMPERATURE\"  |\"MAXIMUM_TEMPERATURE\"  |\"PRECIPITATION\"  |\"DATE\"               |\"DAY_OF_WEEK\"  |\"MONTH\"  |\"DAYTYPE\"  |\"TOTAL_RIDERS\"  |\"PREV_DAY_RIDERS\"  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|USW00014819                |2005-10-15  |8.9                    |20.0                   |0.0              |2005-10-15 00:00:00  |6              |10       |A          |666129          |1087863            |\n|USW00014819                |2019-04-29  |6.1                    |11.7                   |29.0             |2019-04-29 00:00:00  |1              |4        |W          |724030          |332461             |\n|USW00014819                |2019-09-26  |13.9                   |22.8                   |0.0              |2019-09-26 00:00:00  |4              |9        |W          |847678          |852326             |\n|USW00014819                |2006-12-09  |-4.9                   |3.3                    |0.0              |2006-12-09 00:00:00  |6              |12       |A          |586623          |948538             |\n|USW00014819                |2015-05-05  |10.6                   |15.6                   |20.1             |2015-05-05 00:00:00  |2              |5        |W          |913079          |926775             |\n|USW00014819                |2006-05-05  |8.9                    |15.6                   |0.0              |2006-05-05 00:00:00  |5              |5        |W          |1018785         |1042392            |\n|USW00014819                |2019-11-04  |3.9                    |12.2                   |0.0              |2019-11-04 00:00:00  |1              |11       |W          |842258          |354020             |\n|USW00014819                |2013-02-07  |0.0                    |2.2                    |14.5             |2013-02-07 00:00:00  |4              |2        |W          |963866          |1026678            |\n|USW00014819                |2013-08-30  |20.6                   |35.6                   |11.9             |2013-08-30 00:00:00  |5              |8        |W          |1004986         |1029901            |\n|USW00014819                |2007-04-28  |4.4                    |22.2                   |0.0              |2007-04-28 00:00:00  |6              |4        |A          |662079          |1018455            |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\n## Dropping any null values\nfrom snowflake.snowpark.functions import col, is_null\n\n# Create a filter condition for non-finite values across all columns\nnon_finite_filter = None\n\n# Iterate over all columns and update the filter condition\nfor column in joined_df.columns:\n    current_filter = is_null(col(column))\n    non_finite_filter = current_filter if non_finite_filter is None else (non_finite_filter | current_filter)\n\n# Apply the filter to the DataFrame to exclude rows with any non-finite values\ndf_filtered = joined_df.filter(~non_finite_filter)\n\n\n#Split the data into training and test sets\ntrain = df_filtered.filter(col('DATE') &lt; '2019-01-01')\ntest = df_filtered.filter(col('DATE') &gt;= '2019-01-01')\n\n\nprint (train.count())\nprint (test.count())\n\n6570\n790"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#distributed-feature-engineering-in-a-pipeline",
    "href": "Snowpark_Forecasting_Bus.html#distributed-feature-engineering-in-a-pipeline",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "4. Distributed Feature Engineering in a Pipeline",
    "text": "4. Distributed Feature Engineering in a Pipeline\nFeature engineering + XGBoost\n\nsession.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\nsession.sql(\"alter warehouse snowpark_opt_wh set max_concurrency_level = 1\").collect()\n\n[Row(status='Statement executed successfully.')]\n\n\n\n ## Distributed Preprocessing - 25X to 50X faster\nnumeric_features = ['DAY_OF_WEEK','MONTH','PREV_DAY_RIDERS','MINIMUM_TEMPERATURE','MAXIMUM_TEMPERATURE','PRECIPITATION']\nnumeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\ncategorical_cols = ['DAYTYPE']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-99999))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_cols)\n        ])\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', XGBRegressor())])"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#distributed-training",
    "href": "Snowpark_Forecasting_Bus.html#distributed-training",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "5. Distributed Training",
    "text": "5. Distributed Training\nThese operations are done inside the Snowpark warehouse which provides improved performance and scalability with distributed execution for these scikit-learn preprocessing functions and XGBoost training (and many other types of models).\n\n ## Distributed HyperParameter Optimization\nhyper_param = dict(\n        model__max_depth=[2,4],\n        model__learning_rate=[0.1,0.3],\n    )\n\nxg_model = GridSearchCV(\n    estimator=pipeline,\n    param_grid=hyper_param,\n    #cv=5,\n    input_cols=numeric_features + categorical_cols,\n    label_cols=['TOTAL_RIDERS'],\n    output_cols=[\"TOTAL_RIDERS_FORECAST\"],\n)\n\n# Fit and Score\nxg_model.fit(train)\n##Takes 25 seconds\n\n&lt;snowflake.ml.modeling.model_selection.grid_search_cv.GridSearchCV at 0x173418df0&gt;"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#model-evaluation",
    "href": "Snowpark_Forecasting_Bus.html#model-evaluation",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "6. Model Evaluation",
    "text": "6. Model Evaluation\nLook at the results of the mode. cv_results is a dictionary, where each key is a string describing one of the metrics or parameters, and the corresponding value is an array with one entry per combination of parameters\n\nsession.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'SMALL'\").collect()\n\n[Row(status='Warehouse SNOWPARK_OPT_WH successfully created.')]\n\n\n\ncv_results = xg_model.to_sklearn().cv_results_\n\nfor i in range(len(cv_results['params'])):\n    print(f\"Parameters: {cv_results['params'][i]}\")\n    print(f\"Mean Test Score: {cv_results['mean_test_score'][i]}\")\n    print()\n\nParameters: {'model__learning_rate': 0.1, 'model__max_depth': 2}\nMean Test Score: 0.927693653032996\n\nParameters: {'model__learning_rate': 0.1, 'model__max_depth': 4}\nMean Test Score: 0.9440192568004221\n\nParameters: {'model__learning_rate': 0.3, 'model__max_depth': 2}\nMean Test Score: 0.9367972284370352\n\nParameters: {'model__learning_rate': 0.3, 'model__max_depth': 4}\nMean Test Score: 0.9425057277525181\n\n\n\nLook at the accuracy of the model\n\nfrom snowflake.ml.modeling.metrics import mean_absolute_error\ntestpreds = xg_model.predict(test)\nprint('MSE:', mean_absolute_error(df=testpreds, y_true_col_names='TOTAL_RIDERS', y_pred_col_names='\"TOTAL_RIDERS_FORECAST\"'))\ntestpreds.select(\"DATEW\", \"TOTAL_RIDERS\", \"TOTAL_RIDERS_FORECAST\").show(10)         \n\nMSE: 183320.1351068038\n---------------------------------------------------------\n|\"DATEW\"     |\"TOTAL_RIDERS\"  |\"TOTAL_RIDERS_FORECAST\"  |\n---------------------------------------------------------\n|2019-11-09  |476467          |489406.65625             |\n|2019-05-31  |810422          |836847.0625              |\n|2020-12-15  |270178          |633812.25                |\n|2020-08-05  |315741          |710399.9375              |\n|2020-05-17  |118681          |347373.59375             |\n|2019-02-27  |793731          |792628.8125              |\n|2019-01-27  |257918          |286517.4375              |\n|2020-02-07  |771641          |789460.875               |\n|2020-12-06  |143231          |333279.25                |\n|2020-04-02  |213131          |656467.625               |\n---------------------------------------------------------\n\n\n\nMaterialize the results to a table\n\ntestpreds.write.save_as_table(table_name='CHICAGO_BUS_RIDES_FORECAST', mode='overwrite')\n\nUsing metrics from snowpark so calculation is done inside snowflake"
  },
  {
    "objectID": "Snowpark_Forecasting_Bus.html#save-to-the-model-registry-and-use-for-predictions-python-sql",
    "href": "Snowpark_Forecasting_Bus.html#save-to-the-model-registry-and-use-for-predictions-python-sql",
    "title": "Snowflake ML Intro Notebook - ML Forecasting",
    "section": "7. Save to the Model Registry and use for Predictions (Python & SQL)",
    "text": "7. Save to the Model Registry and use for Predictions (Python & SQL)\nConnect to the registry\n\nfrom snowflake.ml.registry import Registry\nreg = Registry(session=session, database_name=\"RAJIV\", schema_name=\"PUBLIC\")\n\n\nmodel_ref = reg.log_model(\n    model_name=\"Forecasting_Bus_Ridership\",\n    version_name=\"v37\",    \n    model=xg_model,\n    conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n    sample_input_data=train,\n    comment=\"XGBoost model, run 36, May13\"\n)\n\n/Users/rajishah/anaconda3/envs/working38/lib/python3.8/contextlib.py:113: UserWarning: `relax_version` is not set and therefore defaulted to True. Dependency version constraints relaxed from ==x.y.z to &gt;=x.y, &lt;(x+1). To use specific dependency versions for compatibility, reproducibility, etc., set `options={'relax_version': False}` when logging the model.\n  return next(self.gen)\n/Users/rajishah/anaconda3/envs/working38/lib/python3.8/site-packages/snowflake/ml/model/_packager/model_packager.py:92: UserWarning: Inferring model signature from sample input or providing model signature for Snowpark ML Modeling model is not required. Model signature will automatically be inferred during fitting. \n  handler.save_model(\n\n\n\nreg.show_models()\n\n\n\n\n\n\n\n\ncreated_on\nname\ndatabase_name\nschema_name\ncomment\nowner\ndefault_version_name\nversions\n\n\n\n\n0\n2024-01-23 18:58:41.929000-08:00\nDIABETES_XGBOOSTER\nRAJIV\nPUBLIC\nNone\nRAJIV\nV2\n[\"V2\",\"V3\",\"V4\",\"V5\",\"V7\"]\n\n\n1\n2024-02-19 17:12:27.005000-08:00\nE5_BASE_V2\nRAJIV\nPUBLIC\nNone\nRAJIV\nV1\n[\"V1\"]\n\n\n2\n2024-02-07 13:00:56.292000-08:00\nFINBERT\nRAJIV\nPUBLIC\nNone\nRAJIV\nV1\n[\"V1\"]\n\n\n3\n2024-02-26 18:55:00.548000-08:00\nFORECASTING_BUS_RIDERSHIP\nRAJIV\nPUBLIC\nNone\nRAJIV\nV7\n[\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V1...\n\n\n4\n2024-02-19 17:19:12.122000-08:00\nMINILMV2\nRAJIV\nPUBLIC\nNone\nRAJIV\nV1\n[\"V1\",\"V2\",\"V4\",\"V5\"]\n\n\n5\n2024-02-07 13:14:44.823000-08:00\nMPNET_BASE\nRAJIV\nPUBLIC\nNone\nRAJIV\nV1\n[\"V1\",\"V2\",\"V3\"]\n\n\n6\n2024-01-25 14:54:04.655000-08:00\nTPCDS_XGBOOST_DEMO\nRAJIV\nPUBLIC\nNone\nRAJIV\nV5\n[\"V5\",\"V6\",\"V7\",\"V8\",\"V9\"]\n\n\n7\n2024-01-23 18:49:09.294000-08:00\nXGBOOSTER\nRAJIV\nPUBLIC\nNone\nRAJIV\nV1\n[\"V1\",\"V2\"]\n\n\n\n\n\n\n\nLet‚Äôs retrieve the model from the registry\n\nreg_model = reg.get_model(\"Forecasting_Bus_Ridership\").version(\"v37\")\n\nLet‚Äôs do predictions inside the warehouse\n\nremote_prediction = reg_model.run(test, function_name='predict')\nremote_prediction.sort(\"DATEW\").select(\"DATEW\",\"TOTAL_RIDERS\",\"TOTAL_RIDERS_FORECAST\").show(10)\n\n---------------------------------------------------------\n|\"DATEW\"     |\"TOTAL_RIDERS\"  |\"TOTAL_RIDERS_FORECAST\"  |\n---------------------------------------------------------\n|2019-01-01  |247279          |290942.375               |\n|2019-01-02  |585996          |668251.3125              |\n|2019-01-03  |660631          |767229.875               |\n|2019-01-04  |662011          |759055.3125              |\n|2019-01-05  |440848          |491881.78125             |\n|2019-01-06  |316844          |351156.84375             |\n|2019-01-07  |717818          |762515.625               |\n|2019-01-08  |779946          |879376.0625              |\n|2019-01-09  |743021          |790567.625               |\n|2019-01-10  |743075          |764690.8125              |\n---------------------------------------------------------\n\n\n\nIf you look in the activity view, you can find the SQL which will run a bit faster. This SQL command is showing the result in a snowflake dataframe. You could use collect to pull the info out into your local session.\nModify the SQL with by adding in your specific model with this line: WITH MODEL_VERSION_ALIAS AS MODEL RAJIV.PUBLIC.DIABETES_XGBOOSTER VERSION V7 and updating the location of your target predictions which is located here: SNOWPARK_ML_MODEL_INFERENCE_INPUT\n\nsqlquery = \"\"\"SELECT \"DATEW\", \"TOTAL_RIDERS\",  CAST (\"TMP_RESULT\"['TOTAL_RIDERS_FORECAST'] AS DOUBLE) AS \"TOTAL_RIDERS_FORECAST\" FROM (WITH SNOWPARK_ML_MODEL_INFERENCE_INPUT AS (SELECT  *  FROM ( SELECT  *  FROM (( SELECT \"NOAA_WEATHER_STATION_ID\" AS \"NOAA_WEATHER_STATION_ID\", \"DATE\" AS \"DATEW\", \"MINIMUM_TEMPERATURE\" AS \"MINIMUM_TEMPERATURE\", \"MAXIMUM_TEMPERATURE\" AS \"MAXIMUM_TEMPERATURE\", \"PRECIPITATION\" AS \"PRECIPITATION\" FROM ( SELECT \"NOAA_WEATHER_STATION_ID\", \"DATE\",  CAST (\"MINIMUM_TEMPERATURE\" AS DOUBLE) AS \"MINIMUM_TEMPERATURE\",  CAST (\"MAXIMUM_TEMPERATURE\" AS DOUBLE) AS \"MAXIMUM_TEMPERATURE\",  CAST (\"PRECIPITATION\" AS DOUBLE) AS \"PRECIPITATION\" FROM CHICAGO_WEATHER)) AS SNOWPARK_LEFT INNER JOIN ( SELECT \"DATE\" AS \"DATE\", \"DAY_OF_WEEK\" AS \"DAY_OF_WEEK\", \"MONTH\" AS \"MONTH\", \"DAYTYPE\" AS \"DAYTYPE\", \"TOTAL_RIDERS\" AS \"TOTAL_RIDERS\", \"PREV_DAY_RIDERS\" AS \"PREV_DAY_RIDERS\" FROM ( SELECT \"DATE\", \"DAY_OF_WEEK\", \"MONTH\", \"DAYTYPE\", \"TOTAL_RIDERS\", LAG(\"TOTAL_RIDERS\", 1, NULL) OVER (  ORDER BY \"DATE\" ASC NULLS FIRST ) AS \"PREV_DAY_RIDERS\" FROM ( SELECT \"DATE\", \"DAY_OF_WEEK\", \"MONTH\",  LISTAGG ( DISTINCT \"DAYTYPE\", '') AS \"DAYTYPE\", sum(\"RIDES\") AS \"TOTAL_RIDERS\" FROM ( SELECT \"ROUTE\", \"DAYTYPE\", \"RIDES\", \"DATE\", dayofweek(\"DATE\") AS \"DAY_OF_WEEK\", month(\"DATE\") AS \"MONTH\" FROM ( SELECT \"ROUTE\", \"DAYTYPE\", \"RIDES\", to_timestamp(\"DATE\", 'MM/DD/YYYY') AS \"DATE\" FROM CHICAGO_BUS_RIDES)) GROUP BY \"DATE\", \"DAY_OF_WEEK\", \"MONTH\") ORDER BY \"DATE\" ASC NULLS FIRST)) AS SNOWPARK_RIGHT ON (\"DATEW\" = \"DATE\"))) WHERE (NOT ((((((((((\"NOAA_WEATHER_STATION_ID\" IS NULL OR \"DATEW\" IS NULL) OR \"MINIMUM_TEMPERATURE\" IS NULL) OR \"MAXIMUM_TEMPERATURE\" IS NULL) OR \"PRECIPITATION\" IS NULL) OR \"DATE\" IS NULL) OR \"DAY_OF_WEEK\" IS NULL) OR \"MONTH\" IS NULL) OR \"DAYTYPE\" IS NULL) OR \"TOTAL_RIDERS\" IS NULL) OR \"PREV_DAY_RIDERS\" IS NULL) AND (\"DATE\" &gt;= '2019-01-01'))),MODEL_VERSION_ALIAS AS MODEL RAJIV.PUBLIC.FORECASTING_BUS_RIDERSHIP VERSION V27\n                SELECT *,\n                    MODEL_VERSION_ALIAS!PREDICT(DAY_OF_WEEK, MONTH, PREV_DAY_RIDERS, MINIMUM_TEMPERATURE, MAXIMUM_TEMPERATURE, PRECIPITATION, DAYTYPE) AS TMP_RESULT\n                FROM SNOWPARK_ML_MODEL_INFERENCE_INPUT) ORDER BY \"DATEW\" ASC NULLS FIRST LIMIT 10\"\"\"\n\n\nresults = session.sql(sqlquery).show()\n\n---------------------------------------------------------\n|\"DATEW\"     |\"TOTAL_RIDERS\"  |\"TOTAL_RIDERS_FORECAST\"  |\n---------------------------------------------------------\n|2019-01-01  |247279          |290942.375               |\n|2019-01-02  |585996          |668251.3125              |\n|2019-01-03  |660631          |767229.875               |\n|2019-01-04  |662011          |759055.3125              |\n|2019-01-05  |440848          |491881.78125             |\n|2019-01-06  |316844          |351156.84375             |\n|2019-01-07  |717818          |762515.625               |\n|2019-01-08  |779946          |879376.0625              |\n|2019-01-09  |743021          |790567.625               |\n|2019-01-10  |743075          |764690.8125              |\n---------------------------------------------------------\n\n\n\n\ntest.show()\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"NOAA_WEATHER_STATION_ID\"  |\"DATEW\"     |\"MINIMUM_TEMPERATURE\"  |\"MAXIMUM_TEMPERATURE\"  |\"PRECIPITATION\"  |\"DATE\"               |\"DAY_OF_WEEK\"  |\"MONTH\"  |\"DAYTYPE\"  |\"TOTAL_RIDERS\"  |\"PREV_DAY_RIDERS\"  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|USW00014819                |2019-07-23  |16.7                   |27.2                   |0.0              |2019-07-23 00:00:00  |2              |7        |W          |751862          |729088             |\n|USW00014819                |2020-12-25  |-12.7                  |-5.5                   |0.0              |2020-12-25 00:00:00  |5              |12       |U          |80199           |199439             |\n|USW00014819                |2020-07-23  |18.3                   |27.2                   |0.0              |2020-07-23 00:00:00  |4              |7        |W          |312243          |303124             |\n|USW00014819                |2021-01-15  |-2.1                   |3.3                    |0.0              |2021-01-15 00:00:00  |5              |1        |W          |274858          |273087             |\n|USW00014819                |2019-06-05  |15.6                   |30.0                   |9.7              |2019-06-05 00:00:00  |3              |6        |W          |814691          |794543             |\n|USW00014819                |2019-05-08  |8.3                    |24.4                   |9.7              |2019-05-08 00:00:00  |3              |5        |W          |820018          |802783             |\n|USW00014819                |2021-01-17  |-2.1                   |1.1                    |0.5              |2021-01-17 00:00:00  |0              |1        |U          |141354          |190486             |\n|USW00014819                |2019-12-09  |-3.8                   |9.4                    |0.5              |2019-12-09 00:00:00  |1              |12       |W          |780897          |341326             |\n|USW00014819                |2020-07-30  |22.2                   |28.3                   |0.0              |2020-07-30 00:00:00  |4              |7        |W          |304656          |302637             |\n|USW00014819                |2019-05-23  |17.8                   |24.4                   |0.5              |2019-05-23 00:00:00  |4              |5        |W          |799367          |805534             |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\nLet‚Äôs calculate and save the metrics to the registry\n\nfrom snowflake.ml.modeling.metrics import mean_absolute_error\ntestpreds = reg_model.run(test, function_name='predict')\nmae = mean_absolute_error(df=testpreds, y_true_col_names='TOTAL_RIDERS', y_pred_col_names='\"TOTAL_RIDERS_FORECAST\"')\nreg_model.set_metric(\"MAE\", value=mae)\n\n\nreg_model.show_metrics()\n\n{'MAE': 183320.1351068038}\n\n\n\nsession.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'SMALL'\").collect()\n\n[Row(status='Warehouse SNOWPARK_OPT_WH successfully created.')]\n\n\n\n#session.close()"
  },
  {
    "objectID": "xgbfi.html",
    "href": "xgbfi.html",
    "title": "Using xgbfi for revealing feature interactions",
    "section": "",
    "text": "Introduction\nTree based methods excel in using feature or variable interactions. As a tree is built, it picks up on the interaction of features. For example, buying ice cream may not be affected by having extra money unless the weather is hot. It is the interaction of both of these features that can affect whether ice cream will be consumed.\nThe traditional manner for examining interactions is relying on measures of variable importance. However, these measures don‚Äôt provide insights into second or third order interactions. Identifying these interactions are important in building better models, especially when finding features to use within linear models.\nIn this post, I show how to find higher order interactions using XGBoost Feature Interactions & Importance. This tool has been available for a while, but outside of kagglers, it has received relatively little attention.\nAs a starting point, I used the Ice Cream dataset to illustrate using xgbfi. This walkthrough is in R, but python instructions are also available at the repo. I am going to break the code into three sections, the initial build of the model, exporting the files necessary for xgbfi, and running xgbi.\n\nBuilding the model\nLets start by loading the data:\nlibrary(xgboost)\nlibrary(Ecdat)\ndata(Icecream)\ntrain.data &lt;- data.matrix(Icecream[,-1])\nThe next step is running xgboost:\nbst &lt;- xgboost(data = train.data, label = Icecream$cons, max.depth = 3, eta = 1, nthread = 2, nround = 2, objective = \"reg:linear\")\nTo better understand how the model is working, lets go ahead and look at the trees:\nxgb.plot.tree(feature_names = names((Icecream[,-1])), model = bst)\n\n\n\nxg tree plot\n\n\nThe results here line up with our intution. Hot days seems to be the biggest variable by just eyeing the plot. This lines up with the results of a variable importance calculation:\n&gt; xgb.importance(colnames(train.data, do.NULL = TRUE, prefix = \"col\"), model = bst)\n   Feature       Gain      Cover Frequency\n1:    temp 0.75047187 0.66896552 0.4444444\n2:  income 0.18846270 0.27586207 0.4444444\n3:   price 0.06106542 0.05517241 0.1111111\nAll of this should be very familiar to anyone who has used decision trees for modeling. But what are the second order interactions? Third order interactions? Can you rank them?\n\n\nExporting the tree\nThe next step involves saving the tree and moving it outside of R so xgbfi can parse the tree. The code below will help to create two files that are needed:xgb.dump and fmap.text.\nfeatureList &lt;- names(Icecream[,-1])\nfeatureVector &lt;- c() \nfor (i in 1:length(featureList)) { \n  featureVector[i] &lt;- paste(i-1, featureList[i], \"q\", sep=\"\\t\") \n}\nwrite.table(featureVector, \"fmap.txt\", row.names=FALSE, quote = FALSE, col.names = FALSE)\nxgb.dump(model = bst, fname = 'xgb.dump', fmap = \"fmap.txt\", with.stats = TRUE)\n\n\nRunning xgbfi\nThe first step is to clone the xgbfi repository onto your computer. Then copy the files xgb.dump and fmap.text to the bin directory.\nGo to your terminal or command line and run: XgbFeatureInteractions.exe application. On a mac, download mono and then run the command: mono XgbFeatureInteractions.exe. There is also a XgbFeatureInteractions.exe.config file that contains configuration settings in the bin directory.\nAfter the application runs, it will write out an excel spreadsheet titled: XgbFeatureInteractions.xlsx. This spreadsheet has the good stuff! Open up the spreadsheet and you should see:\n\n\n\ninteraction depth 0\n\n\nThis tab of the spreadsheet shows the first order interactions. These results are similar to what variable importance showed. The good stuff is when you click on the tab for Interaction Depth 1 or Interaction Depth 2.\n\n\n\ninteraction depth 1\n\n\n\n\n\ninteraction depth 2\n\n\nIt is now possible to rank the higher order interactions. With the simple dataset, you can see that the results out of xgbfi match what is happening in the tree. The real value of this tool is for much larger datasets, where its difficult to examine the trees for the interactions."
  },
  {
    "objectID": "standup.html",
    "href": "standup.html",
    "title": "Stand Up for Best Practices",
    "section": "",
    "text": "Source: Yuriy Guts selection from Shutterstock\n\nStand Up for Best Practices:\n\n\nMisuse of Deep Learning in Nature‚Äôs Earthquake Aftershock Paper\n\n\nThe Dangers of Machine Learning Hype\nPractitioners of AI, machine learning, predictive modeling, and data science have grown enormously over the last few years. What was once a niche field defined by its blend of knowledge is becoming a rapidly growing profession. As the excitement around AI continues to grow, the new wave of ML augmentation, automation, and GUI tools will lead to even more growth in the number of people trying to build predictive models.\nBut here‚Äôs the rub: While it becomes easier to use the tools of predictive modeling, predictive modeling knowledge is not yet a widespread commodity. Errors can be counterintuitive and subtle, and they can easily lead you to the wrong conclusions if you‚Äôre not careful.\nI‚Äôm a data scientist who works with dozens of expert data science teams for a living. In my day job, I see these teams striving to build high-quality models. The best teams work together to review their models to detect problems. There are many hard-to-detect-ways that lead to problematic models (say, by allowing target leakage into their training data).\nIdentifying issues is not fun. This requires admitting that exciting results are ‚Äútoo good to be true‚Äù or that their methods were not the right approach. In other words, it‚Äôs less about the sexy data science hype that gets headlines and more about a rigorous scientific discipline.\n\n\nBad Methods Create Bad Results\nAlmost a year ago, I read an article in Nature that claimed unprecedented accuracy in predicting earthquake aftershocks by using deep learning. Reading the article, my internal radar became deeply suspicious of their results. Their methods simply didn‚Äôt carry many of the hallmarks of careful predicting modeling.\nI started to dig deeper. In the meantime, this article blew up and became widely recognized! It was even included in the release notes for Tensorflow as an example of what deep learning could do. However, in my digging, I found major flaws in the paper. Namely, data leakage which leads to unrealistic accuracy scores and a lack of attention to model selection (you don‚Äôt build a 6 layer neural network when a simpler model provides the same level of accuracy).\nThe testing dataset had a much higher AUC than the training set . . . this is not normal\nTo my earlier point: these are subtle, but incredibly basic predictive modeling errors that can invalidate the entire results of an experiment. Data scientists are trained to recognize and avoid these issues in their work. I assumed that this was simply overlooked by the author, so I contacted her and let her know so that she could improve her analysis. Although we had previously communicated, she did not respond to my email over concerns with the paper.\n\n\nFalling On Deaf Ears\nSo, what was I to do? My coworkers told me to just tweet it and let it go, but I wanted to stand up for good modeling practices. I thought reason and best practices would prevail, so I started a 6-month process of writing up my results and shared them with Nature.\nUpon sharing my results, I received a note from Nature in January 2019 that despite serious concerns about data leakage and model selection that invalidate their experiment, they saw no need to correct the errors, because ‚ÄúDevries et al.¬†are concerned primarily with using machine learning as [a] tool to extract insight into the natural world, and not with details of the algorithm design‚Äù. The authors provided a much harsher response.\nYou can read the entire exchange on my github.\nIt‚Äôs not enough to say that I was disappointed. This was a major paper (it‚Äôs Nature!) that bought into AI hype and published a paper despite it using flawed methods.\nThen, just this week, I ran across articles by Arnaud Mignan and Marco Broccardo on shortcomings that they found in the aftershocks article. Here are two more data scientists with expertise in earthquake analysis who also noticed flaws in the paper. I also have placed my analysis and reproducible code on github.\nGo run the analysis yourself and see the issue\n\n\nStanding Up For Predictive Modeling Methods\nI want to make it clear: my goal is not to villainize the authors of the aftershocks paper. I don‚Äôt believe that they were malicious, and I think that they would argue their goal was to just show how machine learning could be applied to aftershocks. Devries is an accomplished earthquake scientist who wanted to use the latest methods for her field of study and found exciting results from it.\nBut here‚Äôs the problem: their insights and results were based on fundamentally flawed methods. It‚Äôs not enough to say, ‚ÄúThis isn‚Äôt a machine learning paper, it‚Äôs an earthquake paper.‚Äù If you use predictive modeling, then the quality of your results are determined by the quality of your modeling. Your work becomes data science work, and you are on the hook for your scientific rigor.\nThere is a huge appetite for papers that use the latest technologies and approaches. It becomes very difficult to push back on these papers.\nBut if we allow papers or projects with fundamental issues to advance, it hurts all of us. It undermines the field of predictive modeling.\nPlease push back on bad data science. Report bad findings to papers. And if they don‚Äôt take action, go to twitter, post about it, share your results and make noise. This type of collective action worked to raise awareness of p-values and combat the epidemic of p-hacking. We need good machine learning practices if we want our field to continue to grow and maintain credibility.\nAcknowledgments: I want to thank all the great data scientists at DataRobot that collaborated and supported me this past year, a few of these include: Lukas Innig, Amanda Schierz, Jett Oristaglio, Thomas Stearns, and Taylor Larkin.\nThis article was orignally posted on Medium and featured on Reddit"
  },
  {
    "objectID": "setfit.html",
    "href": "setfit.html",
    "title": "Few shot text classification with SetFit",
    "section": "",
    "text": "SetFit\n\n\n\nIntroduction\nData scientists often do not have large amounts of labeled data. This issue is even graver when dealing with problems with tens or hundreds of classes. The reality is very few text classification problems get to the point where adding more labeled data isn‚Äôt improving performance.\nSetFit offers a few-shot learning approach for text classification. The paper‚Äôs results show across many datasets, it‚Äôs possible to get better performance with less labeled data. This technique uses contrastive learning to build a larger dataset for fine-tuning a text classification model. This approach was new to me and was why I did a video explaining how contrastive learning helps with text classification.\nI have created a Colab üìì companion notebook at https://bit.ly/raj_setfit, and the Youtube üé• video that provides a detailed explanation. I walk through a simple churn example to give the intuition behind SetFit. The notebook trains the CR (customer review dataset) highlighted in the SetFit paper.\nThe SetFit github contains the code, and a great deep dive for text classification is found on Philipp‚Äôs blog. For those looking to productionize a SetFit model, Philipp has also documented how to create the Hugging Face endpoint for a SetFit model.\nSo grab your favorite text classification dataset and give it a try!"
  },
  {
    "objectID": "running-code-failing-models.html",
    "href": "running-code-failing-models.html",
    "title": "Running Code and Failing Models",
    "section": "",
    "text": "Source: Yuriy Guts selection from Shutterstock\nMachine learning is a glass cannon. When used correctly, it can be a truly transformative technology, but just a small oversight can cause it to become misleading and even actively harmful. Even if all the code runs and the model seems to be spitting out reasonable answers, it‚Äôs possible for a model to encode fundamental data science mistakes that invalidate its results. These errors might seem small, but the effects can be disastrous when the model is used to make decisions in the real world.\nThe promise and power of AI lead many researchers to gloss over the ways in which things can go wrong when building and operationalizing machine learning models. As a data scientist, one of my passions is to reproduce research papers as a learning exercise. Along the way, I have uncovered cases where the research was published with faulty methodologies. My hope is that this analysis can increase awareness about data science mistakes and raise the standards for machine learning in research. For example, last year I shared an analysis of a project by Harvard and Google researchers that contained fundamental errors. The researchers refused to fix their mistake even when confronted with it directly.\nOver the holidays, I used DataRobot to reproduce a few machine learning benchmarks. I found many examples of machine learning code that ran without errors but that were built using flawed data science practices. The examples I share in this post come from the world‚Äôs best data scientists and affect hundreds of peer-reviewed research publications. As these examples show, errors in machine learning can be subtle. The key to finding these errors is to work with a tool that offers guardrails and insights along the way."
  },
  {
    "objectID": "running-code-failing-models.html#target-leakage-in-a-fast.ai-example",
    "href": "running-code-failing-models.html#target-leakage-in-a-fast.ai-example",
    "title": "Running Code and Failing Models",
    "section": "Target Leakage in a fast.ai Example",
    "text": "Target Leakage in a fast.ai Example\nDeep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD by Jeremy Howard and Sylvain Gugger is a hands-on guide that helps people with little math background understand and use deep learning quickly. In the section about tabular datasets, the authors use the Blue Book for Bulldozers problem, the goal of which is to predict the sale price for heavy equipment at auction. I tried to replicate their machine learning model and wasn‚Äôt able to beat their model‚Äôs predictive performance, which piqued my interest.\nAfter carefully inspecting their code, I found a mistake in their validation dataset. Their code attempted to create a validation test set based on a prediction point of November 1, 2011. The goal was to split the data at this point so that you could train on the data known at prediction time. The performance of the model is then analyzed on a test set, which is located after the prediction point. Unfortunately, the code was not written correctly; there was contamination from the future in the training data.\n\n\n\nLeakage.png\n\n\nThe code below might at first look like it separates data before and after November 1, 2011, but there‚Äôs a subtle mistake that includes future dates. The use of information in the model training process that would not be expected at prediction time is known as target leakage, and it led to an over-optimistic accuracy. Because I used DataRobot, which requires and validates a date when creating a validation dataset based on time, I was able to find the mistake in the fast.ai book.\nAfter the target leakage was fixed, the fast.ai scores dropped, and I was able to reproduce the results outside of fast.ai. This simple coding mistake led to a notebook and model that appeared valid. If this model were put into production, the results would have been much worse on new data. After I identified this issue, Jeremy Howard agreed to add a note in the course materials.\n\n\n\nfastai2.png"
  },
  {
    "objectID": "running-code-failing-models.html#sarcos-dataset-failure",
    "href": "running-code-failing-models.html#sarcos-dataset-failure",
    "title": "Running Code and Failing Models",
    "section": "SARCOS Dataset Failure",
    "text": "SARCOS Dataset Failure\nThe SARCOS dataset is a widely used benchmark dataset in machine learning. Based on predicting the movement of a robotic arm, SARCOS appears in more than one hundred academic papers. I tested this dataset because it appears in various benchmarks by Google and fast.ai.\nThe SARCOS dataset is broken into two parts: a training dataset (sarcos_inv) and a test dataset (sarcos_inv_test). Following common data science practices, DataRobot broke the SARCOS training set into a training partition and a validation partition. I treated the SARCOS test set (sarcos_inv_test) as a holdout. When I looked at the results, I immediately noticed something suspicious. Do you see it?\n\n\n\nsarcos3.png\n\n\nThe large drop between the validation score and the holdout score indicates that something is very different between the validation and holdout datasets. When I examined the holdout dataset (the SARCOS test set), I found that every row in the test set was in the training data too. After some investigation, I discovered that the holdout dataset was built out of the training dataset. Of the 4,449 examples in the test set, 4,445 examples are present in the training set, too. The target leakage here is significant. By overfitting or memorizing the training dataset, it‚Äôs possible to get perfect results on the test set. Overfitting, a well-known issue in machine learning, is illustrated in the following figure. The test dataset should have used out-of-sample testing to prevent overfitting.\n\n\n\noverfit4.png\n\n\nTarget leakage helped to explain the very low scores of the deep learning models. For comparison, a random forest model achieves 2.38 mean squared error (MSE), while a deep learning model overfits and produces 0.038 MSE. Judging from the suspiciously large difference between the models, it appears that the deep learning model just memorized the training data, which is why it had such low error.\nThe consequences of this target leakage are far-reaching. More than one hundred journal articles relied on this dataset. Thousands of data scientists have used it to benchmark their machine learning code. Researcher Kai Arulkumaran has already acknowledged this issue and now the research community is dealing with the ramifications of the target leakage.\nWhy wasn‚Äôt this error discovered earlier? When I reproduced the SARCOS benchmarks, I used a tool that includes technical safeguards for proper validation splits and provides transparency in the display of the results of each split. DataRobot‚Äôs AutoML was designed by data scientists to prevent these sorts of issues. In contrast, working within code, it was quite easy to overlook this fundamental issue. After all, thousands of data scientists have rerun their code and published their results without a second thought."
  },
  {
    "objectID": "running-code-failing-models.html#poker-hand-dataset",
    "href": "running-code-failing-models.html#poker-hand-dataset",
    "title": "Running Code and Failing Models",
    "section": "Poker Hand Dataset",
    "text": "Poker Hand Dataset\nThe Poker Hand dataset is another widely used benchmark dataset in machine learning. It‚Äôs used to predict poker hands (for example, a full house from five cards). The fast.ai and Google benchmarks for this model use the accuracy metric. Accuracy is a measurement for assessing the predictive performance of a model (basically, the percentage of predictions that are correct). Although it‚Äôs easy to get running code with the accuracy metric, it‚Äôs not good data science practice for this problem.\nWhen DataRobot builds a model with the Poker Hand dataset, by default, it uses log loss as an optimization metric. Log loss is a measure of error for a model. At DataRobot, we believe that it isn‚Äôt good practice to use accuracy as your metric on a classification project with imbalanced classes. With imbalanced data, you can easily build a highly accurate model that‚Äôs useless.\nTo understand why accuracy isn‚Äôt the best metric when classifying unbalanced data, consider the following figure. Minesweeper is a popular game where the goal is to identify a few mines that are scattered across a board. Because there are a lot of squares with no mines, you could generate a very accurate model just by predicting that every square is safe. Although a 99% accurate model for Minesweeper sounds impressive, it‚Äôs not very useful.\n\n\n\nminesweeper5.png\n\n\nAutomated feature selection in DataRobot provides a more parsimonious featurelist. In the Poker Hand dataset, DataRobot created a DR Reduced Features list with only six features. The starting feature list for this dataset, Cat+Cont, contained 15 features. The leaderboard below shows that the simpler DR Reduced Features list performs better than the full Cat+Cont feature list. The model below was optimized on log loss, but I am viewing the accuracy metrics for comparison to the existing benchmarks.\n\n\n\nDRreduce6.png"
  },
  {
    "objectID": "running-code-failing-models.html#conclusion",
    "href": "running-code-failing-models.html#conclusion",
    "title": "Running Code and Failing Models",
    "section": "Conclusion",
    "text": "Conclusion\nI have shared simple examples of how data scientists can have running code, but failed models. After spending a week going through a half dozen datasets, I am even more convinced that automation with technical safeguards is a required part of building trusted AI. The mistakes I‚Äôve shared here are not isolated incidents.\nThe issues go beyond the reproducibility crisis for machine learning research. It‚Äôs a great first step for researchers to publish their code and make the data available, but as these examples show, sharing code isn‚Äôt enough to validate models. So, what should you do about this?\nIn regulated industries, there are processes in place to validate running code (for example, building a challenger model using a different technical framework). For its safeguards and transparency, many organizations use DataRobot to validate models. Just rereading or rerunning a project isn‚Äôt enough to identify errors."
  },
  {
    "objectID": "running-code-failing-models.html#links",
    "href": "running-code-failing-models.html#links",
    "title": "Running Code and Failing Models",
    "section": "Links",
    "text": "Links\n\nStand Up for Best Practices (Harvard Leakage)\nFast.AI Issue\nSARCOS"
  },
  {
    "objectID": "outlier_app.html",
    "href": "outlier_app.html",
    "title": "Outlier App",
    "section": "",
    "text": "algorithms\n\n\n\nIntroduction\nI was recently trying various outlier detection algorithms. For me, the best way to understand an algorithm is to tinker with it. I wanted to share my recent work on a shiny app that allows you to play around with various outlier algorithms.\nThe shiny app is available on my site, but even better, the code is on github for you to run locally or improve! I also posted a video that provides background on the app. Let me give you a quick tour of the app:\n\n\nAlgorithms\nThe available algorithms include:\n\nHierarchical Clustering (DMwR)\nKmeans (distance metrics from proxy)\n\nKmeans Euclidean Distance\nKmeans Mahalanobis\nKmeans Manhattan\n\nFuzzy kmeans (all from fclust)\n\nFuzzy kmeans - Gustafson and Kessel\nFuzzy k-medoids\nFuzzy k-means with polynomial fuzzifier\n\nLocal Outlier Factor (dbscan)\nRandomForest (proximity from randomForest)\n\nIsolation Forest (IsolationForest)\n\nAutoencoder (Autoencoder)\nFBOD and SOD (HighDimOut)\n\n\n\n\nalgorithms\n\n\n\n\nDatasets\nThere are also a wide range of datasets to try as well:\n\n\n\ndatasets\n\n\nOnce the data is loaded, you can start exploring. One thing you can do is look at the effect scaling can have. In this example, you can see how outliers differ when scaling is used. The values on the far right no longer dominate the distance measurements, and there are now outliers from other areas:\n\n\n\nscaling\n\n\nBy trying different algorithms, you can see how different algorithms will select outliers. In this case, you see a difference between the outliers selected using an autoencoder versus isolation forest.\nAnother example here is the difference between kmeans and fuzzy kmeans as show below:\n\n\n\nfuzzy\n\n\nA density based algorithm can also select different outliers versus a distance based algorithm. This example nicely shows the difference between kmeans and lof (local outlier factor from dbscan)\n\n\n\ndensity\n\n\nAn important part of using this visualization is studying the distance numbers that are calculated. Are these numbers meshing with your intuition? How big of a quantitative difference is there between outliers and other points?\n\n\n\noutlier_table\n\n\nSo that is the 2D app. Please send me bug fixes, additional algorithms, or tighter code!\n3D+ App?\nThe next thing is whether to expand this to larger datasets. This is something that you would run locally (large datasets take too long to run for my shiny server). The downside of larger datasets is that it gets tricker to visualize them. For now, I am using a TSNE plot. I am open to suggestions, but the intent here is a way to evaluate outlier algorithms on a variety of datasets.\n\n\n\ndatasets"
  },
  {
    "objectID": "openai_mod.html",
    "href": "openai_mod.html",
    "title": "Building Worlds for Reinforcement Learning",
    "section": "",
    "text": "Introduction\nOpenAI‚Äôs Gym places reinforcement learning into the masses. It comes with a wealth of environments from the classic cart pole, board games, Atari, and now the new Universe which adds flash games and PC Games like GTA or Portal. This is great news, but for someone starting out, working on some of these games is overkill. You can learn a lot more in a shorter time, by playing around with some smaller toy environments.\nOne area I like within the gym environments are the classic control problems (besides the fun of eating melon and poop). These are great problems for understanding the basics of reinforcement learning because we intutiively understand the rewards and they run really fast. Its not like pong that can take several days to train, instead, you can train these environments within minutes!\nIf you aren‚Äôt happy with the current environments, it is possible to modify and even add more environments. In this post, I will highlight other environments and share how I modified an Acrobot-v1 environment.\n\n\nRLPy\nTo begin, grab the repo for the OpenAI gym. Inside the repo, navigate to gym/envs/classic_controlwhere you will see the scripts that define the class control environments. If you open one of the scripts, you will see a heading on the top that says:\n__copyright__ = \"Copyright 2013, RLPy http://acl.mit.edu/RLPy\"\nAhh! In the spirit of open source, OpenAI stands on the shoulders of another reinforcement library, RLPy. You can learn a lot more about them at the RLPy site or take a look at their github. If you browse here, you can find the original script that was used in OpenAI under rlpy /rlpy/Domains. The interesting thing here is that there are a ton more interesting reinforcement problems!\n\n\n\nRLlisting\n\n\nYou can run these using RLPy or you can try and hack this into OpenAI.\n\n\nModifying OpenAI Environments\nI decided to modify the Acrobot environment. Acrobot is a 2-link pendulum with only the second joint actuated (it has three states, left, right, and no movement). The goal is to swing the end to a height of at least one link above the base. If you look at the leaderboard on OpenAIs site, they meet that criterion, but its not very impressive. Here is the current highest scoring entry:\n\n\n\ntraining\n\n\nThis is way boring compared to what Hardmaru shows in his demo, where a pendulum is capable of balancing for a short time.\nSo I decided to try and modify the Acrobot demo to make this task a little more interesting, Acrobot gist here. The main change was to the reward system. I added a variable steps_beyond_done that would keep track of successes when the end was swung high. I also changed the reward structure, so it would gradually be rewarded as it swung higher. I also changed g to 0, this removes gravity‚Äôs effect.\nself.rewardx = (-np.cos(s[0]) - np.cos(s[1] + s[0])) ##Swung height is calculated \nif self.rewardx &lt; .5:\n    reward = -1.\n    self.steps_beyond_done = 0\nif (self.rewardx &gt; .5 and self.rewardx &lt; .8):\n    reward = -0.8\n    self.steps_beyond_done = 0  \nif self.rewardx &gt; .8:\n    reward = -0.6 \nif self.rewardx &gt; 1:\n    reward = -0.4\n    self.steps_beyond_done += 1 \nif self.steps_beyond_done &gt; 4:\n    reward = -0.2\nif self.steps_beyond_done &gt; 8:\n    reward = -0.1\nif self.steps_beyond_done &gt; 12:\n    reward = 0.\nAnother important file to be aware of is where the benchmarks are kept for each environment. You can navigate to this at gym/gym/benchmarks/__init__.pyWithin this file, you will see the following:\n{'env_id': 'Acrobot-v1',\n         'trials': 3,\n         'max_timesteps': 100000,\n         'reward_floor': -500.0,\n         'reward_ceiling': 0.0,\n        },\nI then ran an implementation of Asynchronous Advantage Actor Critic A3C) by Arno Moonens. After running for a half hour, you can see the improvement in the algorithm:\n\n\n\ntraining1\n\n\nNow a half hour later:\n\n\n\ntraining2\n\n\nThe result is teaching the pendulum to stay up for an extended time! This is much more interesting and what I was looking for. I hope this will inspire others to build new and interesting environments."
  },
  {
    "objectID": "deeplearningR.html",
    "href": "deeplearningR.html",
    "title": "Deep Learning with R",
    "section": "",
    "text": "For R users, there hasn‚Äôt been a production grade solution for deep learning (sorry MXNET). This post introduces the Keras interface for R and how it can be used to perform image classification. The post ends by providing some code snippets that show Keras is intuitive and powerful üí™üèΩ.\n\nTensorflow\nLast January, Tensorflow for R was released, which provided access to the Tensorflow API from R. This was signficant, as Tensorflow is the most popular library for deep learning. However, for most R users, the Tensorflow for R interface was not very R like. ü§¢ Take a look at this code chunk for training a model:\ncross_entropy &lt;- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))\ntrain_step &lt;- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)\ncorrect_prediction &lt;- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))\naccuracy &lt;- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))\nsess$run(tf$global_variables_initializer())\n\nfor (i in 1:20000) {\n  batch &lt;- mnist$train$next_batch(50L)\n  if (i %% 100 == 0) {\n    train_accuracy &lt;- accuracy$eval(feed_dict = dict(\n        x = batch[[1]], y_ = batch[[2]], keep_prob = 1.0))\n    cat(sprintf(\"step %d, training accuracy %g\\n\", i, train_accuracy))\n  }\n  train_step$run(feed_dict = dict(\n    x = batch[[1]], y_ = batch[[2]], keep_prob = 0.5))\n}\n\ntest_accuracy &lt;- accuracy$eval(feed_dict = dict(\n     x = mnist$test$images, y_ = mnist$test$labels, keep_prob = 1.0))\ncat(sprintf(\"test accuracy %g\", test_accuracy))\nYikes!\nUnless you are familiar with tensorflow, it‚Äôs not readily apparent what is going on. A quick search on Github finds less than a 100 code results using tensorflow for R. üòî\n\n\nKeras\nAll this is going to change with Keras and R! ‚ò∫Ô∏è\nFor background, Keras is a high-level neural network API that is designed for experimentation and can run on top of Tensorflow. Keras is what data scientists like to use. ü§ì Keras has grown in popularity and supported on a wide set of platforms including Tensorflow, CNTK, Apple‚Äôs CoreML, and Theano. It is becoming the de factor language for deep learning.\nAs a simple example, here is the code to train a model in Keras:\nmodel_top %&gt;% fit(\n        x = train_x, y = train_y,\n        epochs=epochs, \n        batch_size=batch_size,\n        validation_data=valid)\n\n\nImage Classification with Keras\nSo if you are still with me, let me show you how to build deep learning models using R, Keras, and Tensorflow together. You will find a Github repo at https://github.com/rajshah4/image_keras/ that contains the code and data you will need. Included is an R notebook (and Python notebooks) that walks through building an image classifier (telling üê± from üê∂), but can easily be generalized to other images. The walk through includes advanced methods that are commonly used for production deep learning work including:\n\naugmenting data\nusing the bottleneck features of a pre-trained network\nfine-tuning the top layers of a pre-trained network\nsaving weights of models\n\n\n\nCode Snippets of Keras\nThe R interface to Keras truly makes it easy to build deep learning models in R. Here are some code snippets based on my example of building an image classifier to illustrate how intuitive and useful Keras for R is:\nTo load üñº from a folder:\ntrain_generator &lt;- flow_images_from_directory(train_directory, generator = image_data_generator(), target_size = c(img_width, img_height), color_mode = \"rgb\",\n  class_mode = \"binary\", batch_size = batch_size, shuffle = TRUE,\n  seed = 123)\nTo define a simple convolutional neural network:\nmodel &lt;- keras_model_sequential()\n\nmodel %&gt;%\n  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, 3)) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% \n  \n  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n  \n  layer_conv_2d(filter = 64, kernel_size = c(3,3)) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n  \n  layer_flatten() %&gt;%\n  layer_dense(64) %&gt;%\n  layer_activation(\"relu\") %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1) %&gt;%\n  layer_activation(\"sigmoid\")\nTo augment data:\naugment &lt;- image_data_generator(rescale=1./255,\n                               shear_range=0.2,\n                               zoom_range=0.2,\n                               horizontal_flip=TRUE)\nTo load a pretrained network:\nmodel_vgg &lt;- application_vgg16(include_top = FALSE, weights = \"imagenet\")\nTo save model weights:\nsave_model_weights_hdf5(model_ft, 'finetuning_30epochs_vggR.h5', overwrite = TRUE)\nThe Keras for R interface makes it much easier for R users to build and refine deep learning models. Its no longer necessary to force everyone to use Python to build, refine, and test deep learning models. I really think this will open up deep learning to a wider audience that was a bit apprehensive on using python.\nTo start with, you can grab my repo, fire up RStudio (or your IDE of choice), and go build a simple classifier using Keras. There are also a wealth of other examples such as generating text from Nietzsche‚Äôs writings, deep dreaming, or creating a variational encoder.\nSo for now, give it a spin!\nAn earlier version of this post was posted at Datascience+."
  },
  {
    "objectID": "QuickDraw.html",
    "href": "QuickDraw.html",
    "title": "Using Google‚Äôs Quickdraw to create an MNIST style dataset!",
    "section": "",
    "text": "https://www.tensorflow.org/images/MNIST.png\n\n\n\nIntroduction\nFor those running deep learning models, MNIST is ubiquotuous. This dataset of handwritten digits serves many purposes from benchmarking numerous algorithms (its referenced in thousands of papers) and as a visualization, its even more prevelant than Napoleon‚Äôs 1812 March. The digits look like this:\nThere are many reasons for its enduring use, but much of it is the lack of an alternative. In this post, I want to introduce an alternative, the Google QuickDraw dataset. The quickdraw dataset was captured in 2017 by Google‚Äôs drawing game, Quick, Draw!. The dataset consists of 50 million drawings across 345 categories. The drawings look like this:\n\n\n\nhttps://github.com/googlecreativelab/quickdraw-dataset/blob/master/preview.jpg\n\n\n\n\nBuild your own Quickdraw dataset\nI want to walk through how you can use this drawings and create your own MNIST like dataset. Google has made available 28x28 grayscale bitmap files of each drawing. These can serve as drop in replacements for the MNIST 28x28 grayscale bitmap images.\nAs a starting point, Google has graciously made the dataset publicly available with documentation on the dataset. All the data is sitting in Google‚Äôs Cloud Console, but for the images, you want this link of the numpy_bitmaps.\n\n\n\nGCP2\n\n\nYou should arrive on a page that allows you to download all the images for any category. So this is when you have fun! Go ahead and pick your own categories. I started with eyeglasses, face, pencil, and television. As I learned from the face, the drawings that have fine points can be more difficult to learn. But you should play around and pick fun categories.\n\n\n\nshortqd\n\n\nThe next challenge is taking these .npy files and using them. Here is a short python gist that I used to read the .npy files and combine them to create a 80,000 images dataset that I could use in place of MNIST. They are saved in a hdf5 format that is cross platform and often used in deep learning.\n\n\n\ngist\n\n\n\n\nUsing Quickdraw instead of MNIST\nThe next thing is to go have fun with it. I used this dataset in place of MNIST for some work playing around with autoencoders in Python from the Keras tutorials. The below picture represents the original images at the top and reconstructed ones at the bottom, using an autoencoder.\n\n\n\nvae10\n\n\nI next used this dataset with a variational autoencoder in R. Here is the code snippet to import the data:\nlibrary(rhdf5)\nx_test &lt;- t(h5read(\"x_test.h5\", \"name-of-dataset\"))\nx_train &lt;- t(h5read(\"x_train.h5\", \"name-of-dataset\"))\ny_test &lt;- (h5read(\"y_test.h5\", \"name-of-dataset\"))\ny_train &lt;- (h5read(\"y_train.h5\", \"name-of-dataset\"))\nHere is a visualization of its latent space using my custom quickdraw dataset. For me, this was a nice fresh alternative to always staring at the MNIST dataset. So next time you see MNIST listed . . . go build your own!\n\n\n\nVAE15"
  },
  {
    "objectID": "HF-Endpoint.html",
    "href": "HF-Endpoint.html",
    "title": "Text style transfer in a spreadsheet using Hugging Face Inference Endpoints",
    "section": "",
    "text": "SetFit\n\n\n\nIntroduction\nWe change our conversational style from informal to formal speech. We often do this without thinking when talking to our friends compared to addressing a judge. Computers now have this capability! I use textual style transfer in this post to convert informal text to formal text. To make this easy to use, we do it in a spreadsheet.\n\n\nStep 1\nThe first step is identifying an informal to formal text style model. Next, we deploy the model using Hugging Face Inference endpoints. Inference endpoints is a production-grade solution for model deployment.\n\n\n\nStep 2\nLet‚Äôs incorporate the endpoint into Google Sheets custom function to make the model easy to use.\n\nI added the code to Google Sheets through the Apps Script extension. Grab it here as a gist. Once that is saved, you can use the new function as a formula. Now, I can use one simple command if I want to do textual style transfer!\n\n\n\nAlt Text\n\n\n\n\nResources\nI created a Youtube üé• video for a more detailed walkthrough.\nGo try this out with your favorite model! For another example, check out the positive style textual model in a Tik Tok video."
  },
  {
    "objectID": "DADC.html",
    "href": "DADC.html",
    "title": "Dynamic Adversarial Data Collection",
    "section": "",
    "text": "img\n\n\nAre you looking for better training data for your models? Let me tell you about dynamic adversarial data collection!\nI had a large enterprise customer asking me to incorporate this workflow into a Hugging Face private hub demo. Here are some resources I found useful: Chris Emezue put together a blog post: ‚ÄúHow to train your model dynamically using adversarial data‚Äù and a real-life example using MNIST using Spaces.\nIf you want an academic paper that details this process, check out: Analyzing Dynamic Adversarial Training Data in the Limit. By using this approach, this paper found models made 26% fewer errors on the expert-curated test set.\nAnd if you prefer a video‚Ää‚Äî‚Äächeck out my Tik Tok:\nhttps://www.tiktok.com/@rajistics/video/7123667796453592366?is_from_webapp=1&sender_device=pc&web_id=7106277315414181422"
  },
  {
    "objectID": "rag-agentic-world.html",
    "href": "rag-agentic-world.html",
    "title": "From Vectors to Agents: Managing RAG in an Agentic World",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "rag-agentic-world.html#video",
    "href": "rag-agentic-world.html#video",
    "title": "From Vectors to Agents: Managing RAG in an Agentic World",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "rag-agentic-world.html#annotated-presentation",
    "href": "rag-agentic-world.html#annotated-presentation",
    "title": "From Vectors to Agents: Managing RAG in an Agentic World",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the slide-by-slide annotated presentation based on the video ‚ÄúFrom Vectors to Agents: Managing RAG in an Agentic World‚Äù by Rajiv Shah.\n\n\n1. Title Slide\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00)\nThe presentation begins with the title slide, introducing the core theme: ‚ÄúFrom Vectors to Agents: Managing RAG in an Agentic World.‚Äù The speaker, Rajiv Shah from Contextual, sets the stage for a technical deep dive into Retrieval-Augmented Generation (RAG).\nHe outlines the agenda, promising to move beyond basic RAG concepts to focus specifically on retrieval approaches. The talk is designed to cover the spectrum from traditional methods like BM25 and Language Models to the emerging field of Agentic Search.\n\n\n2. ACME GPT\n\n\n\nSlide 2\n\n\n(Timestamp: 00:40)\nThis slide displays a stylized logo for ‚ÄúACME GPT,‚Äù representing the typical enterprise aspiration. Companies see tools like ChatGPT and immediately want to apply that capability to their internal data, asking questions like, ‚ÄúCan I get the list of board of directors?‚Äù\nHowever, the speaker notes a common hurdle: generic models don‚Äôt know enterprise-specific knowledge. This sets up the necessity for RAG‚Äîinjecting private data into the model‚Äîrather than relying solely on the model‚Äôs pre-trained knowledge.\n\n\n3. Building RAG is Easy\n\n\n\nSlide 3\n\n\n(Timestamp: 01:10)\nThe speaker illustrates the deceptively simple workflow of a basic RAG demo. The diagram shows the standard path: a user query is converted to vectors, matched against a database, and sent to an LLM.\nShah acknowledges that building a ‚Äúhello world‚Äù version of this is trivial. He notes, ‚ÄúYou can build a very easy RAG demo out of the box by just grabbing some data, using an embedding model, creating vectors, doing the similarity.‚Äù\n\n\n4. Building RAG is Easy (Code Example)\n\n\n\nSlide 4\n\n\n(Timestamp: 01:22)\nA Python code snippet using LangChain is displayed to reinforce how accessible basic RAG has become. The code demonstrates loading a document, chunking it, and setting up a retrieval chain in just a few lines.\nThis slide serves as a foil for the upcoming reality check. While the code works for a demo, it hides the immense complexity required to make such a system robust, accurate, and scalable in a real-world production environment.\n\n\n5. RAG Reality Check\n\n\n\nSlide 5\n\n\n(Timestamp: 01:35)\nThe tone shifts to the challenges of production. The slide highlights a sobering statistic: 95% of Gen AI projects fail to reach production. The speaker details the specific reasons why demos fail when scaled: poor accuracy, unbearable latency, scaling issues with millions of documents, and ballooning costs.\nHe emphasizes a critical, often overlooked factor: Compliance. ‚ÄúInside an enterprise, not everybody gets to read every document.‚Äù A demo ignores entitlements, but a production system cannot.\n\n\n6. Maybe try a different RAG?\n\n\n\nSlide 6\n\n\n(Timestamp: 03:00)\nThis slide lists a dizzying array of RAG variants (GraphRAG, RAPTOR, CRAG, etc.) and retrieval techniques. It represents the ‚Äúanalysis paralysis‚Äù developers face when scouring arXiv papers for a solution to their accuracy problems.\nShah warns against blindly chasing the latest academic paper to fix fundamental system issues. ‚ÄúThe answer is not in here of pulling together like a bunch of archive papers.‚Äù Instead, he advocates for a structured framework to make decisions.\n\n\n7. Ultimate RAG Solution\n\n\n\nSlide 7\n\n\n(Timestamp: 03:30)\nA humorous cartoon depicts a ‚ÄúRube Goldberg‚Äù machine, representing the ‚ÄúUltimate RAG Solution.‚Äù It mocks the tendency to over-engineer systems with too many interconnected, fragile components in the pursuit of performance.\nThe speaker uses this visual to argue for simplicity and deliberate design. The goal is to avoid building a monstrosity that is impossible to maintain, urging the audience to think about trade-offs before complexity.\n\n\n8. RAG as a system\n\n\n\nSlide 8\n\n\n(Timestamp: 03:35)\nThe speaker introduces a clean system architecture for RAG, broken into four distinct stages: Parsing, Querying, Retrieving, and Generation. This framework serves as the mental map for the rest of the presentation.\nHe highlights that ‚ÄúParsing‚Äù is vastly overlooked‚Äîgetting information out of complex documents cleanly is a prerequisite for success. Today‚Äôs talk, however, will zoom in specifically on the Retrieving and Querying components.\n\n\n9. Designing a RAG Solution\n\n\n\nSlide 9\n\n\n(Timestamp: 04:10)\nThis slide presents a ‚ÄúTradeoff Triangle‚Äù for RAG, balancing Problem Complexity, Latency, and Cost. The speaker advises having a serious conversation with stakeholders about these constraints before writing code.\nA key concept introduced here is the ‚ÄúCost of a mistake.‚Äù In coding assistants, a mistake is low-cost (the developer fixes it). In medical RAG systems, the cost of a mistake is high (life or death), which dictates a completely different architectural approach.\n\n\n10. RAG Considerations\n\n\n\nSlide 10\n\n\n(Timestamp: 05:30)\nA detailed table breaks down specific considerations that influence RAG design, such as domain difficulty, multilingual requirements, and data quality. This slide was originally created for sales teams to help scope customer problems.\nShah emphasizes that understanding the nuances of the use case upfront saves heartache later. For instance, knowing if users will ask simple questions or require complex reasoning changes the retrieval strategy entirely.\n\n\n11. Consider Query Complexity\n\n\n\nSlide 11\n\n\n(Timestamp: 06:15)\nThe speaker categorizes queries by complexity, ranging from simple Keywords (‚ÄúTotal Revenue‚Äù) to Semantic variations (‚ÄúHow much bank?‚Äù), to Multi-hop reasoning, and finally Agentic scenarios.\nHe points out a common failure mode: ‚ÄúThe answers aren‚Äôt in the documents‚Ä¶ all of a sudden they‚Äôre asking for knowledge that‚Äôs outside.‚Äù Recognizing the query complexity determines whether you need a simple search engine or a complex agentic workflow.\n\n\n12. Retrieval (Highlighted)\n\n\n\nSlide 12\n\n\n(Timestamp: 07:32)\nThe presentation zooms back into the system diagram, highlighting the ‚ÄúRetrieving‚Äù box. This signals the start of the deep technical dive into retrieval algorithms.\nShah notes that this area causes the most confusion due to the sheer number of model choices and architectures available. He aims to provide a practical guide to selecting the right retrieval tool.\n\n\n13. Retrieval Approaches\n\n\n\nSlide 13\n\n\n(Timestamp: 08:16)\nThree primary retrieval pillars are introduced: 1. BM25: The lexical, keyword-based standard. 2. Language Models: Semantic embeddings and vector search. 3. Agentic Search: The new frontier of iterative reasoning.\nThe speaker emphasizes that documents must be broken into pieces (chunking) because no single model context window is efficient enough to hold all enterprise data for every query.\n\n\n14. Building RAG is Easy (Code Highlight)\n\n\n\nSlide 14\n\n\n(Timestamp: 08:50)\nReturning to the initial code snippet, the speaker highlights the vectorstore and retriever initialization lines. This pinpoints exactly where the upcoming concepts fit into the implementation.\nThis visual anchor helps developers map the theoretical concepts of BM25 and Embeddings back to the actual lines of code they write in libraries like LangChain or LlamaIndex.\n\n\n15. BM25\n\n\n\nSlide 15\n\n\n(Timestamp: 09:18)\nBM25 (Best Match 25) is explained as a probabilistic lexical ranking function. The slide visualizes an inverted index, mapping words (like ‚Äúbutterfly‚Äù) to the specific documents containing them.\nShah explains that this is the 25th iteration of the formula, designed to score documents based on word frequency and saturation. It remains a powerful, fast baseline for retrieval.\n\n\n16. BM25 Performance\n\n\n\nSlide 16\n\n\n(Timestamp: 09:55)\nA table compares the speed of a Linear Scan (Ctrl+F style) versus an Inverted Index (BM25) as the document count grows from 1,000 to 9,000.\nThe data shows that linear search becomes exponentially slower (taking 3,000 seconds for 1k documents in this synthetic test), while BM25 remains orders of magnitude faster. This efficiency is why lexical search is still widely used in production.\n\n\n17. BM25 Failure Cases\n\n\n\nSlide 17\n\n\n(Timestamp: 11:08)\nThe limitations of BM25 are exposed. Because it relies on exact word matches, it fails when users use synonyms. If a user searches for ‚ÄúPhysician‚Äù but the documents only contain ‚ÄúDoctor,‚Äù BM25 will return zero results.\nSimilarly, it struggles with acronyms like ‚ÄúIBM‚Äù vs ‚ÄúInternational Business Machines.‚Äù Despite this, Shah argues BM25 is a ‚Äúvery strong baseline‚Äù that often beats complex neural models on specific keyword-heavy datasets.\n\n\n18. Hands on: BM25s\n\n\n\nSlide 18\n\n\n(Timestamp: 12:14)\nFor developers wanting to implement this, the slide points to a library called bm25s, a high-performance Python implementation available on Hugging Face.\nThis reinforces the practical nature of the talk‚ÄîBM25 isn‚Äôt just a legacy concept; it is an active, installable tool that developers should consider using alongside vector search.\n\n\n19. Enter Language Models\n\n\n\nSlide 19\n\n\n(Timestamp: 12:24)\nThe talk transitions to Language Models (Embeddings). The slide explains how an encoder model turns text into a dense vector (a list of numbers) that captures semantic meaning.\nBecause these models are trained on vast amounts of data, they ‚Äúhave an idea of these similar concepts.‚Äù This solves the synonym problem that plagues BM25.\n\n\n20. Embeddings Visualized\n\n\n\nSlide 20\n\n\n(Timestamp: 12:50)\nA 2D visualization demonstrates how embeddings group related concepts in latent space. The word ‚ÄúDoctor‚Äù and ‚ÄúPhysician‚Äù would be located very close to each other mathematically.\nThis spatial proximity allows for Semantic Search: finding documents that mean the same thing as the query, even if they don‚Äôt share a single word.\n\n\n21. Semantic search is widely used\n\n\n\nSlide 21\n\n\n(Timestamp: 13:15)\nThe speaker validates the importance of semantic search by showing a tweet from Google‚Äôs SearchLiaison regarding BERT, and a screenshot of Hugging Face‚Äôs model repository.\nThis confirms that semantic search is the industry standard for modern information retrieval, having been deployed at massive scale by tech giants to improve result relevance.\n\n\n22. Which language model?\n\n\n\nSlide 22\n\n\n(Timestamp: 13:30)\nA scatter plot compares various models based on Inference Speed (X-axis) and NDCG@10 (Y-axis, a measure of retrieval quality).\nShah places BM25 on the right (fast but lower accuracy) to orient the audience. He points out that there is a massive variety of models with different trade-offs between compute cost and retrieval quality.\n\n\n23. Static Embeddings\n\n\n\nSlide 23\n\n\n(Timestamp: 14:43)\nThe speaker introduces Static Embeddings (like Word2Vec or GloVe) which are located on the far right of the previous scatter plot‚Äîextremely fast, even on CPUs.\nThese models assign a fixed vector to every word. While efficient, they lack context. The word ‚Äúbank‚Äù has the same vector whether referring to a river bank or a financial bank, which limits their accuracy.\n\n\n24. Why Context Matters\n\n\n\nSlide 24\n\n\n(Timestamp: 15:16)\nA cartoon illustrates the difference between Static Embeddings and Transformers. The Transformer can distinguish between ‚ÄúModel‚Äù in a data science context versus ‚ÄúModel‚Äù in a fashion context.\nThis contextual awareness is why modern Transformer-based embeddings (like BERT) generally outperform static embeddings and BM25 in complex retrieval tasks, despite being slower.\n\n\n25. Many more models!\n\n\n\nSlide 25\n\n\n(Timestamp: 15:55)\nReturning to the scatter plot, a red arrow points toward the top-left quadrant‚Äîmodels that are slower but achieve higher accuracy.\nThe speaker notes that the field is constantly evolving, with ‚Äúnewer generations of models‚Äù pushing the boundary of what is possible in terms of retrieval quality.\n\n\n26. MTEB/RTEB\n\n\n\nSlide 26\n\n\n(Timestamp: 16:35)\nTo help developers choose, Shah introduces the MTEB (Massive Text Embedding Benchmark) and RTEB (Retrieval Text Embedding Benchmark). These are leaderboards hosted on Hugging Face.\nHe highlights a key distinction: MTEB uses public datasets, while RTEB uses private, held-out datasets. This is crucial for avoiding ‚Äúdata contamination,‚Äù where models perform well simply because they were trained on the test data.\n\n\n27. Selecting an embedding model\n\n\n\nSlide 27\n\n\n(Timestamp: 16:48)\nThe speaker switches to a live browser view (captured in the slide) of the leaderboard. He discusses the bubble chart visualization where size often correlates with parameter count.\nHe points out an interesting trend: ‚ÄúYou‚Äôll see that there‚Äôs a bunch of models here that are all the same size‚Ä¶ but the performance differs.‚Äù This indicates improvements in training strategies and architecture rather than just throwing more compute at the problem.\n\n\n28. Selecting an embedding model (Other Considerations)\n\n\n\nSlide 28\n\n\n(Timestamp: 19:07)\nBeyond the leaderboard score, Shah lists practical selection criteria: Model Size (can it fit in memory?), Architecture (CPU vs GPU), Embedding Dimension (storage costs), and Training Data (multilingual support).\nHe advises checking if a model is open source and quantizable, as this can significantly reduce latency without a major hit to accuracy.\n\n\n29. Matryoshka Embedding Models\n\n\n\nSlide 29\n\n\n(Timestamp: 20:53)\nA specific innovation is highlighted: Matryoshka Embeddings. These models allow developers to truncate vectors (e.g., from 768 dimensions down to 64) while retaining most of the performance.\nThis is a ‚Äúneat kind of innovation‚Äù for optimizing storage and search speed. OpenAI‚Äôs newer models also support this feature, offering flexibility between cost and accuracy.\n\n\n30. Sentence Transformer\n\n\n\nSlide 30\n\n\n(Timestamp: 21:42)\nThe Sentence Transformer architecture is described as the dominant approach for RAG. Unlike standard BERT which works on tokens, these are fine-tuned to understand full sentences and paragraphs.\nThis architecture uses Siamese networks to ensure that semantically similar sentences are close in vector space, making them ideal for the ‚Äúchunk-level‚Äù retrieval required in RAG.\n\n\n31. Cross Encoder / Reranker\n\n\n\nSlide 31\n\n\n(Timestamp: 22:16)\nThe concept of a Cross Encoder (or Reranker) is introduced. Unlike the bi-encoder (retriever) which processes query and document separately, the cross-encoder processes them together.\nThis allows for a much deeper calculation of relevance. It is typically used as a second stage: retrieve 50 documents quickly with vectors, then use the slow but accurate Cross Encoder to rank the top 5.\n\n\n32. Cross Encoder / Reranker (Duplicate)\n\n\n\nSlide 32\n\n\n(Timestamp: 22:16)\n(This slide reinforces the previous diagram, emphasizing the ‚Äúcrossing‚Äù of the query and document in the model architecture.)\n\n\n33. Cross Encoder / Reranker (Accuracy Boost)\n\n\n\nSlide 33\n\n\n(Timestamp: 23:07)\nA bar chart quantifies the value of reranking. It shows a significant boost in NDCG (accuracy) when a reranker is added to the pipeline.\nThe speaker notes that while you get a ‚Äúbump‚Äù in quality, it ‚Äúdoesn‚Äôt come for free.‚Äù The trade-off is increased latency, as the cross-encoder is computationally expensive.\n\n\n34. Cross Encoder / Reranker (Execution Flow)\n\n\n\nSlide 34\n\n\n(Timestamp: 23:15)\nThe execution flow diagram highlights the reranker‚Äôs position in the pipeline. It sits between the Vector Store retrieval and the LLM generation.\nThis visual reinforces the latency implication: the user has to wait for both the initial search and the reranking pass before the LLM even starts generating an answer.\n\n\n35. Hands On: Retriever & Reranker\n\n\n\nSlide 35\n\n\n(Timestamp: 23:30)\nA screenshot of a Google Colab notebook is shown, demonstrating a practical implementation of the Retrieve and Re-rank strategy using the SentenceTransformer and CrossEncoder libraries.\nThis provides a concrete resource for the audience to test the accuracy vs.¬†speed trade-offs themselves on simple datasets like Wikipedia.\n\n\n36. Instruction Following Reranker\n\n\n\nSlide 36\n\n\n(Timestamp: 23:48)\nShah mentions a specific advancement: Instruction Following Rerankers (developed by his company, Contextual). These allow developers to pass a prompt to the reranker, such as ‚ÄúPrioritize safety notices.‚Äù\nThis adds a ‚Äúknob‚Äù for developers to tune retrieval based on business logic without retraining the model.\n\n\n37. Combine Multiple Retrievers\n\n\n\nSlide 37\n\n\n(Timestamp: 24:19)\nThe presentation suggests that you don‚Äôt have to pick just one method. You can combine BM25, various embedding models (E5, BGE), and rerankers.\nWhile combining them (Ensemble Retrieval) often yields better recall, Shah warns that ‚Äúyou got to engineer this.‚Äù Managing multiple indexes and fusion logic increases operational complexity and compute costs.\n\n\n38. Cascading Rerankers in Kaggle\n\n\n\nSlide 38\n\n\n(Timestamp: 24:56)\nA complex diagram from a Kaggle competition winner illustrates a Cascade Strategy. The solution used three different rerankers, filtering from 64 documents down to 8, and then to 5.\nThis shows the extreme end of retrieval engineering, where multiple models are chained to squeeze out every percentage point of accuracy.\n\n\n39. Best practices\n\n\n\nSlide 39\n\n\n(Timestamp: 25:16)\nShah distills the complexity into a recommended Best Practice: 1. Hybrid Search: Combine Semantic Search (Vectors) and Lexical Search (BM25). 2. Reciprocal Rank Fusion: Merge the results. 3. Reranker: Pass the top results through a cross-encoder.\nThis setup provides a ‚Äúpretty good standard performance out of the box‚Äù and should be the default baseline before trying exotic methods.\n\n\n40. Families of Embedding Models\n\n\n\nSlide 40\n\n\n(Timestamp: 25:42)\nA taxonomy slide categorizes the models discussed: Static (Fastest/Low Accuracy), Bi-Encoders (Fast/Good Accuracy), and Cross-Encoders (Slow/Best Accuracy).\nThis summary helps the audience mentally organize the tools available in their toolbox.\n\n\n41. Lots of New Models\n\n\n\nSlide 41\n\n\n(Timestamp: 25:50)\nLogos for IBM Granite, Google EmbeddingGemma, and others appear. The speaker notes that while new models from major players appear weekly, the improvements are often ‚Äúincremental.‚Äù\nHe advises against ‚Äúripping up‚Äù a working system just to switch to a model that is 1% better on a leaderboard.\n\n\n42. Other retrieval methods\n\n\n\nSlide 42\n\n\n(Timestamp: 26:18)\nAlternative methods are briefly listed: SPLADE (Sparse retrieval), ColBERT (Late interaction), and GraphRAG.\nShah acknowledges these exist and may fit specific niches, but warns against chasing the ‚Äúflavor of the week‚Äù before establishing a solid baseline with hybrid search.\n\n\n43. Operational Concerns\n\n\n\nSlide 43\n\n\n(Timestamp: 27:30)\nThe talk shifts to operations. Libraries like FAISS are mentioned for efficient vector similarity search.\nA key point is that for many use cases, you can simply store embeddings in memory. You don‚Äôt always need a complex vector database if your dataset fits in RAM.\n\n\n44. Vector Database Options\n\n\n\nSlide 44\n\n\n(Timestamp: 27:55)\nA diagram categorizes storage into Hot (In-Memory), Warm (SSD/Disk), and Cold tiers.\nShah notes there are ‚Äútons of vector database options‚Äù (Snowflake, Pinecone, etc.). The choice should be governed by latency requirements. If you need sub-millisecond retrieval, you need in-memory storage.\n\n\n45. Operational Concerns (Datastore Size)\n\n\n\nSlide 45\n\n\n(Timestamp: 28:40)\nA graph shows that as Datastore Size increases (X-axis), retrieval performance naturally degrades (Y-axis).\nTo combat this, the speaker strongly recommends using Metadata Filtering. ‚ÄúIf you‚Äôre not using something like metadata‚Ä¶ it‚Äôs going to be very tough.‚Äù Narrowing the search scope is essential for scaling to millions of documents.\n\n\n46. Search Strategy Comparison\n\n\n\nSlide 46\n\n\n(Timestamp: 29:22)\nThe presentation pivots to the ‚Äúexciting part‚Äù: Agentic RAG. A visual compares ‚ÄúTraditional RAG‚Äù (a linear path) with ‚ÄúAgentic RAG‚Äù (a winding, exploratory path).\nThis represents the shift from a ‚Äúone-shot‚Äù retrieval attempt to an iterative system that can explore, backtrack, and reason.\n\n\n47. Tools use / Reasoning\n\n\n\nSlide 47\n\n\n(Timestamp: 29:40)\nReasoning models (like o1 or DeepSeek R1) enable LLMs to use tools effectively. A code snippet shows an agent loop: query -&gt; generate -&gt; ‚ÄúDid it answer the question?‚Äù\nIf the answer is no, the model can ‚Äúrewrite the query‚Ä¶ try to find that missing information, feed that back into the loop.‚Äù This self-correction is the core of Agentic RAG.\n\n\n48. Agentic RAG (Workflow)\n\n\n\nSlide 48\n\n\n(Timestamp: 30:32)\nA flowchart details the Agentic RAG lifecycle. The model thinks through steps: ‚ÄúOh, this is the query I need to make‚Ä¶ based on those results‚Ä¶ maybe we should do it a different way.‚Äù\nThis workflow allows the system to synthesize answers from multiple sources or clarify ambiguous queries automatically.\n\n\n49. Tools use / Reasoning (Detailed Example)\n\n\n\nSlide 49\n\n\n(Timestamp: 30:35)\nA specific example of a complex query is shown. The agent breaks the problem down, calls tools, and iterates.\nThis demonstrates that the ‚ÄúThinking‚Äù time is where the value is generated, allowing for a depth of research that a single retrieval pass cannot match.\n\n\n50. Open Deep Research\n\n\n\nSlide 50\n\n\n(Timestamp: 31:02)\nShah references ‚ÄúOpen Deep Research‚Äù by LangChain, an open-source framework where sub-agents go out, perform research, and report back.\nThis is a specific category of Agentic RAG focused on generating comprehensive reports rather than quick answers.\n\n\n51. DeepResearch Bench\n\n\n\nSlide 51\n\n\n(Timestamp: 31:30)\nA leaderboard for DeepResearch Bench is shown, testing models on ‚Äú100 PhD level research tasks.‚Äù\nThe speaker warns that this approach ‚Äúcan get very expensive.‚Äù Solving a single complex query might cost significant money due to the number of tokens and iterative steps required.\n\n\n52. Westlaw AI Deep Research\n\n\n\nSlide 52\n\n\n(Timestamp: 31:55)\nA real-world application is highlighted: Westlaw AI. In the legal field, thoroughness is worth the latency and cost.\nThis proves that Agentic RAG isn‚Äôt just a toy; it is being commercialized in high-value verticals where accuracy is paramount.\n\n\n53. Agentic RAG (Self-RAG)\n\n\n\nSlide 53\n\n\n(Timestamp: 32:11)\nThe concept of Self-RAG is introduced, emphasizing the ‚ÄúReflection‚Äù step. The model critiques its own retrieved documents and generation quality.\nShah notes that this isn‚Äôt brand new, but has become practical due to better reasoning models.\n\n\n54. Agentic RAG (LangChain Reddit)\n\n\n\nSlide 54\n\n\n(Timestamp: 34:04)\nA Reddit post is shown where a developer discusses building a self-reflection RAG system. This highlights the community‚Äôs active experimentation with these loops.\n\n\n55. Agentic RAG (Efficiency Concerns)\n\n\n\nSlide 55\n\n\n(Timestamp: 34:15)\nThe discussion turns to the ‚ÄúRub‚Äù: Inefficiency. Agentic loops can be slow and wasteful, re-retrieving data unnecessarily.\nThis sets up the trade-off conversation again: Is the extra time and compute worth the accuracy gain?\n\n\n56. Research: BRIGHT\n\n\n\nSlide 56\n\n\n(Timestamp: 32:11)\nNote: The speaker introduces the BRIGHT benchmark around 32:11, slightly out of slide order in the transcript flow, but connects it here.\nBRIGHT is a benchmark specifically designed for Retrieval Reasoning. Unlike standard benchmarks that test keyword matching, BRIGHT tests questions that require thinking, logic, and multi-step deduction to find the correct document.\n\n\n57. BRIGHT #1: DIVER\n\n\n\nSlide 57\n\n\n(Timestamp: 32:48)\nThe top-performing system on BRIGHT is DIVER. The diagram shows it uses the exact components discussed earlier: Chunking, Retrieving, and Reranking, but wrapped in an iterative loop.\nShah points out, ‚ÄúIt probably doesn‚Äôt look that crazy to you if you‚Äôre used to RAG.‚Äù The innovation is in the process, not necessarily a magical new model architecture.\n\n\n58. BRIGHT #1: DIVER (LLM Instructions)\n\n\n\nSlide 58\n\n\n(Timestamp: 33:31)\nThe specific prompts used in DIVER are shown. The system asks the LLM: ‚ÄúGiven a query‚Ä¶ what do you think would be possibly helpful to do?‚Äù\nThis Query Expansion allows the system to generate new search terms that the user didn‚Äôt think of, bridging the semantic gap through reasoning.\n\n\n59. Agentic RAG on WixQA\n\n\n\nSlide 59\n\n\n(Timestamp: 34:36)\nShah shares his own experiment results on the WixQA dataset (technical support). * One Shot RAG: 5 seconds latency, 76% Factuality. * Agentic RAG: Slower latency, 93% Factuality.\nThis massive jump in accuracy (0.76 to 0.93) is the key takeaway. ‚ÄúThat has a ton of implications.‚Äù It suggests that the limitation of RAG often isn‚Äôt the data, but the lack of reasoning applied to the retrieval process.\n\n\n60. Rethink your Assumptions\n\n\n\nSlide 60\n\n\n(Timestamp: 37:10)\nThis is the climax of the technical argument. A graph from the BRIGHT paper shows that BM25 (lexical search) combined with an Agentic loop (GPT-4) outperforms advanced embedding models (Qwen).\n‚ÄúThis is crazy,‚Äù Shah exclaims. Because the LLM can rewrite queries into many variations, it mitigates BM25‚Äôs weakness (synonyms). This implies you might not need complex vector databases if you have a smart agent.\n\n\n61. Agentic RAG with BM25\n\n\n\nSlide 61\n\n\n(Timestamp: 38:20)\nShah validates the paper‚Äôs finding with his own internal data (Financial 10Ks). Agentic RAG with BM25 performed nearly as well as Agentic RAG with Embeddings.\nHe suggests a radical possibility: ‚ÄúI could throw all that away [vector DBs]‚Ä¶ just stick this in a text-only database and use BM25.‚Äù\n\n\n62. Agentic RAG for Code Search\n\n\n\nSlide 62\n\n\n(Timestamp: 39:46)\nHe connects this finding to Claude Code, which uses a lexical approach (like grep) rather than vectors for code search.\nSince code doesn‚Äôt have the same semantic ambiguity as natural language, and agents can iterate rapidly, lexical search is proving to be superior for coding assistants.\n\n\n63. Combine Retrieval Approaches\n\n\n\nSlide 63\n\n\n(Timestamp: 40:15)\nA DoorDash case study illustrates a two-tier guardrail system. They use simple text similarity first (fast/cheap). If that fails or is uncertain, they kick it to an LLM (slow/expensive).\nThis ‚ÄúTiered‚Äù approach optimizes the trade-off between cost and accuracy in production.\n\n\n64. Hands on: Agentic RAG (Smolagents)\n\n\n\nSlide 64\n\n\n(Timestamp: 41:07)\nThe speaker points to Smolagents, a Hugging Face library, as a way to get hands-on with these concepts. A Colab notebook is provided for the audience to build their own agentic retrieval loops.\n\n\n65. Solutions for a RAG Solution\n\n\n\nSlide 65\n\n\n(Timestamp: 41:18)\nShah updates the ‚ÄúProblem Complexity‚Äù framework from the beginning of the talk with specific recommendations: * Low Latency (&lt;5s): Use BM25 or Static Embeddings. * High Cost of Mistake: Add a Reranker. * Complex Multi-hop: Use Agentic RAG.\n\n\n66. Retriever Checklist\n\n\n\nSlide 66\n\n\n(Timestamp: 41:52)\nA final checklist summarizes the retrieval hierarchy: 1. Keyword/BM25 (The baseline). 2. Semantic Search (The standard). 3. Agentic/Reasoning (The problem solver).\nThis provides the audience with a mental menu to choose from based on their specific constraints.\n\n\n67. RAG as a system (Retrieval with Instruction Following Reranker)\n\n\n\nSlide 67\n\n\n(Timestamp: 42:00)\nThe system diagram is shown one last time, updated to include the Instruction Following Reranker in the retrieval box, solidifying the modern RAG architecture.\n\n\n68. RAG - Generation\n\n\n\nSlide 68\n\n\n(Timestamp: 42:10)\nNote: The speaker concludes the talk at 42:10, stating ‚ÄúI‚Äôm going to end it here.‚Äù Slides 68-70 regarding the Generation stage were included in the deck but skipped in the video recording due to time constraints.\nThis slide would have covered the final stage of RAG: generating the answer. The focus here is typically on reducing hallucinations and ensuring the tone matches the user‚Äôs needs.\n\n\n69. RAG - Generation (Model Selection)\n\n\n\nSlide 69\n\n\n(Timestamp: 42:10)\nSkipped in video. This slide illustrates the choice of LLM for generation (e.g., GPT-4 vs Llama 3 vs Claude). The choice depends on the ‚ÄúCost/Latency budget‚Äù and specific domain requirements.\n\n\n70. Chunking approaches\n\n\n\nSlide 70\n\n\n(Timestamp: 42:10)\nSkipped in video. This slide compares Original Chunking (cutting text at fixed intervals) with Contextual Chunking (adding a summary prefix to every chunk). Contextual chunking significantly improves retrieval because every chunk carries the context of the parent document.\n\n\n71. Title Slide (Duplicate)\n\n\n\nSlide 71\n\n\n(Timestamp: 42:10)\nThe presentation concludes with the title slide. Rajiv Shah thanks the audience, encouraging them to think about trade-offs rather than just chasing the latest models. ‚ÄúHopefully I‚Äôve given you a sense of thinking about these trade-offs‚Ä¶ thank you all.‚Äù\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rajiv Shah - rajistics blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Created - Oldest\n      \n      \n        Created - Newest\n      \n      \n        Title\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nRunning Code and Failing Models\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Practical Guide to Evaluating Generative AI Applications\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Vectors to Agents: Managing RAG in an Agentic World\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Sparse Matrices through Interactive Visualizations\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection Methods and Feature Selection Curves\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Lessons in Building Generative AI: RAG and Text to SQL\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSnowflake ML Intro Notebook - ML Forecasting\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReasoning in Large Language Models\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nText style transfer in a spreadsheet using Hugging Face Inference Endpoints\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFew shot text classification with SetFit\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGetting predictions intervals with conformal inference\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining predictions from ü§ó transformer models\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic Adversarial Data Collection\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStand Up for Best Practices\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization Strategies\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Unlabeled Data to Label Data\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Google‚Äôs Quickdraw to create an MNIST style dataset!\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning with R\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Worlds for Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nTaking an H2O Model to Production\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nUsing xgbfi for revealing feature interactions\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nOutlier App\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nRNN Addition (1st Grade)\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSportVu Analysis\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nShiny front end for Tensorflow demo\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2016\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "genai-evaluation-guide.html",
    "href": "genai-evaluation-guide.html",
    "title": "A Practical Guide to Evaluating Generative AI Applications",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "genai-evaluation-guide.html#video",
    "href": "genai-evaluation-guide.html#video",
    "title": "A Practical Guide to Evaluating Generative AI Applications",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "genai-evaluation-guide.html#annotated-presentation",
    "href": "genai-evaluation-guide.html#annotated-presentation",
    "title": "A Practical Guide to Evaluating Generative AI Applications",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the annotated presentation for Rajiv Shah‚Äôs workshop on ‚ÄúHill Climbing: Best Practices for Evaluating LLMs.‚Äù\n\n1. Title Slide\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00)\nThis slide introduces the workshop titled ‚ÄúHill Climbing: Best Practices for Evaluating LLMs,‚Äù presented by Rajiv Shah, PhD, at the Open Data Science Conference (ODSC). The presentation focuses on the technical nuances of Generative AI and how to build effective evaluation workflows.\nRajiv sets the stage by outlining his three main goals for the session: understanding the technical differences in GenAI evaluation, learning a basic introductory workflow for building evaluation datasets, and inspiring practitioners to start ‚Äúlearning by doing‚Äù rather than just reading papers.\nThe concept of ‚ÄúHill Climbing‚Äù refers to the iterative process of improving LLM applications‚Äîstarting with a baseline and continuously optimizing performance through rigorous testing and error analysis.\n\n\n2. Evaluating for Gen AI Resources\n\n\n\nSlide 2\n\n\n(Timestamp: 00:06)\nThis slide provides a QR code and a GitHub URL, directing the audience to the code and resources associated with the talk. It emphasizes that the workshop is practical, with code examples available for attendees to replicate the evaluation techniques discussed.\nRajiv encourages the audience to access these resources to follow along with the technical implementations of the concepts, such as building LLM judges and creating unit tests, which will be covered later in the presentation.\n\n\n3. Customer Support Use Case\n\n\n\nSlide 3\n\n\n(Timestamp: 00:48)\nTo motivate the need for evaluation, the presentation introduces a common real-world use case: Customer Support. Generative AI is frequently deployed to help agents compose emails or chat responses based on user inquiries.\nThis scenario serves as the baseline example throughout the talk. It represents a high-volume task where automation is desirable, but accuracy and tone are critical for maintaining customer satisfaction and brand reputation.\n\n\n4. Vibe Coding\n\n\n\nSlide 4\n\n\n(Timestamp: 00:59)\nThis slide introduces the concept of ‚ÄúVibe Coding‚Äù‚Äîthe initial phase where developers grab a simple prompt, feed it to a model, and get a result that feels right. It highlights the misconception that GenAI is easy because it works ‚Äúout of the box‚Äù for simple demos.\nRajiv notes that while ‚Äúvibe coding‚Äù might work for a quick demo app, it is insufficient for production systems. Relying on a ‚Äúvibe‚Äù that the model is working prevents teams from catching subtle failures that occur at scale.\n\n\n5. Good Response: Delayed Order\n\n\n\nSlide 5\n\n\n(Timestamp: 01:10)\nHere, we see a successful output generated by the LLM. The customer inquired about a delayed order, and the AI generated a polite, relevant response acknowledging the delay and apologizing.\nThis example reinforces the ‚ÄúVibe Coding‚Äù trap: because the model often produces high-quality, human-sounding text like this, developers can be lulled into a false sense of security regarding the system‚Äôs reliability.\n\n\n6. Good Response: Damaged Product\n\n\n\nSlide 6\n\n\n(Timestamp: 01:12)\nThis slide provides another example of a ‚Äúgood‚Äù response. The AI correctly identifies that the customer received a damaged product and initiates a replacement protocol.\nThese positive examples establish a baseline of expected behavior. The challenge in evaluation is not just confirming that the model can work, but ensuring it works consistently across all edge cases.\n\n\n7. Bad Response: Irrelevance\n\n\n\nSlide 7\n\n\n(Timestamp: 01:26)\nThe presentation shifts to failure modes. In this example, the user asks about an ‚ÄúOrder Delay,‚Äù but the AI responds with information about a ‚ÄúNew Product Launch.‚Äù\nThis illustrates a complete context mismatch. The model failed to attend to the user‚Äôs intent, generating a coherent but completely irrelevant response. This type of failure frustrates users and degrades trust in the automated system.\n\n\n8. Bad Response: Hallucination\n\n\n\nSlide 8\n\n\n(Timestamp: 01:36)\nThis slide shows a more dangerous failure: Hallucination. The AI apologizes for a defective ‚Äúespresso machine,‚Äù but as the speaker notes, ‚ÄúWe don‚Äôt actually sell espresso machines.‚Äù\nThis highlights the risk of the model fabricating facts to be helpful. Such errors can lead to logistical nightmares, such as customers expecting replacements for products that do not exist or that the company never sold.\n\n\n9. Risks of LLM Mistakes\n\n\n\nSlide 9\n\n\n(Timestamp: 01:51)\nRajiv categorizes the risks associated with LLM failures into three buckets: Reputational, Legal, and Financial. He cites the example of Cursor, an IDE company, where a support bot hallucinated a policy restricting users to one device, causing customers to cancel subscriptions.\nThe slide emphasizes that courts may view AI agents as employees; if a bot makes a promise (like a refund or policy change), the company might be legally bound to honor it. This escalates evaluation from a technical nice-to-have to a business necessity.\n\n\n10. The Despair of Gen AI\n\n\n\nSlide 10\n\n\n(Timestamp: 02:38)\nThis visual represents the frustration developers feel when moving from a successful demo to a failing production system. The ‚Äúdespair‚Äù comes from the realization that the stochastic nature of LLMs makes them difficult to control.\nIt serves as an emotional anchor for the audience, acknowledging that while GenAI is exciting, the unpredictability of its failures causes significant stress for engineering teams responsible for deployment.\n\n\n11. High Failure Rates\n\n\n\nSlide 11\n\n\n(Timestamp: 02:48)\nThe slide cites an MIT report stating that ‚Äú95% of GenAI pilots are failing.‚Äù While Rajiv notes this number might be overstated, it reflects a trend where executives are demanding ROI and seeing lackluster results.\nThis shift in 2025 means that evaluation is no longer just for debugging; it is required to prove business value and justify the high costs of running Generative AI infrastructure.\n\n\n12. Evaluation Improves Applications\n\n\n\nSlide 12\n\n\n(Timestamp: 03:14)\nThis slide asserts the core thesis: Evaluation helps you build better GenAI applications. It references a previous viral video by the speaker on the same topic, positioning this talk as an updated, condensed version with fresh content.\nRajiv explains that you cannot improve what you cannot measure. Without a robust evaluation framework, developers are essentially guessing whether changes to prompts or models are actually improving performance.\n\n\n13. Why Evaluation is Necessary\n\n\n\nSlide 13\n\n\n(Timestamp: 03:40)\nThis concentric diagram illustrates the stakeholders involved in evaluation. It starts with ‚ÄúThings Go Wrong‚Äù (technical reality), moves to ‚ÄúBuy-in‚Äù (convincing managers/teams), and ends with ‚ÄúRegulators‚Äù (external compliance).\nEvaluation serves multiple audiences: it helps the developer debug, it provides the metrics needed to convince management that the app is production-ready, and it creates the audit trails required by third-party auditors or regulators.\n\n\n14. Evaluation Dimensions\n\n\n\nSlide 14\n\n\n(Timestamp: 04:18)\nEvaluation must cover three dimensions: Technical (F1 scores, accuracy), Business (ROI, value generated), and Operational (Total Cost of Ownership, latency).\nRajiv highlights that data scientists often focus solely on the technical, but ignoring operational costs (like the expense of hosting GPUs vs.¬†using APIs) can kill a project. A comprehensive evaluation strategy considers the cost-to-quality ratio.\n\n\n15. Public Benchmarks\n\n\n\nSlide 15\n\n\n(Timestamp: 05:06)\nThe slide discusses Public Benchmarks (like MMLU, GSM8K). While useful for a general idea of a model‚Äôs capabilities (e.g., ‚ÄúIs Llama 3 better than Llama 2?‚Äù), they are insufficient for specific applications.\nRajiv warns against using these benchmarks to determine if a model fits your specific use case. Companies promote these numbers for marketing, but they rarely reflect performance on proprietary business data.\n\n\n16. Custom Benchmarks\n\n\n\nSlide 16\n\n\n(Timestamp: 05:22)\nThe solution to the limitations of public benchmarks is Custom Benchmarks. This slide defines a benchmark as a combination of a Task, a Dataset, and an Evaluation Metric.\nThis is a critical definition for the workshop. To ‚Äútame‚Äù GenAI, you must build a dataset that reflects your specific customer queries and define success metrics that matter to your business logic, rather than relying on generic academic tests.\n\n\n17. Taming Gen AI\n\n\n\nSlide 17\n\n\n(Timestamp: 05:28)\nThis title slide signals a transition into the technical ‚Äúhow-to‚Äù section of the talk. ‚ÄúTaming‚Äù implies that the default state of GenAI is wild and unpredictable.\nThe goal of the following sections is to bring structure and control to this chaos through rigorous engineering practices and evaluation workflows.\n\n\n18. Workshop Roadmap\n\n\n\nSlide 18\n\n\n(Timestamp: 05:31)\nThe roadmap outlines the four main sections of the talk: 1. Basics of Gen AI: Understanding variability and technical nuances. 2. Evaluation Workflow: Building the dataset and running the first tests. 3. More Complexity: Adding unit tests and conducting error analysis. 4. Agents: Evaluating complex, multi-step workflows.\n\n\n19. Variability in Responses\n\n\n\nSlide 19\n\n\n(Timestamp: 06:00)\nThis slide visually demonstrates the Non-Determinism of LLMs. It shows two responses to the same prompt generated just minutes apart. While substantively similar, the wording and structure differ slightly.\nThis variability makes exact string matching (a common software testing technique) impossible for LLMs. It necessitates semantic evaluation techniques, which complicates the testing pipeline.\n\n\n20. Input-Model-Output Diagram\n\n\n\nSlide 20\n\n\n(Timestamp: 06:24)\nA simple diagram illustrates the flow: Prompt -&gt; Model -&gt; Output. Rajiv uses this to structure the analysis of where variability comes from.\nHe explains that ‚Äúchaos‚Äù can enter the system at any of these three stages: the input (prompt sensitivity), the model (inference non-determinism), or the output (formatting and evaluation).\n\n\n21. Inconsistent Benchmark Scores\n\n\n\nSlide 21\n\n\n(Timestamp: 06:44)\nThe slide presents a discrepancy between benchmark scores tweeted by Hugging Face and those in the official Llama paper. Both used the same dataset (MMLU), but reported different accuracy numbers.\nThis introduces the problem of Evaluation Harness Sensitivity. Even with standard benchmarks, how you ask the model to take the test changes the score, proving that evaluation is fragile and implementation-dependent.\n\n\n22. MMLU Overview\n\n\n\nSlide 22\n\n\n(Timestamp: 07:25)\nMMLU (Massive Multitask Language Understanding) is explained here. It is a multiple-choice test covering 57 tasks across STEM, the humanities, and more.\nIt is currently the standard for measuring general ‚Äúintelligence‚Äù in models. However, because it is a multiple-choice format, it is susceptible to prompt formatting nuances, as the next slides demonstrate.\n\n\n23. Prompt Sensitivity\n\n\n\nSlide 23\n\n\n(Timestamp: 07:44)\nThis slide reveals why the scores in Slide 21 differed. The three evaluation harnesses used slightly different prompt structures (e.g., using the word ‚ÄúQuestion‚Äù vs.¬†just listing the text).\nThese minor changes resulted in significant accuracy shifts. This proves that LLMs are highly sensitive to syntax, meaning a ‚Äúbetter‚Äù model might just be one that was prompted more effectively for the test, not one that is actually smarter.\n\n\n24. Formatting Changes\n\n\n\nSlide 24\n\n\n(Timestamp: 08:22)\nExpanding on sensitivity, this slide references Anthropic‚Äôs research showing that changing answer choices from (A) to [A] or (1) affects the output.\nThis level of fragility is a key takeaway: seemingly cosmetic changes in how inputs are formatted can alter the model‚Äôs reasoning capabilities or its ability to output the correct token.\n\n\n25. GPT-4o Performance Drop\n\n\n\nSlide 25\n\n\n(Timestamp: 08:38)\nA bar chart demonstrates that this issue persists even in state-of-the-art models like GPT-4o. Subtle changes in wording can lead to a 5-10% drop in performance.\nThis counters the assumption that newer, larger models have ‚Äúsolved‚Äù prompt sensitivity. It remains a persistent variable that evaluators must control for.\n\n\n26. Tone Sensitivity\n\n\n\nSlide 26\n\n\n(Timestamp: 08:46)\nThis slide shows that the tone of a prompt (e.g., being polite vs.¬†direct) affects accuracy. Rajiv jokes, ‚ÄúI guess this is why mom always said to be polite.‚Äù\nThe graph indicates that prompt engineering strategies, like adding emotional weight or politeness, can statistically alter model performance, adding another layer of complexity to evaluation.\n\n\n27. Persistent Sensitivity\n\n\n\nSlide 27\n\n\n(Timestamp: 09:00)\nThe slide reiterates that despite years of progress, models are still sensitive to specific phrases. It shows a ‚ÄúPrompt Engineering‚Äù guide suggesting specific words to use.\nThe takeaway is that developers cannot treat the prompt as a static instruction; it is a hyperparameter that requires optimization and constant testing.\n\n\n28. Falcon LLM Bias\n\n\n\nSlide 28\n\n\n(Timestamp: 09:18)\nThis slide introduces a case study with the Falcon LLM. A user tweet shows the model recommending Abu Dhabi as a technological city with glowing sentiment, which raised suspicions about bias given the model‚Äôs origin in the Middle East.\nThis serves as a detective story: users wondered if the model weights were altered or if specific training data was injected to force this positive association.\n\n\n29. Potential Cover-up?\n\n\n\nSlide 29\n\n\n(Timestamp: 09:50)\nAnother tweet speculates if the model is ‚Äúcovering up human rights abuses‚Äù because it provides different answers for Abu Dhabi compared to other cities.\nThis highlights how model behavior can be misinterpreted as malicious bias or censorship, when the root cause might be something much simpler in the input stack.\n\n\n30. Inspecting the System Prompt\n\n\n\nSlide 30\n\n\n(Timestamp: 10:00)\nThe reveal: The bias wasn‚Äôt in the weights, but in the System Prompt. The slide suggests looking at the hidden instructions given to the model.\nIn Falcon‚Äôs case, the system prompt explicitly told the model, ‚ÄúYou are a model built in Abu Dhabi.‚Äù This context influenced its generation probabilities, causing it to favor Abu Dhabi in its responses.\n\n\n31. Claude System Prompt\n\n\n\nSlide 31\n\n\n(Timestamp: 10:33)\nRajiv points out that most developers never read the system prompts of the models they use. He highlights the Claude System Prompt, which is 1700 words long and takes nearly 10 minutes to read.\nThese extensive instructions define the model‚Äôs personality and safety guardrails. Ignoring them means you don‚Äôt fully understand the inputs driving your application‚Äôs behavior.\n\n\n32. Complexity of a Single Response\n\n\n\nSlide 32\n\n\n(Timestamp: 11:00)\nThe diagram is updated to show that a ‚Äúsingle response‚Äù is actually the result of complex interactions: Tokenization -&gt; Prompt Styles -&gt; Prompt Engineering -&gt; System Prompt.\nThis visual summarizes the ‚ÄúInput‚Äù section of the talk, reinforcing that before the model even processes data, multiple layers of text transformation occur that can alter the result.\n\n\n33. Inter-text Similarity\n\n\n\nSlide 33\n\n\n(Timestamp: 11:15)\nThis heatmap compares Inter-text similarity between models. It highlights Llama 70B and Llama 8B. Even though they are from the same family and likely trained on similar data, they are not identical.\nThis means you cannot swap a smaller model for a larger one (or vice versa) and expect the exact same behavior. Any model change requires a full re-evaluation.\n\n\n34. Sycophantic Models\n\n\n\nSlide 34\n\n\n(Timestamp: 12:16)\nThe slide discusses Sycophancy‚Äîthe tendency of models to agree with the user even when the user is wrong. It mentions how early versions of GPT-4 were sometimes ‚Äúoverly nice.‚Äù\nThis behavior is a specific type of model bias that evaluators must watch for. If a user asks a leading question containing false premises, a sycophantic model might validate the falsehood rather than correct it.\n\n\n35. Model Drift\n\n\n\nSlide 35\n\n\n(Timestamp: 12:37)\n‚ÄúModel Drift‚Äù refers to the phenomenon where commercial APIs (like OpenAI or Anthropic) change their model behavior over time without warning.\nBecause developers do not control the weights of API-based models, the ‚Äúground underneath them‚Äù can shift. A prompt that worked yesterday might fail today because the provider updated the backend or the inference infrastructure.\n\n\n36. Degraded Responses Timeline\n\n\n\nSlide 36\n\n\n(Timestamp: 12:55)\nThis slide shows a timeline of Degraded Responses from an Anthropic incident. Technical issues like context window routing errors led to corrupted outputs for a period of days.\nThis illustrates that drift isn‚Äôt always about model updates; it can be infrastructure failures. Continuous monitoring is required to detect when an external dependency degrades your application‚Äôs performance.\n\n\n37. Hyperparameters\n\n\n\nSlide 37\n\n\n(Timestamp: 13:33)\nThe slide lists Hyperparameters like Temperature, Top-P, and Max Length. Rajiv explains that users can control these ‚Äúknobs‚Äù to influence creativity versus determinism.\nSetting temperature to 0 makes the model less random, but as the next slides show, it does not guarantee perfect determinism due to hardware nuances.\n\n\n38. Non-Deterministic Inference\n\n\n\nSlide 38\n\n\n(Timestamp: 14:03)\nThis slide tackles Non-Deterministic Inference. Unlike traditional ML models (e.g., XGBoost) where a fixed seed guarantees identical output, LLMs on GPUs often produce different results for identical inputs.\nCauses include floating-point accumulation errors and the behavior of Mixture of Experts (MoE) models where different batches might activate different experts.\n\n\n39. Addressing Non-Determinism\n\n\n\nSlide 39\n\n\n(Timestamp: 15:11)\nRajiv references recent work by Thinking Machines and updates to vLLM that attempt to solve the non-determinism problem through correct batching.\nWhile solutions are emerging, the takeaway is that most current setups are non-deterministic by default. Evaluators must design their tests to tolerate this variance rather than expecting bit-wise reproducibility.\n\n\n40. Updated Model Diagram\n\n\n\nSlide 40\n\n\n(Timestamp: 15:43)\nThe diagram expands again. The ‚ÄúModel‚Äù box now includes Model Selection, Hyperparameters, Non-deterministic Inference, and Forced Updates.\nThis visual summarizes the ‚ÄúModel‚Äù section, showing that the ‚Äúblack box‚Äù is actually a dynamic system with internal variables (weights/architecture) and external variables (infrastructure/updates) that all add noise to the output.\n\n\n41. Output Format Issues\n\n\n\nSlide 41\n\n\n(Timestamp: 16:01)\nMoving to the ‚ÄúOutput‚Äù stage, this slide uses MMLU again to show how Output Formatting affects evaluation. How do you ask the model to answer a multiple-choice question?\nDo you ask it to output just the letter ‚ÄúA‚Äù? Or the full text? Or the probability of the token ‚ÄúA‚Äù? Different evaluation harnesses use different methods, leading to the score discrepancies seen earlier.\n\n\n42. Evaluation Harness Variations\n\n\n\nSlide 42\n\n\n(Timestamp: 16:35)\nThis table details the specific differences in implementation between harnesses (e.g., original MMLU vs.¬†HELM vs.¬†EleutherAI).\nIt reinforces that there is no standard ‚Äúruler‚Äù for measuring LLMs. The tool you use to measure the model introduces its own bias and variance into the final score.\n\n\n43. Score Comparison Table\n\n\n\nSlide 43\n\n\n(Timestamp: 16:56)\nA spreadsheet shows the same models scoring differently across different evaluation implementations. The variance is not trivial; it can be large enough to change the ranking of which model is ‚Äúbest.‚Äù\nThis data drives home the point: You must control your own evaluation pipeline. Relying on reported numbers is risky because you don‚Äôt know the implementation details behind them.\n\n\n44. Sentiment Analysis Variance\n\n\n\nSlide 44\n\n\n(Timestamp: 17:09)\nThis slide shows varying Sentiment Analysis outputs. Different models (or the same model with different prompts) might classify a review as ‚ÄúPositive‚Äù while another says ‚ÄúNeutral.‚Äù\nThis introduces the concept that even ‚Äúsimple‚Äù classification tasks in GenAI are subject to interpretation and variance, unlike traditional classifiers that have a fixed decision boundary.\n\n\n45. Tool Use Variance\n\n\n\nSlide 45\n\n\n(Timestamp: 17:23)\nRadar charts illustrate variance in Tool Use. Models might be good at using an ‚ÄúEmail‚Äù tool but fail at ‚ÄúCalendar‚Äù or ‚ÄúTerminal‚Äù tools.\nFurthermore, models exhibit non-determinism in decision making‚Äîsometimes they choose to use a tool, and sometimes they try to answer from memory. This adds a layer of logic errors on top of text generation errors.\n\n\n46. Summary: Why Responses Differ\n\n\n\nSlide 46\n\n\n(Timestamp: 17:49)\nThis comprehensive slide aggregates all the factors discussed: Inputs (prompts, system prompts), Model (drift, hyperparams), Outputs (formatting), and Infrastructure.\nIt serves as a checklist for the audience. If your application is behaving inconsistently, investigate these specific layers of the stack to find the source of the noise.\n\n\n47. Chaos is Okay\n\n\n\nSlide 47\n\n\n(Timestamp: 18:17)\nRajiv reassures the audience that ‚ÄúChaos is Okay.‚Äù The slide presents a chart of evaluation methods ranging from flexible/expensive (human eval) to rigid/cheap (code assertions).\nThe message is that while the technology is chaotic, there is a spectrum of tools available to manage it. We don‚Äôt need to solve every source of variance; we just need a robust process to measure it.\n\n\n48. From Chaos to Control\n\n\n\nSlide 48\n\n\n(Timestamp: 18:27)\nThis transition slide marks the beginning of the Evaluation Workflow section. The presentation shifts from describing the problem to prescribing the solution.\nThe goal here is to move from ‚ÄúVibe Coding‚Äù to a structured engineering discipline where changes are measured against a stable baseline.\n\n\n49. Build the Evaluation Dataset\n\n\n\nSlide 49\n\n\n(Timestamp: 18:37)\nThe first step in the workflow is to Build the Evaluation Dataset. The slide lists examples of prompts for tasks like summarization, extraction, and translation.\nRajiv emphasizes that this dataset should reflect your actual use case. It is the foundation of the ‚ÄúCustom Benchmark‚Äù concept introduced earlier.\n\n\n50. Get Labeled Outputs (Gold)\n\n\n\nSlide 50\n\n\n(Timestamp: 18:46)\nStep two is to get Labeled Outputs, also known as Gold Outputs, Reference, or Ground Truth. The slide adds a column showing the ideal answer for each prompt.\nThis is the standard against which the model will be judged. While obtaining these labels can be expensive (requiring human effort), they are essential for calculating accuracy.\n\n\n51. Compare to Model Output\n\n\n\nSlide 51\n\n\n(Timestamp: 19:00)\nStep three is to generate responses from your system and place them alongside the Gold Outputs. The slide adds a ‚ÄúModel Output‚Äù column.\nThis visual comparison allows developers (and automated judges) to see the delta between what was expected and what was produced.\n\n\n52. Measure Equivalence\n\n\n\nSlide 52\n\n\n(Timestamp: 19:10)\nStep four is to Measure Equivalence. Since LLMs rarely produce exact string matches, we use an LLM Judge (another model) to determine if the Model Output means the same thing as the Gold Output.\nThe slide shows a prompt for the judge: ‚ÄúAre these two responses semantically equivalent?‚Äù This converts a fuzzy text comparison problem into a binary (Pass/Fail) metric.\n\n\n53. Optimize Using Equivalence\n\n\n\nSlide 53\n\n\n(Timestamp: 19:57)\nOnce you have an equivalence metric, you can Optimize. The slide shows Config A vs.¬†Config B. By changing prompts or models, you can track if your ‚ÄúEquivalence Score‚Äù goes up or down.\nThis treats GenAI engineering like traditional hyperparameter tuning. The goal is to maximize the equivalence score on your custom dataset.\n\n\n54. Why Global Metrics Aren‚Äôt Enough\n\n\n\nSlide 54\n\n\n(Timestamp: 20:28)\nThe slide discusses the limitations of the ‚ÄúEquivalence‚Äù approach. While good for a general sense of quality, Global Metrics miss nuances.\nSometimes it‚Äôs hard to get a Gold Answer for open-ended creative tasks. Furthermore, a simple ‚ÄúPass/Fail‚Äù doesn‚Äôt tell you why the model failed (e.g., was it tone, length, or factuality?).\n\n\n55. From Global to Targeted Evaluation\n\n\n\nSlide 55\n\n\n(Timestamp: 20:55)\nThis slide argues for Targeted Evaluation. To maximize performance, you need to dig deeper into the data and identify specific error modes.\nThis transitions the talk from ‚ÄúBasic Workflow‚Äù to ‚ÄúAdvanced Testing,‚Äù where we break down ‚ÄúQuality‚Äù into specific, testable components like tone, length, and safety.\n\n\n56. Building Tests\n\n\n\nSlide 56\n\n\n(Timestamp: 21:14)\nThe section title ‚ÄúBuilding Tests‚Äù appears. This is where the presentation moves into the ‚ÄúUnit Testing‚Äù philosophy for GenAI.\nJust as software engineering relies on unit tests to verify specific functions, GenAI engineering should use targeted tests to verify specific attributes of the generated text.\n\n\n57. Good vs.¬†Bad Examples\n\n\n\nSlide 57\n\n\n(Timestamp: 21:20)\nThe slide displays a Good Example and a Bad Example of a response. The bad example is visibly shorter and less polite.\nRajiv asks the audience to identify why it is bad. This exercise is crucial: you cannot build a test until you can articulate exactly what makes a response a failure.\n\n\n58. Develop an Evaluation Mindset\n\n\n\nSlide 58\n\n\n(Timestamp: 21:46)\nTo define ‚ÄúBad,‚Äù developers need an Evaluation Mindset. This involves observing real-world user interactions and problems.\nData scientists often want to stay in their ‚Äúchair‚Äù and optimize algorithms, but Rajiv argues that effective evaluation requires understanding the user‚Äôs pain points.\n\n\n59. Collaborate with Experts\n\n\n\nSlide 59\n\n\n(Timestamp: 21:58)\nThe slide stresses Collaboration. You must talk to domain experts (e.g., the customer support team) to define what a ‚Äúgood‚Äù answer looks like.\nNaive bootstrapping‚Äîpretending to be a user‚Äîis a good start, but long-term success requires input from the people who actually know the business domain.\n\n\n60. Identify and Categorize Failures\n\n\n\nSlide 60\n\n\n(Timestamp: 22:52)\nOnce you understand the domain, you can Categorize Failure Types. The slide shows a chart grouping errors into categories like ‚ÄúHarmful Content,‚Äù ‚ÄúBias,‚Äù or ‚ÄúIncorrect Info.‚Äù\nThis clustering allows you to see patterns. Instead of just knowing ‚Äúthe model failed 20% of the time,‚Äù you know ‚Äúthe model has a specific problem with tone.‚Äù\n\n\n61. Define What Good Looks Like\n\n\n\nSlide 61\n\n\n(Timestamp: 23:11)\nUsing the categorization, you can explicitly Define What Good Looks Like. The slide contrasts the good/bad examples again, but now with labels: ‚ÄúToo short,‚Äù ‚ÄúLacks professional tone.‚Äù\nThis transforms a subjective feeling (‚Äúthis response sucks‚Äù) into objective criteria (‚Äúresponse must be &gt;50 words and use polite honorifics‚Äù).\n\n\n62. Document Every Issue\n\n\n\nSlide 62\n\n\n(Timestamp: 23:32)\nThe slide shows a spreadsheet where humans evaluate responses and Document Every Issue. Columns track specific attributes like ‚ÄúIs it helpful?‚Äù or ‚ÄúIs the tone right?‚Äù\nThis manual annotation is the training data for your automated tests. You need humans to establish the ground truth before you can automate the checking.\n\n\n63. Evaluation Tooling\n\n\n\nSlide 63\n\n\n(Timestamp: 23:53)\nRajiv mentions that Tooling Can Help. The slide shows a custom chat viewer designed to make human review easier.\nHowever, he warns against getting sidetracked by building fancy tools. Simple spreadsheets often suffice for the early stages. The goal is the data, not the interface.\n\n\n64. Test 1: Length Check\n\n\n\nSlide 64\n\n\n(Timestamp: 24:05)\nNow we build the automated tests. Test 1 is a Length Check. The slide shows Python code asserting that the word count is between 8 and 200.\nThis is a deterministic test. You don‚Äôt need an LLM to count words. Rajiv encourages using simple Python assertions wherever possible because they are fast, cheap, and reliable.\n\n\n65. Test 2: Tone and Style\n\n\n\nSlide 65\n\n\n(Timestamp: 24:22)\nTest 2 checks Tone and Style. Since ‚Äútone‚Äù is subjective, we use an LLM Judge (OpenAI model) to classify the response.\nThe prompt asks the judge to identify the style. This allows us to automate the ‚Äúvibe check‚Äù that humans were previously doing manually.\n\n\n66. Adding Metrics to Documentation\n\n\n\nSlide 66\n\n\n(Timestamp: 24:41)\nThe spreadsheet is updated with new columns: Length_OK and Tone_OK. These are the results of the automated tests.\nNow, for every row in the dataset, we have granular pass/fail metrics. This helps pinpoint exactly why a specific response failed, rather than just a generic failure.\n\n\n67. Check Judges Against Humans\n\n\n\nSlide 67\n\n\n(Timestamp: 25:12)\nA critical step: Check LLM Judges Against Humans. You must verify that your automated ‚ÄúTone Judge‚Äù agrees with your human experts.\nIf the human says the tone is rude, but the LLM Judge says it‚Äôs polite, your metric is useless. You must iterate on the judge‚Äôs prompt until alignment is high.\n\n\n68. Self-Evaluation Bias\n\n\n\nSlide 68\n\n\n(Timestamp: 26:06)\nThe slide illustrates Self-Evaluation Bias. LLMs tend to rate their own outputs higher than outputs from other models. GPT-4 prefers GPT-4 text.\nTo mitigate this, Rajiv suggests mixing models‚Äîuse Claude to judge GPT-4, or Gemini to judge Claude. This helps ensure a more neutral evaluation.\n\n\n69. Alignment Checks\n\n\n\nSlide 69\n\n\n(Timestamp: 26:46)\nThis slide reinforces the need for Continuous Alignment. Just because your judge aligned with humans last month doesn‚Äôt mean it still does (due to model drift).\nHuman spot-checks should be a permanent part of the pipeline to ensure the automated judges haven‚Äôt drifted.\n\n\n70. Biases in LLM Judges\n\n\n\nSlide 70\n\n\n(Timestamp: 27:02)\nThe slide lists known Biases in LLM Judges, such as Position Bias (favoring the first answer presented) or Verbosity Bias (favoring longer answers).\nEvaluators must be aware of these. For example, you should shuffle the order of answers when asking a judge to compare two options to cancel out position bias.\n\n\n71. Best Practices for LLM Judges\n\n\n\nSlide 71\n\n\n(Timestamp: 27:11)\nA summary of Best Practices: Calibrate with human data, use ensembles (multiple judges), avoid asking for ‚Äúrelevance‚Äù (too vague), and use discrete rating scales (1-5) rather than continuous numbers.\nThese tips help stabilize the inherently noisy process of using AI to evaluate AI.\n\n\n72. Error Analysis Chart\n\n\n\nSlide 72\n\n\n(Timestamp: 27:46)\nWith tests in place, we move to Error Analysis. The bar chart shows the number of failed cases categorized by error type (Length, Tone, Professional, Context).\nThis visualization tells you where to focus your efforts. If ‚ÄúTone‚Äù is the biggest bar, you work on the system prompt‚Äôs tone instructions. If ‚ÄúContext‚Äù is the issue, you might need better Retrieval Augmented Generation (RAG).\n\n\n73. Comparing Prompts\n\n\n\nSlide 73\n\n\n(Timestamp: 27:58)\nThe chart can compare Prompt A vs.¬†Prompt B. This allows for A/B testing of prompt engineering strategies.\nYou can see if a new prompt improves ‚ÄúTone‚Äù but accidentally degrades ‚ÄúContext.‚Äù This tradeoff analysis is impossible with a single global score.\n\n\n74. Explanations Guide Improvement\n\n\n\nSlide 74\n\n\n(Timestamp: 28:14)\nRajiv suggests asking the LLM Judge for Explanations. Don‚Äôt just ask for a score; ask for ‚Äúone sentence explaining why.‚Äù\nThese explanations act as metadata that helps developers understand the judge‚Äôs reasoning, making it easier to debug discrepancies between human and AI judgments.\n\n\n75. Limits to Explanations\n\n\n\nSlide 75\n\n\n(Timestamp: 28:35)\nA warning: Explanations are not causal. When an LLM explains why it did something, it is generating a plausible justification, not a trace of its actual neural activations.\nTreat explanations as a heuristic or a helpful hint, not as absolute truth about the model‚Äôs internal state.\n\n\n76. The Evaluation Flywheel\n\n\n\nSlide 76\n\n\n(Timestamp: 28:46)\nThe Evaluation Flywheel describes the iterative cycle: Build Eval -&gt; Analyze -&gt; Improve -&gt; Repeat.\nThis concept, credited to Hamill, emphasizes that evaluation is not a one-time event but a continuous loop that spins faster as you gather more data and build better tests.\n\n\n77. Financial Analyst Agent Example\n\n\n\nSlide 77\n\n\n(Timestamp: 29:20)\nTo demonstrate advanced unit testing, Rajiv introduces a Financial Analyst Agent. The goal is to assess the specific ‚Äústyle‚Äù of a financial report.\nThis is a complex domain where ‚Äúgood‚Äù is highly specific (regulated, precise, risk-aware), making it a perfect candidate for granular unit tests.\n\n\n78. Use a Global Test?\n\n\n\nSlide 78\n\n\n(Timestamp: 29:43)\nYou could use a Global Test: ‚ÄúWas this explained as a financial analyst would?‚Äù\nWhile simple, this test is opaque. If it fails, you don‚Äôt know if it was because of compliance issues, lack of clarity, or poor formatting.\n\n\n79. Global vs.¬†Unit Tests\n\n\n\nSlide 79\n\n\n(Timestamp: 29:54)\nThe slide contrasts the Global approach with Unit Tests. Instead of one question, we ask six: Context, Clarity, Precision, Compliance, Actionability, and Risks.\nThis breakdown allows for targeted debugging. You might find the model is great at ‚ÄúClarity‚Äù but terrible at ‚ÄúCompliance.‚Äù\n\n\n80. Scoring Radar Chart\n\n\n\nSlide 80\n\n\n(Timestamp: 30:16)\nA Radar Chart visualizes the unit test scores. This allows for a quick visual assessment of the model‚Äôs profile.\nIt facilitates comparison: you can overlay the profiles of two different models to see which one has the better balance of attributes for your specific needs.\n\n\n81. Analyzing Failures with Clusters\n\n\n\nSlide 81\n\n\n(Timestamp: 30:37)\nWith enough unit test data, you can use Clustering (e.g., K-Means) to group failures. The slide shows clusters like ‚ÄúSynthesis,‚Äù ‚ÄúContext,‚Äù and ‚ÄúHallucination.‚Äù\nThis moves error analysis from reading individual logs to analyzing aggregate trends, helping you prioritize which class of errors to fix first.\n\n\n82. Designing Good Unit Tests\n\n\n\nSlide 82\n\n\n(Timestamp: 30:52)\nAdvice on Designing Unit Tests: Keep them focused (one concept per test), use unambiguous language, and use small rating ranges.\nGood unit tests are the building blocks of a reliable evaluation pipeline. If the tests themselves are noisy or vague, the entire system collapses.\n\n\n83. Examples of Unit Tests\n\n\n\nSlide 83\n\n\n(Timestamp: 30:55)\nThe slide lists specific examples of tests for Legal (Compliance, Terminology), Retrieval (Relevance, Completeness), and Bias/Fairness.\nThis serves as a menu of options for the audience, showing that unit tests can cover almost any dimension of quality required by the business.\n\n\n84. Evaluating New Prompts\n\n\n\nSlide 84\n\n\n(Timestamp: 30:58)\nA bar chart shows how unit tests are used to Evaluate New Prompts. By running the full suite of unit tests on a new prompt, you get a ‚Äúscorecard‚Äù of its performance.\nThis data-driven approach removes the guesswork from prompt engineering.\n\n\n85. Tools - No Silver Bullet\n\n\n\nSlide 85\n\n\n(Timestamp: 31:02)\nRajiv reminds the audience that Tools are No Silver Bullet. You must master the basics (datasets, metrics) first.\nHe advises logging traces and experiments and practicing Dataset Versioning. Tools facilitate these practices, but they cannot replace the fundamental engineering discipline.\n\n\n86. Forest and Trees\n\n\n\nSlide 86\n\n\n(Timestamp: 31:04)\nAn analogy helps structure the analysis: Forest (Global/Integration) vs.¬†Trees (Test Case/Unit Tests).\nYou need to look at both. The forest tells you the overall health of the app, while the trees tell you specifically what needs pruning or fixing.\n\n\n87. Change One Thing at a Time\n\n\n\nSlide 87\n\n\n(Timestamp: 31:17)\nA crucial scientific principle: Change One Thing at a Time. With so many knobs (prompt, temp, model, RAG settings), changing multiple variables simultaneously makes it impossible to know what caused the improvement (or regression).\nIsolate your variables to conduct valid experiments.\n\n\n88. Error Analysis Tips\n\n\n\nSlide 88\n\n\n(Timestamp: 31:32)\nA summary of Error Analysis Tips: Use ablation studies (removing parts to see impact), categorize failures, save interesting examples, and leverage logs/traces.\nThese are the daily habits of successful GenAI engineers.\n\n\n89. The Evaluation Story\n\n\n\nSlide 89\n\n\n(Timestamp: 32:08)\nThe slide shows the ‚ÄúStory We Tell‚Äù‚Äîa linear graph of improvement over time. This is the idealized version of progress often presented in case studies.\nIt suggests a smooth journey from ‚ÄúOut of the box‚Äù to ‚ÄúSpecialized‚Äù to ‚ÄúUser Feedback.‚Äù\n\n\n90. The Reality of Progress\n\n\n\nSlide 90\n\n\n(Timestamp: 32:24)\nThe Reality is a messy, non-linear graph. You take two steps forward, one step back. Sometimes an ‚Äúimprovement‚Äù breaks the model.\nRajiv encourages resilience. Experienced practitioners know that this messy graph is normal and that sticking to the process eventually yields results.\n\n\n91. Continual Process\n\n\n\nSlide 91\n\n\n(Timestamp: 33:01)\nEvaluation is a Continual Process. It involves Problem ID, Data Collection, Optimization, User Acceptance Testing (UAT), and Updates.\nCrucially, UAT is your holdout set. Since you don‚Äôt have a traditional test set in GenAI, your real users act as the final validation layer.\n\n\n92. Eating the Elephant\n\n\n\nSlide 92\n\n\n(Timestamp: 34:03)\nThe metaphor ‚ÄúHow do you eat an elephant?‚Äù addresses the overwhelming nature of building a comprehensive evaluation suite.\nThe answer, of course, is ‚Äúone bite at a time.‚Äù You don‚Äôt need 100 tests on day one.\n\n\n93. Adding Tests Over Time\n\n\n\nSlide 93\n\n\n(Timestamp: 34:10)\nThe slide visualizes the ‚Äúelephant‚Äù being broken down into bites. You start with a few critical tests. As the app matures and you discover new failure modes, you add more tests.\nSix months in, you might have 100 tests, but you built them incrementally. This makes the task manageable.\n\n\n94. Doing Evaluation the Right Way\n\n\n\nSlide 94\n\n\n(Timestamp: 34:39)\nA summary slide listing best practices: Annotated Examples, Systematic Documentation, Continuous Error Analysis, Collaboration, and awareness of Generalization.\nThis concludes the core methodology section of the talk.\n\n\n95. Agentic Use Cases\n\n\n\nSlide 95\n\n\n(Timestamp: 34:50)\nThe final section covers Agentic Use Cases, symbolized by a dragon. Agents add a layer of complexity because the model is now making decisions (routing, tool use) rather than just generating text.\nThis ‚Äúagency‚Äù makes the system harder to track and evaluate.\n\n\n96. Crossing the River\n\n\n\nSlide 96\n\n\n(Timestamp: 35:06)\nA conceptual slide asking, ‚ÄúHow should it cross the river?‚Äù (Fly, Swim, Bridge?). This represents the decision-making step in an agent.\nEvaluating an agent requires evaluating how it made the decision (the router) separately from how well it executed the action.\n\n\n97. Chat-to-Purchase Router\n\n\n\nSlide 97\n\n\n(Timestamp: 35:22)\nA complex flowchart shows a Chat-to-Purchase Router. The agent must decide if the user wants to search for a product, get support, or track a package.\nRajiv suggests breaking this down: evaluate the Router component first (did it pick the right path?), then evaluate the specific workflow (did it track the package correctly?).\n\n\n98. Text to SQL Agent\n\n\n\nSlide 98\n\n\n(Timestamp: 36:17)\nAnother example: Text to SQL Agent. This workflow involves classification, feature extraction, and SQL generation.\nYou can isolate the ‚ÄúClassification‚Äù step (is this a valid SQL question?) and build a test just for that, before testing the actual SQL generation.\n\n\n99. Evaluating Office-Style Agents\n\n\n\nSlide 99\n\n\n(Timestamp: 36:46)\nThe slide discusses OdysseyBench, a benchmark for office tasks. It highlights failure modes like ‚ÄúFailed to create folder‚Äù or ‚ÄúFailed to use tool.‚Äù\nEvaluating agents involves checking if they successfully manipulated the environment (files, APIs), which is a functional test rather than a text similarity test.\n\n\n100. Error Analysis for Agents\n\n\n\nSlide 100\n\n\n(Timestamp: 37:00)\nError Analysis for Agentic Workflows requires assessing the overall performance, the routing decisions, and the individual steps.\nIt is the same ‚Äúaction error analysis‚Äù process but applied recursively to every node in the agent‚Äôs decision tree.\n\n\n101. Evaluating Workflow vs.¬†Response\n\n\n\nSlide 101\n\n\n(Timestamp: 37:19)\nThis slide distinguishes between evaluating a Response (text) and a Workflow (process). The flowchart shows a conversational flow.\nEvaluating a workflow might mean checking if the agent successfully moved the user from ‚ÄúGreeting‚Äù to ‚ÄúResolution,‚Äù regardless of the exact words used.\n\n\n102. Agentic Frameworks\n\n\n\nSlide 102\n\n\n(Timestamp: 37:48)\nRajiv warns that ‚ÄúAgentic Frameworks Help ‚Äì Until They Don‚Äôt.‚Äù Frameworks (like LangChain or AutoGen) are great for demos because they abstract complexity.\nHowever, in production, these abstractions can break or become outdated. He often recommends using straight Python for production agents to maintain control and reliability.\n\n\n103. Abstraction for Workflows\n\n\n\nSlide 103\n\n\n(Timestamp: 38:32)\nThe slide illustrates the trade-off in Abstraction. You can build rigid workflows (orchestration) where you control every step, or use general agents where the LLM decides.\nOrchestration is more reliable but rigid. General agents are flexible but prone to non-deterministic errors.\n\n\n104. When Abstractions Break\n\n\n\nSlide 104\n\n\n(Timestamp: 38:53)\nModel providers are training models to handle workflows internally (removing the need for external orchestration).\nHowever, until models are perfect, developers often need to break tasks down into specific pieces to ensure reliability. The choice between ‚Äúletting the model do it‚Äù and ‚Äúscripting the flow‚Äù depends on the application‚Äôs risk tolerance.\n\n\n105. Lessons from Agent Benchmarks\n\n\n\nSlide 105\n\n\n(Timestamp: 39:15)\nThe slide lists Lessons from Reproducing Agent Benchmarks: Standardize evaluation, measure efficiency, detect shortcuts, and log real behavior.\nThese are advanced tips for those pushing the boundaries of what agents can do.\n\n\n106. Conclusion\n\n\n\nSlide 106\n\n\n(Timestamp: 39:27)\nThe final slide, ‚ÄúWe did it!‚Äù, concludes the presentation. Rajiv thanks the audience and provides the QR code again.\nHis final message is one of empowerment: he hopes the audience now has the confidence to go out, build their own evaluation datasets, and start ‚Äúhill climbing‚Äù their own applications.\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "practical-rag-text-to-sql.html",
    "href": "practical-rag-text-to-sql.html",
    "title": "Practical Lessons in Building Generative AI: RAG and Text to SQL",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "practical-rag-text-to-sql.html#video",
    "href": "practical-rag-text-to-sql.html#video",
    "title": "Practical Lessons in Building Generative AI: RAG and Text to SQL",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "practical-rag-text-to-sql.html#annotated-presentation",
    "href": "practical-rag-text-to-sql.html#annotated-presentation",
    "title": "Practical Lessons in Building Generative AI: RAG and Text to SQL",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the annotated presentation based on the video transcript and slide summaries.\n\n1. Title Slide: A Practical Perspective on Generative AI\n\n\n\nSlide 1\n\n\n(Timestamp: 00:01)\nThis presentation begins with an introduction by Rajiv Shah from Snowflake. The talk focuses on distinguishing ‚Äúwhat‚Äôs easy to do with LLMs, what‚Äôs hard to do with LLMs, and where that boundary is for generative AI.‚Äù The content is framed as a practical guide for enterprises navigating the hype versus the reality of implementing these technologies.\nThe speaker sets the stage for a narrative-driven presentation that will move away from abstract theory and into concrete examples. The goal is to walk through the basics of Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) before applying them to real-world scenarios involving legal research and enterprise data analysis.\n\n\n2. Presentation Goals\n\n\n\nSlide 2\n\n\n(Timestamp: 00:31)\nThe agenda for the talk is outlined here. The speaker intends to cover the foundational mechanisms of how to use LLMs effectively, specifically focusing on RAG. To make the concepts relatable, the presentation uses two storytelling devices: a fictional law firm (‚ÄúDewey, Cheatham, and Howe‚Äù) and a hypothetical company (‚ÄúFrosty‚Äù).\nThese two stories serve to illustrate how people are currently using Generative AI, the specific limitations they encounter, and the engineering required to build a robust application. The speaker emphasizes that the talk will explore ‚Äúwhat does it take to actually develop a generative AI application‚Äù beyond just simple prompting.\n\n\n3. The Avianca Case\n\n\n\nSlide 3\n\n\n(Timestamp: 01:00)\nThe speaker introduces the concept of hallucinations through a famous real-world example involving the airline Avianca. A lawyer, attempting to speed up his work on a brief regarding a personal injury case, used ChatGPT for legal research. The AI ‚Äúfound some cases that were unpublished,‚Äù which the lawyer cited in court.\nHowever, ChatGPT had ‚Äúmade up those cases.‚Äù The lawyer was admonished by the bar for submitting fictitious legal precedents. This slide serves as a warning: while LLMs are powerful tools, they cannot be blindly trusted for factual research because they are prone to fabricating information when they don‚Äôt know the answer.\n\n\n4. Generative AI in Action\n\n\n\nSlide 4\n\n\n(Timestamp: 02:14)\nTo demonstrate the variability of LLMs, the speaker presents a side-by-side comparison of two models (Google Gemma and a ‚ÄúWoflesh‚Äù model) answering the same prompt: ‚ÄúHow many vehicles will Rivian manufacture in Normal, Illinois?‚Äù The models provide different answers.\nThis illustrates a key characteristic of Generative AI: ‚ÄúTwo different manufacturers, two different methods for training these models are probably going to lead to two different results.‚Äù It highlights that out-of-the-box models rely on their specific training data, which may be outdated or weighted differently, leading to inconsistent factual accuracy.\n\n\n5. Next Token Prediction\n\n\n\nSlide 5\n\n\n(Timestamp: 03:02)\nThis technical diagram explains why models hallucinate. The speaker clarifies that LLMs function by trying to predict the next word or token based on statistical likelihood. They are not databases of facts; they are engines designed to construct coherent sentences.\n‚ÄúThey‚Äôre not worried about truth and false; they‚Äôre really trying to tell what the most cohesive, coherent story is.‚Äù Because the model is optimizing for the most probable next word to complete a pattern, it will confidently generate plausible-sounding but factually incorrect information if that sequence of words is statistically likely.\n\n\n6. LLM Mistakes\n\n\n\nSlide 6\n\n\n(Timestamp: 03:30)\nHere, the speaker provides examples of the ‚ÄúNext Token Prediction‚Äù logic failing to provide truth. If asked for the ‚ÄúCapital of Mars,‚Äù the model doesn‚Äôt know Mars has no capital; it simply tries to ‚Äúcomplete that story‚Äù by inventing a name. Similarly, when asked to perform math, the model isn‚Äôt calculating; it is predicting the next characters in a math-like sequence.\nThe slide shows the model failing at basic arithmetic because ‚Äúit looks like it‚Äôs read too many release notes, not actually enough math.‚Äù This reinforces that LLMs are linguistic tools, not calculators or knowledge bases, and they lack an internal concept of ‚Äúfictional‚Äù versus ‚Äúfactual.‚Äù\n\n\n7. Risks for Enterprises\n\n\n\nSlide 7\n\n\n(Timestamp: 04:03)\nThis slide highlights the liability risks for companies, citing the Air Canada chatbot case. In this instance, a chatbot invented a refund policy that did not exist. When the customer sued, the airline argued the chatbot was responsible, but the tribunal ruled the company was liable for its agent‚Äôs statements.\nThe speaker notes, ‚ÄúWe‚Äôre going to treat this chatbot just like one of your employees‚Ä¶ you‚Äôre responsible for what this model says.‚Äù This legal precedent explains why enterprises are hesitant to deploy Gen AI and why ‚ÄúGen AI committees‚Äù are forming to manage governance and risk before public deployment.\n\n\n8. Retrieval-Augmented Generation (RAG)\n\n\n\nSlide 8\n\n\n(Timestamp: 04:51)\nTo solve the hallucination problem, the presentation introduces Retrieval-Augmented Generation (RAG). The speaker describes this as a solution from academia designed to ‚Äúground‚Äù the model. Instead of relying solely on the model‚Äôs internal training data, RAG surrounds the model with external context.\nThe core idea is simple: ‚ÄúWe‚Äôre going to ground it with information so it uses that information in answering the question.‚Äù This technique attempts to bridge the gap between the model‚Äôs linguistic capabilities and the need for factual accuracy in enterprise applications.\n\n\n9. How RAG Works\n\n\n\nSlide 9\n\n\n(Timestamp: 05:20)\nThis diagram breaks down the RAG architecture. When a user asks a question, the system does not send it directly to the LLM. First, it goes out to ‚Äúsearch and look for is there relevant information that‚Äôs related to this question.‚Äù\nOnce relevant documents are collected from a knowledge base, they are bundled with the original question and sent to the LLM. The LLM then generates an answer based only on that provided context. This ensures the ‚Äúfinal answer is grounded‚Äù by factual documents rather than the model‚Äôs statistical predictions alone.\n\n\n10. Grounding with 10-K Forms\n\n\n\nSlide 10\n\n\n(Timestamp: 05:51)\nThe speaker sets up a practical RAG demonstration using 10-K forms (annual reports filed by public companies). These documents are chosen because ‚Äúyou can trust that they‚Äôre factual.‚Äù\nThis slide prepares the audience to see how the previous question about Rivian‚Äôs manufacturing capacity‚Äîwhich generated inconsistent answers earlier‚Äîcan be answered accurately when the model is forced to look at Rivian‚Äôs official financial filings.\n\n\n11. Rivian Manufacturing Answer\n\n\n\nSlide 11\n\n\n(Timestamp: 06:07)\nThe slide shows the output of a RAG application. The question ‚ÄúHow many vehicles do you manufacture in Normal?‚Äù is asked again. This time, the application provides a specific, fact-based answer derived from the uploaded documents.\nThis demonstrates the immediate utility of RAG: it turns the LLM from a creative writing engine into a synthesis engine that can read specific enterprise documents and extract the correct answer, mitigating the hallucination issues seen in Slide 4.\n\n\n12. Context and Citations\n\n\n\nSlide 12\n\n\n(Timestamp: 06:21)\nA critical feature of RAG is displayed here: Citations. The application shows exactly which document the answer came from. The speaker notes, ‚ÄúI can see exactly what‚Äôs the document that this answer came from‚Ä¶ a nice source.‚Äù\nThis transparency is why RAG is the ‚Äúnumber one most popular generative AI application.‚Äù It allows users to verify the AI‚Äôs work, building trust in the system‚Äîsomething impossible with a standard ‚Äúblack box‚Äù LLM response.\n\n\n13. Chatbot for Legal Research\n\n\n\nSlide 13\n\n\n(Timestamp: 07:00)\nThe narrative shifts to the fictional law firm ‚ÄúDewey, Cheatham, and Howe.‚Äù The firm wants to use AI to reduce the heavy workload of legal research. The initial thought process is to use raw LLMs because they are knowledgeable.\nThe speaker introduces a colleague who assumes, ‚ÄúI know it could pass the bar exam‚Ä¶ why don‚Äôt I just wire it up directly?‚Äù This sets up the common misconception that because a model has general knowledge (passing a test), it is suitable for specialized professional work without further engineering.\n\n\n14. GPT Models on the Bar Exam\n\n\n\nSlide 14\n\n\n(Timestamp: 07:22)\nThis chart reinforces the previous assumption, showing the progression of GPT models on the Multistate Bar Exam (MBE). GPT-4 significantly outperforms its predecessors, achieving a passing score.\nWhile this suggests the model ‚Äúknows something about the law,‚Äù the speaker hints that this is merely a multiple-choice test. Success here does not necessarily translate to the nuance required for actual legal practice, foreshadowing the errors to come in the story.\n\n\n15. Hallucinating Statutes\n\n\n\nSlide 15\n\n\n(Timestamp: 07:50)\nThe first failure of the ‚Äúraw LLM‚Äù approach is revealed. A lawyer asks for statutes regarding ‚Äúonline dating services in Connecticut.‚Äù The model confidently provides ‚ÄúConnecticut General Statute ¬ß 42-290.‚Äù\nHowever, the lawyer discovers ‚Äúthere is no statute; this was entirely hallucinated.‚Äù Despite passing the bar exam, the model fabricated a law that sounded plausible but did not exist. This forces the firm to pivot toward a RAG approach to ground the AI in real legal literature.\n\n\n16. Lexis+ AI\n\n\n\nSlide 16\n\n\n(Timestamp: 08:30)\nThe firm decides to use professional tools. They turn to Lexis+ AI, a commercial product that promises ‚ÄúHallucination-Free Linked Legal Citations.‚Äù This tool uses the RAG approach discussed earlier, retrieving from a database of real case law.\nThe expectation is that by using a trusted vendor with a RAG architecture, the hallucination problem will be solved, and lawyers will receive accurate, citable information.\n\n\n17. Conceptual Hallucinations\n\n\n\nSlide 17\n\n\n(Timestamp: 08:50)\nEven with RAG and real citations, a new problem emerges: Conceptual confusion. The AI provides a real case but confuses the ‚ÄúEquity Cleanup Doctrine‚Äù with the ‚ÄúDoctrine of Clean Hands.‚Äù The speaker explains that while the words are similar, the legal concepts are distinct (one is about consolidating claims, the other about a plaintiff‚Äôs conduct, illustrated by a joke about P. Diddy).\nThe model found a document containing the words but failed to understand the meaning. This shows that RAG ensures the document exists, but not necessarily that the reasoning or application of that document is correct.\n\n\n18. The Fictional Judge\n\n\n\nSlide 18\n\n\n(Timestamp: 09:50)\nThe model‚Äôs failure deepens with an example of an ‚Äúinside joke.‚Äù A lawyer asks for opinions by ‚ÄúJudge Luther A. Wilgarten.‚Äù Wilgarten is a fictional judge created as a prank in law reviews.\nThe AI, treating the law reviews as factual text, retrieves ‚Äúcases‚Äù by this fake judge. It fails to distinguish between a real judicial opinion and a satirical article within its knowledge base. This illustrates the ‚Äúgarbage in, garbage out‚Äù risk even within RAG systems if the model cannot discern the nature of the source material.\n\n\n19. Hallucination Rates in Legal AI\n\n\n\nSlide 19\n\n\n(Timestamp: 10:44)\nThe speaker references a Stanford paper analyzing hallucination rates across major legal AI tools (Lexis, Westlaw, GPT-4). The chart shows that these tools still hallucinate or provide incomplete answers 17% to 33% of the time.\nThis data point serves as a reality check: ‚ÄúThese models hallucinate using real questions.‚Äù Despite marketing claims of being ‚Äúhallucination-free,‚Äù the complexity of the domain means that errors are still frequent, posing significant risks for professional use.\n\n\n20. Limits of RAG\n\n\n\nSlide 20\n\n\n(Timestamp: 11:03)\nThis slide summarizes the limitations discovered in the legal example. RAG works well when documents are ‚ÄúTrue, Authoritative, and Applicable.‚Äù However, in complex domains like law, these attributes are often contested.\n‚ÄúSometimes all these things are very contested and it gets really hard to separate it.‚Äù If the underlying documents contain conflicting information, satire, or outdated facts, the RAG system (which assumes retrieved text is ‚Äútruth‚Äù) will propagate those errors to the user.\n\n\n21. Why Legal is Hard\n\n\n\nSlide 21\n\n\n(Timestamp: 11:28)\nThe speaker elaborates on the complexity of legal research. It involves navigating different specialties (Tort vs.¬†Maritime), jurisdictions (Federal vs.¬†State), and authorities (Supreme Court vs.¬†Law Reviews). Furthermore, the element of time is crucial‚Äîknowing if a case has been overturned.\n‚ÄúYou really have to have a lot of knowledge to be able to weave everything in and out.‚Äù An LLM often lacks the meta-knowledge to weigh these factors, treating a lower court opinion from 1950 with the same weight as a Supreme Court ruling from 2024.\n\n\n22. Conclusion on Legal Chatbots\n\n\n\nSlide 22\n\n\n(Timestamp: 13:00)\nThe conclusion for the legal use case is that human expertise remains essential. While AI can ‚Äúget you a stack of documents,‚Äù you still need ‚Äúfacts people to actually tease out the insights.‚Äù\nThe current state of technology is an aid, not a replacement. The speaker transitions away from the legal example to a new story about building a data application, suggesting that while law is hard, structured data might offer different challenges and solutions.\n\n\n23. Building Generative AI (Text-to-SQL)\n\n\n\nSlide 23\n\n\n(Timestamp: 13:20)\nThe presentation shifts to the story of ‚ÄúFrosty,‚Äù a company building a Text-to-SQL application. The goal is to turn natural language questions (e.g., ‚ÄúHow many orders do I have in each state?‚Äù) into SQL code that can query a database.\nThis is a ‚Äúvery common application‚Äù for Gen AI, allowing non-technical users to interact with data. This section will focus on the engineering steps required to build this system, moving beyond the simple RAG implementation discussed previously.\n\n\n24. Evaluating SQL Queries\n\n\n\nSlide 24\n\n\n(Timestamp: 14:14)\nThe first challenge in building this app is evaluation. How do you know if the AI‚Äôs generated SQL is good? The slide shows a ‚ÄúGold Standard‚Äù query (the correct answer) and a ‚ÄúCandidate SQL‚Äù (the AI‚Äôs attempt).\nIn this example, the AI added an extra column (‚Äúlatitude‚Äù) that wasn‚Äôt requested. While the query might still work, it isn‚Äôt an exact match. The speaker notes, ‚ÄúWe really need to have a way to give partial credit,‚Äù because simple string matching would mark this helpful addition as a failure.\n\n\n25. Model Based Evaluation\n\n\n\nSlide 25\n\n\n(Timestamp: 15:45)\nTo solve the grading problem at scale, the speaker introduces Model-Based Evaluation. This involves using an LLM (like GPT-4) to act as the ‚Äújudge‚Äù for the output of another model.\nInstead of humans manually grading thousands of SQL queries, ‚Äúwe‚Äôre going to use a large language model to do this.‚Äù This allows for nuanced grading (partial credit) that strict code comparison cannot provide.\n\n\n26. Skepticism of Model Evaluation\n\n\n\nSlide 26\n\n\n(Timestamp: 15:59)\nThe speaker acknowledges the common reaction to this technique: ‚ÄúIs that going to work? I mean that‚Äôs like the fox guarding the outhouse.‚Äù There is a fear of ‚Äúmodel collapse‚Äù or circular logic when AI evaluates AI.\nDespite this intuition, the speaker assures the audience that this is a standard and effective practice in modern AI development, and proceeds to explain how to implement it correctly.\n\n\n27. The Evaluation Prompt\n\n\n\nSlide 27\n\n\n(Timestamp: 16:14)\nThis slide reveals the system prompt used for the model-based judge. It instructs the LLM to act as a ‚Äúdata quality analyst‚Äù and provides a specific grading rubric (0 to 3 scale).\nBy explicitly defining what constitutes a ‚ÄúPerfect Match,‚Äù ‚ÄúGood Match,‚Äù or ‚ÄúNo Match,‚Äù the engineer can control how the AI judges the output. This turns a subjective assessment into a structured, automated process.\n\n\n28. The ‚ÄúTX‚Äù vs ‚ÄúTEXAS‚Äù Problem\n\n\n\nSlide 28\n\n\n(Timestamp: 16:50)\nA specific example of why strict matching fails. The user asked for data in ‚ÄúTexas.‚Äù The database uses the abbreviation ‚ÄòTX‚Äô, but the AI generated a query looking for ‚ÄòTEXAS‚Äô.\n‚ÄúIt‚Äôs a natural mistake here to confuse TX and Texas‚Ä¶ but if we go with the strict criteria of that exact match, we don‚Äôt get an exact match.‚Äù A standard code test would fail this, even though the intent is correct and easily fixable.\n\n\n29. Execution Accuracy Failure\n\n\n\nSlide 29\n\n\n(Timestamp: 17:16)\nThis slide confirms that under ‚ÄúExecution Accuracy‚Äù (strict matching), the query is a failure (‚ÄúNo Match‚Äù). This metric is too harsh for development because it obscures progress; a model that gets the logic right but misses an abbreviation is much better than one that writes gibberish.\n\n\n30. Execution Score Success\n\n\n\nSlide 30\n\n\n(Timestamp: 17:30)\nUsing the Model-Based Evaluation, the same ‚ÄòTX‚Äô vs ‚ÄòTEXAS‚Äô error is graded differently. The ‚ÄúExecution Score‚Äù is a ‚ÄúPerfect Match‚Äù because the judge recognizes the semantic intent was captured.\n‚ÄúIt captures the user‚Äôs intent‚Ä¶ the user could easily fix this.‚Äù This allows developers to optimize the model for logic and reasoning first, handling minor syntax issues separately.\n\n\n31. Correlation with Other Metrics\n\n\n\nSlide 31\n\n\n(Timestamp: 17:50)\nThe speaker presents data showing a strong correlation between the model-based scores and other evaluation methods. When the model judge gives a 5/5, other metrics generally agree.\nThis validation step is crucial. The engineer in the story checked her results and found ‚Äú80% were the exact same when she scored them.‚Äù This high level of agreement gives confidence in automating the evaluation pipeline.\n\n\n32. Research on Model Evaluation\n\n\n\nSlide 32\n\n\n(Timestamp: 18:19)\nSupporting the anecdote, this slide references broader research indicating that LLMs correlate with human judges about 80% of the time regarding correctness and readability.\n‚ÄúI got tired of adding research sites here‚Ä¶ universally we see that often in many contexts that these large language models correlate about 80% of the time to humans.‚Äù This establishes model-based evaluation as an industry standard.\n\n\n33. Initial Benchmark Results\n\n\n\nSlide 33\n\n\n(Timestamp: 18:50)\nAfter setting up the evaluation pipeline and creating an internal enterprise benchmark (not a public dataset), the initial results are poor: only 33% accuracy.\nThe speaker emphasizes the importance of using internal data for benchmarks: ‚ÄúYou can‚Äôt trust those public data sets‚Ä¶ they‚Äôre far too easy.‚Äù The low score sets the stage for the iterative engineering process required to improve the application.\n\n\n34. Using Multiple Models\n\n\n\nSlide 34\n\n\n(Timestamp: 19:15)\nThe first improvement strategy is Ensembling. The engineer noticed different models had different strengths, so she combined them.\n‚ÄúIn traditional machine learning, we often Ensemble models‚Ä¶ she decided to try the same thing here.‚Äù By using multiple Text-to-SQL models and combining their outputs, performance improved.\n\n\n35. Error Correction (Self-Reflection)\n\n\n\nSlide 35\n\n\n(Timestamp: 19:40)\nThe next optimization is Error Correction via self-reflection. When the model generates an error, the system asks the model to ‚Äúreflect upon it‚Äù or think ‚Äústep-by-step.‚Äù\n‚ÄúThat actually makes the model spend more time thinking about it‚Ä¶ and actually they can use all of that to get a better answer.‚Äù This technique, often called Chain of Thought, leverages the model‚Äôs ability to debug its own output when prompted correctly.\n\n\n36. Screening Inputs\n\n\n\nSlide 36\n\n\n(Timestamp: 20:30)\nImproving the input data is just as important as improving the model. The engineer adds a Screening layer to filter out questions that are ambiguous or irrelevant (non-SQL questions).\n‚ÄúShe noticed that a lot of what the users were typing in just didn‚Äôt make sense.‚Äù By catching bad queries early and asking the user for clarification, the system avoids processing garbage data, thereby increasing overall success rates.\n\n\n37. Feature Extraction\n\n\n\nSlide 37\n\n\n(Timestamp: 21:40)\nRecognizing that different questions require different handling, the engineer implements Feature Extraction. A time-series question needs different context than a ranking question.\n‚ÄúIf I‚Äôm cooking macaroni and cheese I need different ingredients than if I‚Äôm making tacos.‚Äù The system now identifies the type of question and extracts the specific features (metadata, table schemas) relevant to that type before generating SQL.\n\n\n38. The Semantic Layer\n\n\n\nSlide 38\n\n\n(Timestamp: 22:50)\nTo bridge the gap between messy enterprise databases and user language, a Semantic Layer is added. This involves human experts defining the data structure in business terms.\n‚ÄúWe‚Äôre going to use the expertise‚Ä¶ to give us details of the data structure in a way that deals with all this confusing structure.‚Äù This layer translates business logic (e.g., what defines a ‚Äúchurned customer‚Äù) into a schema the AI can understand, significantly boosting accuracy.\n\n\n39. Generative AI Decision App\n\n\n\nSlide 39\n\n\n(Timestamp: 24:10)\nThis flowchart represents the final, production-grade system. It is no longer just a prompt sent to a model. It includes classification, feature extraction, multiple SQL generation agents, error correction, and a semantic layer.\nThe lesson is that ‚ÄúGenerative AI is not about a data scientist sitting out an Island by themselves‚Ä¶ instead it‚Äôs building a system like this.‚Äù It requires a cross-functional team of analysts, engineers, and domain experts to build a reliable application.\n\n\n40. The Future of AI\n\n\n\nSlide 40\n\n\n(Timestamp: 25:00)\nThe speaker pivots to the future, acknowledging the rapid pace of innovation from companies like OpenAI and Google DeepMind. He addresses the audience‚Äôs potential skepticism: ‚ÄúThe future is you‚Äôre just going to be able to just take all your data cram it into one thing it‚Äôs just going to solve it all for you.‚Äù\nThis sets up the final section on Reasoning and Planning, moving beyond simple retrieval and text generation.\n\n\n41. Can LLMs Reason and Plan? (Block World)\n\n\n\nSlide 41\n\n\n(Timestamp: 25:38)\nTo test reasoning, the speaker introduces the Block World benchmark. The task is to stack colored blocks in a specific order. This requires multi-step planning.\n‚ÄúYou have to logically think and plan for maybe five, six, ten, even 20 steps to be able to solve it.‚Äù This tests the model‚Äôs ability to handle dependencies and sub-tasks, rather than just predicting the next word.\n\n\n42. GPT-4 Planning Performance\n\n\n\nSlide 42\n\n\n(Timestamp: 26:40)\nThe results for GPT-4 are shown. While it achieves 34% on the standard Block World, its performance collapses to 3% in ‚ÄúMystery World.‚Äù Mystery World is the same problem, but the block names are randomized (e.g., obfuscated).\n‚ÄúWhat you call them doesn‚Äôt matter [to a human]‚Ä¶ but for a large language model, what you does call them matters a lot.‚Äù The collapse in performance proves the model was relying on memorized patterns (approximate reasoning) rather than true logical planning.\n\n\n43. o1 Models and Progress\n\n\n\nSlide 43\n\n\n(Timestamp: 27:50)\nThe speaker updates the data with the very latest OpenAI o1 model results. This model uses ‚ÄúChain of Thought on steroids‚Äù (reinforcement learning). It shows a massive improvement, jumping to nearly 100% on Block World and significantly higher on Mystery World (around 37-53%).\nWhile this is ‚Äúsolid progress,‚Äù the speaker notes it ‚Äústill has a ways to go.‚Äù The models are getting better at approximate reasoning, but they are not infallible logic engines yet.\n\n\n44. Be Skeptical of Benchmarks\n\n\n\nSlide 44\n\n\n(Timestamp: 29:36)\nA warning accompanies the new capabilities: Be skeptical. As models get better at approximating reasoning, their mistakes will become harder to spot. They will sound incredibly convincing even when they are logically flawed.\n‚ÄúYou‚Äôre going to have to have an expert to be able to tell when this models are going off the rails‚Ä¶ because the Baseline for these models is so good.‚Äù Just as legal experts were needed for RAG, domain experts are needed to verify AI reasoning.\n\n\n45. Common Gen AI Use Cases Summary\n\n\n\nSlide 45\n\n\n(Timestamp: 30:13)\nThe speaker summarizes the key technical concepts covered: Hallucinations, RAG, Reasoning, Evaluation, Model as a Judge, and Data Enrichment.\nThese pillars form the basis of current Gen AI development. The presentation has moved from the simple idea of ‚Äúasking a chatbot‚Äù to the complex reality of building systems that manage retrieval, evaluation, and reasoning.\n\n\n46. Project Reality\n\n\n\nSlide 46\n\n\n(Timestamp: 30:35)\nThe final takeaway emphasizes the organizational aspect. ‚ÄúGenerative AI is like any other project and doesn‚Äôt go as planned.‚Äù It is not magic; it is engineering.\nSuccess requires a diverse team (‚Äúsystem of people‚Äù) including evaluators, analysts, and technical builders. It is an iterative process that involves handling messy data and managing expectations.\n\n\n47. Closing Title\n\n\n\nSlide 47\n\n\n(Timestamp: 30:53)\nThe presentation concludes. The speaker thanks the audience, hoping the stories of the law firm and the data company provided a realistic ‚ÄúPractical Perspective‚Äù on the current state of Generative AI.\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "sparsedataframe.html",
    "href": "sparsedataframe.html",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "",
    "text": "When working with machine learning models, preparing data properly is essential. One common preprocessing technique is one-hot encoding, which transforms categorical data into a format algorithms can understand. However, this transformation often creates sparse matrices - dataframes where most values are zero."
  },
  {
    "objectID": "sparsedataframe.html#basic-one-hot-encoding",
    "href": "sparsedataframe.html#basic-one-hot-encoding",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "Basic One-Hot Encoding",
    "text": "Basic One-Hot Encoding\nThe first animation illustrates the fundamental concept of one-hot encoding. This transformation converts a single categorical column (like ‚Äúcity‚Äù) into multiple binary columns, where each column represents one possible category value.\nView the basic one-hot encoding animation\nThis visualization walks through the transformation step-by-step:\n\nStarting with the original dataset containing categorical values\nAdding binary indicator columns for each category\nShowing how the dataset becomes wider but sparse (mostly filled with zeros)\nDemonstrating how the original categorical column becomes redundant\n\nIn traditional tabular data processing, we often don‚Äôt see this sparsity visually. The animation makes it clear how one-hot encoding dramatically changes the structure of our data."
  },
  {
    "objectID": "sparsedataframe.html#the-curse-of-dimensionality",
    "href": "sparsedataframe.html#the-curse-of-dimensionality",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "The Curse of Dimensionality",
    "text": "The Curse of Dimensionality\nThe second animation takes the concept further by demonstrating what happens with high-cardinality categorical features - those with many possible values.\nView the curse of dimensionality animation\nThis more advanced visualization shows how one-hot encoding can lead to the ‚Äúcurse of dimensionality‚Äù:\n\nStarting with a modest 4-column dataset\nExpanding to over 150 columns when encoding a categorical feature with many values\nCreating an extremely sparse matrix where 99% of values are zeros\nIllustrating the practical challenges this presents for machine learning"
  },
  {
    "objectID": "sparsedataframe.html#why-it-matters",
    "href": "sparsedataframe.html#why-it-matters",
    "title": "Understanding Sparse Matrices through Interactive Visualizations",
    "section": "Why It Matters",
    "text": "Why It Matters\nUnderstanding the sparsity that results from one-hot encoding is crucial for several reasons:\n\nMemory usage: Sparse matrices can consume excessive memory if not properly handled\nComputational efficiency: Processing mostly-zero matrices is inefficient\nModel performance: Many algorithms struggle with extremely sparse data\nFeature selection: With hundreds of binary columns, feature selection becomes critical\n\nFor high-cardinality features, consider alternatives like feature hashing, target encoding, or embeddings to avoid the dimensionality explosion shown in the second animation.\nThese visualizations help build intuition about what‚Äôs happening ‚Äúunder the hood‚Äù when we preprocess data - something that‚Äôs often hidden when we use high-level libraries that handle these transformations automatically.\nRelated videos: Sparsity in AI or Curse of Dimensionality or Reality of Models"
  },
  {
    "objectID": "H2O_prod.html",
    "href": "H2O_prod.html",
    "title": "Taking an H2O Model to Production",
    "section": "",
    "text": "Introduction\nOne of the best feelings as a Data Scientist is when the model you have poured your heart and soul into, moves into production. Your model is now grown-up and you get to watch it mature.\nThis post shows how to take a H2O model and move it into a production environment. In this post, we will develop a simple H2O based predictive model, convert it into a Plain Old Java Object (POJO), compile it along with other Java packages, and package the compiled class files into a deployable JAR file so that it can readily be deployed onto any Java based application servers. This model will accept the input data set in the form of CSV file and return the predicted output in CSV format.\nH2O is one of my favorite tools for building models because it is well designed from an algorithm perspective, easy to use, and can scale to larger datasets. However, H2O‚Äôs documentation, though voluminous, doesn‚Äôt have clear instructions for moving a POJO model into production. This post will discuss this approach in greater detail besides providing code for how to do this. (H2O does have a post on doing real time predictions with storm). Special thanks to Socrates Krishnamurthy who co-wrote this post with me.\n\n\nBuilding H2O Model\nAs a starting point, lets use our favorite ice cream dataset to create a toy model in H2O:\n  library(h2o)  \n  library(Ecdat)  \n  data(Icecream)  \n  h2o.init()  \n  train.h2o &lt;- as.h2o(Icecream)  \n  rf &lt;- h2o.randomForest(x=2:4, y=1, ntrees=2, training_frame=train.h2o)   \nOnce you have developed your model in H2O, then the next step is downloading the POJO:\nh2o.download_pojo(rf, getjar=TRUE, path=\"~/Code/h2o-3.9.1.3459/test/\")\n# you must give a path to download a file\nThis will save two files, a H2O jar file about the model and an actual model file (that begins with DRF and ends with .java). Go ahead and open the model file in a text editor if you want to have a look at it.\n\n\nCompiling the H2O Model\nThe next step is to compile and run the model (say, the downloaded model name is DRF_model_R_1470416938086_15.java), then type:\n&gt; javac -cp h2o-genmodel.jar -J-Xmx2g DRF_model_R_1470416938086_15.java  \nThis creates a bunch of java class files.\n\n\nScoring the Input Data\nThe final step is scoring some input data. Prior to running the model, it is necessary to have files created for the input and output. For the input, the default setting is to read the first row as a header. The assumption is that the csv is well formed (this approach is not using the H2O parser). Once that is done, run:\n&gt; java -cp .:h2o-genmodel.jar hex.genmodel.tools.PredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv\nIf you open the output.csv file, it can be noticed that the predicted values are in Hexadecimal and not in Numeric format. For example, the output will be something like this:\n0x1.a24dd2p-2\n\n\nFixing the Hexadecimal Issue\nThe model is now predicting, but the predictions are in the wrong format. Yikes! To fix this issue requires some hacking of the java code. The rest of this post will show you how to hack the java code in PredictCsv, which can fix this issue and other unexpected issues with PredictCsv (for example, if your input comes tab separated).\nIf we take a deeper look at the PredictCsv java file located in the h2o github, the myDoubleToString method returns Hexadecimal string. But the challenge is this method being static in nature, cannot be overridden in a subclass or cannot be updated directly since it was provided by H2O jar file, to return regular numeric value in String format.\nThis can be fixed by creating a new java file (say, NewPredictCsv.java) by copying the entire content of PredictCsv.java from the above location and saving it locally. You then need to:\n\ncomment out the first line, so it should be //package hex.genmodel.tools;\n\nchange the name of the class name (~line 20) to read: public class NewPredictCsv {\n\ncorrect the hexadecimal issue by changing the return statement of myDoubleToString method to .toString() in lieu of .toHexString() (~line 131).\n\nAfter creating NewPredictCsv.java, compile it using the following command:\n&gt; javac -cp h2o-genmodel.jar -J-Xmx2g NewPredictCsv.java DRF_model_R_1470416938086_15.java\nRun the compiled file by providing input and output CSV files using the following command (Ensure that the input.csv file is in the current folder where you will run this):\n&gt; java -cp .:h2o-genmodel.jar NewPredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv\nIf you open the output.csv file now, it will be in the proper numeric format as follows:\n0.40849998593330383\n\n\nDeploying the Solution into Production:\nAt this point, we have a workable flow for using our model to score new data. But we can clean up the code to make it a little friendlier for our data engineers. First, create a jar file out of the class files created in previous steps. To do that, issue the following command:\n&gt; jar cf my-RF-model.jar *.class\nThis will place all the class files and our NewPredictCsv inside the jar. This is helpful when we have a model with say 500 trees. Now all we need is three files to run our scorer. So copy the above two jar files along with input.csv file in any folder/directory from where the program has to be executed. After copying, the folder should contain following files:\n&gt; my-RF-model.jar  \n&gt; h2o-genmodel.jar  \n&gt; input.csv  \nThe above input.csv file contains the dataset for which the dependent variable has to be predicted. To compute/ predict the values, run the java command as below:\n&gt; java -cp .:my-RF-model.jar:h2o-genmodel.jar NewPredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv\n\n\nNote:\nReplace : with ; in above commands if you are working in Windows (yuck)."
  },
  {
    "objectID": "HF-Reasoning.html",
    "href": "HF-Reasoning.html",
    "title": "Reasoning in Large Language Models",
    "section": "",
    "text": "Reasoning\n\n\n\nIntroduction\nI was wowed by ChatGPT. While I understood tasks like text generation and summarization, something was different with ChatGPT. When I looked at the literature, I saw this work exploring reasoning. Models reasoning, c‚Äômon. As a very skeptical data scientist, that seemed far-fetched to me. But I had to explore.\nI came upon the Big Bench Benchmark, composed of more than 200 reasoning tasks. The tasks include playing chess, describing code, guessing the perpetrator of a crime in a short story, identifying sarcasm, and even recognizing self-awareness. A common benchmark to test models is the Big Bench Hard (BBH), a subset of 23 tasks from Big Bench. Early models like OpenAI‚Äôs text-ada-00 struggle to reach a random score of 25. However, several newer models reach and surpass the average human rater score of 67.7. You can see results for these models in these publications: 1, 2, and 3.\n\n\n\nBig Bench Hard (23 Tasks) (1).png\n\n\nA survey of the research pointed out some common starting points for evaluating reasoning in models, including Arithmetic Reasoning, Symbolic Reasoning, and Commonsense Reasoning. This blog post provides examples of reasoning, but you should try out all these examples yourself. Hugging Face has a space where you can try to test a Flan T5 model yourself.\n\n\nArithmetic Reasoning\nLet‚Äôs start with the following problem.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: The answer is 5\nIf you ask an older text generation model like GPT-2 to complete this, it doesn‚Äôt understand the question and instead continues to write a story like this.\n\nWhile I don‚Äôt have access to PalM - 540B parameter model in the Big Bench, I was able to work with the Flan-T5 XXL using this publicly available space. I entered the problem and got this answer!\n\n\n\nR-Cars-Flan.png\n\n\nIt solved it! I tried messing with it and changing the words, but it still answered correctly. To my untrained eye, it is trying to take the numbers and perform a calculation using the surrounding information. This is an elementary problem, but this is more sophisticated than the GPT-2 response. I next wanted to do a more challenging problem like this:\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\nThe model gave an answer of 8, which isn‚Äôt correct. Recent research has found using chain-of-thought prompting can improve the ability of models. This involves providing intermediate reasoning to help the model determine the answer.\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is \nThe model correctly answers 11. To solve the juggling problem, I used this chain-of-thought prompt as an example. Giving the model some examples is known as few-shot learning. The new combined prompt using chain-of-thought and few-shot learning is:\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\nQ: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\nA:\nTry it, it works! Giving it an example and making it think everything through step by step was beneficial. This was fascinating for me. We don‚Äôt train the model in the sense of updating it‚Äôs weights. Instead, we are guiding it purely by the inference process.\n\n\nSymbolic Reasoning\nThe first symbolic reasoning was doing a reversal and the Flan-T5 worked very well on this type of problem.\nReverse the sequence \"glasses, pen, alarm, license\".\nA more complex problem on coin flipping was more interesting for me.\nQ: A coin is heads up. Tom does not flip the coin. Mike does not flip the coin. Is the coin still heads up?\nA:\nFor this one, I played around with different combinations of people flipping and showing the coin and the model, and it answered correctly. It was following the logic that was going through.\n\n\nCommon sense reasoning\nThe last category was common sense reasoning and much less obvious to me how models know how to solve these problems correctly.\nQ: What home entertainment equipment requires cable?\nAnswer Choices: (a) radio shack (b) substation (c) television (d) cabinet\nA: The answer is\nI was amazed at how well the model did, even when I changed the order.\n\n\n\nReasongif\n\n\nAnother common reasoning example goes like this:\nQ: Can Barack Obama have a conversation with George Washington? Give the rationale before answering.\nI changed around people to someone currently living, and it still works well.\n\n\nThoughts\nAs the first step, please, go try out these models for yourself. Google‚Äôs Flan-T5 is available with an Apache 2.0 license. Hugging Face has a space where you can try all these reasoning examples yourself. You can also replicate this using OpenAI‚Äôs GPT or other language models. I have a short video on the reasoning that also shows several examples.\nThe current language models have many known limitations. The next generation of models will likely be able to retrieve relevant information before answering. Additionally, language models will likely be able to delegate tasks to other services. You can see a demo of this integrating ChatGPT with Wolfram‚Äôs scientific API. By letting language models offload other tasks, the role of language models will emphasize communication and reasoning.\nThe current generation of models is starting to solve some reasoning tasks and match average human raters. It also appears that performance can still keep increasing. What happens when there are a set of reasoning tasks that computers are better than humans? While plenty of academic literature highlights the limitations, the overall trajectory is clear and has extraordinary implications."
  },
  {
    "objectID": "conformal_predictions.html",
    "href": "conformal_predictions.html",
    "title": "Getting predictions intervals with conformal inference",
    "section": "",
    "text": "Conformal\n\n\n\nIntroduction\nData scientists often overstate the certainty of their predictions. I have had engineers laugh at my point predictions and point out several types of errors in my model that create uncertainty. Prediction intervals are an excellent counterbalance for communicating the uncertainty of predictions.\nConformal inference offers a model agnostic technique for prediction intervals. It‚Äôs well known within statistics but not as well established in machine learning. This post focuses on a straightforward conformal inference technique, but there are more sophisticated techniques that provide more adaptable prediction intervals.\nI have created a Colab üìì companion notebook at https://bit.ly/raj_conf, and the Youtube üé• video that provides a detailed explanation. This explanation is a toy example to learn how conformal inference works. Typical applications will use a more sophisticated methodology along with implementations found within the resources below.\nFor python folks, a great package to start using conformal inference is MAPIE - Model Agnostic Prediction Interval Estimator. It works for tabular and time series problems.\n\n\nFurther Resources:\nQuick intro to conformal prediction using MAPIE in medium\nA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification, paper link\nAwesome Conformal Prediction (lots of resources)"
  },
  {
    "objectID": "explaining_transformers.html",
    "href": "explaining_transformers.html",
    "title": "Explaining predictions from ü§ó transformer models",
    "section": "",
    "text": "Banner\n\n\n\nIntroduction\nThis post covers 3 easy-to-use üì¶ packages to get started. You can also check out the Colab üìì companion notebook at https://bit.ly/raj_explain and the Youtube üé• video for a deeper treatment.\nExplanations are useful for explaining predictions. In the case of text, they highlight how the text influenced the prediction. They are helpful for ü©∫ diagnosing model issues, üëÄ showing stakeholders understand how a model is working, and üßë‚Äç‚öñÔ∏è meeting regulatory requirements. Here is an explanation üëá using shap. For more on explanations, check out the explanations in machine learning video.\n\n\n\nScreen Shot 2022-08-12 at 9.25.07 AM\n\n\nLet‚Äôs review 3 packages you can use to get explanations. All of these work with transformers, provide visualizations, and only require a few lines of code.\n\n\n\nRed and Purple Real Estate Soft Gradients Twitter Ad (1)\n\n\n\n\nShap\n\nSHAP is a well-known, well-regarded, and robust package for explanations. In working with text, SHAP typically defers to using a Partition Shap explainer. This method makes the shap computation tractable by using hierarchical clustering and Owens values. The image here shows the clustering for a simple phrase. If you want to learn more about Shapley values, I have a video on shapley values and a deep dive on Partition Shap explainer is here.\n\n\n\n\nScreen Shot 2022-08-12 at 9.35.34 AM\n\n\n\n\nTransformers Interpret\n\nTransformers Interpret uses Integrated Gradients from Captum to calculate the explanations. This approach is üêá quicker than shap! Check out this space to see a demo.\n\n\n\n\nScreen Shot 2022-08-12 at 9.27.04 AM\n\n\n\n\nFerret\n\nFerret is built for benchmarking interpretability techniques and includes multiple explanation methodologies (including Partition Shap and Integrated Gradients). A spaces demo for ferret is here along with a paper that explains the various metrics incorporated in ferret.\nYou can see below how explanations can differ when using different explanation methods. A great reminder that explanations for text are complicated and need to be appropriately caveated.\n\n\n\nScreen Shot 2022-08-11 at 1.19.05 PM\n\n\nReady to dive in? üü¢\nFor a longer walkthrough of all the üì¶ packages with code snippets, web-based demos, and links to documentation/papers, check out:\nüëâ Colab notebook: https://bit.ly/raj_explain\nüé• https://youtu.be/j6WbCS0GLuY"
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Optimization Strategies",
    "section": "",
    "text": "fanduel\n\n\n\nIntroduction\nAs a data scientist, you spend a lot of your time helping to make better decisions. You build predictive models to provide improved insights. You might be predicting whether an image is a cat or dog, store sales for the next month, or the likelihood if a part will fail. In this post, I won‚Äôt help you with making better predictions, but instead how to make the best decision.\nThe post strives to give you some background on optimization. It starts with a simply toy example show you the math behind an optimization calculation. After that, this post tackles a more sophisticated optimization problem, trying to pick the best team for fantasy football. The FanDuel image below is a very common sort of game that is widely played (ask your inlaws). The optimization strategies in this post were shown to consistently win! Along the way, I will show a few code snippets and provide links to working code in R, Python, and Julia. And if you do win money, feel free to share it :)\n\n\nSimple Optimization Example\nA simple example, which I found online, starts with a carpenter making bookcases in two sizes, large and small. It takes 6 hours to make a large bookcase and 2 hours to make a small one. The profit on a large bookcase is $50, and the profit on a small bookcase is $20. The carpenter can spend only 24 hours per week making bookcases and must make at least 2 of each size per week. Your job as a data scientist is to help your carpenter maximize her revenue.\nYour initial inclination could be that since the large bookcase is the most profitable, why not focus on them. In that case, you would profit (2*$20) + (3*$50) which is $190. That is a pretty good baseline, but not the best possible answer. It is time to get the algebra out and create equations that define the problem. First, we start with the constraints:\nx&gt;=2    ## large bookcases\n\ny&gt;=2    ## small bookcases\n\n6x + 2y &lt;= 24  (labor constraint)\nOur objective function which we are trying to maximize is:\nP = 50x + 20y\nIf we do the algebra by hand, we can convert out constraints to y &lt;= 12 - 3x. Then we graph all the constraints and find the feasible area for the portion of making small and large bookcases:\n\n\n\ngraph\n\n\nThe next step is figuring out the optimal point. Using the corner-point principle of linear programming, the maximum and minimum values of the objective function each occur at one of the vertices of the feasible region. Looking here, the maximum values (2,6) is when we make 2 large bookcases and 6 small bookcases, which results in an income of $220.\nThis is a very simple toy problem, typically there are many more constraints and the objective functions can get complicated. There are lots of classic problems in optimization such as routing algorithms to find the best path, scheduling algorithms to optimize staffing, or trying to find the best way to allocate a group of people to set of tasks. As a data scientist, you need to dissect what you are trying to maximize and identify the constraints in the form of equations. Once you can do this, we can hand this over to a computer to solve. So lets next walk through a bit more complicated example.\n\n\nFantasy Football\nOver the last few years, fantasy sports have increasingly grown in popularity. One game is to pick a set of football players to make the best possible team. Each football player has a price and there is a salary cap limit. The challenge is to optimize your team to produce the highest total points while staying within a salary cap limit. This type of optimization problem is known as the knapsack problem or an assignment problem.\n\n\nSimple Linear Optimization\nSo for this problem, let‚Äôs start by loading a dataset and taking a look at the raw data. You need to know both the salary as well as the expected points. Most football fans spend a lot of time trying to predict how many points a player will score. If you want to build a model for predicting the expected performance of a player, take a look at Ben‚Äôs blog post.\n\n\n\nQB points\n\n\nThe goal here is to build the best possible team for a salary cap, let‚Äôs say $50,000. A team consists of a quarterback, running backs, wide receivers, tight ends, and a defense. We can use the lpSolve package in R to set up the problem. Here is a code snippet for setting up the constraints.\n\n\n\nconstraints\n\n\nIf you parse through this, you can see we have set a minimum and maximum for QB of 1 player. However, for the RB, we have allowed a maximum of 3 and a minimum of 2. This is not unusual in fantasy football, be because there is a role called a flex player, which anyone can choose and they can either be a RB, WR, or TE. Now let‚Äôs look at the code for the objective:\n\n\n\nobjective\n\n\nThe code shows that we have set up the problem to maximize the objective of the most points and include our constraints. Once the code is run, it outputs an optimal team! I forked an existing repo and have made the R code and dataset are available here. A more sophisticated python optimization repo is also available.\n\n\n\nfinalteam\n\n\n\n\nAdvanced steps\nSo far, we have built a very simple optimization to solve the problem. There are several other strategies to further improve the optimizer. First, the variance of our teams can be increased by using a strategy called stacking, where you make sure your QB and WR are on the same team. A simple optimization is a constraint for selecting a QB and WR from the same team. Another strategy is using an overlap constraint for selecting multiple lineups. An overlap constraint ensures a diversity of players and not the same set of players for each optimized team. This strategy is particularly effective when submitting multiple lineups. You can read more about these strategies here and run the code in Julia here. An code snippet of the stacking constraint (this is for a hockey optimization):\n.\nLast year, at Sloan sports conference, Haugh and Sighal , presented a paper with additional optimization constraints. They include what an opponents team is likely to look like. After all, there are some players that are much more popular. Using this knowledge, you can predict the likely teams that will oppose your team. The approach here used Dirichlet regressions for modeling players. The result was a much-improved optimizer that was capable of consistently winning!\nI hope this post has shown you how optimization strategies can help you find the best possible solution.\n‚Äã"
  },
  {
    "objectID": "rnn_addition.html",
    "href": "rnn_addition.html",
    "title": "RNN Addition (1st Grade)",
    "section": "",
    "text": "Introduction\nEver since I ran across RNNs, they have intrigued me with their ability to learn. The best background is Denny Britz‚Äôs tutorial, Karpathy‚Äôs totally accessible and fun post on character-level language models, and Colah‚Äôs detailed descriptions of LSTMs. Besides all the fun examples of generating content with RNNs, other people have been applying them and winning Kaggle competitions and the ECML/PKDD challenge.\nI am still blown away by how RNN‚Äôs can learn to add. RNNs are trained through thousands of examples and can learn how to sum numbers. For example, the Keras addition example show how to add two sets of numbers up to 5 digital long each (e.g., 54678 + 78967). It achieves 99% train/test accuracy in 30 epochs with a one layer LSTM (128 HN) and 550k training examples.\nMy eventual goal is to use RNNs to study various sequenced data (such as the NBA SportVu), so I thought I should start simple. I wanted to teach a RNN to add a series of numbers. For example: 5+7+9. The rest of the post discusses this journey.\n\n\n1st Grade Model\nMy first model was teaching an RNN to add between 5 to 15 single digit numbers. This would be at the level of a first grader in the US. For example, using a 2 layer LSTM network with 100 hidden units, a batch of 50 training examples, and 5000 epochs, the RNN summed up:\n8+6+4+4+0+9+1+1+7+3+9+2+8 as 66.2154007\nThis isn‚Äôt too far from the actual answer of 62. The Keras addition example show that with even more examples/training, the RNN can get much better. The code for this RNN is available as a gist using tensorflow. I made this in a notebook format so its easy to play with.\nThere are lots of parameters to tweak with RNN models, such as the number of hidden units, epochs, batch size, dropout, and training rate. Each of these has different sorts of effects on the model. For example, increasing the number of hidden units will provide more space for learning, but consequently take longer to learn/train. The chart below shows the effect of different choices. Please take the time to really study/investigate the role of hidden units. Its a dynamic plot so you can zoom in and examine each series individually by clicking on the legend."
  },
  {
    "objectID": "semi_sup.html",
    "href": "semi_sup.html",
    "title": "Using Unlabeled Data to Label Data",
    "section": "",
    "text": "https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg\n\n\nYour boss hands you a pile of a 100,000 unlabeled images and asks you to categorize whether they are sandals, pants, boots, etc.\nSo now you have a massive set of unlabeled data and you need labels. What should you do?\nThis problem is commonplace. Lots of companies are swimming with data, whether its transactional, IoT sensors, security logs, images, voice, or more, and its all unlabeled. With so little labeled data, it is a tedious and slow process for data scientists to build machine learning models in most all enterprises.\nTake Google‚Äôs street view data. Gebru had to figure out how to label cars in 50 million images with very little labeled data. Over at Facebook, they used algorithms to label half a million videos, a task that would have otherwise taken 16 years.\nThis post shows you how to label hundreds of thousands of images in an afternoon. You can use the same approach whether you are labeling images or labeling traditional tabular data (e.g, identifying cyber security atacks or potential part failures.)\n\nThe Manual Method\nFor most data scientists when asked to do something, the first step is to calculate who else should do this.\n\n\n\nhttp://vni.s3.amazonaws.com/130905163633604.jpg\n\n\nBut 100,000 images could cost you at least $30,000 on Mechanical Turk or some other competitor. Your boss expects this done cheaply, since after all, they hired you because you use free software. Now, she doesn‚Äôt budget for anything other than your salary (if you don‚Äôt believe me, ask to go to pydata).\nYou take a deep breath and figure you can probably label 200 images in an hour. So that means in three weeks of non stop work, you can get this done!! Yikes!\n\n\nJust Build a Model\nThe first idea is to label a handful of the images, train a machine learning algorithm, and then predict the remaining set of labels. For this exercise, I am using the Fashion-MNIST dataset (you could also make your own using quickdraw). There are ten classes of images to identify and here is a sample of what they look like:\n\n\n\nhttps://pbs.twimg.com/media/DJNuE7BWAAAo3eC.jpg\n\n\nI like this dataset, because each image is 28 by 28 pixels, which means it contains 784 unique features/variables. For a blog post this works great, but its also not like any datasets you see in the real world, which are often either much narrower (traditional tabluar business problem datasets) or much wider (real images are much bigger and include color).\nI built models using the most common data science algorithms: logistic regression, support vector machines (SVM), random forest and gradient boosted machines (GBM).\nI evaluated the performance based on labeling 100, 200, 500, 1000, and 2000 images.\n\n\n\nAll\n\n\nAt this point in the post, if you are still with me, slow down and mull this graph over. There is a lot of good stuff here. Which algorithm does the best? (If you a data scientist, you shouldn‚Äôt fall for that question.) It really depends on the context.\nYou want something quick and dependable out of the box, you could go for the logistic regression. While the random forest starts way ahead, the SVM is coming on fast. If we had more labeled data the SVM would pass the random forest. And the GBM works great, but can take a bit of work to perform their best. The scores here are using out of the box implementations in R (e1071, randomForest, gbm, nnet).\nIf our benchmark is 80% accuracy for ten classes of images, we could get there by building a Random Forest model with 1000 images. But 1000 images is still a lot of data to label, 5 hours by my estimate. Lets think about ways we can improve.\n\n\nLet‚Äôs Think About Data\nAfter a little reflection, you remember what you often tell others ‚Äî that data isn‚Äôt random, but has patterns. By taking advantage of these patterns we can get insight in our data.\nLets start with an autoencoder (AE). An autoencoder squeezes and compresses your data, kind of like turning soup into a bouillon cube. Autoencoders are the hipster‚Äôs Principle Component Analysis (PCA) , since they support nonlinear transformations.\nEffectively this means we are taking our wide data (784 features/variables) reducing it down to 128 features. We then take this new compressed data and train our machine learning algorithm (SVM in this case). The graph below shows the difference in performance between an SVM fed with an autoencoder (AE_SVM) versus the SVM on the raw data.\n\n\n\nAE\n\n\nBy squeezing the information down to 128 features, we were able to actually improve the performance of the SVM algorithm at the low end. At the 100 labels mark, accuracy went from 44% to 59%. At the 1000 labels mark, the autoencoder was still helping, we see an improvement from 74% to 78%. So we are on to something here. We just need think a bit more about the distribution and patterns in our data that we can take advantage of.\n\n\nThinking Deeper About Your Data\nWe know that our data are images and since 2012, the hammer for images is a convolutional neural network (CNN). There are a couple of ways we could use a CNN, from a pretrained network or as a simple model to pre-process the images. For this post, I am going to use a Convolutional Variational Autoencoder as a path towards the technique by Kingma for semi-supervised learning.\nSo lets build a Convolutional Variational Autoencoder (CVAE). The leap here is twofold. First, ‚Äúvariational‚Äù means the autoenconder compress the information down into a probability distribution. Second is the addition of using a convolutional neural networks as an encoder. This is a bit of deep learning, but the emphasis here is on how we are solving the problem, not the latest shiny toy.\nFor coding my CVAE, I used the example CVAE from the list of examples over at RStudio‚Äôs Keras page. Like the previous autoencoder, we design the latent space to reduce the data to 128 features. We then use this new data to train an SVM model. Below is a plot of the performance of the CVAE as compared to the SVM and RandomForest on the raw data.\n\n\n\nCVAE\n\n\nWow! The new model is much more accurate. We can get well past 80% accuracy with just 500 labels. By using these techniques we get better performance and require less labelled images! At the top end, we can also do much better than the RandomForest or SVM model.\n\n\nNext Steps\nBy using some very simple semi-supervised techniques with autoencoders, its possible to quickly and accurately label data. But the takeaway is not to use deep learning auto encoders! Instead, I hope you understand the methodology here of starting very simple and then trying gradually more complex solutions. Don‚Äôt fall for the latest shiny toy ‚Äî pratical data science is not about using the latest approaches found in arxiv.\nIf this idea of semi-supervised learning inspires you, this post is the logistic regression of semi-supervised learning. If you want to dig further into Semi-Supervised Learning and Domain Adaptation, check out Brian Keng‚Äôs great walkthrough of using variational autoencoders (which goes beyond what we have done here) or the work of Curious AI, which has been advancing semi-supervised learning using deep learning and sharing their code. But at the very least, don‚Äôt reflexively think all your data has to be hand labeled."
  },
  {
    "objectID": "sportvu_analysis.html",
    "href": "sportvu_analysis.html",
    "title": "SportVu Analysis",
    "section": "",
    "text": "Introduction\nThis post shares some of the code that I have created for analyzing NBA SportVu data.\nFor background, the NBA SportVu data is motion data for the basketball and players taken 25 times a second. For a typical NBA game, this means about 2 million rows of data. The data for over 600 NBA games (first half of the 2015-2016 season) is available. This is over a billion rows of telematics (iOT) type data. This is a gold mine and here are some early pieces from studying that data.\n\n\nEDA\nThe first is basic EDA on the movement data. This code allows you to start analyzing the ball and player movement. \n\n\nPBP\nThe next markdown, PBP, shows how to merge play by play data with the SportVu movement data. This allows using the annotated data which contains information on the type of play, score, and home/visitor info.\n\n\nChull\n The next set of documents start analyzing the data. The first measures player spacing using convex hulls. The next shows how to calculate player velocity, acceleration, and jerk. (I really wanted to do a post on the biggest jerk in the NBA, but unfortunately the jerk data is way too noisy.)  The third document offers a few different ways for analyzing player and ball trajectories.\nYou can find all these files at my SportVu Github repo."
  },
  {
    "objectID": "tensorflow_shiny.html",
    "href": "tensorflow_shiny.html",
    "title": "Shiny front end for Tensorflow demo",
    "section": "",
    "text": "shiny_tensor\n\n\n\nIntroduction\nI built a GUI front end for tensorflow from shiny, the code is available at Github. The shiny app allows trying different inputs, RNN cell types, and even optimizers. The results are shown with plots as well as a link to tensorboard. The app allows anyone to try out these models with a variety of modelling options.\nThe code for the shiny web app was based around work by Sachin Jogleka. Sachin focused on RNNs that had two numeric inputs. (This is slightly different than most RNN examples which focus on language models.)\nSachin‚Äôs code was modified to allow different cell types and reworked so it could be called from rPython. The shiny web app relies on rPython to run the tensorflow models. There is also an iPython notebook in the repository if you would like to test this outside of shiny.\n\n\n\nshiny_tensor\n\n\n\n\nLive Demo:\nI have a live demo of this app, but it‚Äôs flaky. Building RNN models is computationally intensive and the shiny front end is intended to be used on development boxes with tensorflow. My live demo app is limited in several ways. First, the server lacks the horsepower to build models quickly. Second, if the instructions below are not carefully followed the app will crash. Third, its not designed for multiple people building different types of models at the same time. Finally, tensorboard application sometimes stops running, so the link to tensorboard within the live demo app may not work. Again, to really use this app, please install it locally.\nThe requirements for the app include tensorflow and numpy on the Python side. Shiny, Metrics, plotly, and rPython on the R side. rPython can be difficult to install/configure, so please verify that rPython is working correctly if you are having problems running the code.\n\n\nUsing the App:\nTo use the app, select your model options. For the inputs, there are three options of increasing complexity. Steps for prediction window refers to how far ahead is the model suppose to predict. For this data, 20s seemed a reasonable window. For Cell Type, select one of the cell types and press Initialize Model. Then select iterations (max of 10,000) and press Train. After a few seconds, you will see the output.\nTake advantage of the plots to zoom in and out and see the shape of the actual and predicted outputs. To further improve the model, you can add iterations by pressing the train button. The plots show how the RNN model is learning and getting better at predicting the output.\nTo try a new model, select a new cell type and press initialize model. Then select the number of iterations and press train.\nIf the app crashes, no worries, it happens. I have not accounted for everything that could go wrong."
  },
  {
    "objectID": "Feature_Selection.html",
    "href": "Feature_Selection.html",
    "title": "Feature Selection Methods and Feature Selection Curves",
    "section": "",
    "text": "‚Äî title: Feature Selection Methods and Feature Selection Curves date: ‚Äú2024-10-08‚Äùcategories: [featureselection, MLOps]image: Feature_Selection_files/figure-html/cell-19-output-1.png‚Äî\nHow to Select the Best Features for Machine Learning!\nLet‚Äôs deep dive into several feature selection techniques and help you figure out when to use each one. The notebook includes two data sources: the MNIST dataset and the Madelon dataset. The MNIST dataset is a collection of 28x28 pixel images of handwritten digits. The Madelon dataset is a synthetic dataset that you can control.\nThe notebook uses the following feature selection techniques:\n\nF-statistic\nMutual Information\nLogistic Regression\nLogistics Regression with Lasso (L1) Regularization\nFeature Importance\nBoruta\nMRMR (Minimum Redundancy Maximum Relevance)\nRecursive Feature Elimination\nFeature importance rank ensembling (FIRE)\n\nTo help visualize the feature selection process, the notebook includes a feature selection curve. The feature selection curve plots the number of features against the accuracy of the model. This helps you understand how many features you need to achieve a certain level of accuracy.\nThis notebook is based on the following articles:\nFeature Selection: How to Throw Away 95% of Your Features and Get 95% Accuracy and the associated notebook.\nA companion video to this can be found on my youtube site, @rajistics: Feature Selection Methods and Feature Selection Curves, it‚Äôs about 15 minutes and gives more context to the notebook.\nThis blog post can be found at http://bit.ly/raj_fs or https://projects.rajivshah.com/blog/Feature_Selection.html\n\nimport warnings; warnings.filterwarnings(\"ignore\")\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"\nimport keras\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nImport data\n\nA. MNIST (Very visual dataset)\n\nfrom keras.datasets import mnist\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nX_train = X_train.reshape(60000, 28 * 28)\nX_test = X_test.reshape(10000, 28 * 28)\n\n\nprint(list(X_train[7, :]))\n\n\nplt.imshow(X_train[7, :].reshape(28, 28), cmap = 'binary', vmin = 0, vmax = 255)\nplt.xticks([])\nplt.yticks([])\nplt.savefig('sample_image.png')\n\n\nX_train = pd.DataFrame(X_train)  # Assuming X_mnist is the MNIST feature data\ny_train = pd.Series(y_train)   \nX_test = pd.DataFrame(X_test)  # Assuming X_mnist is the MNIST feature data\ny_test = pd.Series(y_test)  \n\n\n\nB. Madelon (Very high-dimensional dataset that you control)\nIf you run the following cells, Madelon will be the dataset you use. If you want to use MNIST, you should skip the following cells.\nMadelon is a favorite of mine because you know which features are carrying the signal and which ones are noise. In this case, the first 5 features will be informative. I often modify Madelon to include other types of noisy features, interactions, correlations, and then use this dataset to test various machine learning techniques. Since I know what the true signal is, this is very effective at helping me guage the effectiveness of these methods.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=10000,\n                           n_features=40,\n                           n_informative=5,\n                           n_classes=3,\n                           n_redundant = 3,\n                           random_state=0,\n                           flip_y =0.05,\n                           class_sep = 0.5,\n                           n_clusters_per_class=3,\n                           shuffle=False)\n\nX_df = pd.DataFrame(X, columns=[f'{i}' for i in range(X.shape[1])])\ny_df = pd.Series(y, name='target')\n\nX_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n\nX_train.columns = X_train.columns.astype(int)\nX_test.columns = X_test.columns.astype(int)\n\n\nX_train\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n\n\n\n\n9254\n0.926694\n-1.773357\n0.172527\n0.217298\n-1.733944\n0.319415\n0.185012\n1.907097\n-0.649596\n0.121499\n...\n0.706101\n-0.880984\n-0.980460\n-1.040219\n-1.495820\n2.793184\n0.206932\n-0.357897\n-1.633463\n-0.358298\n\n\n1561\n1.692096\n-1.412235\n1.294343\n-0.672776\n-0.576808\n1.088448\n0.446408\n1.081032\n-0.355654\n-0.940438\n...\n-2.334601\n-0.446046\n-0.577543\n-0.692218\n-0.311946\n0.329447\n-1.312834\n0.339797\n-0.291047\n0.931088\n\n\n1670\n-0.721183\n-1.430124\n0.776395\n0.226875\n-1.209252\n-0.458278\n-1.011414\n1.682210\n-1.048116\n-1.783993\n...\n1.440083\n-0.666334\n-0.909174\n0.377606\n1.303421\n-0.655019\n0.003210\n-0.802838\n-1.305648\n-0.170390\n\n\n6087\n1.429094\n1.539467\n0.230706\n0.256132\n-0.478975\n-1.493286\n1.738055\n0.888900\n0.164039\n-2.488486\n...\n1.454662\n0.493267\n0.079875\n-1.390000\n1.330840\n0.212113\n1.955695\n-0.567808\n-0.883676\n-0.472567\n\n\n6669\n0.207305\n0.600810\n0.477484\n-0.784978\n-0.651178\n-0.362503\n1.032674\n0.369245\n-0.659173\n-1.210180\n...\n-3.207426\n0.423698\n1.538654\n-0.856037\n0.343482\n-0.119711\n-0.355270\n0.724913\n1.702261\n-1.597048\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5734\n0.484935\n0.695846\n1.481478\n0.223780\n-1.012330\n-2.116814\n0.613437\n2.080326\n-0.730603\n1.408916\n...\n0.328236\n0.413631\n0.337577\n-0.747556\n0.020008\n-0.202360\n1.484470\n-0.465176\n1.391591\n0.294199\n\n\n5191\n-0.715630\n-1.895183\n-1.091845\n-0.579646\n-0.474871\n2.217163\n-0.666726\n-0.763180\n0.261672\n1.570425\n...\n0.280017\n0.836381\n0.115396\n-0.044588\n0.516398\n-0.630678\n0.755802\n0.016894\n0.183862\n-0.401010\n\n\n5390\n1.091513\n-1.606975\n-1.678945\n-0.706068\n-0.547585\n2.905629\n0.827132\n-0.997257\n-0.983815\n-0.609981\n...\n0.001994\n0.411601\n-0.809740\n-0.163079\n0.020689\n-0.731637\n-0.154384\n0.599125\n1.094542\n-1.020837\n\n\n860\n-1.696127\n1.277728\n0.043566\n0.659020\n0.537680\n-1.793380\n-0.878325\n-0.168647\n-0.712758\n2.285642\n...\n0.233483\n0.551602\n0.139338\n0.805881\n-0.628342\n0.532257\n-0.107130\n1.449110\n-0.499819\n-0.826810\n\n\n7270\n2.113646\n-2.644226\n-0.097455\n-0.645787\n-1.748579\n2.288712\n0.920350\n1.266990\n0.545140\n-0.652207\n...\n-0.048207\n1.331188\n-0.683416\n-0.014705\n0.185842\n1.100054\n-0.244144\n-1.529671\n-1.914142\n0.072284\n\n\n\n\n8000 rows √ó 40 columns\n\n\n\n\n\n\nFeature selection\nLet‚Äôs go through a couple of different methods for feature selection\n\n\nFeature Selection Methods Comparison\n\n\n\n\n\n\n\n\n\n\nMethod\nPros\nCons\nBest Used When\nComputational Complexity\n\n\n\n\nF-statistic\n- Fast and simple- Works well for linear relationships- Easy to interpret\n- Assumes linear relationship- Considers features independently- May miss interaction effects\n- Initial screening- Linear problems- Need interpretable results\nO(n)\n\n\nMutual Information\n- Captures non-linear relationships- No assumptions about distribution\n- Can be computationally intensive- May overfit with small samples\n- Non-linear relationships- Complex interactions\nO(n log n)\n\n\nLogistic Regression\n- Fast for high-dimensional data- Provides feature coefficients\n- Assumes linear decision boundary- Sensitive to correlated features\n- Binary classification- Need interpretable coefficients\nO(n^2)\n\n\nLasso (L1)\n- Fast for high-dimensional data- Automatically does feature selection\n- May struggle with correlated features- Can be sensitive to outliers\n- High-dimensional data- Need sparse solutions\nO(n^2)\n\n\nLightGBM\n- Handles non-linear relationships- Considers feature interactions\n- Can be computationally intensive- May overfit with small samples\n- Complex relationships- Large datasets\nO(n log n)\n\n\nMRMR\n- Considers feature redundancy- Good for correlated features\n- Can be computationally intensive- May struggle with non-linear relationships\n- Datasets with correlated features- Need diverse feature set\nO(n^2)\n\n\nRFE\n- Considers feature interactions- Can capture complex relationships\n- Computationally intensive- Can be unstable with small changes in data\n- When computational cost isn‚Äôt an issue- Need very precise feature selection\nO(n^2 log n)\n\n\n\n\n1. F-statistic\nf_classif relies on the Analysis of Variance (ANOVA) F-statistic to evaluate the relationship between each feature and the target variable. It tests whether the mean values of the target variable differ significantly across the groups defined by each feature. The higher the F-value, the more likely it is that the feature discriminates between different classes. Assumes a linear relationship between the features and the target, and that the target is categorical.\nSee: https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n\nfrom sklearn.feature_selection import f_classif\n\nf = f_classif(X_train, y_train)[0]\nf\n\narray([2.40810399e+02, 6.64390517e+01, 5.42587843e+01, 1.71099993e-01,\n       4.27226542e+01, 9.64735668e+01, 4.72136845e+01, 5.71846457e+01,\n       1.27834979e+00, 1.42650284e+00, 3.40020422e-01, 4.08232233e-01,\n       1.30120819e-01, 3.36734714e+00, 4.55665866e-01, 3.62300510e-01,\n       1.57485437e-01, 6.93572687e-02, 1.64816305e+00, 2.91782944e+00,\n       1.24026934e+00, 9.32533895e-01, 7.07099908e-01, 1.87544216e+00,\n       1.10130690e+00, 3.54044700e-01, 1.15417945e+00, 2.59156089e-01,\n       7.45820681e-01, 7.75403854e-01, 1.35835715e-01, 3.34985292e+00,\n       8.36576456e-02, 5.15026453e-02, 4.33788709e-01, 3.12140721e-01,\n       3.55118575e+00, 7.37076241e+00, 1.17274619e+00, 4.36532461e+00])\n\n\n\n\n2. Mutual information\nmutual_info_classif uses the concept of mutual information, which measures the dependency between each feature and the target variable. Mutual information quantifies the amount of information gained about the target by knowing the value of the feature. It captures both linear and non-linear dependencies.\nSee: https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n\nfrom sklearn.feature_selection import mutual_info_classif\n\nmi = mutual_info_classif(X_train, y_train)\nmi\n\narray([0.05548077, 0.01340894, 0.04092611, 0.00099355, 0.00516085,\n       0.06952759, 0.02752171, 0.04043945, 0.00083431, 0.        ,\n       0.        , 0.        , 0.        , 0.01146672, 0.        ,\n       0.00298292, 0.        , 0.        , 0.0068234 , 0.00961735,\n       0.00935105, 0.00586449, 0.00561433, 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.00876386, 0.00049355,\n       0.        , 0.00478042, 0.00487523, 0.00268551, 0.00118896,\n       0.        , 0.00583264, 0.        , 0.        , 0.        ])\n\n\n\n\n3. Logistic regression\nLogistic regression is a linear model for classification rather than regression. It is used to estimate the probability that an instance belongs to a particular class. The coefficients of the model can be used to determine feature importance.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression().fit(X_train, y_train)\nlogreg.coef_\n\narray([[-0.08669758,  0.06064912, -0.04063592,  0.00704782,  0.05079063,\n        -0.02366404, -0.03246196, -0.07613473, -0.02216845,  0.00405511,\n         0.00890925, -0.01289459, -0.00446435,  0.00330386,  0.01287983,\n        -0.00599418,  0.00494212, -0.00385749,  0.03721175, -0.03849129,\n        -0.0032749 , -0.01534965, -0.00908255,  0.02016669, -0.00175419,\n         0.00918138,  0.01908963,  0.01357562, -0.01804835,  0.00266229,\n         0.00180036, -0.00624841, -0.00351875,  0.00131487, -0.01573702,\n        -0.00485053, -0.03744854,  0.05047984,  0.0174477 , -0.00658735],\n       [ 0.22615114, -0.09497312, -0.04870109,  0.08462927, -0.00434942,\n         0.0845374 ,  0.05186658,  0.05312074,  0.01121456,  0.0271935 ,\n        -0.00027397,  0.01737956,  0.0080553 , -0.04770429, -0.00379082,\n        -0.00963934,  0.00274429,  0.00688572, -0.03159925,  0.02737958,\n        -0.0220797 , -0.00789503, -0.01134082, -0.02724488, -0.02432216,\n         0.00543336, -0.02498671, -0.00919296,  0.01043275,  0.01323123,\n        -0.00059363, -0.02632891, -0.00159478, -0.00753653,  0.01697607,\n         0.01247471,  0.04553467, -0.05091659, -0.02028138,  0.03615948],\n       [-0.13945356,  0.03432401,  0.089337  , -0.09167709, -0.04644121,\n        -0.06087336, -0.01940462,  0.02301399,  0.0109539 , -0.03124862,\n        -0.00863527, -0.00448497, -0.00359095,  0.04440043, -0.00908901,\n         0.01563352, -0.00768641, -0.00302823, -0.0056125 ,  0.01111171,\n         0.02535461,  0.02324468,  0.02042338,  0.0070782 ,  0.02607635,\n        -0.01461474,  0.00589708, -0.00438266,  0.0076156 , -0.01589352,\n        -0.00120673,  0.03257732,  0.00511353,  0.00622166, -0.00123904,\n        -0.00762418, -0.00808614,  0.00043674,  0.00283369, -0.02957212]])\n\n\n\n\n3.5 Feature Selection with L1 (Lasso) Regularization\nLasso is a great feature selection technique. It‚Äôs fast, easy to use, and works well with high-dimensional data. I have often used it when very wide data, greater than 100 features (or even &gt;10k features) to help parse down the number of features. It uses L1 regularization to penalize the absolute size of the coefficients. This leads to sparse solutions, where many of the coefficients are zero. The features with non-zero coefficients are selected. Lasso can be used for feature selection by setting the regularization parameter to a value that results in a sparse solution. The regularization parameter can be tuned using cross-validation.\nTry modifying the regularization parameter to see how it affects the number of features selected.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Step 4: Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Step 5: Apply Logistic Regression with L1 regularization for feature selection\nlogregL1 = LogisticRegression(penalty='l1', solver='saga', multi_class='multinomial', C=0.01)  # C is inverse of regularization strength\nlogregL1.fit(X_train_scaled, y_train)\n\n# Step 6: Get the selected features using the original DataFrame 'X'\nselected_features = X_train.columns[(logregL1.coef_ != 0).any(axis=0)]\nprint(\"Selected features: \", selected_features)\n\n# Optional: Check the coefficients\n#print(\"Logistic Regression coefficients: \", logreg.coef_)\n\nSelected features:  Index([0, 1, 2, 4, 5, 13, 19, 36, 37], dtype='int64')\n\n\n\n\n4. LightGBM\nLightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed for efficiency and can handle large datasets. It can be used to determine feature importance.\n\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(\n    objective = 'multiclass',\n    metric = 'multi_logloss',\n    importance_type = 'gain'\n).fit(X_train, y_train)\n\nlgbm.feature_importances_\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001096 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10200\n[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 40\n[LightGBM] [Info] Start training from score -1.108284\n[LightGBM] [Info] Start training from score -1.094371\n[LightGBM] [Info] Start training from score -1.093252\n\n\narray([7030.24782425, 4036.90086633, 7197.39050466, 5339.74033117,\n       2017.793881  , 8113.67321557, 3905.26838762, 4383.72206521,\n        550.03625131,  531.02187729,  472.12624365,  678.28547454,\n        526.57803982,  586.75292325,  552.92263156,  433.08122051,\n        552.18078488,  534.15573859,  566.58704376,  630.01932001,\n        635.43262064,  636.71719581,  560.95981157,  586.52648336,\n        553.7755444 ,  563.13766581,  547.99060541,  523.01072556,\n        676.76891661,  616.94216621,  634.27822083,  489.91742009,\n        680.71264285,  620.95509708,  618.59545827,  418.22946733,\n        568.21738124,  592.29172051,  553.43465978,  655.03435677])\n\n\n\n\n5. Boruta\nBoruta is an all-relevant feature selection method. It is an extension of the Random Forest algorithm. It selects all features that are relevant to the target variable, rather than just the most important features.\n\n### long training time &gt; 1 hour\nfrom boruta import BorutaPy\nfrom sklearn.ensemble import RandomForestClassifier\n\n#boruta = BorutaPy(\n#    estimator = RandomForestClassifier(max_depth = 5), \n#    n_estimators = 'auto', \n#    max_iter = 100\n#).fit(X_train, y_train)\n\n\n\n6. MRMR\nMRMR (Minimum Redundancy Maximum Relevance) is a feature selection method that selects features based on their relevance to the target variable and their redundancy with other features. It aims to select features that are highly correlated with the target variable but uncorrelated with each other.\nThere are several implementations of MRMR available in Python: https://github.com/smazzanti/mrmr https://koaning.github.io/scikit-lego/api/feature-selection/ https://github.com/AutoViML/featurewiz?tab=readme-ov-file\n\nimport pandas as pd\nfrom mrmr import mrmr_classif\n\n#mrmr = mrmr_classif(pd.DataFrame(X_train), pd.Series(y_train), K = 784)\nmrmr = mrmr_classif(pd.DataFrame(X_train), pd.Series(y_train), K = X_train.shape[1])\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:03&lt;00:00, 11.45it/s]\n\n\n\n\nStore results\n\nimport numpy as np\n\nranking = pd.DataFrame(index = range(X_train.shape[1]))\n\nranking['f'] = pd.Series(f, index = ranking.index).fillna(0).rank(ascending = False)\nranking['mi'] = pd.Series(mi, index = ranking.index).fillna(0).rank(ascending = False)\nranking['logreg'] = pd.Series(np.abs(logreg.coef_).mean(axis = 0), index = ranking.index).rank(ascending = False)\nranking['lasso']= pd.Series(np.abs(logregL1.coef_).mean(axis = 0), index = ranking.index).rank(ascending = False)\nranking['lightgbm'] = pd.Series(lgbm.feature_importances_, index = ranking.index).rank(ascending = False)\n#ranking['boruta'] = boruta.support_* 1 + boruta.support_weak_ * 2 + (1 - boruta.support_ - boruta.support_weak_) * X_train.shape[1]\nranking['mrmr'] = pd.Series(\n    list(range(1, len(mrmr) + 1)) + [len(mrmr) + 1] * (X_train.shape[1] - len(mrmr)),\n    index = mrmr + list(set(ranking.index) - set(mrmr))\n).sort_index()\nranking['lasso']= pd.Series(np.abs(logregL1.coef_).mean(axis = 0), index = ranking.index).rank(ascending = False)\n\n\nranking = ranking.replace(to_replace = ranking.max(), value = X_train.shape[1])\n#ranking.to_csv('ranking.csv', index = False)\n\n\n\n\nEvaluate Feature Selection Methods\nLet‚Äôs see how the predictive performance of the model changes as we add more features. We will use the top features selected by each method to train a model and evaluate its performance.\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n## 22 minutes for mnist\n\nalgos = ['f', 'mi', 'logreg', 'lasso', 'lightgbm', 'mrmr'] ##Feel free to change this\nks = [1, 2, 5, 10, 15, 20, 30, 40] \nks = [1, 2, 3, 5, 10, 20, 30, 40] ##Feel free to change this\n\naccuracy = pd.DataFrame(index = ks, columns = algos)\nroc = pd.DataFrame(index = ks, columns = algos)\n\nfor algo in algos:\n    print (algo)\n    for k in ks:\n    \n        cols = ranking[algo].sort_values().head(k).index.to_list()\n                \n        clf = CatBoostClassifier().fit(\n            X_train[cols], y_train,\n            eval_set=(X_test[cols], y_test),\n            early_stopping_rounds = 20,\n            verbose = False\n        )\n                \n        # Store accuracy\n        accuracy.loc[k, algo] = accuracy_score(\n            y_true=y_test, y_pred=clf.predict(X_test[cols])\n        )\n        \naccuracy.to_csv('accuracyMC.csv', index = True)\nroc.to_csv('rocMC.csv', index = True)\n\nf\nmi\nlogreg\nlasso\nlightgbm\nmrmr\n\n\n\n\nFeature Selection Curves\nLet‚Äôs visualize how the model‚Äôs accuracy changes as a function of feature selection.\nNotice how for Madelon, there is an optimal number of features. Too many features that are noise end up reducing the performance of the model\n\nfor algo, label, color in zip(\n    ['mrmr', 'f', 'mi', 'lightgbm', 'logreg',\"lasso\"],\n    ['MRMR', 'F-statistic', 'Mutual Info', 'LightGBM', 'Log Reg','Log Reg (L1/Lasso)'],\n    ['orangered', 'blue', 'yellow', 'lime', 'black', 'pink']):\n        plt.plot(accuracy.index, accuracy[algo], label = label, color = color, lw = 3)\n\nplt.plot(\n    [1, 40], [pd.Series(y_test).value_counts(normalize = True).iloc[0]] * 2, \n    label = '[Random]', color = 'grey', ls = '--', lw = 3\n)\n\nplt.legend(fontsize = 13, loc = 'center left', bbox_to_anchor = (1, 0.5))\nplt.grid()\nplt.xlabel('Number of features', fontsize = 13)\nplt.ylabel('Accuracy', fontsize = 13)\nplt.savefig('accuracy.png', dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n\n\n\n\n\nFeature Selection combined with Feature Elimination Techniques\n\nRecursive Feature Elimination\nOne of the best methods for feature selection consistently is feature importance with LightGBM. We can refine and improve this in several ways: Recursive Feature Elimination uses the same feature importance method, but then iteratively removes the least important features. This iterative process requires training a model several times, but can provide an improvement in feature selection. This method is a version of Recursive Feature Elimination that is widely accepted as a best practice for feature selection.\n\nfrom sklearn.feature_selection import RFE\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVR\nmodel = XGBClassifier(random_state=42)\n#model = SVR(kernel=\"linear\")  #took 3 minutes, ok results but not as good as XGB on Madelon\nrfe = RFE(model, n_features_to_select=7, step=1)\n#rfe = RFE(model, n_features_to_select=50, step=200,verbose=2) #for MNIST\nrfe.fit(X_train, y_train)\nrfe.support_\n\narray([ True,  True,  True,  True, False,  True,  True,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False])\n\n\n\n# Train an XGBoost model with the selected features from RFE\nmodel_selected = XGBClassifier(random_state=42)\nX_selected = X_train.loc[:, rfe.support_]\nmodel_selected.fit(X_selected, y_train)\n\n# Make predictions on the test set with both models\ny_pred_selected = model_selected.predict(X_test.loc[:, rfe.support_])\naccuracy_selected = accuracy_score(y_test, y_pred_selected)\nprint(f\"Accuracy with selected features: {accuracy_selected:.4f}\")\n\nAccuracy with selected features: 0.7135\n\n\nCompare with perfect on Madelon\n\nperfect = [ True,  True,  True,  True, True,  True,  True,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False]\n\n\n# Train an XGBoost model with the selected features from RFE\nmodel_selected = XGBClassifier(random_state=42)\nX_selected = X_train.loc[:, perfect]\nmodel_selected.fit(X_selected, y_train)\n\n# Make predictions on the test set with both models\ny_pred_selected = model_selected.predict(X_test.loc[:, perfect])\naccuracy_selected = accuracy_score(y_test, y_pred_selected)\nprint(f\"Accuracy with selected features: {accuracy_selected:.4f}\")\n\nAccuracy with selected features: 0.7140\n\n\n\n\nFeature Elimination with FIRE\nAt DataRobot, we had a mighty AutoML engine that showed you how feature importance aggregated across different models (this is feature importance from four diverse models). \nYou can use this variance as part of feature selection. It takes a lot more compute, but in our experiments, can perform even better feature selection. Read more about feature importance rank ensembling (FIRE) here - https://docs.datarobot.com/en/docs/api/accelerators/adv-approaches/fire.html and a code snippet is here - https://github.com/datarobot-community/examples-for-data-scientists/blob/master/Feature%20Lists%20Manipulation/Python/Advanced%20Feature%20Selection.ipynb\n\n\nFeatureViz\nFeatureviz looks like a cool feature selection package, but I wasn‚Äôt able to get it to work. It‚Äôs worth checking out. add links\n\n\n\nOther great feature selection resources:\nA classic dataset where many feature selection techniques have been applied is the Kaggle Santader Customer Satisfaction competition.\nFeature Selection with Feature Engine\nAdvance Feature Selection Tutorial"
  },
  {
    "objectID": "spark-of-ai-transfer-learning.html",
    "href": "spark-of-ai-transfer-learning.html",
    "title": "Spark of AI: How Transfer Learning Unlocked AI‚Äôs Potential",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "spark-of-ai-transfer-learning.html#video",
    "href": "spark-of-ai-transfer-learning.html#video",
    "title": "Spark of AI: How Transfer Learning Unlocked AI‚Äôs Potential",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "spark-of-ai-transfer-learning.html#annotated-presentation",
    "href": "spark-of-ai-transfer-learning.html#annotated-presentation",
    "title": "Spark of AI: How Transfer Learning Unlocked AI‚Äôs Potential",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the annotated presentation based on the provided video transcript and slide summaries.\n\n1. The Spark of the AI Revolution\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00)\nThe presentation begins with the title slide, ‚ÄúThe Spark of the AI Revolution: Transfer Learning,‚Äù presented by Rajiv Shah from Snowflake. This talk was originally given at the University of Cincinnati and recorded later to share the insights with a broader audience.\nRajiv sets the stage by explaining that this is not a deep technical dive into code, but rather a descriptive history and analysis of the drivers behind the current AI boom. The goal is to explain how AI learns and how individuals can start to interrogate and understand these technologies in their own lives.\nThe core premise is that Transfer Learning is the catalyst that shifted AI from academic curiosity to a revolutionary force. The talk aims to bridge the gap for those unfamiliar with the underlying mechanics of how models like ChatGPT came to be.\n\n\n2. Sparks of AGI: Early Experiments\n\n\n\nSlide 2\n\n\n(Timestamp: 01:00)\nThis slide illustrates an early experiment conducted by researchers investigating GPT-4. To understand how the model was learning, they gave it a concept and asked it to draw it using code (SVG). The slide displays a progression of abstract animal figures, showing how the model‚Äôs ability to represent concepts improved over time during training.\nThis references the paper ‚ÄúSparks of Artificial General Intelligence,‚Äù which caused significant waves in the tech community. It suggests that these models were beginning to show signs of Artificial General Intelligence (AGI)‚Äîreasoning capabilities that extend beyond narrow tasks.\nThe visual progression from crude shapes to recognizable forms serves as a metaphor for the rapid evolution of these models. It highlights the mystery and potential power hidden within the training process of Large Language Models (LLMs).\n\n\n3. Extinction Level Threat?\n\n\n\nSlide 3\n\n\n(Timestamp: 01:36)\nThe presentation addresses the extreme concerns surrounding the rapid scaling of AI technologies. The slide features a dramatic image reminiscent of the Terminator, referencing fears that unchecked AI development could pose an ‚Äúextinction-level‚Äù threat to humanity.\nRajiv notes that as these technologies scale, there is a segment of the research and safety community worried about catastrophic outcomes. This sets up a contrast between the theoretical existential risks and the practical, everyday reality of how AI is currently being used.\nThis slide acknowledges the ‚Äúhype and fear‚Äù cycle that dominates the media narrative, validating the audience‚Äôs anxiety before pivoting to a more grounded explanation of how the technology actually works.\n\n\n4. The New AI Overlords\n\n\n\nSlide 4\n\n\n(Timestamp: 01:42)\nShifting to a lighter tone, this slide highlights the widespread adoption of AI by the younger generation. It cites a statistic that 89% of students have used ChatGPT for homework, humorously suggesting that children have already ‚Äúaccepted our new AI overlords.‚Äù\nThe slide points out a discrepancy in honesty, noting that while 89% use it, a significant portion (implied by the ‚Äú11% are lying‚Äù joke) might not admit it. This reflects a fundamental shift in education and information retrieval that has already taken place.\nThis context emphasizes that the AI revolution is not just a future possibility but a current reality affecting how the next generation learns and works. It underscores the urgency of understanding these tools.\n\n\n5. Fundamental Questions\n\n\n\nSlide 5\n\n\n(Timestamp: 01:53)\nThis slide poses the central questions that the presentation will answer: ‚ÄúWhat is AI doing?‚Äù and ‚ÄúHow should you think about AI?‚Äù It serves as an agenda setting for the technical explanation that follows.\nRajiv transitions here from the societal impact of AI to the mechanics of machine learning. He prepares the audience to look ‚Äúunder the hood‚Äù to demystify the ‚Äúmagic‚Äù of tools like ChatGPT.\nThe goal is to move the audience from passive consumers of AI hype to critical thinkers who understand the limitations and capabilities of the technology based on how it is built.\n\n\n6. How We Teach Computers\n\n\n\nSlide 6\n\n\n(Timestamp: 02:02)\nThe presentation begins its technical explanation with a fundamental question: ‚ÄúHow do we teach computers?‚Äù The slide uses imagery of blueprints and tools, likening the traditional process of building AI models to craftsmanship.\nThis introduces the concept of Supervised Learning in a relatable way. Before discussing neural networks, Rajiv grounds the audience in traditional analytics, where humans explicitly guide the machine on what to look for.\nThe focus here is on the human element in traditional machine learning‚Äîthe ‚Äúartisan‚Äù who must carefully select inputs to get a desired output.\n\n\n7. Identifying Features\n\n\n\nSlide 7\n\n\n(Timestamp: 02:11)\nUsing a real estate example, this slide explains the concept of Features (or variables). To teach a computer to value a house, one must identify specific characteristics like square footage, number of bedrooms, or closet space.\nRajiv explains that we capture these characteristics and organize them into a tabular format. This process is known as Feature Engineering, where the data scientist decides which attributes are relevant for the problem at hand.\nThis is the bedrock of traditional enterprise AI: converting real-world objects into structured data points that a machine can process mathematically.\n\n\n8. Historical Data Patterns\n\n\n\nSlide 8\n\n\n(Timestamp: 02:50)\nThis slide displays a scatter plot correlating ‚ÄúSales Price‚Äù with ‚ÄúSquare Feet.‚Äù It illustrates how enterprises gather historical data to look for patterns and relationships backwards in time.\nRajiv notes that much of traditional analytics is simply looking at this historical data to understand what happened. However, the power of AI lies in using this data for forward-looking purposes.\nThe visual clearly shows a trend: as square footage increases, the price generally increases. This linear relationship is what the machine needs to ‚Äúlearn.‚Äù\n\n\n9. Learning the Model\n\n\n\nSlide 9\n\n\n(Timestamp: 03:03)\nHere, a line is drawn through the data points on the scatter plot. This line represents the Model. Learning, in this context, is simply the mathematical process of fitting this line to the historical data to minimize error.\nRajiv explains that the model ‚Äúunderstands the relationships‚Äù defined by the data. Instead of a human manually writing rules, the algorithm finds the best-fit trend based on the input features.\nThis simplifies the concept of training a model down to its essence: finding a mathematical representation of a trend within a dataset.\n\n\n10. Making Predictions\n\n\n\nSlide 10\n\n\n(Timestamp: 03:16)\nThis slide demonstrates the utility of the trained model. When a ‚ÄúNew House‚Äù comes onto the market, the model uses the learned line to predict its value based on its square footage.\nThis defines the Inference stage of machine learning. The model is no longer learning; it is applying its ‚Äúknowledge‚Äù (the line) to unseen data to generate a prediction.\nIt highlights the portability of a model‚Äîonce trained, it can be used to make rapid assessments of new data points without human intervention.\n\n\n11. The Domain Limitation\n\n\n\nSlide 11\n\n\n(Timestamp: 03:30)\nThe presentation introduces a critical limitation of traditional models. The slide shows the model trained on San Francisco data being applied to houses in South Carolina. The result is labeled ‚ÄúPoor Model.‚Äù\nRajiv explains that while you can technically take the model with you, it will fail because the underlying relationships between features (size) and targets (price) are different in different domains (geographies).\nThis illustrates the concept of Domain Shift or lack of generalization. A model is only as good as the data it was trained on, and it assumes the future (or new location) looks exactly like the past.\n\n\n12. The Thinking Emoji\n\n\n\nSlide 12\n\n\n(Timestamp: 03:43)\nThis slide reinforces the previous point with a thinking emoji, emphasizing the realization that the existing model is inadequate. The ‚ÄúSan Francisco Model‚Äù does not fit the ‚ÄúSouth Carolina Data.‚Äù\nIt serves as a visual pause to let the problem sink in: traditional machine learning is brittle. It requires the data distribution to remain constant.\nRajiv uses this to set up the labor-intensive nature of traditional analytics, where models cannot simply be ‚Äútransferred‚Äù across different contexts.\n\n\n13. Train New Model\n\n\n\nSlide 13\n\n\n(Timestamp: 03:55)\nThe solution in the traditional paradigm is presented here: ‚ÄúTrain New Model.‚Äù To get accurate predictions for South Carolina, one must collect local data and repeat the entire training process from scratch.\nThis highlights the ‚ÄúNever-Ending Battle‚Äù of enterprise analytics. Data scientists are constantly retraining models for every specific region, product line, or use case.\nThis sets the baseline for why Transfer Learning (introduced later) is such a revolution. In the old way, knowledge was not portable; every problem required a bespoke solution.\n\n\n14. Artisan AI\n\n\n\nSlide 14\n\n\n(Timestamp: 04:15)\nRajiv coins the term ‚ÄúArtisan AI‚Äù to describe this traditional approach. The slide features an image of a craftsman, symbolizing that these models are hand-built and rely heavily on human-crafted features.\nThis approach is slow and difficult to scale. Just as an artisan can only produce a limited number of goods, a data science team using these methods can only maintain a limited number of models.\nIt emphasizes that the intelligence in these systems comes largely from the human who engineered the features, not the machine itself.\n\n\n15. Enterprise AI Use Cases\n\n\n\nSlide 15\n\n\n(Timestamp: 04:23)\nThis slide lists common Enterprise AI applications: Forecasting, Pricing, Customer Churn, and Fraud. It notes that 80% of production models currently fall into this category.\nRajiv grounds the talk in the reality of today‚Äôs business world. Despite the hype around Generative AI, most companies are still running on these ‚ÄúArtisan‚Äù structured data models.\nThis distinction is crucial for understanding the market. There is ‚ÄúOld AI‚Äù (highly effective, structured, labor-intensive) and ‚ÄúNew AI‚Äù (generative, unstructured, scalable), and they solve different problems.\n\n\n16. The Computer Science Perspective\n\n\n\nSlide 16\n\n\n(Timestamp: 04:41)\nThe presentation shifts from the enterprise view to the academic Computer Science view. The slide asks, ‚ÄúHow should we teach computers?‚Äù signaling a move toward more advanced methodologies.\nRajiv indicates that computer scientists were trying to find ways to move beyond the limitations of manual feature engineering. They wanted machines to learn the features themselves.\nThis transition introduces the concept of Deep Learning and the move toward processing unstructured data like audio, images, and text.\n\n\n17. Frederick Jelinek‚Äôs Insight\n\n\n\nSlide 17\n\n\n(Timestamp: 04:49)\nThis slide introduces a quote from Frederick Jelinek, a pioneer in speech recognition: ‚ÄúEvery time I fire a linguist, the performance of the speech recognizer goes up.‚Äù\nThis provocative quote encapsulates a major shift in AI philosophy. It suggests that human expertise (linguistics) often gets in the way of raw data processing. Instead of hard-coding grammar rules, it is better to let the model learn patterns directly from the data.\nRajiv asks the audience to ‚Äúchew on that,‚Äù as it foreshadows the ‚ÄúBitter Lesson‚Äù of AI: massive compute and data often outperform human domain expertise.\n\n\n18. Computer Vision in 2010\n\n\n\nSlide 18\n\n\n(Timestamp: 05:47)\nThe slide depicts the state of Computer Vision around 2010. It shows a process of manual feature extraction (like HOG - Histogram of Oriented Gradients) used to identify shapes and edges.\nRajiv explains that even in vision, researchers were essentially doing ‚ÄúArtisan AI.‚Äù They sat around thinking about how to mathematically describe the shape of a car or a truck to a computer.\nThis illustrates that before the deep learning boom, computer vision was stuck in the same ‚Äúfeature engineering‚Äù trap as tabular analytics.\n\n\n19. SVM Classification\n\n\n\nSlide 19\n\n\n(Timestamp: 06:05)\nFollowing feature extraction, this slide shows a Support Vector Machine (SVM) classifier separating data points (cars vs.¬†trucks). This was the standard approach: extract features manually, then use a simple algorithm to classify them.\nThis reinforces the previous point about the limitations of the time. The intelligence was in the manual extraction, not the classification model.\nRajiv mentions his own work at Caterpillar, noting that this was exactly how they tried to separate images of machinery‚Äîa tedious and specific process.\n\n\n20. Fei-Fei Li and Big Data\n\n\n\nSlide 20\n\n\n(Timestamp: 06:15)\nThe slide introduces Professor Fei-Fei Li, a visionary in computer vision. It features a collage of images, hinting at the need for scale.\nRajiv explains that Fei-Fei Li recognized that for computer vision to advance, it needed to move away from tiny datasets (100-200 images) and toward massive scale. She understood that deep learning required vast amounts of data to generalize.\nThis marks the beginning of the ‚ÄúBig Data‚Äù era in AI, where the focus shifted from better algorithms to better and larger datasets.\n\n\n21. ImageNet\n\n\n\nSlide 21\n\n\n(Timestamp: 06:41)\nThis slide details ImageNet, the dataset Fei-Fei Li helped create. It contains 14 million images across 1000 classes.\nRajiv highlights the sheer effort involved, noting the use of Mechanical Turk to crowdsource the labeling of these images. He calls this the ‚Äúdirty secret‚Äù of AI‚Äîthat it is powered by low-wage human labor labeling data.\nImageNet became the benchmark that drove the AI revolution. It provided the ‚Äúfuel‚Äù necessary for neural networks to finally work.\n\n\n22. AlexNet and GPUs\n\n\n\nSlide 22\n\n\n(Timestamp: 07:44)\nThe presentation introduces Alex Krizhevsky, a graduate student under Geoffrey Hinton. The slide mentions ‚ÄúAlexNet‚Äù and the use of GPUs (Graphics Processing Units).\nRajiv tells the story of how Alex decided to use NVIDIA gaming cards to train neural networks. Traditional CPUs were too slow for the math required by deep learning.\nThis moment‚Äîcombining the massive ImageNet dataset with the parallel processing power of GPUs‚Äîwas the ‚Äúbig bang‚Äù of modern AI.\n\n\n23. AlexNet Training Details\n\n\n\nSlide 23\n\n\n(Timestamp: 08:05)\nThis slide provides the technical specs of AlexNet: trained on 1.2 million images, using 2 GPUs, taking roughly 6 days, with 60 million parameters.\nRajiv emphasizes that while 6 days seems long, the result was a model vastly superior to anything else. It proved that neural networks, which had been theoretical for decades, were now practical.\nThe ‚Äú60 million parameters‚Äù figure is a precursor to the ‚Äúbillions‚Äù and ‚Äútrillions‚Äù we see today, marking the start of the parameter scaling race.\n\n\n24. Crushing the Competition\n\n\n\nSlide 24\n\n\n(Timestamp: 08:27)\nA chart displays the results of the ImageNet Large Scale Visual Recognition Challenge. It shows AlexNet achieving a significantly lower error rate than the competitors.\nRajiv notes that the performance jump was so dramatic that by the following year, every competitor had switched to using the AlexNet architecture.\nThis visualizes the paradigm shift. The ‚ÄúArtisan‚Äù methods were instantly obsolete, replaced by Deep Learning.\n\n\n25. Feature Engineering vs.¬†Deep Learning\n\n\n\nSlide 25\n\n\n(Timestamp: 08:37)\nUsing a humorous meme format, this slide compares the ‚ÄúOld Way‚Äù (Feature Engineering + SVM) with the ‚ÄúNew Way‚Äù (AlexNet). The AlexNet side is depicted as a powerful, overwhelming force.\nThis solidifies the takeaway: Deep Learning didn‚Äôt just improve upon the old methods; it completely replaced them for unstructured data tasks like vision.\nIt emphasizes that the model learned the features itself (edges, textures, shapes) rather than having humans manually code them.\n\n\n26. The 1000 Classes\n\n\n\nSlide 26\n\n\n(Timestamp: 08:51)\nThis slide shows examples of the 1000 classes in ImageNet, ranging from specific dog breeds to everyday objects.\nRajiv explains that this model learned to identify a vast array of things from the raw pixels. It went from raw vision to understanding textures, shapes, and objects.\nHowever, he sets up the next problem: What if you want to identify something not in those 1000 classes?\n\n\n27. The Hot Dog Problem\n\n\n\nSlide 27\n\n\n(Timestamp: 09:01)\nreferencing a famous scene from the show Silicon Valley, this slide presents the specific challenge of classifying ‚ÄúHot Dogs.‚Äù\nRajiv uses this to ask: How do you help a buddy with a startup who needs to find hot dogs if ‚Äúhot dog‚Äù isn‚Äôt one of the primary categories, or if they need a specific type of hot dog? Do you have to start from scratch?\nThis sets the stage for Transfer Learning‚Äîthe solution to avoiding the need for 14 million images every time you have a new problem.\n\n\n28. Pre-Trained Models\n\n\n\nSlide 28\n\n\n(Timestamp: 09:12)\nThe slide introduces the concept of a Pre-trained Model. This is the model that has already learned the 1000 classes from ImageNet.\nRajiv explains that this model already ‚Äúknows‚Äù how to see. It understands edges, curves, and textures. This knowledge is contained in the ‚Äúweights‚Äù of the neural network.\nThe key idea is that we don‚Äôt need to relearn how to ‚Äúsee‚Äù every time we want to identify a new object.\n\n\n29. Transfer Learning Mechanics\n\n\n\nSlide 29\n\n\n(Timestamp: 09:30)\nThis technical slide illustrates how Transfer Learning works. It shows the layers of a neural network. We keep the early layers (which know shapes and textures) and only retrain the final layers for the new task (e.g., identifying boats).\nRajiv explains that we can transfer ‚Äúmost of that knowledge‚Äù and only change a small amount of parameters (less than 10%).\nThis is the revolution: You can build a world-class model with a small amount of data by standing on the shoulders of the giant ImageNet model.\n\n\n30. The Revolution\n\n\n\nSlide 30\n\n\n(Timestamp: 09:54)\nA graph titled ‚ÄúTransfer Learning Revolution‚Äù shows the dramatic improvement in accuracy when using transfer learning versus training from scratch. It includes a quote from Andrew Ng stating that transfer learning will be the next driver of commercial success.\nRajiv emphasizes that this capability allowed startups and companies to build powerful AI without needing Google-sized datasets. It democratized access to high-performance computer vision.\nThis wraps up the vision section of the talk, establishing Transfer Learning as the ‚ÄúSpark.‚Äù\n\n\n31. The Implications\n\n\n\nSlide 31\n\n\n(Timestamp: 10:11)\nThe slide shows a YouTube video thumbnail from 2016 featuring Geoffrey Hinton. This transitions the talk to the societal and professional implications of this technology.\nRajiv prepares to share a famous prediction by Hinton regarding the medical field, specifically radiology. It signals a shift from ‚Äúhow it works‚Äù to ‚Äúwhat it does to jobs.‚Äù\n\n\n32. The Coyote Moment\n\n\n\nSlide 32\n\n\n(Timestamp: 10:19)\nThe slide displays a webpage for the University of Cincinnati Radiology Fellows. Rajiv quotes Hinton: ‚ÄúRadiologists are like the coyote that‚Äôs already over the edge of the cliff but hasn‚Äôt yet looked down.‚Äù\nHinton suggested people should stop training radiologists because AI interprets images better. Rajiv humorously notes that since he was speaking at U of C, he had to show the ‚Äúcoyotes‚Äù in the audience.\nThis highlights the tension between AI capabilities and human expertise, a recurring theme in the presentation.\n\n\n33. NLP: The Academic View\n\n\n\nSlide 33\n\n\n(Timestamp: 10:42)\nThe presentation switches domains from Computer Vision to Natural Language Processing (NLP). The slide depicts a traditional academic setting, representing the text researchers.\nRajiv explains that while Computer Vision was having its revolution with AlexNet, the text folks were still doing things the ‚ÄúOld Way‚Äù‚Äîcrafting features and rules for language.\nThey saw the success in vision and wondered how to replicate it for text, but language proved more difficult to model than images initially.\n\n\n34. Traditional NLP Tasks\n\n\n\nSlide 34\n\n\n(Timestamp: 11:04)\nThis slide lists various NLP tasks: Classification, Information Extraction, and Sentiment Analysis.\nRajiv notes that traditionally, each of these was a separate discipline. You built a specific model for sentiment, a different one for translation, and another for summarization. There was no ‚Äúone model to rule them all.‚Äù\nThis fragmentation made NLP difficult and resource-intensive, as knowledge didn‚Äôt transfer between tasks.\n\n\n35. The GLUE Benchmark\n\n\n\nSlide 35\n\n\n(Timestamp: 11:25)\nThe slide introduces the GLUE Benchmark (General Language Understanding Evaluation). This was a collection of different text tasks put together to measure general language ability.\nRajiv explains this was an attempt to push the field toward general-purpose models. Researchers wanted a single metric to see if a model could understand language broadly, not just solve one specific trick.\n\n\n36. The Transformer Architecture\n\n\n\nSlide 36\n\n\n(Timestamp: 11:37)\nThis slide marks the turning point for text: the introduction of the Transformer architecture by Google researchers in 2017 (the ‚ÄúAttention Is All You Need‚Äù paper).\nRajiv highlights that this architecture was not only more accurate (higher BLEU scores) but, crucially, more efficient.\nThe Transformer allowed for parallel processing of text, unlike previous sequential models (RNNs/LSTMs), unlocking the ability to train on massive datasets.\n\n\n37. Lower Training Costs\n\n\n\nSlide 37\n\n\n(Timestamp: 11:46)\nThe slide emphasizes the Training Cost reduction associated with Transformers.\nRajiv points out that because the architecture used less processing power per unit of data, researchers immediately asked: ‚ÄúWhat happens if we give it more processing?‚Äù\nThis efficiency paradox‚Äîmaking something cheaper allows you to do vastly more of it‚Äîsparked the scaling era of LLMs.\n\n\n38. Exponential Growth\n\n\n\nSlide 38\n\n\n(Timestamp: 11:56)\nA graph demonstrates the exponential growth in the size of Transformer models (measured in parameters) over just a few years. The curve shoots upward vertically.\nRajiv explains that this scaling‚Äîsimply making the models bigger and feeding them more data‚Äîled to the performance of GPT-4.\nThis visualizes the ‚ÄúScale‚Äù aspect of modern AI. We haven‚Äôt necessarily changed the architecture since 2017; we‚Äôve just made it significantly larger.\n\n\n39. GPT-4 and Images\n\n\n\nSlide 39\n\n\n(Timestamp: 12:11)\nThe presentation circles back to the GPT-4 generated images from Slide 2.\nRajiv connects the Transformer architecture and scaling directly to these ‚ÄúSparks of AGI.‚Äù The ability to reason and draw emerged from simply predicting the next word at a massive scale.\n\n\n40. The Era of ChatGPT\n\n\n\nSlide 40\n\n\n(Timestamp: 12:16)\nThe slide displays the ChatGPT logo, symbolizing the current era where these technical advancements reached the public consciousness.\nRajiv sets up the next section of the talk: explaining exactly how a model like ChatGPT is trained. He moves from history to the ‚ÄúRecipe.‚Äù\n\n\n41. The Learning Process\n\n\n\nSlide 41\n\n\n(Timestamp: 12:20)\nA visual diagram outlines the evolutionary stages of ChatGPT. It previews the three steps Rajiv will cover: Pre-training, Fine-tuning, and Alignment.\nThis roadmap helps the audience understand that ChatGPT isn‚Äôt just one static thing; it‚Äôs the result of a multi-stage pipeline involving different types of learning.\n\n\n42. Recipe Step 1: Foundation Model\n\n\n\nSlide 42\n\n\n(Timestamp: 12:27)\nThe first step identified is the ‚ÄúFoundation Model‚Äù (or Base Model).\nRajiv explains that the core capability of these models is Next Word Prediction. Before it can answer questions or be helpful, it must simply learn the statistical structure of language.\n\n\n43. Predictive Keyboards\n\n\n\nSlide 43\n\n\n(Timestamp: 12:31)\nTo make the concept relatable, the slide compares LLMs to the predictive text feature on a smartphone keyboard.\nRajiv notes that while the game on your phone is simple, scaling that concept up to the entire internet makes it incredibly powerful. It grounds the ‚Äúmagic‚Äù of AI in a familiar user experience.\n\n\n44. Next Token Prediction\n\n\n\nSlide 44\n\n\n(Timestamp: 12:40)\nThis technical slide defines ‚ÄúNext Token Prediction.‚Äù It explains that the model looks at a sequence of text and calculates the probability of what comes next.\nRajiv emphasizes that this is a hard statistical problem. There are many possibilities for the next word, and the model must learn to weigh them based on context.\n\n\n45. The Homer Simpson Challenge\n\n\n\nSlide 45\n\n\n(Timestamp: 13:05)\nRajiv introduces a specific experiment: Training a Transformer to speak like Homer Simpson. He mentions using 7MB of Simpsons scripts (~7 million tokens).\nThis serves as a concrete example to show how training data size affects model performance.\n\n\n46. 4 Million Tokens\n\n\n\nSlide 46\n\n\n(Timestamp: 13:32)\nThe slide shows the output of the model when trained on only 4 Million tokens. The text is ‚Äúnonsensical and random.‚Äù\nRajiv demonstrates that with insufficient data, the model hasn‚Äôt learned grammar or structure yet. It‚Äôs just outputting characters.\n\n\n47. 16 Million Tokens\n\n\n\nSlide 47\n\n\n(Timestamp: 13:37)\nAt 16 Million tokens, the output improves slightly. It contains random words and incorrect grammar, but it‚Äôs recognizable as language.\nThis illustrates the ‚Äúgrokking‚Äù phase where the model starts to pick up on basic syntax but lacks semantic meaning.\n\n\n48. 64 Million Tokens\n\n\n\nSlide 48\n\n\n(Timestamp: 13:39)\nWith 64 Million tokens, the model generates text that is ‚Äúclose to a proper sentence‚Äù and sounds vaguely like Homer Simpson.\nRajiv uses this progression to prove that these models are statistical engines. With enough data, they mimic the patterns of the training set effectively.\n\n\n49. GPT-2 Specifications\n\n\n\nSlide 49\n\n\n(Timestamp: 13:54)\nThe slide details GPT-2 (released in 2019), which had 1.5 Billion parameters.\nRajiv recalls that when GPT-2 came out, he wasn‚Äôt excited because it was just a ‚Äúcreative storytelling model.‚Äù It wasn‚Äôt factually accurate. He wants the audience to remember that at their core, these models are just predicting the next word, not checking facts.\n\n\n50. Llama 3.1 and Scale\n\n\n\nSlide 50\n\n\n(Timestamp: 14:27)\nUpdating the timeline, this slide shows Llama 3.1. It highlights the training data: 15 Trillion Tokens and the compute: 40 Million GPU Hours.\nRajiv emphasizes that 15 trillion tokens is an ‚Äúunfathomable amount of information.‚Äù The scale has increased 10,000x since GPT-2.\nThis underscores the energy and compute intensity of modern AI‚Äîit requires massive infrastructure.\n\n\n51. Hallucinations\n\n\n\nSlide 51\n\n\n(Timestamp: 15:15)\nThis slide addresses Hallucinations. It uses an example of asking for the ‚ÄúCapital of Mars.‚Äù The model will confidently invent an answer.\nRajiv argues that ‚Äúhallucination‚Äù isn‚Äôt the right metaphor because the model isn‚Äôt malfunctioning. It is doing exactly what it was designed to do: predict the most likely next word. It has no concept of ‚Äútruth,‚Äù only statistical likelihood.\n\n\n52. GPT-2 Failure on Sentiment\n\n\n\nSlide 52\n\n\n(Timestamp: 16:17)\nRajiv shows an example of trying to use the base GPT-2 model for a specific task: Customer Sentiment. When prompted, the model just continues the story instead of classifying the sentiment.\nThis illustrates that Base Models are creative but not useful for following instructions. They don‚Äôt know they are supposed to solve a problem; they just want to write text.\n\n\n53. Recipe Step 2: Instruction Fine-Tuned\n\n\n\nSlide 53\n\n\n(Timestamp: 16:38)\nThis introduces the second step in the ChatGPT recipe: ‚ÄúInstruction Fine-Tuned Model.‚Äù\nRajiv explains that to make the model useful, we must teach it to follow orders. This is done via Transfer Learning‚Äîtaking the base model and training it further on examples of instructions and answers.\n\n\n54. Fine-Tuning for Sentiment\n\n\n\nSlide 54\n\n\n(Timestamp: 16:46)\nThe slide shows the process of fine-tuning the language model specifically for Sentiment Analysis.\nBy showing the model examples of ‚ÄúSentence -&gt; Sentiment,‚Äù we can tweak the parameters so it learns to perform classification rather than just storytelling.\n\n\n55. Multi-Task Fine-Tuning\n\n\n\nSlide 55\n\n\n(Timestamp: 17:22)\nRajiv expands the concept. We don‚Äôt just fine-tune for one task; we fine-tune for Topic Classification as well.\nThe key insight is that one model can now solve multiple problems. Unlike the ‚ÄúOld NLP‚Äù where you needed separate models, the LLM can swap between tasks based on the instruction.\n\n\n56. Translation Task\n\n\n\nSlide 56\n\n\n(Timestamp: 17:24)\nThe slide adds Translation to the mix, using about 10,000 examples.\nThis reinforces the ‚ÄúGeneral Purpose‚Äù nature of LLMs. They are Swiss Army knives for text.\n\n\n57. Generalization to New Tasks\n\n\n\nSlide 57\n\n\n(Timestamp: 17:30)\nRajiv poses a challenge: What happens if you give the model a task it hasn‚Äôt seen before?\nThe slide indicates the model will try to solve it. This is the breakthrough of Generalization. Because it understands language so well, it can interpolate and attempt tasks it wasn‚Äôt explicitly trained on.\n\n\n58. Practical Applications\n\n\n\nSlide 58\n\n\n(Timestamp: 17:55)\nThis slide showcases the wide array of use cases: Code explanation, Creative writing, Information extraction, etc.\nRajiv explains that these capabilities exist because we have ‚Äútrained these models to follow instructions.‚Äù This is why we can talk to them via Prompts.\n\n\n59. Zero Shot Learning\n\n\n\nSlide 59\n\n\n(Timestamp: 18:19)\nThe slide introduces ‚ÄúZero shot learning‚Äù and ‚ÄúPrompting.‚Äù\nThis is the ability to get a result without showing the model any examples (zero shots). Rajiv notes that there is a ‚Äúwhole language‚Äù around prompting, but fundamentally, it‚Äôs just giving the model the instruction we trained it to expect.\n\n\n60. Weeks vs.¬†Days\n\n\n\nSlide 60\n\n\n(Timestamp: 18:41)\nA comparison slide contrasts ‚ÄúTraining a ML Model (weeks)‚Äù with ‚ÄúPrompting a LLM (days).‚Äù\nRajiv highlights the efficiency shift. In the old days, solving a sentiment problem meant weeks of data collection and training. Now, it takes minutes to write a prompt. This is a massive productivity booster for NLP tasks.\n\n\n61. Reasoning and Planning\n\n\n\nSlide 61\n\n\n(Timestamp: 19:25)\nThe presentation pivots to the limitations of LLMs, specifically regarding Reasoning and Planning. The slide shows a ‚ÄúBlock Stacking‚Äù puzzle.\nRajiv explains that stacking blocks requires planning several steps ahead. It is not a one-step prediction problem; it requires maintaining a state of the world in memory.\n\n\n62. Mystery World Failure\n\n\n\nSlide 62\n\n\n(Timestamp: 20:40)\nThe slide introduces ‚ÄúMystery World,‚Äù a variation of the block problem where the names of the blocks are changed to random words.\nWhile a human (or a 4-year-old) understands that changing the name doesn‚Äôt change the physics of stacking, GPT-4 fails (3% accuracy). Rajiv explains that the model gets distracted by the creative aspect of the words and loses the logical thread. It shows these models struggle with abstract reasoning.\n\n\n63. Recipe Step 3: Aligned Model\n\n\n\nSlide 63\n\n\n(Timestamp: 21:56)\nThe final step in the recipe is the ‚ÄúAligned Model.‚Äù\nRajiv introduces the need for safety and helpfulness. A model that follows instructions perfectly might follow bad instructions. We need to align it with human values.\n\n\n64. Galactica: Science LLM\n\n\n\nSlide 64\n\n\n(Timestamp: 22:00)\nThe slide presents Galactica, a model released by Meta focused on science.\nRajiv describes the intent: a helpful assistant for researchers to write code, summarize papers, and generate scientific content. It was meant to be a specialized tool.\n\n\n65. Galactica Output\n\n\n\nSlide 65\n\n\n(Timestamp: 22:20)\nAn example of Galactica‚Äôs output shows it generating technical content.\nRajiv highlights the potential utility. It looked like a powerful tool for accelerating scientific discovery.\n\n\n66. Galactica Pulled\n\n\n\nSlide 66\n\n\n(Timestamp: 22:47)\nThe slide reveals that Meta pulled the model shortly after release.\nRajiv explains why: users found they could ask it for the ‚Äúbenefits of eating crushed glass‚Äù or ‚Äúbenefits of suicide,‚Äù and the model would happily generate a scientific-sounding justification. It lacked a safety layer. This incident underscored the necessity of Red Teaming and alignment before release.\n\n\n67. Learning What is Helpful\n\n\n\nSlide 67\n\n\n(Timestamp: 23:59)\nTo explain how we define ‚Äúhelpful,‚Äù Rajiv shows a Stack Overflow question.\nHe notes that defining ‚Äúhelpful‚Äù mathematically is difficult. Unlike ‚Äúsquare footage,‚Äù helpfulness is subjective and nuanced.\n\n\n68. Technical Answer\n\n\n\nSlide 68\n\n\n(Timestamp: 24:05)\nThe slide shows a detailed technical answer.\nRajiv points out that trying to create a ‚Äúfeature list‚Äù for what makes this answer helpful is nearly impossible. We can‚Äôt write a rule-based program to detect helpfulness.\n\n\n69. The Dating App Analogy\n\n\n\nSlide 69\n\n\n(Timestamp: 24:26)\nRajiv uses a humorous Dating App analogy. He compares the ‚ÄúOld Way‚Äù (filling out long compatibility forms/features) with the ‚ÄúNew Way‚Äù (Swiping).\nHe explains that Swiping is a way of capturing human preferences without asking the user to explicitly define them. This is how we teach AI what is helpful.\n\n\n70. Collect Human Feedback\n\n\n\nSlide 70\n\n\n(Timestamp: 24:55)\nThe slide details the process: ‚ÄúCollect Human Feedback.‚Äù\nWe present the model with two options and ask a human, ‚ÄúWhich is better?‚Äù By collecting thousands of these ‚Äúswipes,‚Äù we build a dataset of human preference.\n\n\n71. RLHF (Reinforcement Learning from Human Feedback)\n\n\n\nSlide 71\n\n\n(Timestamp: 25:05)\nThis slide introduces the technical term: RLHF.\nRajiv explains this is the layer that turns a raw instruction-following model into a safe, helpful product like ChatGPT. It is an active curation process, similar to curating an Instagram feed.\n\n\n72. The Makeover Example\n\n\n\nSlide 72\n\n\n(Timestamp: 25:27)\nA ‚ÄúBefore and After‚Äù makeover image illustrates the effect of RLHF.\nThe ‚ÄúBefore‚Äù is the raw model (messy, potentially harmful). The ‚ÄúAfter‚Äù is the aligned model (polished, safe, presentable).\n\n\n73. Tuning Responses\n\n\n\nSlide 73\n\n\n(Timestamp: 25:44)\nThe slide shows different ways an AI can answer a question: Sycophantic (sucking up to the user), Baseline Truthful (blunt), or Helpful Truthful.\nRajiv notes we can train models to have specific personalities. We can make them polite, or we can make them ‚Äúkiss your butt‚Äù if the user wants validation.\n\n\n74. AI Conversations\n\n\n\nSlide 74\n\n\n(Timestamp: 26:17)\nThis slide references Character.ai and the trend of people spending hours talking to AI personas.\nRajiv mentions research showing people sometimes prefer AI doctors over human ones because the AI is patient, listens, and is polite (due to alignment). This suggests a future where AI handles high-touch conversational roles.\n\n\n75. The Full Recipe\n\n\n\nSlide 75\n\n\n(Timestamp: 27:19)\nThe presentation summarizes the full pipeline: Foundation Model -&gt; Instruction Fine-Tuned -&gt; Aligned Model.\nThis visual recap cements the three-stage process in the audience‚Äôs mind.\n\n\n76. Learning Mechanisms Recap\n\n\n\nSlide 76\n\n\n(Timestamp: 27:23)\nRajiv maps the learning mechanisms to the stages: 1. Next Word Prediction (Foundation) 2. Multi-task Training (Instruction) 3. Human Preferences (Alignment)\nHe reiterates that understanding these three mechanics helps explain why the models behave the way they do (hallucinations, ability to code, politeness).\n\n\n77. Key Takeaways\n\n\n\nSlide 77\n\n\n(Timestamp: 27:43)\nThe presentation transitions to the conclusion with three main takeaways: 1. Measure Twice 2. Respect Scale 3. Critical Thinking\nRajiv notes in the video that he skimmed these in the original talk, but the slides provide the detail for how to work effectively with AI.\n\n\n78. Measure Twice (Benchmarks)\n\n\n\nSlide 78\n\n\n([Timestamp: End of Transcript])\nThis slide displays a collage of AI benchmarks (MMLU, HumanEval, etc.).\nThe concept ‚ÄúMeasure Twice‚Äù emphasizes that because AI models are probabilistic and prone to hallucination, we cannot trust them blindly. We must rely on rigorous benchmarking to understand their capabilities and failures before deployment.\n\n\n79. Targets for Evaluation\n\n\n\nSlide 79\n\n\n([Timestamp: End of Transcript])\nThis slide likely elaborates on the need for clear ‚Äútargets‚Äù or ground truth when evaluating models.\nYou cannot improve what you cannot measure. In the context of ‚ÄúPrompt Engineering,‚Äù this means you shouldn‚Äôt just tweak prompts randomly; you need a systematic way to measure if a prompt change actually improved the output.\n\n\n80. Respect Scale\n\n\n\nSlide 80\n\n\n([Timestamp: End of Transcript])\nThis slide illustrates the exponential growth in single-chip inference performance.\n‚ÄúRespect Scale‚Äù refers to the lesson that betting against hardware and data scaling is usually a losing bet. The capabilities of these models grow faster than our intuition expects.\n\n\n81. The Scaling Lesson (Humans)\n\n\n\nSlide 81\n\n\n([Timestamp: End of Transcript])\nThis slide likely discusses how human expertise fits into the scaling laws. As technology scales, the role of the human shifts from doing the work to evaluating the work.\n\n\n82. The Plateau\n\n\n\nSlide 82\n\n\n([Timestamp: End of Transcript])\nA visual showing that human contribution or specific ‚Äúhacks‚Äù tend to plateau, whereas general-purpose methods that leverage scale (like Transformers) continue to improve.\nThis reinforces the ‚ÄúBitter Lesson‚Äù: specialized, hand-crafted solutions eventually lose to general methods that can consume more compute.\n\n\n83. AlexNet vs Transformers\n\n\n\nSlide 83\n\n\n([Timestamp: End of Transcript])\nA comparison between AlexNet (the start of the deep learning era) and Transformers (the current era).\nIt highlights the massive increase: 10,000x more data and 1,000x more compute. This illustrates that the fundamental driver of progress has been scale.\n\n\n84. The Bitter Lesson\n\n\n\nSlide 84\n\n\n([Timestamp: End of Transcript])\nThis slide explicitly references Rich Sutton‚Äôs ‚ÄúThe Bitter Lesson.‚Äù\nThe lesson is that researchers often try to build their knowledge into the system (like Jelinek‚Äôs linguists), but in the long run, the only thing that matters is leveraging computation. AI succeeds when we stop trying to teach it how to think and just give it enough power to learn on its own.\n\n\n85. Text to SQL\n\n\n\nSlide 85\n\n\n([Timestamp: End of Transcript])\nThe slide examines Text to SQL, a common enterprise use case. It compares AI performance to human experts.\nIt notes that while AI is good, humans still achieve higher exact match accuracy. This nuances the ‚ÄúRespect Scale‚Äù argument‚Äîfor high-precision tasks, human oversight is still required.\n\n\n86. Critical Thinking\n\n\n\nSlide 86\n\n\n([Timestamp: End of Transcript])\nThe final takeaway is ‚ÄúCritical Thinking.‚Äù\nIn an age where AI can generate convincing but false information, human judgment becomes the most valuable skill. We must critically evaluate the outputs of these models.\n\n\n87. Predictions and Concerns\n\n\n\nSlide 87\n\n\n([Timestamp: End of Transcript])\nThis slide recaps expert predictions, ranging from job displacement to existential threats.\nIt serves as a reminder that even experts disagree on the timeline and impact, reinforcing the need for individual critical thinking rather than blind faith in pundits.\n\n\n88. Practical Limits: Bezos and Alexa\n\n\n\nSlide 88\n\n\n([Timestamp: End of Transcript])\nA humorous slide showing Jeff Bezos and Alexa. It likely references an instance where Alexa failed to understand a simple context despite Amazon‚Äôs massive resources.\nThis illustrates the ‚ÄúPractical Limits of Learning.‚Äù Despite the hype, current AI still struggles with basic context that humans find trivial.\n\n\n89. Autonomous Driving Limits\n\n\n\nSlide 89\n\n\n([Timestamp: End of Transcript])\nImages of an autonomous driving interface and a car accident.\nThis points out that in high-stakes physical environments, ‚Äú99% accuracy‚Äù isn‚Äôt enough. The ‚Äúlong tail‚Äù of edge cases remains a massive hurdle for AI.\n\n\n90. Chatbot Failures\n\n\n\nSlide 90\n\n\n([Timestamp: End of Transcript])\nExamples of chatbots failing simple math or making ‚Äúlegally binding offers‚Äù (referencing the Air Canada chatbot lawsuit).\nThis warns against deploying these models in critical business flows without guardrails. They can confidently make costly mistakes.\n\n\n91. Interaction Principles\n\n\n\nSlide 91\n\n\n([Timestamp: End of Transcript])\nThis slide summarizes the three principles for interacting with AI: ‚ÄúMeasure twice,‚Äù ‚ÄúRespect scale,‚Äù and ‚ÄúThink critically.‚Äù\nIt acts as the final instructional slide, giving the audience a mantra for navigating the AI landscape.\n\n\n92. Evolution of Generative Capabilities\n\n\n\nSlide 92\n\n\n([Timestamp: End of Transcript])\nThe slide shows a series of Unicorn images generated by GPT-4 over time.\nThis visualizes the rapid improvement in generative capabilities. Just as the ‚ÄúSparks of AGI‚Äù images improved, the fidelity of these outputs continues to evolve, reminding us that we are looking at a moving target.\n\n\n93. Conclusion\n\n\n\nSlide 93\n\n\n([Timestamp: End of Transcript])\nThe final slide concludes the presentation with Rajiv Shah‚Äôs name and affiliation (Snowflake).\nIt wraps up the narrative: from the spark of Transfer Learning to the fire of the Generative AI revolution, offering a practical, technical, and critical perspective on the technology shaping our future.\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "interpretable-ml-models.html",
    "href": "interpretable-ml-models.html",
    "title": "Interpretable Machine Learning Models Simply Explained",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "interpretable-ml-models.html#video",
    "href": "interpretable-ml-models.html#video",
    "title": "Interpretable Machine Learning Models Simply Explained",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "interpretable-ml-models.html#annotated-presentation",
    "href": "interpretable-ml-models.html#annotated-presentation",
    "title": "Interpretable Machine Learning Models Simply Explained",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the annotated presentation for ‚ÄúRules: A Simple & Effective Machine Learning Approach‚Äù by Rajiv Shah.\n\n1. Title Slide\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00:00)\nThe presentation begins by introducing the core topic: Interpretable Models and the use of rules in machine learning. Rajiv Shah sets the stage by contrasting this talk with previous discussions on explainability (using tools to explain complex models). Instead, this session focuses on choosing models that are inherently easy to understand.\nShah expresses his interest in how machine learning helps us understand the world. He notes that while tools like SHAP or LIME help unpack complex models, there is immense value in approaching the problem differently: by selecting model architectures that are transparent by design.\nThe speaker invites the audience to view this not just as a technical lecture but as a discussion on the trade-offs between model complexity and interpretability, setting a collaborative tone for the presentation.\n\n\n2. Table of Contents\n\n\n\nSlide 2\n\n\n(Timestamp: 00:02:30)\nThis slide outlines the roadmap for the presentation. Shah explains that he will begin with the ‚ÄúBig Picture‚Äù concepts‚Äîspecifically the ‚ÄúWhy?‚Äù and the ‚ÄúBaseline‚Äù‚Äîbefore diving into four specific technical approaches to rule-based modeling.\nThe four specific methods to be covered are Rulefit, GA2M (Generalized Additive Models with interactions), Rule Lists, and Scorecards. This structure moves from theoretical justification to practical application, comparing different algorithms that prioritize transparency.\nShah also mentions that a GitHub repository is available with code examples for everything shown, allowing the audience to reproduce the results for the tabular datasets discussed.\n\n\n3. Section 1: Why?\n\n\n\nSlide 3\n\n\n(Timestamp: 00:03:09)\nThis section header introduces the fundamental question: Why do we want rules? The speaker moves past the obvious statement that ‚ÄúAI is important‚Äù to investigate the influences that drive data scientists toward complex, opaque models.\nShah prepares to discuss the cultural and competitive pressures in data science that prioritize raw accuracy over usability. This section serves as a critique of the ‚Äúaccuracy at all costs‚Äù mindset often found in the industry.\n\n\n4. Mark Cuban Quote\n\n\n\nSlide 4\n\n\n(Timestamp: 00:03:17)\nThe slide features a quote from Mark Cuban: ‚ÄúArtificial Intelligence, deep learning, machine learning ‚Äî whatever you‚Äôre doing if you don‚Äôt understand it ‚Äî learn it. Because otherwise you‚Äôre going to be a dinosaur within 3 years.‚Äù\nShah briefly references this as the ‚Äúobligatory‚Äù acknowledgment of AI‚Äôs massive importance in the current landscape. It reinforces that while the field is moving fast, the understanding of these systems is paramount, which ties into the presentation‚Äôs focus on interpretability.\n\n\n5. Influences: Kaggle & Academia\n\n\n\nSlide 5\n\n\n(Timestamp: 00:03:40)\nShah identifies Kaggle competitions and academic research as two primary influences on data scientists. He notes that these platforms heavily incentivize accuracy above all else. For example, in the Zillow Prize, the difference between the top scores is minuscule, yet teams fight for that fraction of a percentage.\nHe argues that this environment trains data scientists to focus solely on improving metrics (like RMSE or AUC), often ignoring other critical trade-offs like model complexity, deployment difficulty, or explainability.\nAs he states, ‚ÄúOne of the byproducts of Kaggle is a very heavy focus on making sure you improve your models around accuracy‚Ä¶ and that‚Äôs how you can get a conference paper.‚Äù This sets up the problem of complexity creep.\n\n\n6. The Netflix Prize Winners\n\n\n\nSlide 6\n\n\n(Timestamp: 00:05:39)\nThis slide shows the winners of the famous Netflix Prize, a competition held about 15 years ago where a team won $1 million for improving Netflix‚Äôs recommendation algorithm by 10%.\nShah uses this story to illustrate the peak of the ‚Äúaccuracy‚Äù mindset. The competition drew massive interest and drove innovation, but it also encouraged teams to prioritize the leaderboard score over the practicality of the solution.\n\n\n7. Netflix Prize Progress Graph\n\n\n\nSlide 7\n\n\n(Timestamp: 00:06:14)\nThe graph displays the progress of teams over time during the Netflix competition. Shah points out that after an initial period of rapid improvement using standard algorithms, progress plateaued.\nTo break through these plateaus, teams began using Ensembling‚Äîcombining multiple models together. The winning solution was an ensemble of 107 different models. Shah emphasizes that while this strategy is powerful for eking out the last bit of performance, it creates immense complexity.\n\n\n8. The Engineering Cost of Complexity\n\n\n\nSlide 8\n\n\n(Timestamp: 00:07:39)\nThis slide reveals the ironic conclusion of the Netflix Prize: the winning model was never implemented. The engineering costs to deploy an ensemble of 107 models were simply too high compared to the marginal gain in accuracy.\nShah uses this as a cautionary tale: ‚ÄúIf your focus is on accuracy‚Ä¶ it drives you down towards this complexity‚Ä¶ but often you end up with these complex models [that] are often very difficult to implement.‚Äù This highlights the disconnect between competitive data science and enterprise reality.\n\n\n9. Understandable White Box Model (CLEAR-2)\n\n\n\nSlide 9\n\n\n(Timestamp: 00:08:04)\nShah transitions to the alternative: Interpretable Models. This slide shows a simple linear model (CLEAR-2) with only two features. This is a classic ‚ÄúWhite Box‚Äù model where the relationship between inputs and outputs is transparent.\nThe speaker contrasts this with the ‚ÄúBlack Box‚Äù nature of complex ensembles. He argues that if you cannot understand what is going on inside a model, you cannot effectively debug it, nor can you easily convince stakeholders to trust it.\n\n\n10. Complex White Box Model (CLEAR-8)\n\n\n\nSlide 10\n\n\n(Timestamp: 00:11:51)\nThis slide presents a linear model with eight features (CLEAR-8). While technically still a ‚ÄúWhite Box‚Äù model, Shah implies that as feature counts grow, true understandability diminishes.\nHe touches on this concept later in the ‚ÄúCaveats‚Äù section, noting that even linear models can become confusing if there is multicollinearity (features moving in the same direction). Just because we can see the coefficients doesn‚Äôt mean the model is intuitively ‚Äúexplainable‚Äù to a human if the variables interact in complex, non-obvious ways.\n\n\n11. Easy to Understand Decision Tree\n\n\n\nSlide 11\n\n\n(Timestamp: 00:19:15)\nHere, a simple Decision Tree is presented. Shah connects this to the history of rule-based learning, noting that early research found that keeping decision trees ‚Äúshort and stumpy‚Äù made them very easy for humans to explain.\nThis visual represents the ideal of interpretability: a clear path of logic (e.g., ‚ÄúIf X is less than 3, go left‚Äù) that leads to a prediction. This is the foundation for the Rulefit method discussed later.\n\n\n12. Too Much to Comprehend\n\n\n\nSlide 12\n\n\n(Timestamp: 00:07:46)\nContrasting the previous slide, this image shows a chaotic forest of decision trees. This represents modern ensemble methods like Random Forests or Gradient Boosted Machines.\nShah uses this visual to reinforce the point that while ensembles offer ‚ÄúBetter Performance,‚Äù the sheer number of decision paths makes them ‚Äútoo much to Comprehend.‚Äù You lose the ability to trace the ‚Äúwhy‚Äù behind a specific prediction, turning the system into a Black Box.\n\n\n13. Pedro Domingos Tweet\n\n\n\nSlide 13\n\n\n(Timestamp: 00:08:22)\nShah acknowledges the counter-argument by showing a tweet from Pedro Domingos, a prominent machine learning researcher, who suggests that demanding explainability limits the potential of AI.\nShah respectfully disagrees with this stance in the context of enterprise data science. He argues that in the real world, ‚ÄúIf you don‚Äôt understand what‚Äôs going on in your model, it‚Äôs hard for you to debug it, it‚Äôs hard to convince somebody else to adopt your model.‚Äù Practicality and trust often outweigh raw theoretical power.\n\n\n14. Benefits of Interpretable Models\n\n\n\nSlide 14\n\n\n(Timestamp: 00:09:16)\nThis slide summarizes the key benefits of using interpretable models, referencing the work of Cynthia Rudin. The main advantages are: 1. Debugging: It is easier to spot weird behaviors. 2. Trust: Stakeholders and legal/risk teams are more likely to approve the model. 3. Deployment: These models can often be deployed as simple SQL queries or basic code, avoiding the need for heavy GPU infrastructure.\nShah emphasizes the deployment aspect: ‚ÄúYou don‚Äôt have to go out and get a GPU‚Ä¶ you can actually deploy directly within a database.‚Äù\n\n\n15. Caveats of Interpretable Models\n\n\n\nSlide 15\n\n\n(Timestamp: 00:11:00)\nShah provides a necessary reality check. He clarifies that selecting an interpretable algorithm is only one part of the process. True interpretability depends on the entire data pipeline.\nIssues like data labeling, feature engineering, and multicollinearity can render even a simple model confusing. For example, if two correlated features have opposite coefficients in a linear model, it becomes very difficult to explain the logic to a business user, even if the math is simple.\n\n\n16. Section 2: Baseline\n\n\n\nSlide 16\n\n\n(Timestamp: 00:12:15)\nThis slide introduces the Baseline section. Shah advocates for always starting a project with a simple baseline model to establish a performance benchmark.\nHe shares an anecdote about people spending a year on a project only to be nearly matched by a simple model built in two hours. Establishing a baseline helps determine how much effort should be spent chasing incremental accuracy improvements.\n\n\n17. The Problem: UCI Adult Dataset\n\n\n\nSlide 17\n\n\n(Timestamp: 00:12:54)\nShah introduces the dataset he will use for all examples in the talk: the UCI Adult Dataset (Census Income). The goal is a binary classification problem: predicting whether someone has a high or low income based on demographics.\nHe chooses this dataset because it represents typical enterprise tabular data: it has 30,000 rows, a mix of numerical and categorical features, and contains collinearity and interaction effects. This makes it a realistic test bed for the models he will demonstrate.\n\n\n18. Baseline Models\n\n\n\nSlide 18\n\n\n(Timestamp: 00:13:53)\nThe speaker outlines the three baseline models he built to bracket the performance possibilities: 1. Logistic Regression: The standard statistical approach. 2. AutoML (H2O): A stacked ensemble of many models (Neural Networks, GBMs, etc.) representing the ‚Äúmaximum‚Äù possible performance. 3. OneR: A very simple rule-based algorithm.\nThese baselines provide the context for evaluating the interpretable models later.\n\n\n19. Baseline Models Plot\n\n\n\nSlide 19\n\n\n(Timestamp: 00:14:12)\nThis plot visualizes Complexity vs.¬†AUC (Area Under the Curve). * OneR is at the bottom (AUC ~0.60) with very low complexity. * Logistic Regression is in the middle (AUC ~0.91). * Stacked Ensemble is at the top (AUC ~0.93) but with massive complexity.\nShah notes that while the Stacked Ensemble wins on accuracy, the Logistic Regression is surprisingly close, highlighting that simpler models can often be ‚Äúgood enough.‚Äù\n\n\n20. OneR Example\n\n\n\nSlide 20\n\n\n(Timestamp: 00:15:17)\nShah explains the OneR (One Rule) algorithm. This method finds the single feature in the dataset that best predicts the target. In the example shown (Iris dataset), utilizing just ‚ÄúPetal Width‚Äù classifies 96% of instances correctly.\nHe suggests OneR is a great way to detect Target Leakage‚Äîif one feature predicts the target perfectly, it might be ‚Äúcheating.‚Äù It also sets the floor for performance; if a complex model can‚Äôt beat OneR, something is wrong.\n\n\n21. Baseline Models Plot (Recap)\n\n\n\nSlide 21\n\n\n(Timestamp: 00:16:36)\nReturning to the complexity plot, Shah reiterates the performance gap. The AutoML model sets the ‚Äúceiling‚Äù at 0.93 AUC.\nThe goal for the rest of the presentation is to see where the interpretable models (Rulefit, GA2M, etc.) fall on this graph. Can they approach the 0.93 AUC of the ensemble without incurring the massive complexity penalty?\n\n\n22. Section 3: Rulefit\n\n\n\nSlide 22\n\n\n(Timestamp: 00:18:18)\nThis slide introduces the first major interpretable technique: Rulefit. Shah mentions familiarity with this from his time at Data Robot and notes that it is a powerful way to combine the benefits of trees and linear models.\n\n\n23. What is Rulefit?\n\n\n\nSlide 23\n\n\n(Timestamp: 00:18:30)\nRulefit is an algorithm developed by Friedman and Popescu (2008). It works by: 1. Building a random forest of short, ‚Äústumpy‚Äù decision trees. 2. Extracting each path through the trees as a ‚ÄúRule.‚Äù 3. Using these rules as binary features in a sparse linear model (Lasso).\nThis approach allows the model to capture interactions (via the trees) while maintaining the interpretability of a linear equation.\n\n\n24. H2O Rulefit Output\n\n\n\nSlide 24\n\n\n(Timestamp: 00:22:19)\nShah displays the output from the H2O Rulefit implementation. The model generates human-readable rules, such as: ‚ÄúIf Education &lt; 12 AND Capital Gain &lt; $7000, THEN Coefficient is negative.‚Äù\nHe notes that while the rules are readable, the raw output can look like ‚Äúcomputer-ese.‚Äù However, it allows a data scientist to identify specific segments of the population (e.g., low education, low capital gain) that strongly drive the prediction.\n\n\n25. Overlapping Rules\n\n\n\nSlide 25\n\n\n(Timestamp: 00:24:30)\nA key characteristic of Rulefit is that the rules overlap. A single data point might satisfy multiple rules simultaneously.\nShah points out that this adds a layer of complexity to interpretability. To understand a prediction, you have to sum up the coefficients of all the rules that apply to that person. This is different from a decision tree where you fall into exactly one leaf node.\n\n\n26. H2O Rulefit with Linear Terms\n\n\n\nSlide 26\n\n\n(Timestamp: 00:25:55)\nOne limitation of pure rules is handling continuous variables (like age or miles driven). Rules have to ‚Äúbin‚Äù these variables (e.g., Age &lt; 30, Age 30-40).\nShah explains that H2O Rulefit solves this by including Linear Terms. The model can use rules for non-linear interactions and standard linear coefficients for continuous trends. This hybrid approach boosts the AUC significantly (up to 0.88 in this example) by capturing linear relationships more naturally.\n\n\n27. Rulefit Results\n\n\n\nSlide 27\n\n\n(Timestamp: 00:27:04)\nThis slide plots the performance of Rulefit models with varying numbers of rules. Shah demonstrates that by increasing the number of rules (complexity), the AUC climbs closer to the Stacked Ensemble.\nHe concludes that Rulefit is a versatile tool. You can tune the ‚Äúdial‚Äù of complexity: fewer rules for more interpretability, or more rules for higher accuracy, often getting very competitive performance.\n\n\n28. Section 4: GA2M\n\n\n\nSlide 28\n\n\n(Timestamp: 00:31:35)\nThe presentation moves to the second technique: GA2M (Generalized Additive Models with pairwise interactions). Shah notes that while GAMs have existed for a while, modern implementations like Microsoft‚Äôs Explainable Boosting Machines (EBM) have made them much more accessible and powerful.\n\n\n29. What is GA2M?\n\n\n\nSlide 29\n\n\n(Timestamp: 00:32:02)\nGA2M is essentially a linear model where features are binned, and pairwise interactions are automatically detected. Shah highlights InterpretML, an open-source library from Microsoft that implements this via EBMs.\nThe model structure is additive: \\(g(E[y]) = \\beta_0 + \\sum f_j(x_j) + \\sum f_{ij}(x_i, x_j)\\). This means the final score is just the sum of individual feature scores and interaction scores, making it very transparent.\n\n\n30. GA2M Binning\n\n\n\nSlide 30\n\n\n(Timestamp: 00:32:42)\nShah explains how GA2M handles numerical data. Instead of a single slope coefficient (like in logistic regression), the model bins the continuous feature (e.g., dividing ‚Äúcriminal history‚Äù into ranges).\nEach bin gets its own coefficient. This allows the model to learn non-linear patterns (e.g., risk might go up, then down, then up again as a variable increases) while remaining easy to inspect.\n\n\n31. Interactions in GA2M\n\n\n\nSlide 31\n\n\n(Timestamp: 00:33:08)\nThe ‚Äú2‚Äù in GA2M stands for pairwise interactions. Shah emphasizes that this is the model‚Äôs superpower. While standard linear models struggle with interactions (e.g., the combined effect of age and education), GA2M has an efficient algorithm to automatically find the most important pairs.\nThis allows the model to achieve accuracy levels comparable to complex ensembles (AUC 0.93) because it captures the interaction signal that simple linear models miss.\n\n\n32. GA2M Visualization\n\n\n\nSlide 32\n\n\n(Timestamp: 00:35:14)\nShah showcases the InterpretML dashboard. It provides clear visualizations of how each feature contributes to the prediction.\nIn the example, we see the coefficients for different marital statuses. This acts like a ‚Äúlookup table‚Äù for risk. Shah argues that this is very ‚Äúmodel risk management friendly‚Äù because stakeholders can validate every single coefficient and interaction term to ensure they make business sense.\n\n\n33. Section 5: Rule Lists\n\n\n\nSlide 33\n\n\n(Timestamp: 00:40:28)\nThe third approach is Rule Lists. Shah introduces this as a method to solve the ‚Äúoverlapping rules‚Äù problem found in Rulefit.\n\n\n34. What are Rule Lists?\n\n\n\nSlide 34\n\n\n(Timestamp: 00:40:48)\nRule Lists are ordered sets of IF-THEN-ELSE statements. Unlike Rulefit, where you sum up multiple rules, here an observation triggers only the first rule it matches.\nShah mentions implementations like CORELS and SBRL (Scalable Bayesian Rule Lists). The goal is to produce a concise list that a human can read from top to bottom to make a decision.\n\n\n35. SBRL Process\n\n\n\nSlide 35\n\n\n(Timestamp: 00:41:09)\nCreating an optimal rule list is computationally expensive because the algorithm must search through many permutations to find the best order.\nShah explains the logic: The algorithm finds a rule that covers a subset of data, removes those instances, and then finds the next rule for the remaining data. This sequential ‚Äúpeeling off‚Äù of data creates the IF-ELSE structure.\n\n\n36. SBRL Output Example\n\n\n\nSlide 36\n\n\n(Timestamp: 00:41:45)\nThe output of an SBRL model is shown. It reads like a checklist: 1. IF Capital Gain &gt; $7500 -&gt; High Income (99% prob) 2. ELSE IF Education &lt; 4 -&gt; Low Income (90% prob) 3. ELSE‚Ä¶\nShah highlights the simplicity: ‚ÄúYou just go down the list until you find the rule‚Ä¶ much easier to explain to those marketing people.‚Äù The trade-off is a drop in accuracy (AUC 0.86) compared to GA2M or Rulefit.\n\n\n37. Section 6: Scorecard\n\n\n\nSlide 37\n\n\n(Timestamp: 00:44:52)\nThe final approach is the Scorecard. Shah introduces this as perhaps the simplest and most widely recognized format for decision-making in industries like credit and criminal justice.\n\n\n38. What are Scorecards?\n\n\n\nSlide 38\n\n\n(Timestamp: 00:45:04)\nScorecards are simple additive models where features are assigned integer ‚Äúpoints.‚Äù To get a prediction, you simply add up the points.\nShah mentions tools like Optbinning and SLIM (Sparse Linear Integer Models). This format is beloved in operations because it can be printed on a physical card or implemented in a basic spreadsheet.\n\n\n39. Scorecard Example\n\n\n\nSlide 39\n\n\n(Timestamp: 00:46:08)\nThis slide shows a scorecard built for the Adult dataset. * Capital Gain &gt; 7000? +29 points. * Age &lt; 25? -5 points.\nShah expresses a personal preference for this over raw coefficients: ‚ÄúI actually like this better‚Ä¶ I think it‚Äôs a little easier to understand which features are most important.‚Äù The integer points make the ‚Äúweight‚Äù of each factor immediately obvious to a layperson.\n\n\n40. Summary\n\n\n\nSlide 40\n\n\n(Timestamp: 00:51:11)\nShah begins to wrap up the presentation, preparing to consolidate the four methods (Rulefit, GA2M, Rule Lists, Scorecards) into a final comparison.\n\n\n41. Complexity vs AUC Summary Plot\n\n\n\nSlide 41\n\n\n(Timestamp: 00:51:13)\nThis is the definitive comparison graph of the talk. It places all discussed models on the Complexity vs.¬†AUC plane. * GA2M (EBM) and Rulefit sit high up, offering near-SOTA accuracy with moderate interpretability. * Scorecards and Rule Lists sit lower on accuracy but offer maximum simplicity.\nShah summarizes the trade-off: ‚ÄúThe Rule Lists and Scorecard‚Ä¶ you lose a little bit [of accuracy]‚Ä¶ but we talked about the trade-offs of being able to easily understand.‚Äù\n\n\n42. Take Away\n\n\n\nSlide 42\n\n\n(Timestamp: 00:52:08)\nThe final message is a call to action: Try these approaches.\nShah encourages data scientists to add these tools to their toolkit. He asks them to consider the specific needs of their problem: Is it about transparency in calculation (Scorecard)? Or understanding factors (GA2M)? Often, a simple model that gets deployed is far better than a complex model that gets stuck in review.\n\n\n43. Conclusion\n\n\n\nSlide 43\n\n\n(Timestamp: 00:52:52)\nThe presentation concludes with Rajiv Shah‚Äôs contact information. He mentions an upcoming blog post that will synthesize these topics and invites the audience to reach out with questions or feedback.\nHe reiterates that these interpretable models are often easier to get ‚Äúbuy-in‚Äù for, making them a pragmatic choice for real-world data science success.\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "evaluating-llms-deep-dive.html",
    "href": "evaluating-llms-deep-dive.html",
    "title": "Evaluation for Large Language Models (LLMs) and Generative AI - A Deep Dive",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "evaluating-llms-deep-dive.html#video",
    "href": "evaluating-llms-deep-dive.html#video",
    "title": "Evaluation for Large Language Models (LLMs) and Generative AI - A Deep Dive",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "evaluating-llms-deep-dive.html#annotated-presentation",
    "href": "evaluating-llms-deep-dive.html#annotated-presentation",
    "title": "Evaluation for Large Language Models (LLMs) and Generative AI - A Deep Dive",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the annotated presentation for ‚ÄúEvaluating LLMs‚Äù by Rajiv Shah.\n\n1. Title Slide: Evaluating LLMs\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00)\nThe presentation begins with the title slide, introducing the speaker, Rajiv Shah, and the topic of Evaluating Large Language Models (LLMs). The slide includes a link to a GitHub repository (LLM-Evaluation), which serves as a companion resource containing notebooks and code examples referenced throughout the talk.\nRajiv sets the stage by explaining his motivation: he sees many enterprises treating Generative AI as ‚Äúscience experiments‚Äù that fail to reach production. He argues that a major reason for this failure is a lack of proper evaluation strategies.\nThe goal of this talk is to move beyond experimentation and discuss how to rigorously evaluate models to get them into production and keep them there, covering technical, business, and operational perspectives.\n\n\n2. No Impact!\n\n\n\nSlide 2\n\n\n(Timestamp: 00:05)\nThis slide humorously illustrates the current state of many LLM projects. It depicts a chaotic lab scene and a cartoon character in a strange vehicle, captioned ‚ÄúNo impact!‚Äù This visualizes the frustration of data scientists building cool things that never deliver real-world value.\nRajiv uses this to highlight the ‚Äúscience experiment‚Äù nature of current GenAI work. Without proper evaluation, teams cannot prove the reliability or value of their models, preventing deployment.\nThe slide emphasizes the necessity of shifting from ‚Äúplaying around‚Äù with models to applying rigorous engineering discipline, starting with evaluation.\n\n\n3. Three Pillars of Evaluation\n\n\n\nSlide 3\n\n\n(Timestamp: 00:41)\nThis slide breaks down Generative AI evaluation into three critical dimensions: Technical (F1), Business ($$), and Operational (TCO). While the talk focuses heavily on technical metrics, Rajiv stresses that the other two are equally vital for production success.\nThe Business dimension asks about the return on investment and the cost of errors, while the Operational dimension considers the Total Cost of Ownership (TCO), latency, and maintenance.\nUnderstanding all three pillars is what distinguishes a successful production deployment from a mere prototype.\n\n\n4. Generative AI Evaluation Methods\n\n\n\nSlide 4\n\n\n(Timestamp: 01:03)\nThis chart is the central framework of the presentation. It categorizes technical evaluation methods based on Cost (y-axis) and Flexibility (x-axis). The methods range from rigid, low-cost approaches like Exact Matching to flexible, high-cost approaches like Red Teaming.\nThe slide lists specific methodologies: Exact matching, Similarity (BLEU/ROUGE), Functional correctness (Unit tests), Benchmarks (MMLU), Human evaluation, Model-based approaches (LLM-as-a-Judge), and Red teaming.\nRajiv notes that these categories overlap and are not mutually exclusive. This visual guide helps practitioners choose the right tool for their specific stage of development and resource constraints.\n\n\n5. Application to RAG\n\n\n\nSlide 5\n\n\n(Timestamp: 01:16)\nThis slide previews the case study at the end of the talk: Retrieval Augmented Generation (RAG). It shows a diagram splitting the RAG process into two distinct components: Retrieval (finding the data) and Augmented Generation (synthesizing the answer).\nRajiv introduces this here to promise a practical application of the concepts. He explains that after covering the evaluation methods, he will demonstrate how to apply them specifically to a RAG system.\nThis foreshadows the importance of component-wise evaluation‚Äîevaluating the retriever and the generator separately rather than just the system as a whole.\n\n\n6. Evaluating LLMs (Title Repeat)\n\n\n\nSlide 6\n\n\n(Timestamp: 01:31)\nThis slide serves as a transition point, reiterating the talk‚Äôs title and contact information. It signals the end of the introduction and the beginning of the deep dive into the current state of LLMs.\nRajiv notes that this will be a long, detailed talk, encouraging viewers to use the video timeline to skip around. He sets expectations for the pace and depth of the technical content to follow.\n\n\n7. Many Ways to Use LLMs\n\n\n\nSlide 7\n\n\n(Timestamp: 01:45)\nThis slide illustrates the versatility of LLMs, showing examples of Question Answering and Code Generation. It highlights that LLMs are not limited to a single task like classification; they can summarize, chat, write code, and reason.\nRajiv explains that this versatility makes evaluation difficult. Unlike traditional ML where a simple confusion matrix might suffice, LLMs produce varied, open-ended outputs that require more complex assessment strategies.\nThe slide sets up the problem statement: because LLMs can do so much, we need a diverse set of evaluation tools to measure their performance across different modalities.\n\n\n8. Open Source LLM Leaderboard\n\n\n\nSlide 8\n\n\n(Timestamp: 02:18)\nThis slide shows a screenshot of the Hugging Face Open LLM Leaderboard. It notes that over 2,000 LLMs have been evaluated, visualizing the sheer volume of models available to practitioners.\nRajiv describes the experience of looking for a model as ‚Äúoverwhelming.‚Äù With new models releasing weekly, relying solely on public leaderboards to pick a model is daunting and potentially misleading.\nThis introduces the concept of ‚ÄúLeaderboard Fatigue‚Äù and questions whether these general-purpose rankings are useful for specific enterprise use cases.\n\n\n9. HELM Framework\n\n\n\nSlide 9\n\n\n(Timestamp: 02:52)\nThis slide introduces HELM (Holistic Evaluation of Language Models) from Stanford. It displays the framework‚Äôs structure, which evaluates models across various scenarios (datasets) and metrics (accuracy, bias, toxicity).\nRajiv presents HELM as the academic approach to the evaluation problem. It attempts to be comprehensive by measuring everything across many dimensions, offering a more rigorous alternative to simple leaderboards.\nHowever, he points out that even this comprehensive approach has its downsides, primarily the sheer volume of data it produces.\n\n\n10. Overwhelming Information\n\n\n\nSlide 10\n\n\n(Timestamp: 03:25)\nThis slide displays a screenshot of the HELM research paper, emphasizing its length (163 pages). The caption ‚ÄúOverwhelming!‚Äù reflects the difficulty a data scientist faces when trying to digest this amount of information.\nRajiv humorously compares the paper‚Äôs size to a ‚ÄúHarry Potter book,‚Äù illustrating that while the academic rigor is high, the practical barrier to entry is also significant.\nThe key takeaway is that while comprehensive benchmarks exist, they are often too dense for quick, practical decision-making in an enterprise setting.\n\n\n11. Feeling Overwhelmed\n\n\n\nSlide 11\n\n\n(Timestamp: 03:46)\nThis visual slide features a person looking frustrated and burying their face in their hands. It represents the emotional state of a data scientist trying to navigate the complex, rapidly changing landscape of LLM evaluation.\nRajiv uses this to empathize with the audience. Between the thousands of models on Hugging Face and the hundreds of pages of academic papers, it is easy to feel lost.\nThis sets the stage for the need for simpler, more fundamental principles to guide evaluation.\n\n\n12. Reliability of HELM\n\n\n\nSlide 12\n\n\n(Timestamp: 03:57)\nThis slide questions the reliability of benchmarks like HELM. It presents data showing that minor changes in dataset selection can lead to different scoring and winners 22% of the time. A correlation matrix visualizes the relationships between different metrics.\nRajiv points out that benchmarks are fragile. If you change the specific datasets used to represent a ‚Äúscenario,‚Äù the ranking of the models changes.\nThis implies that ‚Äúwinners‚Äù on leaderboards are often dependent on the specific composition of the benchmark rather than inherent superiority across all tasks.\n\n\n13. Davinci-002 vs Davinci-003\n\n\n\nSlide 13\n\n\n(Timestamp: 04:15)\nThis slide highlights a specific anomaly in HELM results where an older model (text-davinci-002) appears to outperform a newer, better model (text-davinci-003) in accuracy.\nRajiv expresses skepticism, noting that OpenAI is unlikely to release a newer model that is objectively worse. This discrepancy suggests that the benchmark might not be capturing the improvements in the newer model, such as better instruction following or safety.\nThe slide serves as a warning: Do not blindly trust benchmark rankings, as they may not reflect the actual capabilities or ‚Äúquality‚Äù of a model for your specific needs.\n\n\n14. Leaderboard Reliability\n\n\n\nSlide 14\n\n\n(Timestamp: 04:53)\nThis slide examines the Open LLM Leaderboard again, pointing out that rankings are heavily influenced by specific datasets like TruthfulQA. It asks, ‚ÄúIs this impactful?‚Äù\nRajiv argues that if a model‚Äôs high ranking is driven primarily by its performance on a dataset like TruthfulQA, it might not be relevant to a user whose use case (e.g., summarizing financial documents) has nothing to do with that specific benchmark.\nThis reinforces the idea that general-purpose leaderboards may not align with specific business goals.\n\n\n15. Model Evals vs System Evals\n\n\n\nSlide 15\n\n\n(Timestamp: 05:33)\nThis slide distinguishes between Model Evals (selecting the best model from n options) and System Evals (optimizing a single model for a specific task).\nRajiv explains that most public benchmarks focus on the former‚Äîcomparing thousands of models. However, in enterprise settings, the goal is usually the latter: you pick a model (like GPT-4 or Llama 2) and need to evaluate how to optimize it for your specific application.\nThe talk focuses on bridging this gap, helping practitioners evaluate their specific implementation rather than just comparing base models.\n\n\n16. Lost in the Maze\n\n\n\nSlide 16\n\n\n(Timestamp: 06:33)\nThis slide features an image of a hedge maze with the word ‚ÄúLost,‚Äù symbolizing the confusion in the current evaluation landscape.\nRajiv uses this to pivot back to fundamentals. When lost in complex new technology, the best approach is to return to first principles of data science evaluation.\nHe prepares the audience to look at a classic machine learning problem to ground the upcoming LLM concepts.\n\n\n17. Evaluating Customer Churn\n\n\n\nSlide 17\n\n\n(Timestamp: 06:49)\nThis slide introduces a classic ‚ÄúData Science 101‚Äù problem: Customer Churn. It depicts an exit door and a pie chart, setting up a scenario where a data scientist must evaluate a model designed to predict which customers will leave.\nRajiv uses this familiar example to contrast different levels of evaluation maturity, which he will then map onto GenAI.\n\n\n18. Junior Data Scientist Approach\n\n\n\nSlide 18\n\n\n(Timestamp: 07:07)\nThis slide shows standard classification metrics: ROC curve, Confusion Matrix, F1 Score, and True Positive Rate. Rajiv labels this as the ‚ÄúJunior Data Scientist‚Äù approach.\nWhile these metrics are technically correct, they are abstract. A junior data scientist presents these to a boss and says, ‚ÄúLook, I improved the AUC,‚Äù which often fails to communicate business value.\nThis represents the Technical pillar of evaluation‚Äînecessary, but insufficient for business stakeholders.\n\n\n19. Senior Data Scientist Approach\n\n\n\nSlide 19\n\n\n(Timestamp: 07:40)\nThis slide introduces Profit Curves. It translates the confusion matrix into dollar values (cost of false positives vs.¬†value of true positives). Rajiv calls this the ‚ÄúSenior Data Scientist‚Äù approach.\nHere, the evaluation focuses on Business Value: ‚ÄúHow much profit will this model generate compared to the baseline?‚Äù This aligns the technical model with business goals ($$).\nThe lesson is that LLM evaluation must eventually map to business outcomes, not just technical benchmarks.\n\n\n20. Data Science Leader Approach\n\n\n\nSlide 20\n\n\n(Timestamp: 08:27)\nThis slide discusses the Total Cost of Ownership (TCO) and Monitoring. It reflects the ‚ÄúData Science Leader‚Äù perspective, which looks at the system holistically.\nA leader asks: ‚ÄúIs it worth spending 5 more weeks to get 3% more accuracy?‚Äù and ‚ÄúHow will we monitor this when customer behavior changes?‚Äù\nThis corresponds to the Operational pillar. It emphasizes that evaluation includes considering the cost of building, maintaining, and running the model over time.\n\n\n21. Evaluate Generative AI Tasks?\n\n\n\nSlide 21\n\n\n(Timestamp: 09:23)\nThis slide transitions back to Generative AI, showing examples of code generation and summarization. It asks how to apply the principles just discussed (Technical, Business, Operational) to these new, complex tasks.\nRajiv acknowledges that while the outputs (text, code) are different from simple classification labels, the fundamental need to evaluate across three dimensions remains.\n\n\n22. Three Pillars (GenAI Context)\n\n\n\nSlide 22\n\n\n(Timestamp: 09:36)\nThis slide repeats the Technical, Business, Operational framework, asserting ‚ÄúStill the same principles!‚Äù\nRajiv reinforces that despite the hype and novelty of LLMs, we must not abandon standard engineering practices. We still need to measure technical accuracy (F1 equivalent), business impact ($$), and operational costs (TCO).\n\n\n23. Evaluation in the ML Lifecycle\n\n\n\nSlide 23\n\n\n(Timestamp: 09:45)\nThis slide displays a ‚Äúmulti-headed llama‚Äù graphic representing the ML lifecycle: Development, Training, and Deployment.\nRajiv explains that evaluation is not a one-time step. It happens: 1. Before: To decide if a project is viable. 2. During: To train and tune the model. 3. After: To monitor the model in production (Monitoring is the ‚Äúsibling‚Äù of Evaluation).\n\n\n24. Faster, Better, Cheaper\n\n\n\nSlide 24\n\n\n(Timestamp: 10:33)\nThis slide features a tweet by Eugene Yan, stating that automated evaluations lead to ‚Äúfaster, better, cheaper‚Äù LLMs. It mentions that good eval pipelines allow for safer deployments and faster experiments.\nRajiv cites the example of Hugging Face‚Äôs Zephyr model. The team built it in just a few days because they had spent months building a robust evaluation pipeline.\nThe key insight is that investing in evaluation infrastructure upfront accelerates actual model development and iteration.\n\n\n25. Traditional NLP Tasks\n\n\n\nSlide 25\n\n\n(Timestamp: 11:51)\nThis slide advises that if you are using GenAI for a traditional NLP task (like sentiment analysis), you should ‚Äústart with traditional metrics/datasets.‚Äù\nHowever, Rajiv warns about Data Leakage. Because LLMs are trained on the internet, they may have already seen the test sets of standard benchmarks.\nThe takeaway: Use standard metrics if applicable, but be skeptical of results that seem too good, as the model might be memorizing the test data.\n\n\n26. Breaking Existing Evaluations\n\n\n\nSlide 26\n\n\n(Timestamp: 12:38)\nThis slide explains that LLMs can ‚Äúbreak existing evaluations.‚Äù It cites research where LLMs scored poorly on automated metrics but were rated highly by humans.\nRajiv explains that LLMs have such a fluid and rich understanding of language that they often produce correct answers that old, rigid metrics fail to recognize.\nThis highlights the limitation of using pre-LLM automated metrics for modern models; the models have outpaced the measurement tools.\n\n\n27. Beating Human Baselines\n\n\n\nSlide 27\n\n\n(Timestamp: 13:29)\nThis slide presents data showing LLMs (GPT-3/4) beating human baselines in tasks like summarization. The charts show LLMs scoring higher in faithfulness, coherence, and relevance.\nRajiv mentions recent research where GPT-4 wrote medical reports that doctors preferred over those written by other humans.\nThis poses a challenge: How do you evaluate a model when it is better than the human annotators you would typically use as a gold standard?\n\n\n28. Methods Chart (Recap)\n\n\n\nSlide 28\n\n\n(Timestamp: 14:03)\nThis slide brings back the Generative AI Evaluation Methods chart (Cost vs.¬†Flexibility). An arrow points to ‚ÄúRaj guess,‚Äù indicating that the placement of these methods is an estimation.\nRajiv uses this to reorient the audience before diving into the specific methods one by one, starting from the bottom left (least flexible/cheapest).\n\n\n29. Progression of Evaluation\n\n\n\nSlide 29\n\n\n(Timestamp: 14:27)\nThis slide shows a directional arrow moving ‚Äúup‚Äù the chart, from Exact Matching toward Red Teaming.\nRajiv explains the flow of the presentation: we will start with rigid, simple metrics and move toward more complex, flexible, and human-centric evaluation methods.\n\n\n30. Exact Matching Approach\n\n\n\nSlide 30\n\n\n(Timestamp: 15:22)\nThis slide highlights the ‚ÄúExact matching approach‚Äù box on the chart.\nThis is the starting point: the simplest form of evaluation where the model‚Äôs output must be identical to a reference answer.\n\n\n31. How Hard Could It Be?\n\n\n\nSlide 31\n\n\n(Timestamp: 16:02)\nThis slide asks, ‚ÄúHow hard could evaluation be?‚Äù It shows simple outputs (Yes/No, A/B/C/D) and suggests that checking if string A equals string B should be trivial.\nRajiv uses this to set up a contrast. While it looks simple like a basic Python script, the reality of LLMs makes even this basic task complicated due to formatting and non-determinism.\n\n\n32. Consistent Prediction Workflow\n\n\n\nSlide 32\n\n\n(Timestamp: 16:20)\nThis slide outlines a workflow: Inputs (Tokenization, Prompts) -&gt; Model (Hyperparameters) -&gt; Outputs (Evaluation).\nRajiv emphasizes that to get exact matching to work, you need extreme consistency across this entire pipeline. He warns that you must plan for multiple iterations because things will go wrong at every step.\n\n\n33. Story Time: MMLU Leaderboards\n\n\n\nSlide 33\n\n\n(Timestamp: 16:40)\nThis slide shows a tweet announcing a new LLM topping the leaderboard, but points out a discrepancy: ‚ÄúWhy did we have two different MMLU scores?‚Äù\nRajiv tells the story of how a model claimed a high score on Twitter, but the actual paper showed a lower score. This discrepancy triggered an investigation into why the same model on the same benchmark produced different results.\n\n\n34. What is MMLU?\n\n\n\nSlide 34\n\n\n(Timestamp: 17:37)\nThis slide defines MMLU (Massive Multitask Language Understanding). It is a benchmark covering 57 tasks (Math, History, CS), designed to measure the ‚Äúknowledge‚Äù of a model.\nRajiv shows examples of questions (Microeconomics, Physics) to illustrate that these are multiple-choice questions used to gauge general intelligence.\n\n\n35. Why MMLU Evaluation Differed\n\n\n\nSlide 35\n\n\n(Timestamp: 18:16)\nThis slide reveals the culprit behind the score discrepancy: Prompt Formatting. It shows three different prompt styles (HELM, Eleuther, Original) used by different evaluation harnesses.\nRajiv challenges the audience to spot the differences. They are subtle: an extra space, a different bracket style around the letter (A) vs A., or the inclusion of a subject line.\n\n\n36. Style Changes Accuracy\n\n\n\nSlide 36\n\n\n(Timestamp: 19:13)\nThis slide states that these simple style changes resulted in a ~5% change in accuracy.\nRajiv underscores the significance: a 5% swing is massive on a leaderboard. This proves that LLMs are incredibly sensitive to prompt syntax. It also serves as a warning to be skeptical of reported benchmark scores, as they can be ‚Äúmassaged‚Äù simply by tweaking the prompt format.\n\n\n37. Story: Falcon Model Bias\n\n\n\nSlide 37\n\n\n(Timestamp: 20:35)\nThis slide introduces the Falcon model story. Users noticed that when asked for a ‚Äútechnologically advanced city,‚Äù Falcon would almost always suggest Abu Dhabi.\nRajiv sets up the mystery: Was the model biased because it was trained in the Middle East? Why was it so fixated on this specific city?\n\n\n38. Biased Model (Human Rights)\n\n\n\nSlide 38\n\n\n(Timestamp: 21:59)\nThis slide shows that the Falcon model also refused to discuss human rights abuses in Abu Dhabi.\nThis fueled speculation that the model had been censored or biased during training to avoid sensitive topics regarding its region of origin.\n\n\n39. Demo Placeholder\n\n\n\nSlide 39\n\n\n(Timestamp: 21:16)\nThis slide simply says ‚ÄúLet‚Äôs try to demo this.‚Äù In the video, Rajiv switches to a live recording of him interacting with the model to demonstrate the bias firsthand.\n\n\n40. Check the System Prompt\n\n\n\nSlide 40\n\n\n(Timestamp: 22:26)\nThis slide reveals the answer to the Falcon mystery: The System Prompt.\nIt turns out the model had a hidden system instruction explicitly stating it was built in Abu Dhabi. When researchers changed this prompt (e.g., to ‚ÄúMexico‚Äù), the model‚Äôs behavior changed, and it stopped forcing Abu Dhabi into answers.\nThe lesson: System prompts heavily influence evaluation results. Small changes in hidden instructions can radically alter model behavior.\n\n\n41. Prompt Engineering\n\n\n\nSlide 41\n\n\n(Timestamp: 23:26)\nThis slide discusses Prompt Engineering techniques like Chain-of-Thought (COT). It shows how asking a model to ‚Äúthink step by step‚Äù improves reasoning on math problems.\nRajiv emphasizes that identifying the best prompt is a crucial part of the evaluation workflow. You aren‚Äôt just evaluating the model; you are evaluating the model plus the prompt.\n\n\n42. Hands on: Prompting\n\n\n\nSlide 42\n\n\n(Timestamp: 24:05)\nThis slide introduces a hands-on exercise. It encourages users to use OpenAI‚Äôs playground to experiment with different prompts, specifically COT and system prompt variations.\n\n\n43. Hands on: GLaDOS\n\n\n\nSlide 43\n\n\n(Timestamp: 24:14)\nThis slide shows a fun example where the system prompt turns ChatGPT into GLaDOS (from the game Portal).\nRajiv uses this to demonstrate the power of the system prompt to change the persona and tone of the model completely.\n\n\n44. Workflow: Inputs Recap\n\n\n\nSlide 44\n\n\n(Timestamp: 24:26)\nThis slide updates the Consistent Prediction Workflow. Under ‚ÄúInputs,‚Äù it now explicitly lists System Prompt, Tokenization, Prompt Styles, and Prompt Engineering.\nThis summarizes the section: to get consistent evaluation, you must control all these input variables.\n\n\n45. Variability of LLM Models\n\n\n\nSlide 45\n\n\n(Timestamp: 24:39)\nThis slide shifts focus to the Model component. It notes that model size affects scores (Llama-2 example) and introduces the concept of Non-deterministic inference.\nRajiv points out that GPU calculations introduce slight randomness, meaning you might not get bit-wise reproducibility even with the same settings.\n\n\n46. GPT-4 vs GPT-3.5\n\n\n\nSlide 46\n\n\n(Timestamp: 24:46)\nThis slide compares GPT-4 vs GPT-3.5. It shows that even models from the same ‚Äúfamily‚Äù give very different answers to political opinion questions.\nRajiv uses this to show that you cannot swap models (e.g., using a cheaper model for dev and a larger one for prod) without re-evaluating, as their behaviors diverge significantly.\n\n\n47. Non-deterministic Inference\n\n\n\nSlide 47\n\n\n(Timestamp: 25:28)\nThis slide dives deeper into Non-deterministic inference. It explains that floating-point calculations on GPUs can have tiny variances that ripple out to affect token selection.\nFor data scientists coming from deterministic systems (like logistic regression), this lack of 100% reproducibility can be a shock and complicates ‚Äúexact match‚Äù testing.\n\n\n48. Reliability of Commercial APIs\n\n\n\nSlide 48\n\n\n(Timestamp: 26:03)\nThis slide addresses Model Drift in commercial APIs. It shows graphs of GPT-3.5 and GPT-4 performance changing over time on tasks like identifying prime numbers.\nRajiv warns that if you don‚Äôt own the model (i.e., you use an API), the vendor might update it behind the scenes, breaking your evaluation baselines.\n\n\n49. Hyperparameters\n\n\n\nSlide 49\n\n\n(Timestamp: 26:27)\nThis slide shows the UI for Hyperparameters (Temperature, Max Length, Top P).\nRajiv reminds the audience that these settings drastically influence predictions. Evaluation must be done with the exact same hyperparameters intended for production.\n\n\n50. Output Evaluation\n\n\n\nSlide 50\n\n\n(Timestamp: 26:40)\nThis slide highlights the ‚ÄúOutput evaluation‚Äù step in the workflow.\nNow that we‚Äôve covered inputs and models, Rajiv moves to the challenge of parsing and judging the text the model actually produces.\n\n\n51. Generating Multiple Choice Output\n\n\n\nSlide 51\n\n\n(Timestamp: 26:46)\nThis slide discusses the difficulty of evaluating Multiple Choice answers. * First Letter Approach: Just look for ‚ÄúA‚Äù or ‚ÄúB‚Äù. Fails if the model says ‚ÄúThe answer is A‚Äù. * Entire Answer: Look for the full text. Fails if the model phrases it slightly differently.\nRajiv illustrates that even ‚Äúsimple‚Äù multiple-choice evaluation requires complex parsing logic because LLMs love to ‚Äúchat‚Äù and add extra text.\n\n\n52. Evaluating MMLU: Different Outputs\n\n\n\nSlide 52\n\n\n(Timestamp: 27:28)\nThis slide compares how HELM, AI Harness, and the Original MMLU implementation parsed outputs.\nIt reveals that the discrepancy in MMLU scores wasn‚Äôt just about prompts; it was also about how the evaluation code extracted the answer from the model‚Äôs response.\n\n\n53. Consistency is Hard!\n\n\n\nSlide 53\n\n\n(Timestamp: 27:40)\nThis slide summarizes the MMLU saga: ‚ÄúConsistency is hard!‚Äù It shows the table of scores again.\nThe takeaway is that ‚ÄúExact Match‚Äù is a misnomer. It requires rigorous standardization of inputs, models, and output parsing to be reliable.\n\n\n54. Hands on: Evaluating Outputs\n\n\n\nSlide 54\n\n\n(Timestamp: 27:59)\nThis slide introduces a hands-on exercise evaluating sentiment analysis. It shows a spreadsheet where different models output sentiment in different formats (some verbose, some concise).\nRajiv uses this to show the messy reality of parsing LLM outputs.\n\n\n55. Solutions: Standardizing Outputs\n\n\n\nSlide 55\n\n\n(Timestamp: 28:34)\nThis slide presents solutions for the output problem: 1. OpenAI Function Calling: Forces the model to output structured JSON. 2. Guardrails AI: A library for validating outputs against a schema.\nRajiv suggests that using these tools to tame the model into structured output makes ‚ÄúExact Match‚Äù evaluation much more feasible.\n\n\n56. Workflow: Types of Prompts\n\n\n\nSlide 56\n\n\n(Timestamp: 28:59)\nThis slide adds ‚ÄúTypes of Prompts‚Äù to the Input section of the workflow diagram.\nRajiv reiterates the need to plan for multiple iterations. You will likely need to tweak your prompts and parsing logic many times to get a stable evaluation pipeline.\n\n\n57. Resources: Prompting\n\n\n\nSlide 57\n\n\n(Timestamp: 29:10)\nThis slide lists resources for learning prompting, including the OpenAI Cookbook and the DAIR.AI Prompt Engineering Guide.\n\n\n58. Similarity Approach\n\n\n\nSlide 58\n\n\n(Timestamp: 29:13)\nThis slide moves up the chart to the Similarity approach.\nRajiv introduces this as the next level of flexibility. If exact matching is too rigid, we check if the output is ‚Äúsimilar enough‚Äù to the reference.\n\n\n59. Story: Translation\n\n\n\nSlide 59\n\n\n(Timestamp: 29:29)\nThis slide presents a translation challenge. It shows three human references for a Chinese-to-English translation and two computer candidates.\nRajiv asks the audience to guess which candidate is better. This exercise builds intuition for how similarity metrics work: we look for overlapping words and phrases between the candidate and the references.\n\n\n60. BLEU Metric\n\n\n\nSlide 60\n\n\n(Timestamp: 30:47)\nThis slide introduces BLEU (Bilingual Evaluation Understudy). It explains that BLEU calculates scores based on n-gram overlap (1-gram to 4-gram) between the generated text and reference text.\nThis is the mathematical formalization of the intuition from the previous slide. It‚Äôs a standard metric for translation.\n\n\n61. Many Similarity Methods\n\n\n\nSlide 61\n\n\n(Timestamp: 31:15)\nThis slide lists various similarity metrics: Exact match, Edit distance, ROUGE, WER, METEOR, Cosine similarity.\nRajiv notes the pros and cons: They are fast and easy to calculate, but they don‚Äôt consider meaning (semantics) and are biased toward shorter text. They measure lexical overlap, not understanding.\n\n\n62. Taxonomy of Similarity Methods\n\n\n\nSlide 62\n\n\n(Timestamp: 32:06)\nThis slide shows a complex flow chart categorizing similarity methods into Untrained (lexical, character-based) and Trained (embedding-based).\nIt illustrates the depth of research in this field, showing that there are dozens of ways to calculate ‚Äúsimilarity.‚Äù\n\n\n63. Similarity Methods for Code\n\n\n\nSlide 63\n\n\n(Timestamp: 32:15)\nThis slide asks if similarity works for Code. It shows a Python function incr_list.\nRajiv argues that similarity ‚ÄúDoesn‚Äôt work for code.‚Äù In code, variable names can change, and logic can be refactored, resulting in zero string similarity even if the code functions identically. Conversely, a single missing character (syntax error) can break code that is 99% similar textually.\n\n\n64. Functional Correctness\n\n\n\nSlide 64\n\n\n(Timestamp: 32:37)\nThis slide highlights Functional Correctness on the chart.\nThis is the solution to the code evaluation problem. Instead of checking if the text looks right, we execute it to see if it works.\n\n\n65. Problem: Evaluating Code\n\n\n\nSlide 65\n\n\n(Timestamp: 32:54)\nThis slide reinforces the failure of BLEU for code. It shows that a correct solution might have a low BLEU score because it uses different variable names than the reference.\n\n\n66. Evaluating Code with Unit Tests\n\n\n\nSlide 66\n\n\n(Timestamp: 33:17)\nThis slide introduces the Unit Test approach. We take the generated code, run it against a set of test cases (inputs and expected outputs), and check for a pass/fail result.\nRajiv advocates for this approach because it is unambiguous. The code either runs and produces the right result, or it doesn‚Äôt.\n\n\n67. HumanEval Benchmark\n\n\n\nSlide 67\n\n\n(Timestamp: 34:16)\nThis slide presents HumanEval, a famous benchmark for code LLMs that uses functional correctness (pass@1). It lists models like GPT-4 and WizardCoder and their scores.\nThis validates that functional correctness is the industry standard for evaluating coding capabilities.\n\n\n68. Hands on: Building Functional Tests (Email)\n\n\n\nSlide 68\n\n\n(Timestamp: 34:28)\nThis slide asks how to apply functional correctness to Text (e.g., drafting emails).\nRajiv suggests defining ‚Äúfunctional‚Äù properties for text: Is it concise? Does it include a call to action? Is the tone polite? These are testable assertions we can make about text output.\n\n\n69. Hands on: Python Test for Text\n\n\n\nSlide 69\n\n\n(Timestamp: 35:19)\nThis slide shows a Python snippet that tests if an email uses ‚Äúinformal language.‚Äù\nIt demonstrates that we can write code to evaluate text properties, effectively treating text generation as a ‚Äúfunctional‚Äù problem with pass/fail criteria.\n\n\n70. Evaluation Benchmarks\n\n\n\nSlide 70\n\n\n(Timestamp: 35:54)\nThis slide highlights Evaluation Benchmarks on the chart.\nRajiv moves to this category, explaining that benchmarks are essentially collections of the previous methods (exact match, functional tests) aggregated into large suites.\n\n\n71. Story: GLUE Benchmark\n\n\n\nSlide 71\n\n\n(Timestamp: 36:05)\nThis slide tells the history of GLUE (2018). Before GLUE, models were specialized for single tasks. GLUE introduced the idea of a General Language Understanding Evaluation, pushing the field toward models that could handle many different tasks well.\nRajiv credits GLUE with driving the progress that led to modern LLMs by giving researchers a unified target.\n\n\n72. So Many Benchmarks\n\n\n\nSlide 72\n\n\n(Timestamp: 37:47)\nThis slide introduces successors to GLUE: HellaSwag (commonsense) and Big Bench (reasoning).\nRajiv notes that Big Bench Hard compares models to average and max human performance, providing a measuring stick for how close AI is getting to human-level reasoning.\n\n\n73. Even More Benchmarks\n\n\n\nSlide 73\n\n\n(Timestamp: 38:40)\nThis slide scrolls through a massive list of over 80 benchmarks.\nRajiv uses this to illustrate the explosion of evaluation datasets. There is a benchmark for almost everything, but this abundance can be paralyzed.\n\n\n74. Multi-task Benchmarks\n\n\n\nSlide 74\n\n\n(Timestamp: 39:02)\nThis slide explains that Multi-task benchmarks aggregate many specific tasks (stories, code, legal) into a single score.\nThis allows for a robust, high-level view of a model‚Äôs general capability, though it risks hiding specific weaknesses.\n\n\n75. Gaming Benchmarks\n\n\n\nSlide 75\n\n\n(Timestamp: 39:36)\nThis slide discusses Gaming and Data Contamination. It mentions AlpacaEval and how models might cheat by training on the test data.\nRajiv warns that high benchmark scores might just mean the model has memorized the answers, making the benchmark useless for measuring true generalization.\n\n\n76. Hands on: Langtest\n\n\n\nSlide 76\n\n\n(Timestamp: 40:21)\nThis slide introduces Langtest by John Snow Labs. It is a library with 50+ test types for accuracy, bias, and robustness.\nRajiv recommends it as a tool for running standard benchmarks on your own models.\n\n\n77. Hands on: Eleuther Harness\n\n\n\nSlide 77\n\n\n(Timestamp: 40:40)\nThis slide introduces the Eleuther AI Evaluation Harness. Rajiv calls this the ‚ÄúOG‚Äù (original gangster) framework. It supports over 200 tasks.\nHe provides a code snippet showing how easy it is to run a benchmark like MMLU on a Hugging Face model using this harness.\n\n\n78. OpenAI Evals\n\n\n\nSlide 78\n\n\n(Timestamp: 41:20)\nThis slide mentions OpenAI Evals, another framework for evaluating LLMs.\nRajiv notes it is useful but emphasizes that standardized templates work best when content variation is low.\n\n\n79. Benchmarking Test Suites Summary\n\n\n\nSlide 79\n\n\n(Timestamp: 41:29)\nThis slide summarizes the Pros and Cons of benchmarks. * Pros: Wide coverage, cheap, automated. * Cons: Limited to easily measured tasks (often multiple choice), risk of leakage.\nRajiv reminds us that benchmarks are proxies for quality, not definitive proof of utility for a specific business case.\n\n\n80. So Many Leaderboards\n\n\n\nSlide 80\n\n\n(Timestamp: 42:07)\nThis slide visualizes the ecosystem of leaderboards: Open LLM, Mosaic Eval Gauntlet, HELM.\n\n\n81. Pro Tip: Build Your Own Benchmark\n\n\n\nSlide 81\n\n\n(Timestamp: 42:18)\nThis is a key takeaway: ‚ÄúBuild your own benchmark / leaderboards.‚Äù\nRajiv argues that for an enterprise, public leaderboards are insufficient. You should curate a set of tasks that reflect your specific domain (e.g., legal, IT ops) and evaluate models against that.\n\n\n82. Custom Leaderboard Example\n\n\n\nSlide 82\n\n\n(Timestamp: 43:31)\nThis slide shows an example of a custom internal leaderboard (‚ÄúAtmosBank‚Äù). It tracks how different models perform on the specific datasets that matter to that organization.\nThis allows a company to quickly vet new models (like a new Llama release) against their specific needs.\n\n\n83. Benchmark Dataset: OWL\n\n\n\nSlide 83\n\n\n(Timestamp: 43:42)\nThis slide details OWL, a benchmark for IT Operations. It highlights the effort required to build it: manual review of hundreds of questions.\nRajiv uses this to be realistic: building a custom benchmark has a cost. You need to invest human time to create the ‚ÄúGold Standard‚Äù questions and answers.\n\n\n84. Averaging Can Mask Issues\n\n\n\nSlide 84\n\n\n(Timestamp: 44:39)\nThis slide warns that ‚ÄúAveraging can mask issues.‚Äù If Model 2 is amazing at your specific task but terrible at 9 others, an average score will hide its value.\nRajiv advises looking at individual task scores rather than just the aggregate number on a leaderboard.\n\n\n85. Human Evaluation\n\n\n\nSlide 85\n\n\n(Timestamp: 45:13)\nThis slide highlights Human Evaluation on the chart.\nRajiv moves to the high-cost, high-flexibility zone. Humans are the ultimate judges of quality, capturing nuance that automated metrics miss.\n\n\n86. Human Evaluation - Best Practices\n\n\n\nSlide 86\n\n\n(Timestamp: 45:38)\nThis slide lists best practices: Inter-annotator agreement, clear guidelines, and training.\nRajiv notes that we know how to do this from traditional data labeling. If humans can‚Äôt agree on the quality of an output (e.g., only 80% agreement), you can‚Äôt expect the model to do better.\n\n\n87. Human Evaluation - Limitations\n\n\n\nSlide 87\n\n\n(Timestamp: 46:25)\nThis slide discusses limitations. Humans are bad at checking factuality (it takes effort to Google facts) and are easily swayed by assertiveness.\nIf an LLM sounds confident, humans tend to rate it highly even if it is wrong.\n\n\n88. Sycophancy Bias\n\n\n\nSlide 88\n\n\n(Timestamp: 46:43)\nThis slide defines Sycophancy: LLMs tend to generate responses that please the user rather than telling the truth.\nRajiv shows an example where a model reinforces a user‚Äôs misconception because it wants to be ‚Äúhelpful.‚Äù Humans often rate these pleasing answers higher, reinforcing the bias.\n\n\n89. Human Evaluation Summary\n\n\n\nSlide 89\n\n\n(Timestamp: 47:03)\nThis slide summarizes Human Eval. * Strengths: Gold standard, handles variety. * Weaknesses: Expensive, slow, high variance, subject to bias.\n\n\n90. Hands on: Argilla\n\n\n\nSlide 90\n\n\n(Timestamp: 47:57)\nThis slide showcases Argilla, an open-source tool for data annotation.\nRajiv encourages teams to set up tools like this to make it easy for domain experts (doctors, lawyers) to provide feedback on model outputs.\n\n\n91. Annotation Tools\n\n\n\nSlide 91\n\n\n(Timestamp: 48:20)\nThis slide lists other tools: LabelStudio and Prodigy. The message is: don‚Äôt reinvent the wheel, use existing tooling to gather human feedback.\n\n\n92. LongEval\n\n\n\nSlide 92\n\n\n(Timestamp: 48:39)\nThis slide references LongEval, a study on evaluating long summaries. It emphasizes that guidelines for humans need to be specific (coarse vs fine-grained) to get reliable results.\n\n\n93. Human Comparison/Arena\n\n\n\nSlide 93\n\n\n(Timestamp: 49:04)\nThis slide highlights Human Comparison/Arena on the chart.\nThis is a specific subset of human evaluation focused on preferences rather than absolute scoring.\n\n\n94. Story: Dating (Preferences)\n\n\n\nSlide 94\n\n\n(Timestamp: 49:26)\nThis slide uses a dating analogy. Old dating sites used long forms (detailed evaluation), but modern apps use swiping (binary preference).\nRajiv argues that it is much easier and faster for humans to say ‚ÄúI prefer A over B‚Äù (swiping) than to fill out a detailed scorecard. This is the logic behind Arena evaluations.\n\n\n95. Head to Head Preferences\n\n\n\nSlide 95\n\n\n(Timestamp: 50:16)\nThis slide shows a ‚ÄúHead to Head‚Äù interface. The user sees two model outputs and clicks the one they like better.\nThis method is widely used (e.g., in RLHF) because it scales well and reduces cognitive load on annotators.\n\n\n96. Head to Head Leaderboards\n\n\n\nSlide 96\n\n\n(Timestamp: 50:50)\nThis slide introduces the LM-SYS Arena. It uses an Elo rating system (like in Chess) based on thousands of anonymous battles between models.\nRajiv notes this is a very effective way to rank models based on general human preference.\n\n\n97. Arena Solutions\n\n\n\nSlide 97\n\n\n(Timestamp: 51:44)\nThis slide provides links to the code for the LM-SYS arena. Rajiv suggests that enterprises can set up their own internal arenas to gamify evaluation for their employees.\n\n\n98. Model Based Approaches\n\n\n\nSlide 98\n\n\n(Timestamp: 51:55)\nThis slide highlights Model based Approaches on the chart.\nThis is the most rapidly evolving area: using LLMs to evaluate other LLMs (LLM-as-a-Judge).\n\n\n99. Evaluating Factuality\n\n\n\nSlide 99\n\n\n(Timestamp: 52:24)\nThis slide discusses the limitation of reference-based factuality (comparing to a known ground truth). It notes that this is ‚ÄúPretty limited utility‚Äù because we often don‚Äôt have ground truth for every new query.\n\n\n100. Model Based Evaluation\n\n\n\nSlide 100\n\n\n(Timestamp: 52:54)\nThis slide illustrates the core concept: Instead of a human checking if the story is grammatical, we ask GPT-3 (or GPT-4) to do it.\nRajiv explains that models are now good enough to act as proxy evaluators.\n\n\n101. Assertions\n\n\n\nSlide 101\n\n\n(Timestamp: 53:12)\nThis slide lists simple model-based checks called Assertions: Language Match, Sentiment, Toxicity, Length.\nThese act like unit tests but use the LLM to classify the output (e.g., ‚ÄúIs this text toxic? Yes/No‚Äù).\n\n\n102. G-Eval\n\n\n\nSlide 102\n\n\n(Timestamp: 55:07)\nThis slide introduces G-Eval, a framework that uses Chain-of-Thought (CoT) to generate a score. It provides the model with evaluation criteria and steps, asking it to reason before assigning a grade.\n\n\n103. SelfCheckGPT\n\n\n\nSlide 103\n\n\n(Timestamp: 55:26)\nThis slide describes SelfCheckGPT. This method detects hallucinations by sampling the model multiple times. If the model tells the same story consistently, it‚Äôs likely true. If the details change every time, it‚Äôs likely hallucinating.\n\n\n104. Which Model for Evaluation?\n\n\n\nSlide 104\n\n\n(Timestamp: 55:50)\nThis slide asks which model to use as the judge. * GPT-4: Strongest evaluator, best for reasoning. * GPT-3.5: Cheaper, good for simple tasks. * JudgeLM: Fine-tuned specifically for evaluation.\n\n\n105. Human Alignment\n\n\n\nSlide 105\n\n\n(Timestamp: 56:44)\nThis slide presents data showing high Human Alignment. GPT-4 judges agree with human judges 80-95% of the time.\nThis validates the approach: LLM judges are a scalable, cheap proxy for human evaluation.\n\n\n106. Model Evaluation Biases\n\n\n\nSlide 106\n\n\n(Timestamp: 57:32)\nThis slide warns about biases in LLM judges: * Position Bias: Preferring the first answer. * Verbosity Bias: Preferring longer answers. * Self-Enhancement: Preferring its own outputs.\nRajiv suggests mitigations like swapping order and using different models for judging.\n\n\n107. Summary: Model Based Evaluation\n\n\n\nSlide 107\n\n\n(Timestamp: 58:31)\nThis slide categorizes model-based methods: Assertions (simple), Concept based (G-Eval), Sampling based (SelfCheck), and Preference based (RLHF).\n\n\n108. Pros and Cons\n\n\n\nSlide 108\n\n\n(Timestamp: 59:17)\nThis slide summarizes the trade-offs. * Pros: Cheaper/faster than humans, good alignment. * Cons: Sensitive to prompts, known biases.\n\n\n109. Ragas\n\n\n\nSlide 109\n\n\n(Timestamp: 59:49)\nThis slide introduces Ragas, a framework specifically for evaluating RAG pipelines. It calculates a score based on Faithfulness and Relevancy.\n\n\n110. DeepEval\n\n\n\nSlide 110\n\n\n(Timestamp: 1:00:10)\nThis slide mentions DeepEval, another tool that treats evaluation like unit tests for LLMs, checking for bias, toxicity, etc.\n\n\n111. Hands on: Using Ragas\n\n\n\nSlide 111\n\n\n(Timestamp: 1:00:19)\nThis slide shows code for using Ragas. It demonstrates how to pass a dataset to the evaluate function and get metrics like context_precision and answer_relevancy.\n\n\n112. Hands on: Prompts (SALMONN)\n\n\n\nSlide 112\n\n\n(Timestamp: 1:01:00)\nThis slide shows prompts from the SALMONN paper. Rajiv includes these to show real-world examples of how researchers craft prompts to evaluate specific qualities like coherence.\n\n\n113. Quality Prompt\n\n\n\nSlide 113\n\n\n(Timestamp: 1:01:24)\nThis slide shows a prompt for evaluating Data Quality. It asks the model to rate the helpfulness and relevance of text on a scale.\n\n\n114. RAG Relevancy Prompt\n\n\n\nSlide 114\n\n\n(Timestamp: 1:01:35)\nThis slide details a ‚ÄúRAG RELEVANCY PROMPT TEMPLATE.‚Äù It instructs the model to compare a question and a reference text to determine if the reference contains the answer.\n\n\n115. Impartial Judge Prompt\n\n\n\nSlide 115\n\n\n(Timestamp: 1:01:49)\nThis slide shows a prompt for an ‚ÄúImpartial Judge.‚Äù It asks the model to be an assistant that evaluates the quality of a response, ensuring it is helpful, accurate, and detailed.\n\n\n116. Resources: Model Based Eval\n\n\n\nSlide 116\n\n\n(Timestamp: 1:02:08)\nThis slide lists libraries: Ragas, Microsoft llm-eval, TrueLens, Guardrails.\nRajiv notes that while libraries are great, many people end up writing their own hand-crafted prompts to fit their specific needs.\n\n\n117. Red Teaming\n\n\n\nSlide 117\n\n\n(Timestamp: 1:02:23)\nThis slide highlights Red Teaming on the chart.\nThis is the final, most flexible, and rigorous technical evaluation method.\n\n\n118. Story: Microsoft Tay\n\n\n\nSlide 118\n\n\n(Timestamp: 1:02:37)\nThis slide tells the cautionary tale of Microsoft Tay (2016). The chatbot learned from Twitter users and became racist/genocidal in less than 24 hours.\nRajiv cites this as the ‚ÄúOrigin of Red Teaming in AI‚Äù‚Äîthe realization that we must proactively attack our models to find vulnerabilities before the public does.\n\n\n119. Why Red Teaming?\n\n\n\nSlide 119\n\n\n(Timestamp: 1:04:11)\nThis slide defines Red Teaming: Eliciting model vulnerabilities to prevent undesirable behaviors.\nIt is about adversarial testing‚Äîtrying to trick the model into doing something bad.\n\n\n120. Every Use Case Should Be Red Teamed\n\n\n\nSlide 120\n\n\n(Timestamp: 1:04:23)\nThis slide argues that ‚ÄúEvery use case should be Red Teamed.‚Äù\nRajiv explains that fine-tuning a model (even slightly) can destroy the safety alignment (RLHF) provided by the base model creator. You cannot assume a model is safe just because it was safe before you fine-tuned it.\n\n\n121. How to: Red Teaming with a Model\n\n\n\nSlide 121\n\n\n(Timestamp: 1:05:07)\nThis slide suggests a technique: Use a separate ‚ÄúRisk Assessment‚Äù model (like Llama-2) to monitor the inputs and outputs of your main model, logging any risky queries.\n\n\n122. How to: Red Teaming from Meta\n\n\n\nSlide 122\n\n\n(Timestamp: 1:05:22)\nThis slide describes Meta‚Äôs approach to Llama 2. They hired diverse teams to attack the model regarding specific risks (criminal planning, trafficking).\nRajiv notes that Meta actually held back a specific model (33b) because it failed these red team tests.\n\n\n123. Red Teaming Process\n\n\n\nSlide 123\n\n\n(Timestamp: 1:05:54)\nThis slide outlines the workflow: Generate prompts (multilingual), Annotate risk (Likert scale), and use data for safety training.\n\n\n124. Technical Methods Recap\n\n\n\nSlide 124\n\n\n(Timestamp: 1:06:04)\nThis slide shows the full Generative AI Evaluation Methods chart again.\nRajiv concludes the technical section, having covered the spectrum from Exact Match to Red Teaming.\n\n\n125. Operational (TCO)\n\n\n\nSlide 125\n\n\n(Timestamp: 1:06:16)\nThis slide highlights the Operational (TCO) pillar.\nRajiv shifts gears to discuss the cost and maintenance of running these models.\n\n\n126. Story: GitHub Copilot Costs\n\n\n\nSlide 126\n\n\n(Timestamp: 1:06:33)\nThis slide references a story that GitHub Copilot was losing money per user (costing $20-$80/month while charging $10).\nRajiv uses this to warn about the ‚ÄúEpidemic of cloud laundering.‚Äù You must calculate the inference costs upfront, or your successful product might bankrupt you.\n\n\n127. Monitoring\n\n\n\nSlide 127\n\n\n(Timestamp: 1:07:28)\nThis slide introduces Monitoring as the ‚ÄúSibling of Evaluate.‚Äù\nIt lists things to watch: Functional metrics (latency, errors), Prompt Drift, and Response Monitoring.\n\n\n128. Monitoring Metrics (GPU/Responsible AI)\n\n\n\nSlide 128\n\n\n(Timestamp: 1:07:41)\nThis slide lists specific metrics. * GPU: Error rates (429), token counts. * Responsible AI: How often is the content filter triggering?\n\n\n129. Performance Metrics\n\n\n\nSlide 129\n\n\n(Timestamp: 1:07:53)\nThis slide lists Performance Metrics: * Time to first token (TTFT): Critical for user experience. * Requests Per Second (RPS). * Token render rate.\n\n\n130. User Engagement Funnel\n\n\n\nSlide 130\n\n\n(Timestamp: 1:08:01)\nThis slide suggests monitoring User Engagement. * Funnel: Trigger -&gt; Response -&gt; User Keeps/Accepts Response.\nRajiv notes that OpenAI monitors the KV Cache utilization to understand real usage patterns better than simple GPU utilization.\n\n\n131. Application to RAG\n\n\n\nSlide 131\n\n\n(Timestamp: 1:09:14)\nThis slide acts as a section header: APPLICATION TO RAG.\nRajiv will now apply all the previous concepts to a specific use case: Retrieval Augmented Generation.\n\n\n132. Bring Your Own Facts\n\n\n\nSlide 132\n\n\n(Timestamp: 1:09:26)\nThis slide explains the core philosophy of RAG: ‚ÄúIf you need facts - bring them yourself.‚Äù Don‚Äôt rely on the LLM‚Äôs training data; provide the context.\n\n\n133. What is RAG?\n\n\n\nSlide 133\n\n\n(Timestamp: 1:09:33)\nThis slide defines RAG: Improving responses by grounding the model on external knowledge sources.\n\n\n134. Evaluating RAG (The Wrong Way)\n\n\n\nSlide 134\n\n\n(Timestamp: 1:09:51)\nThis slide shows a ‚Äúrecipe‚Äù for RAG evaluation focusing solely on factuality precision (95%).\nRajiv presents this as a trap. He asks, ‚ÄúWhat‚Äôs wrong with this?‚Äù\n\n\n135. Missing the Point\n\n\n\nSlide 135\n\n\n(Timestamp: 1:10:07)\nThis slide explicitly states that focusing only on technical details misses the larger point of view.\nRajiv is baiting the audience to remember the Three Pillars.\n\n\n136. Three Pillars (RAG Context)\n\n\n\nSlide 136\n\n\n(Timestamp: 1:10:20)\nThis slide brings back the Technical, Business, Operational pillars.\nRajiv insists we must start with the Business metrics before jumping into technical precision.\n\n\n137. Business Metric for RAG\n\n\n\nSlide 137\n\n\n(Timestamp: 1:10:28)\nThis slide outlines the Business questions: * What is the value of a correct answer? * What is the cost/consequence of a wrong answer?\nRajiv warns against building a ‚Äúscience experiment‚Äù without knowing the ROI.\n\n\n138. Operational Metrics for RAG\n\n\n\nSlide 138\n\n\n(Timestamp: 1:10:58)\nThis slide lists Operational questions: * Labeling effort? * Running costs? * Is IT ready to support this?\n\n\n139. Three Pillars (Transition)\n\n\n\nSlide 139\n\n\n(Timestamp: 1:11:43)\nThis slide shows the three pillars again, preparing to zoom in on the Technical side.\n\n\n140. Technical Pillar\n\n\n\nSlide 140\n\n\n(Timestamp: 1:11:45)\nThis slide highlights Technical (F1). Now that we‚Äôve justified the business case, how do we technically evaluate RAG?\n\n\n141. Current Approaches (Eyeballing)\n\n\n\nSlide 141\n\n\n(Timestamp: 1:11:51)\nThis slide critiques the current state: ‚ÄúEyeballing a few examples.‚Äù\nRajiv notes that most developers just look at a few chats and say ‚Äúlooks good.‚Äù This is insufficient for production.\n\n\n142. Evaluate LLM System\n\n\n\nSlide 142\n\n\n(Timestamp: 1:12:14)\nThis slide lists system-level questions: Accuracy, references, understandability, query time.\n\n\n143. Decomposing RAG\n\n\n\nSlide 143\n\n\n(Timestamp: 1:12:37)\nThis slide brings back the RAG diagram, emphasizing decomposition. 1. Retrieval 2. Augmented Generation\nRajiv argues we must evaluate these independently to find the bottleneck. Often, the problem is the Retriever, not the LLM.\n\n\n144. Component Metrics\n\n\n\nSlide 144\n\n\n(Timestamp: 1:12:53)\nThis slide details metrics for each component: * Retrieval: Precision, Recall, Order. * Augmentation: Correctness, Toxicity, Hallucination.\n\n\n145. Analyze Retrieval\n\n\n\nSlide 145\n\n\n(Timestamp: 1:13:53)\nThis slide explains how to evaluate retrieval. You need a dataset of (Query, Relevant Documents).\nYou run your retriever and check if it found the documents in your ground truth set.\n\n\n146. Methods for Retrieval\n\n\n\nSlide 146\n\n\n(Timestamp: 1:14:13)\nThis slide highlights Exact Matching on the chart.\nFor retrieval, we can use exact matching (or set intersection) because we know exactly which document IDs should be returned.\n\n\n147. Retrieval Metrics\n\n\n\nSlide 147\n\n\n(Timestamp: 1:14:16)\nThis slide lists retrieval metrics: Success rate (Hit-rate) and Mean Reciprocal Rank (MRR).\n\n\n148. Analyze Augmentation\n\n\n\nSlide 148\n\n\n(Timestamp: 1:14:48)\nThis slide explains how to evaluate the generation step. You need (Context, Generated Response, Ground Truth).\n\n\n149. Methods for Augmentation\n\n\n\nSlide 149\n\n\n(Timestamp: 1:15:02)\nThis slide highlights Human and Model-based approaches on the chart.\nFor generation, exact match doesn‚Äôt work. We need flexible evaluators (Humans or LLMs) to judge faithfulness and relevancy.\n\n\n150. Augmentation Modules\n\n\n\nSlide 150\n\n\n(Timestamp: 1:15:05)\nThis slide lists modules to test: * Label-free: Faithfulness (did it stick to context?), Relevancy. * With-labels: Correctness (compared to ground truth).\n\n\n151. Pro Tip: Imbalance\n\n\n\nSlide 151\n\n\n(Timestamp: 1:15:28)\nThis slide warns about Imbalanced Data. If most retrieved documents are irrelevant, accuracy is a bad metric. Use Precision and Recall.\n\n\n152. Pro Tip: Synthetic Data\n\n\n\nSlide 152\n\n\n(Timestamp: 1:15:35)\nThis slide suggests generating Synthetic Evaluation Datasets.\nYou can use an LLM to read your documents and generate Question/Answer pairs. This creates a ‚ÄúGold Standard‚Äù dataset for retrieval evaluation without manual labeling.\n\n\n153. Notebooks Used\n\n\n\nSlide 153\n\n\n(Timestamp: 1:16:14)\nThis slide lists the notebooks available in the GitHub repo: Prompting, Guidance, Eleuther Harness, Langtest, Ragas.\n\n\n154. Final Slide\n\n\n\nSlide 154\n\n\n(Timestamp: 1:16:32)\nThe presentation concludes with the title slide again, providing the speaker‚Äôs contact info and the GitHub link one last time. Rajiv thanks the audience and promises updates as the field evolves.\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  },
  {
    "objectID": "model-interpretability-explainability.html",
    "href": "model-interpretability-explainability.html",
    "title": "Model Interpretability and Explainability for Machine Learning Models",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "model-interpretability-explainability.html#video",
    "href": "model-interpretability-explainability.html#video",
    "title": "Model Interpretability and Explainability for Machine Learning Models",
    "section": "",
    "text": "Watch the full video"
  },
  {
    "objectID": "model-interpretability-explainability.html#annotated-presentation",
    "href": "model-interpretability-explainability.html#annotated-presentation",
    "title": "Model Interpretability and Explainability for Machine Learning Models",
    "section": "Annotated Presentation",
    "text": "Annotated Presentation\nBelow is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.\nHere is the slide-by-slide annotated presentation based on the technical talk ‚ÄúA Quest for Interpretability.‚Äù\n\n1. A Quest for Interpretability\n\n\n\nSlide 1\n\n\n(Timestamp: 00:00)\nThe presentation opens with the title slide, introducing the core mission of the talk: demystifying machine learning models. The speaker sets the stage for both data science novices and experts, promising to provide methods to ‚Äúask any particular machine learning model you see and be able to explain it.‚Äù\nThe goal is to move beyond simply generating predictions to understanding the ‚Äúwhy‚Äù behind them. The speaker emphasizes that whether you are new to the field or comfortable with interpretability, the session will dive deeper into techniques that provide transparency to complex algorithms.\n\n\n2. Predictive Model Around Aggression\n\n\n\nSlide 2\n\n\n(Timestamp: 00:41)\nTo make the concepts more engaging, the speaker introduces a ‚ÄúDragon theme‚Äù as a visual metaphor. The hypothetical problem presented is building a Predictive Model Around Aggression. The objective is practical and dire: ‚Äúwe want to use machine learning to help us figure out which dragons are likely to eat us.‚Äù\nThis metaphor serves as a stand-in for real-world risk assessment models. Instead of dry financial or medical data initially, the audience is asked to consider the stakes of a model that must accurately predict danger (getting eaten) based on various dragon attributes.\n\n\n3. Trust: The Big Picture\n\n\n\nSlide 3\n\n\n(Timestamp: 01:28)\nThe speaker broadens the scope to explain that interpretability is just one component of a much larger ecosystem called ‚ÄúTrust.‚Äù This slide illustrates that trusting a model involves asking questions about bias, correctness, ethical purposes (like facial recognition debates), and model health over time.\nWhile acknowledging these critical factors‚Äîsuch as ‚Äúis your data biased‚Äù or ‚Äúis your model being used for an ethical purpose‚Äù‚Äîthe speaker clarifies that this specific presentation will focus on the interpretability slice of the pie: ‚Äúcan we explain what‚Äôs going on‚Ä¶ inside that model.‚Äù\n\n\n4. Interpretable Predictive Model Around Aggression\n\n\n\nSlide 4\n\n\n(Timestamp: 02:28)\nReturning to the dragon metaphor, this slide reiterates the specific technical goal: building an Interpretable Predictive Model Around Aggression. The speaker distinguishes this from simply dumping data into a ‚Äúblack box‚Äù like TensorFlow and deploying it based solely on performance metrics.\nThe focus here is on the deliberate choice to build a model that is not just predictive, but understandable. This sets up the central tension of the talk: the trade-off between model complexity (accuracy) and the ability to explain how the model works.\n\n\n5. Why Interpretability?\n\n\n\nSlide 5\n\n\n(Timestamp: 02:38)\nThis slide outlines the three key audiences for interpretability. First, for Yourself: debugging is essential because ‚Äúit‚Äôs very easy for things to go wrong.‚Äù Second, for Stakeholders: managers and bosses will demand to know how a model works, regardless of how high the AUC (Area Under the Curve) is.\nThird, the speaker highlights Regulators in high-risk industries like insurance, finance, and healthcare. In these sectors, there is a ‚Äúhigher standard set‚Äù where you must prove you understand the model‚Äôs behavior to mitigate risks to the financial system or public health.\n\n\n6. An Understandable White Box Model (CLEAR-2)\n\n\n\nSlide 6\n\n\n(Timestamp: 04:21)\nThe presentation begins with the ‚Äúsimplest, easiest, most interpretable model‚Äù: a linear regression for housing prices. This White Box Model uses only two features: the number of bathrooms and square footage.\nThe transparency is total; you can see the coefficients directly (e.g., multiplying bathrooms by a value). The audience is asked to confirm that this is intuitive, and the consensus is that yes, this is an easily explainable model where the inputs have a clear, logical relationship to the output.\n\n\n7. White Box Model (CLEAR-8)\n\n\n\nSlide 7\n\n\n(Timestamp: 05:41)\nComplexity is introduced by adding more features to improve accuracy. However, this slide reveals a paradox of linear models: Multicollinearity. The speaker points out that while the model might be ‚Äútransparent‚Äù (you can see the math), the logic breaks down.\nSpecifically, the model shows that ‚Äúas the total rooms gets higher, the value of my house goes down.‚Äù This counter-intuitive finding occurs because features are not independent. While technically a ‚Äúwhite box,‚Äù the interpretability suffers because the coefficients no longer align with human intuition due to correlations between variables.\n\n\n8. Understandable White Box Model? (Tree - AUC 0.74)\n\n\n\nSlide 8\n\n\n(Timestamp: 09:16)\nMoving to Decision Trees, the speaker presents a simple tree based on the Titanic dataset (predicting survival). With only two features (gender and age), the logic is stark and easy to follow: ‚Äúif you‚Äôre a male and your age is greater than 10 years old‚Ä¶ chance of survival is very low.‚Äù\nThis model has an AUC of 0.74. It is highly interpretable, acting as a flowchart that anyone can trace. However, the speaker hints at the limitation: simplicity often comes at the cost of accuracy.\n\n\n9. Understandable White Box Model? (Tree - AUC 0.78)\n\n\n\nSlide 9\n\n\n(Timestamp: 11:19)\nTo improve the model, more features are added, raising the AUC to 0.78. The tree grows branches, becoming visually more cluttered. The speaker notes that ‚Äúby adding more features or variables‚Ä¶ the performance of our model increases.‚Äù\nThis slide represents the tipping point where the visual representation of the model starts to become less of a helpful flowchart and more of a complex web, though it is still technically possible to trace a single path.\n\n\n10. Understandable White Box Model? (Tree - AUC 0.79)\n\n\n\nSlide 10\n\n\n(Timestamp: 11:38)\nThe optimization continues, pushing the AUC to 0.79. The tree on the slide is now dense and difficult to read. The question mark in the title ‚ÄúUnderstandable White Box Model?‚Äù becomes more relevant.\nThe speaker emphasizes that data scientists ‚Äúdon‚Äôt have to kind of stop there.‚Äù The drive for higher accuracy encourages adding more depth and complexity to the tree, sacrificing the immediate ‚Äúglance-value‚Äù interpretability that smaller trees possess.\n\n\n11. Better Performance but too much to Comprehend (AUC 0.81)\n\n\n\nSlide 11\n\n\n(Timestamp: 11:44)\nThis slide shows a massive, unreadable decision tree with an AUC of 0.81. The speaker notes, ‚Äúit gets a little tricky‚Ä¶ lot harder to understand what‚Äôs going on.‚Äù This illustrates the ‚ÄúBlack Box‚Äù problem even within models considered interpretable.\nFurthermore, the speaker points out that data scientists rarely stop at one tree; they use Random Forests (collections of trees). Interpreting a forest by looking at the trees is impossible, necessitating new tools for explanation.\n\n\n12. So Many Algorithms to Try\n\n\n\nSlide 12\n\n\n(Timestamp: 13:07)\nThis heatmap, derived from a study by Randy Olssen, visualizes the performance of different algorithms across 165 datasets. It illustrates the No Free Lunch Theorem: there is not one single algorithm that always works best.\nBecause of this, data scientists must try various complex algorithms (Gradient Boosting, Neural Networks, Ensembles) to find the best solution. We cannot simply restrict ourselves to linear regression just for the sake of interpretability if it fails to solve the problem.\n\n\n13. Algorithms Matter\n\n\n\nSlide 13\n\n\n(Timestamp: 13:49)\nThe speaker reinforces that model choice is critical. Using a simple model that yields inaccurate predictions is dangerous: ‚Äúif we can‚Äôt figure out if this model is going to work or not we‚Äôre in trouble.‚Äù\nThe slide emphasizes that accuracy is paramount (‚Äúwe are toast‚Äù if we are wrong). Therefore, we need methods that allow us to use complex, accurate algorithms without flying blind regarding how they work.\n\n\n14. Simple Models != Accurate\n\n\n\nSlide 14\n\n\n(Timestamp: 14:11)\nThis slide counters the argument that we should only use simple models. The speaker asserts, ‚Äúmost simple models are just not very accurate.‚Äù Real-world problems are complex, and if they could be solved with a few simple rules, machine learning wouldn‚Äôt be necessary.\nResources are provided on the slide for further reading, including defenses of black box models. The takeaway is that complexity is often a requirement for accuracy, so we must find ways to explain complex models rather than avoiding them.\n\n\n15. Tools That Can Explain Any Black Box Model\n\n\n\nSlide 15\n\n\n(Timestamp: 14:59)\nThis is the pivot point of the presentation. The speaker introduces the solution: ‚ÄúThere are tools here that can explain any blackbox model.‚Äù This promises a methodology that is Model Agnostic‚Äîmeaning it works regardless of whether you are using a Random Forest, a Neural Network, or an SVM.\n\n\n16. Model Agnostic Explanation Tools\n\n\n\nSlide 16\n\n\n(Timestamp: 15:07)\nThe speaker outlines the three specific pillars of interpretability that the rest of the talk will cover: 1. Feature Importance: Understanding what variables are most impactful. 2. Partial Dependence: Understanding the directionality of features (e.g., does age increase or decrease risk?). 3. Prediction Explanations: Explaining why a specific prediction was made for a specific individual (using techniques like SHAP).\n\n\n17. Feature Importance\n\n\n\nSlide 17\n\n\n(Timestamp: 16:11)\nThe first pillar is Feature Importance. Returning to the dragon example, the speaker discusses the data collection process: asking domain experts (or watching Game of Thrones) to determine factors like age, weight, or number of children.\nThe goal is to determine which of these collected variables actually drives the model. This is crucial for debugging, feature selection, and explaining the model to stakeholders.\n\n\n18. Dragon Reading: Milk vs.¬†Age\n\n\n\nSlide 18\n\n\n(Timestamp: 17:41)\nTo illustrate the pitfalls of feature importance, the speaker introduces a new scenario: ‚Äúhow dragons learn to read.‚Äù We intuitively know that Age affects reading ability (older children read better).\nThe speaker then asks about Milk Consumption. While one might guess milk helps (calcium), the reality is that milk consumption is negatively correlated with age (babies drink milk, teenagers don‚Äôt). Therefore, milk consumption appears related to reading ability, but it is a spurious correlation. It has ‚Äúnothing at all to do with the ability to read,‚Äù yet the data might suggest otherwise.\n\n\n19. Split Based Variable Importance\n\n\n\nSlide 19\n\n\n(Timestamp: 20:39)\nThis slide shows what happens when you use the default ‚ÄúSplit Based‚Äù importance metric in algorithms like LightGBM. The chart shows milk_consumption as the most important feature, ranking higher than age.\nThis happens because the model uses milk consumption as a proxy for age during the tree-splitting process. The speaker warns that relying on default metrics can lead to incorrect conclusions where spurious correlations mask the true drivers of the model.\n\n\n20. Permutation Based Variable Importance\n\n\n\nSlide 20\n\n\n(Timestamp: 21:11)\nBy switching to a Permutation Based approach, the chart flips. Now, Age is correctly identified as the dominant feature, and milk consumption drops to near zero importance.\nThe speaker emphasizes that this technique ‚Äúcuts right through‚Äù the noise. It correctly identifies that while milk varies with age, it does not actually influence the reading score when age is accounted for.\n\n\n21. Spurious Correlations (Nicolas Cage)\n\n\n\nSlide 21\n\n\n(Timestamp: 21:38)\nThis slide references the famous spurious correlation between Nicolas Cage films and swimming pool drownings. The speaker uses this to highlight the danger of ‚ÄúEnterprise Data Lakes.‚Äù\nWhen data scientists grab massive tables of data without domain knowledge, they risk finding these coincidental patterns. Machine learning models are excellent at finding patterns, even ones that are nonsensical, making robust feature importance techniques vital.\n\n\n22. Feature Impact Ranking\n\n\n\nSlide 22\n\n\n(Timestamp: 17:09)\nThe presentation shows a ranked list of features for the dragon model. The speaker reiterates that getting this ranking right has ‚Äúreal consequences.‚Äù\nIf you tell a business stakeholder that a specific variable is driving the risk, they will make decisions based on that. Understanding the true hierarchy of influence is essential for trust and actionable insight.\n\n\n23. If Your Feature Impact is Wrong‚Ä¶\n\n\n\nSlide 23\n\n\n(Timestamp: 17:17)\nA humorous but serious warning: ‚ÄúIf your feature impact is wrong, you are toast.‚Äù\nThis underscores the professional risk. If a data scientist attributes a prediction to the wrong cause (like milk instead of age), they lose credibility and potentially cause the business to pull the wrong levers to try and optimize the outcome.\n\n\n24. Feature Importance: Ablation Methodology\n\n\n\nSlide 24\n\n\n(Timestamp: 22:15)\nThe speaker explains the logic behind feature importance using an Ablation Methodology. He presents three models: 1. Model AB (Both features): R-squared 0.9 2. Model A (Feature A only): R-squared 0.7 3. Model B (Feature B only): R-squared 0.8\nHe asks the audience to intuit which feature is more important based on these scores.\n\n\n25. Ablation Methodology Definition\n\n\n\nSlide 25\n\n\n(Timestamp: 22:57)\nThe audience correctly identifies that Feature B is more important because it carries more signal (higher R-squared) on its own.\nThe speaker defines Ablation as comparing the model performance with and without specific features. It is a scientific control method: ‚Äútry something with it and without it,‚Äù similar to testing if coffee makes a person happy by withholding it for a day.\n\n\n26. ‚ÄòLeave it Out‚Äô Feature Importance\n\n\n\nSlide 26\n\n\n(Timestamp: 23:49)\nThis slide formalizes the ‚ÄúLeave One Out‚Äù approach. By calculating the drop in performance when a feature is removed, we quantify its value. * Remove B: Performance drops by 0.2 (0.9 -&gt; 0.7). * Remove A: Performance drops by 0.1 (0.9 -&gt; 0.8).\nSince removing B causes a larger drop in accuracy, B is the more important feature. However, the speaker notes a problem: with 100 features, you would have to build 100 different models, which is computationally expensive.\n\n\n27. Permutation Based Feature Importance\n\n\n\nSlide 27\n\n\n(Timestamp: 26:53)\nTo solve the computational cost of retraining models, the speaker introduces Permutation Importance (attributed to Breiman/Random Forests). Instead of removing a column and retraining, you simply shuffle the values of that column (permute them) within the existing test data.\nBy shuffling the data, you break the relationship between that feature and the target, effectively ‚Äúremoving‚Äù the signal while keeping the model structure intact. If the model‚Äôs error increases significantly after shuffling a feature, that feature was important.\n\n\n28. R Package: randomForest\n\n\n\nSlide 28\n\n\n(Timestamp: 27:31)\nThe speaker highlights that this is a standard technique available in common tools. In the R language, the randomForest package has supported permutation-based importance for a long time.\nThis slide serves as a resource pointer for R users, confirming that these advanced interpretability checks are accessible within their standard toolkits.\n\n\n29. Python: scikit-learn\n\n\n\nSlide 29\n\n\n(Timestamp: 27:36)\nSimilarly, for Python users, scikit-learn has added support for permutation importance. This accessibility reinforces the speaker‚Äôs point that there is no excuse for not using these techniques to validate model behavior.\n\n\n30. Multicollinearity\n\n\n\nSlide 30\n\n\n(Timestamp: 28:10)\nThe speaker addresses a complex issue: Multicollinearity. The Venn diagrams illustrate that features often share information (variance).\nWhen features are highly correlated, they ‚Äúshare the signal.‚Äù This makes it difficult for the model (and the interpreter) to assign credit. Does the credit go to Feature A or Feature B if they both describe the same underlying phenomenon?\n\n\n31. 10 Different Models, 10 Different Importances\n\n\n\nSlide 31\n\n\n(Timestamp: 28:25)\nDue to multicollinearity, running the same algorithm on the same data multiple times (with different random seeds or data partitions) can result in different feature rankings.\nThis instability is frustrating. In one run, ‚ÄúMilk‚Äù might be important; in another, ‚ÄúAge‚Äù takes the lead. This happens because the model arbitrarily chooses one of the correlated features to split on, and this choice changes based on randomness in the training process.\n\n\n32. Multicollinearity Affects Interpreting Models\n\n\n\nSlide 32\n\n\n(Timestamp: 29:08)\nThis chart visualizes the ‚Äútrading off‚Äù effect. You can see features swapping positions in importance rankings across different model runs.\nThe speaker notes that you cannot simply remove correlated features without potentially hurting accuracy, as they might contain slight unique signals. This trade-off between accuracy and stable interpretability is a core challenge in data science.\n\n\n33. Pro Tip: Aggregate Feature Importance (Same Model)\n\n\n\nSlide 33\n\n\n(Timestamp: 30:09)\nTo handle instability, the speaker suggests a ‚ÄúPro Tip‚Äù: Aggregate Feature Importance. Run the feature importance calculation multiple times on the same model and plot the variability (the box plots in the slide).\nThis gives a ‚Äúricher understanding.‚Äù Instead of a single number, you see a range. If the range is huge, you know the feature‚Äôs importance is unstable due to correlation or noise.\n\n\n34. Aggregate Feature Importance (Different Models)\n\n\n\nSlide 34\n\n\n(Timestamp: 30:15)\nExpanding on the previous tip, you can also aggregate importance across different models (e.g., comparing importance in a Random Forest vs.¬†a Gradient Boosted Machine).\nIf a feature is consistently important across different algorithms and multiple runs, you can be much more confident that it is a true driver of the target variable.\n\n\n35. Pro Tips: Add Random Features\n\n\n\nSlide 35\n\n\n(Timestamp: 30:20)\nAnother technique mentioned is adding a Random Feature (noise) to the dataset. If a real feature ranks lower in importance than the random noise variable, it is likely not a significant predictor.\nThis serves as a baseline or ‚Äúsanity check‚Äù to distinguish true signal from statistical noise in the feature ranking list.\n\n\n36. Permutation Based Importance Conclusion\n\n\n\nSlide 36\n\n\n(Timestamp: 33:34)\nThe section concludes by asserting that Permutation based importance is the ‚Äúbest practice.‚Äù It offers a ‚Äúgood balance of computation and performance for any model.‚Äù\nReferences to academic papers (like Strobl) are provided for those who want to dive into the edge cases, but for general application, this is the recommended approach for determining what matters in a model.\n\n\n37. Partial Dependence\n\n\n\nSlide 37\n\n\n(Timestamp: 34:22)\nThe second tool introduced is Partial Dependence. While feature importance tells us which variables matter, Partial Dependence tells us how they matter.\nThe slide shows example plots for Age and Weight. The goal is to understand the functional relationship: as age increases, does the predicted aggression go up, down, or follow a complex curve?\n\n\n38. Effect of Age on Our Target\n\n\n\nSlide 38\n\n\n(Timestamp: 34:41)\nThe speaker reiterates that in complex ‚Äúblack box‚Äù models, we don‚Äôt have coefficients (positive or negative signs) like in linear regression. We cannot simply say ‚Äúage is positive.‚Äù\nTherefore, we need a visualization that maps the input value to the prediction output to understand the behavior of the model across the range of the feature.\n\n\n39. Calculating Partial Dependence (Step 1)\n\n\n\nSlide 39\n\n\n(Timestamp: 35:13)\nTo explain how Partial Dependence is calculated, the speaker walks through the process. Step 1: Take a single observation (one Dragon).\nStep 2: Keep all features constant except the one we are interested in (Age). Manually force the age to different values (e.g., 5, 10, 15 years old) and ask the model for a prediction at each point. This generates a hypothetical curve for that specific dragon.\n\n\n40. Calculating Partial Dependence (Step 2)\n\n\n\nSlide 40\n\n\n(Timestamp: 35:53)\nThe process is repeated for a second dragon. Because the other features (weight, color, etc.) are different for this dragon, the curve might look slightly different (higher or lower baseline), but it follows the model‚Äôs logic for age.\n\n\n41. Calculating Partial Dependence (Step 3)\n\n\n\nSlide 41\n\n\n(Timestamp: 36:06)\nThis is repeated for many observations in the dataset. The slide shows multiple data points being generated. This creates a ‚Äúwhat-if‚Äù scenario for every dragon in the dataset across the spectrum of ages.\n\n\n42. Individual Conditional Expectation (ICE) Curves\n\n\n\nSlide 42\n\n\n(Timestamp: 36:15)\nWhen you draw lines connecting these predictions for each individual instance, you get ICE Curves (Individual Conditional Expectation).\nThis visualizes the relationship between the feature and the prediction for every single data point. It shows the variability: for some dragons, age might have a steep effect; for others, it might be flatter.\n\n\n43. Partial Dependence Plots (PDPs)\n\n\n\nSlide 43\n\n\n(Timestamp: 36:27)\nTo get the Partial Dependence Plot (PDP), you simply average all the ICE curves.\nThis single line represents the average effect of the feature on the model‚Äôs prediction, holding everything else constant. It distills the complex interactions into a single, interpretable trend line.\n\n\n44. Resulting Partial Dependence\n\n\n\nSlide 44\n\n\n(Timestamp: 37:07)\nThe final plot shows the isolated effect of Age. The speaker notes this gives ‚Äúreally good insight.‚Äù We can now see if the risk rises linearly with age, or if (as often happens in nonlinear models) it plateaus or dips at certain points.\n\n\n45. ICE Plots\n\n\n\nSlide 45\n\n\n(Timestamp: 40:48)\nThis slide formally defines ICE Plots. While the PDP shows the average, ICE plots are useful for seeing heterogeneity. For example, if the model treats males and females differently, the ICE curves might show two distinct clusters of lines that the average PDP would obscure.\n\n\n46. Partial Dependence to Show Price Elasticity\n\n\n\nSlide 46\n\n\n(Timestamp: 37:20)\nThe speaker moves to a real-world example: Orange Juice Sales. The goal is to understand Price Elasticity‚Äîif we raise the price, do sales go down?\nEconomics 101 says yes, but the model includes complex factors like store location, coupons, and competitor prices (10 other brands), making it a high-dimensional problem.\n\n\n47. Change in Price Affects Sales?\n\n\n\nSlide 47\n\n\n(Timestamp: 38:45)\nThis chart shows the raw data (orange line) of Price vs.¬†Sales. It is ‚Äúall over the place.‚Äù There is no clear linear relationship visible because the data is noisy and confounded by other variables (e.g., maybe high prices occurred during a holiday when sales were high anyway).\nLooking just at the raw data fails to isolate the specific impact of the price change on consumer behavior.\n\n\n48. Ahh, Price Does Affect Sales!\n\n\n\nSlide 48\n\n\n(Timestamp: 39:16)\nBy applying Partial Dependence, the signal emerges from the noise. The blue line clearly shows that as price increases, sales generally decrease.\nCrucially, the plot reveals a non-linear drop at exactly $3.50. The speaker interprets this as a psychological threshold where customers decide ‚Äúmaybe I‚Äôll buy something else.‚Äù This insight‚Äîa specific price point where demand collapses‚Äîis only visible through this interpretability technique.\n\n\n49. Distributions and Partial Dependence\n\n\n\nSlide 49\n\n\n(Timestamp: 40:30)\nA warning is issued regarding Distributions. Partial Dependence assumes you can vary a feature independently of others. However, if features are correlated, you might create impossible combinations (like a 5-year-old dragon that weighs 5 tons).\nMaking predictions on these ‚Äúimpossible‚Äù data points means extrapolating outside the training distribution, which can lead to unreliable explanations.\n\n\n50. Partial Dependence Conclusion\n\n\n\nSlide 50\n\n\n(Timestamp: 40:25)\nThe speaker concludes that Partial Dependence is a ‚Äúbest practice‚Äù for understanding feature behavior. References to Goldstein and Friedman (classic papers) are provided.\nThis tool answers the ‚Äúdirectionality‚Äù question, proving that the model aligns with domain knowledge (e.g., higher prices = lower sales).\n\n\n51. Predictions\n\n\n\nSlide 51\n\n\n(Timestamp: 41:52)\nThe final section focuses on Predictions. The speaker shows three dragons with their associated risk scores (9.1, 2.4, etc.).\nWhile the model successfully identifies the red dragon as high risk, the next logical question from a user is ‚ÄúWhy?‚Äù\n\n\n52. Predictions & Explanations\n\n\n\nSlide 52\n\n\n(Timestamp: 42:25)\nThis slide introduces Prediction Explanations. Alongside the score of 9.1, the model provides a list of contributing factors: ‚ÄúNumber of past kills‚Äù increased the score, while ‚ÄúGender‚Äù might have decreased it.\nThis moves from global interpretability (how the model works generally) to local interpretability (why this specific instance was scored this way).\n\n\n53. Floor Map with Readmission Probability\n\n\n\nSlide 53\n\n\n(Timestamp: 43:01)\nA real-world application is shown: a hospital dashboard predicting patient readmission. The interface doesn‚Äôt just show a risk score (63.7%); it lists the reasons (e.g., ‚ÄúAbdominal pain,‚Äù ‚ÄúMedical specialty unspecified‚Äù).\nThe speaker highlights that these explanations build Trust with end-users (nurses/doctors) and provide Context that helps them decide how to intervene, rather than just knowing that they should intervene.\n\n\n54. Local Interpretable Model-Agnostic Explanations (LIME)\n\n\n\nSlide 54\n\n\n(Timestamp: 45:01)\nThe speaker mentions LIME, one of the ‚Äútraditional‚Äù or early techniques for this type of explanation. LIME works by fitting a simple local model around a single prediction to approximate the complex model‚Äôs behavior.\n\n\n55. LIME Flaw: Explanations Should Be Identical\n\n\n\nSlide 55\n\n\n(Timestamp: 45:12)\nThe tone shifts to a critique of LIME. The slide asserts a fundamental requirement: ‚ÄúEXPLANATIONS SHOULD BE IDENTICAL‚Äù for the same data and same model.\nIf you ask the model twice why it predicted a score for the same dragon, the answer should be the same both times.\n\n\n56. LIME: Two Different Explanations\n\n\n\nSlide 56\n\n\n(Timestamp: 45:15)\nThis slide provides code evidence of LIME‚Äôs instability. Running LIME twice on the ‚ÄúSAME DATA, SAME MODEL‚Äù produces ‚ÄúTWO DIFFERENT EXPLANATIONS.‚Äù\nThis occurs because LIME relies on random sampling to build its local approximation. This randomness makes it unreliable for serious applications where consistency is required for trust.\n\n\n57. Explanations Should Have Fidelity\n\n\n\nSlide 57\n\n\n(Timestamp: 45:20)\nThe speaker argues that explanations must have Fidelity to the data. If two data points are very similar, their explanations should be similar. LIME often fails this test, producing vastly different explanations for minor changes in input.\n\n\n58. LIME Isn‚Äôt Responsive to Data\n\n\n\nSlide 58\n\n\n(Timestamp: 45:22)\nFurther criticism of LIME. The slide suggests that LIME explanations sometimes lack ‚Äúlocal fidelity,‚Äù meaning the explanation doesn‚Äôt accurately reflect the model‚Äôs behavior in that specific region of the data.\n\n\n59. Anyone Relying on LIME is Toast\n\n\n\nSlide 59\n\n\n(Timestamp: 45:25)\nA blunt conclusion: ‚ÄúAnyone relying on LIME is toast.‚Äù The speaker strongly advises against using LIME due to these flaws, suggesting that while it was a pioneering method, it is no longer the standard for reliable interpretability.\n\n\n60. What Can We Learn From This?\n\n\n\nSlide 60\n\n\n(Timestamp: 45:30)\nThis slide summarizes the requirements for a good explanation method derived from LIME‚Äôs failures: consistency, accuracy, and fidelity. It sets the stage for introducing the superior method: Shapley values.\n\n\n61. Your Model or a Surrogate Model?\n\n\n\nSlide 61\n\n\n(Timestamp: 45:30)\nThe speaker questions whether we are explaining the actual model or a surrogate (approximation). LIME explains a surrogate. Ideally, we want to explain the actual model directly.\n\n\n62. What is Local?\n\n\n\nSlide 62\n\n\n(Timestamp: 45:30)\nAnother critique of LIME involves the definition of ‚Äúlocal.‚Äù The ‚Äúkernel width‚Äù is a hyperparameter that changes the explanation. If the explanation depends on how you tune the explainer, rather than just the data, it is problematic.\n\n\n63. Explanations Should Be Model Agnostic\n\n\n\nSlide 63\n\n\n(Timestamp: 45:06)\nThe speaker reiterates the requirement that the method must work for any model type (Trees, Neural Nets, SVMs). This is a strength of LIME, but also a requirement for its replacement.\n\n\n64. Explanations Should Be Fast\n\n\n\nSlide 64\n\n\n(Timestamp: 45:33)\nSpeed is critical. The slide compares LIME‚Äôs speed across datasets. If an explanation takes too long to generate, it cannot be used in real-time applications (like the hospital dashboard).\n\n\n65. Shapley Values for Explanations\n\n\n\nSlide 65\n\n\n(Timestamp: 45:36)\nThe speaker introduces Shapley Values as the modern standard. Originating from Game Theory (and Nobel Prize-winning economics), this method provides a mathematically sound way to attribute the ‚Äúmarginal effect‚Äù of features to a prediction.\n\n\n66. Shapley Values Metaphor: Pushing a Car\n\n\n\nSlide 66\n\n\n(Timestamp: 46:00)\nTo explain the concept, the speaker uses a metaphor: Pushing a car stuck in the snow. It‚Äôs a cooperative game. Several people (features) are pushing to achieve an outcome (moving the car/making a prediction).\nThe goal is to determine how much each person contributed. Did the teenager actually push, or just stand there?\n\n\n67. Intuition of Shapley Values\n\n\n\nSlide 67\n\n\n(Timestamp: 47:25)\nThe speaker expands the metaphor. If ‚ÄúThe Rock‚Äù joins the pushing, he might only need to add a small amount of force (10 units) to get the car moving because the others are already pushing.\nHowever, if The Rock was pushing alone, he would contribute much more. Shapley values calculate the average contribution across all possible ‚Äúcoalitions‚Äù (combinations of people pushing).\n\n\n68. Calculating Average Contribution\n\n\n\nSlide 68\n\n\n(Timestamp: 46:50)\nThe slide visually represents different scenarios (orders of arrival). The contribution of a person depends on who is already there. Shapley values ‚Äúunpack‚Äù this by averaging the marginal contribution of a feature across all possible permutations of features.\n\n\n69. Calculating Shapley Values: Subsets\n\n\n\nSlide 69\n\n\n(Timestamp: 48:43)\nMathematically, this means looking at all possible subsets of features. The slide lists the combinations (A alone, B alone, A+B, etc.) and the model output (‚ÄúForce‚Äù) for each.\n\n\n70. Calculating Shapley Values: Marginal Contributions\n\n\n\nSlide 70\n\n\n(Timestamp: 48:51)\nBy comparing the output of a subset with a feature to the subset without it, we find the marginal contribution for that specific scenario.\n\n\n71. Calculating Shapley Values: The Average\n\n\n\nSlide 71\n\n\n(Timestamp: 48:54)\nThe final Shapley value is the average of these marginal contributions. This provides a fair distribution of credit among the features that sums up to the total prediction.\n\n\n72. Shapley Values Formula\n\n\n\nSlide 72\n\n\n(Timestamp: 48:40)\nThe slide presents the formal mathematical formula. It is defined as the ‚Äúaverage marginal contribution of a feature with respect to all subsets of other features.‚Äù While complex, it guarantees unique properties like consistency that LIME lacks.\n\n\n73. Shapley Values for Feature Attribution\n\n\n\nSlide 73\n\n\n(Timestamp: 49:05)\nApplying this to Machine Learning: The ‚ÄúGame‚Äù is the prediction task. The ‚ÄúPlayers‚Äù are the features. The ‚ÄúPayout‚Äù is the prediction score.\nThe slide shows a Boston Housing prediction. The Shapley values tell us that for this specific house, the ‚ÄúLSTAT‚Äù feature pushed the price down, while ‚ÄúRM‚Äù (rooms) pushed it up, relative to the average house price.\n\n\n74. So Many Methods for Shapley Values\n\n\n\nSlide 74\n\n\n(Timestamp: 50:35)\nThe speaker notes that calculating exact Shapley values is computationally expensive (2^N combinations). Therefore, many approximation methods exist. The slide lists implementations in R (iml, fastshap) and Python (shap).\n\n\n75. Calculating Shapley Values - Linear Model\n\n\n\nSlide 75\n\n\n(Timestamp: 50:10)\nFor a simple Linear Model, Shapley values are easy to calculate. Because features in a linear model are additive and independent (conceptually), the coefficient * value roughly equals the contribution.\n\n\n76. Linear Model Example\n\n\n\nSlide 76\n\n\n(Timestamp: 50:12)\nThe slide shows that if you change the Age, the prediction changes by a specific amount. In linear models, the difference between the prediction and the baseline is simply the sum of these changes.\n\n\n77. Simple to Get Shapley Values for Linear Model\n\n\n\nSlide 77\n\n\n(Timestamp: 50:15)\nThis reinforces that for linear models, we don‚Äôt need complex approximations. The structure of the model allows for exact calculation easily.\n\n\n78. Shapley Values for Trees: Tree Shap\n\n\n\nSlide 78\n\n\n(Timestamp: 50:45)\nFor tree-based models (Random Forest, XGBoost, LightGBM), there is a specific, fast algorithm called Tree SHAP (developed by Scott Lundberg). It computes exact Shapley values in polynomial time by leveraging the tree structure, making it feasible for large models.\n\n\n79. Tree Shap Calculation\n\n\n\nSlide 79\n\n\n(Timestamp: 50:45)\nThis slide visualizes how Tree SHAP works by tracing paths down the decision tree to calculate expectations. This efficiency is why SHAP has become the industry standard for boosting models.\n\n\n80. Approximating Shapley Values\n\n\n\nSlide 80\n\n\n(Timestamp: 50:55)\nFor other ‚ÄúBlack Box‚Äù models (like Neural Networks or SVMs) where exact calculation is intractable due to the combinatorial explosion (100 features = impossible to compute all subsets), we must use approximations.\n\n\n81. Approximating Shapley Values: Strumbelj\n\n\n\nSlide 81\n\n\n(Timestamp: 51:04)\nOne method is Strumbelj‚Äôs algorithm, a sampling-based approach. It uses Monte Carlo sampling to estimate the difference between predictions with and without a feature, approximating the average marginal contribution.\n\n\n82. Strumbelj Visualization\n\n\n\nSlide 82\n\n\n(Timestamp: 51:04)\nThe slide visualizes the sampling process: creating synthetic instances by mixing the feature of interest with random values from the dataset to estimate its effect.\n\n\n83. Approximating Shapley Values: Shap Kernel\n\n\n\nSlide 83\n\n\n(Timestamp: 51:04)\nKernel SHAP is introduced as a model-agnostic method. It connects LIME and Shapley values. It uses a weighted linear regression (like LIME) but uses specific ‚ÄúShapley weights‚Äù to ensure the result is a valid Shapley value approximation.\n\n\n84. Shap Kernel: Generating Data (1)\n\n\n\nSlide 84\n\n\n(Timestamp: 51:15)\nNote: The speaker skips detailed explanations of these calculation slides due to time constraints, but the slides detail the technical steps.\nThis slide shows the setup for Kernel SHAP, defining a ‚Äúbackground dataset‚Äù to serve as the reference value for ‚Äúmissing‚Äù features.\n\n\n85. Shap Kernel: Generating Data (2)\n\n\n\nSlide 85\n\n\n(Timestamp: 51:15)\nThe method involves treating features as ‚Äúmissing‚Äù by replacing them with background values to simulate their absence from a coalition.\n\n\n86. Shap Kernel: Generating Data (3)\n\n\n\nSlide 86\n\n\n(Timestamp: 51:15)\nPermutations of feature coalitions are generated to create a synthetic dataset for the local regression.\n\n\n87. Shap Kernel: Generating Data (4)\n\n\n\nSlide 87\n\n\n(Timestamp: 51:15)\nA linear model is fit to this synthetic data. The coefficients of this linear model, when weighted correctly, correspond to the Shapley values.\n\n\n88. Shap Kernel: Generating Data (5)\n\n\n\nSlide 88\n\n\n(Timestamp: 51:15)\nThe result is the attribution value for the specific prediction.\n\n\n89. Mimic Shap\n\n\n\nSlide 89\n\n\n(Timestamp: 51:15)\nMimic SHAP is another approximation where a global surrogate model (like a Gradient Boosted Tree) is trained to mimic the black box, and then Tree SHAP is used on the surrogate.\n\n\n90. Gradient Shap\n\n\n\nSlide 90\n\n\n(Timestamp: 51:15)\nGradient SHAP is designed for Deep Learning models (differentiable models). It combines Integrated Gradients with Shapley values for efficient computation in neural networks.\n\n\n91. GkmExplain\n\n\n\nSlide 91\n\n\n(Timestamp: 51:15)\nA specialized method for non-linear Support Vector Machines (SVMs).\n\n\n92. DASP\n\n\n\nSlide 92\n\n\n(Timestamp: 51:15)\nDASP is a polynomial-time algorithm for approximating Shapley values specifically in Deep Neural Networks.\n\n\n93. Aggregating Shapley Values: Feature Importance\n\n\n\nSlide 93\n\n\n(Timestamp: 51:32)\nThe speaker returns to practical applications. Once you have local SHAP values for every prediction, you can aggregate them.\nBy summing the absolute SHAP values across all data points, you get a global Feature Importance plot. This tells you which features are most important overall, derived directly from the local explanations.\n\n\n94. Aggregating Shapley Values: Feature Interactions\n\n\n\nSlide 94\n\n\n(Timestamp: 51:46)\nSHAP can also quantify Interactions. The slide shows the interaction between Age and Sex. It reveals that for males, a certain age range increases risk (prediction), whereas for females, it might be different.\nThis allows data scientists to see exactly how features modify each other‚Äôs effects, solving the problem of hidden interactions in complex models.\n\n\n95. Aggregating Shapley Values: Feature Selection\n\n\n\nSlide 95\n\n\n(Timestamp: 52:24)\nSHAP values can be used for Feature Selection. By ranking features by their mean absolute SHAP value, you can identify the top contributors and remove noise variables, potentially simplifying the model without losing accuracy.\n\n\n96. Aggregating Shapley Values: Supervised Clustering\n\n\n\nSlide 96\n\n\n(Timestamp: 52:45)\nA ‚Äúcool advanced technique‚Äù is Explanation Clustering (Supervised Clustering). Instead of clustering the raw data, you cluster the explanations (the SHAP values).\nThis groups data points not by their raw values, but by why the model made a prediction for them. This can reveal distinct subpopulations or ‚Äúreasons‚Äù for high risk (e.g., a group of high-risk dragons due to age vs.¬†a group due to weight).\n\n\n97. Model Agnostic Explanation Tools Summary\n\n\n\nSlide 97\n\n\n(Timestamp: 53:26)\nThe presentation wraps up by reviewing the three key tools covered: 1. Feature Importance (Permutation based) 2. Partial Dependence (for directionality) 3. Prediction Explanations (Shapley Values)\nThe speaker encourages the audience to use these tools to build trust and understanding in their machine learning workflows.\n\n\n98. Question Time\n\n\n\nSlide 98\n\n\n(Timestamp: 53:34)\nThe final slide opens the floor for questions and provides contact information. The speaker mentions that the slides and notebooks (including the age/milk and LIME examples) are available on his GitHub for those who want to explore the code.\n\nThis annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations."
  }
]