{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building Worlds for Reinforcement Learning\n",
        "\n",
        "### Introduction\n",
        "\n",
        "[OpenAI’s Gym](https://gym.openai.com/) places reinforcement learning\n",
        "into the masses. It comes with a wealth of environments from the classic\n",
        "cart pole, board games, Atari, and now the new\n",
        "[Universe](https://universe.openai.com/) which adds flash games and PC\n",
        "Games like GTA or Portal. This is great news, but for someone starting\n",
        "out, working on some of these games is overkill. You can learn a lot\n",
        "more in a shorter time, by playing around with some smaller toy\n",
        "environments.\n",
        "\n",
        "One area I like within the gym environments are the classic control\n",
        "problems (besides the fun of [eating melon and\n",
        "poop](http://tinyurl.com/eatmelon)). These are great problems for\n",
        "understanding the basics of reinforcement learning because we\n",
        "intutiively understand the rewards and they run really fast. Its not\n",
        "like pong that can take several days to train, instead, you can train\n",
        "these environments within minutes!\n",
        "\n",
        "If you aren’t happy with the current environments, it is possible to\n",
        "modify and even add more environments. In this post, I will highlight\n",
        "other environments and share how I modified an Acrobot-v1 environment.\n",
        "\n",
        "### RLPy\n",
        "\n",
        "To begin, grab the [repo for the OpenAI\n",
        "gym](https://github.com/openai/gym). Inside the repo, navigate to\n",
        "`gym/envs/classic_control`where you will see the scripts that define the\n",
        "class control environments. If you open one of the scripts, you will see\n",
        "a heading on the top that says:\n",
        "\n",
        "`__copyright__ = \"Copyright 2013, RLPy http://acl.mit.edu/RLPy\"`\n",
        "\n",
        "Ahh! In the spirit of open source, OpenAI stands on the shoulders of\n",
        "another reinforcement library,\n",
        "[RLPy](http://rlpy.readthedocs.io/en/latest/). You can learn a lot more\n",
        "about them at the RLPy site or take a look at their\n",
        "[github](https://github.com/rlpy/rlpy). If you browse here, you can find\n",
        "the original script that was used in OpenAI under `rlpy /rlpy/Domains`.\n",
        "The interesting thing here is that there are a ton more interesting\n",
        "reinforcement problems!\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/RLlisting.png\"\n",
        "alt=\"RLlisting\" />\n",
        "<figcaption aria-hidden=\"true\">RLlisting</figcaption>\n",
        "</figure>\n",
        "\n",
        "You can run these using RLPy or you can try and hack this into OpenAI.\n",
        "\n",
        "### Modifying OpenAI Environments\n",
        "\n",
        "I decided to modify the\n",
        "[Acrobot](https://gym.openai.com/envs/Acrobot-v1) environment. Acrobot\n",
        "is a 2-link pendulum with only the second joint actuated (it has three\n",
        "states, left, right, and no movement). The goal is to swing the end to a\n",
        "height of at least one link above the base. If you look at the\n",
        "leaderboard on OpenAIs site, they meet that criterion, but its not very\n",
        "impressive. Here is the current [highest scoring\n",
        "entry](https://gym.openai.com/evaluations/eval_Ig1wrPzQlGipmBAhZ5Tw):\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/training.gif\"\n",
        "alt=\"training\" />\n",
        "<figcaption aria-hidden=\"true\">training</figcaption>\n",
        "</figure>\n",
        "\n",
        "This is way boring compared to what Hardmaru shows in his\n",
        "[demo](http://otoro.net/ml/pendulum-cne/), where a pendulum is capable\n",
        "of balancing for a short\n",
        "time.![hardmaru](https://rajivshah.com/blog/images/hardmaru.gif)\n",
        "\n",
        "So I decided to try and modify the Acrobot demo to make this task a\n",
        "little more interesting, [Acrobot gist\n",
        "here](https://gist.github.com/rajshah4/677698166b860bc11d34507ee18b2d41).\n",
        "The main change was to the reward system. I added a variable\n",
        "`steps_beyond_done` that would keep track of successes when the end was\n",
        "swung high. I also changed the reward structure, so it would gradually\n",
        "be rewarded as it swung higher. I also changed g to 0, this removes\n",
        "gravity’s effect.\n",
        "\n",
        "    self.rewardx = (-np.cos(s[0]) - np.cos(s[1] + s[0])) ##Swung height is calculated \n",
        "    if self.rewardx < .5:\n",
        "        reward = -1.\n",
        "        self.steps_beyond_done = 0\n",
        "    if (self.rewardx > .5 and self.rewardx < .8):\n",
        "        reward = -0.8\n",
        "        self.steps_beyond_done = 0  \n",
        "    if self.rewardx > .8:\n",
        "        reward = -0.6 \n",
        "    if self.rewardx > 1:\n",
        "        reward = -0.4\n",
        "        self.steps_beyond_done += 1 \n",
        "    if self.steps_beyond_done > 4:\n",
        "        reward = -0.2\n",
        "    if self.steps_beyond_done > 8:\n",
        "        reward = -0.1\n",
        "    if self.steps_beyond_done > 12:\n",
        "        reward = 0.\n",
        "\n",
        "Another important file to be aware of is where the benchmarks are kept\n",
        "for each environment. You can navigate to this at\n",
        "`gym/gym/benchmarks/__init__.py`Within this file, you will see the\n",
        "following:\n",
        "\n",
        "    {'env_id': 'Acrobot-v1',\n",
        "             'trials': 3,\n",
        "             'max_timesteps': 100000,\n",
        "             'reward_floor': -500.0,\n",
        "             'reward_ceiling': 0.0,\n",
        "            },\n",
        "\n",
        "I then ran an [implementation of Asynchronous Advantage Actor Critic\n",
        "A3C)](https://github.com/arnomoonens/DeepRL) by Arno Moonens. After\n",
        "running for a half hour, you can see the improvement in the algorithm:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/training1.gif\"\n",
        "alt=\"training1\" />\n",
        "<figcaption aria-hidden=\"true\">training1</figcaption>\n",
        "</figure>\n",
        "\n",
        "Now a half hour later:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/training2.gif\"\n",
        "alt=\"training2\" />\n",
        "<figcaption aria-hidden=\"true\">training2</figcaption>\n",
        "</figure>\n",
        "\n",
        "The result is teaching the pendulum to stay up for an extended time!\n",
        "This is much more interesting and what I was looking for. I hope this\n",
        "will inspire others to build new and interesting environments."
      ],
      "id": "1675728e-8e2f-4e81-bbad-b63f4a9b9ddd"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}