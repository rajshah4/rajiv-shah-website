<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-11-01">

<title>A Practical Guide to Evaluating Generative AI Applications – Rajiv Shah - rajistics blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ddb7102b129bb408a3919432018bab43.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-475ad4fe1e4ce2c827a237f0e4cf2c17.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-ddb7102b129bb408a3919432018bab43.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Rajiv Shah - rajistics blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://www.rajivshah.com"> 
<span class="menu-text"><u>About Me</u></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/rajistics/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/rajistics"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/rajistics/"> <i class="bi bi-instagram" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.tiktok.com/@rajistics"> <i class="bi bi-tiktok" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCu9fxVjTz5AJO7FR1upY02w"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rajshah4"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A Practical Guide to Evaluating Generative AI Applications</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">GenAI</div>
                <div class="quarto-category">Evaluation</div>
                <div class="quarto-category">LLM</div>
                <div class="quarto-category">Testing</div>
                <div class="quarto-category">Annotated Talk</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 1, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#video" id="toc-video" class="nav-link active" data-scroll-target="#video">Video</a></li>
  <li><a href="#annotated-presentation" id="toc-annotated-presentation" class="nav-link" data-scroll-target="#annotated-presentation">Annotated Presentation</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="genai-evaluation-guide.ipynb" download="genai-evaluation-guide.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<section id="video" class="level2">
<h2 class="anchored" data-anchor-id="video">Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/qPHsWTZP58U" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Watch the <a href="https://youtu.be/qPHsWTZP58U">full video</a> | <a href="https://youtu.be/qPHsWTZP58U">Slides</a></p>
<hr>
</section>
<section id="annotated-presentation" class="level2">
<h2 class="anchored" data-anchor-id="annotated-presentation">Annotated Presentation</h2>
<p>Below is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.</p>
<p>Here is the annotated presentation for Rajiv Shah’s workshop on “Hill Climbing: Best Practices for Evaluating LLMs.”</p>
<section id="title-slide" class="level3">
<h3 class="anchored" data-anchor-id="title-slide">1. Title Slide</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_1.png" class="img-fluid figure-img"></p>
<figcaption>Slide 1</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=0s">Timestamp: 00:00</a>)</p>
<p>This slide introduces the workshop titled <strong>“Hill Climbing: Best Practices for Evaluating LLMs,”</strong> presented by Rajiv Shah, PhD, at the Open Data Science Conference (ODSC). The presentation focuses on the technical nuances of Generative AI and how to build effective evaluation workflows.</p>
<p>Rajiv sets the stage by outlining his three main goals for the session: understanding the technical differences in GenAI evaluation, learning a basic introductory workflow for building evaluation datasets, and inspiring practitioners to start “learning by doing” rather than just reading papers.</p>
<p>The concept of “Hill Climbing” refers to the iterative process of improving LLM applications—starting with a baseline and continuously optimizing performance through rigorous testing and error analysis.</p>
</section>
<section id="evaluating-for-gen-ai-resources" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-for-gen-ai-resources">2. Evaluating for Gen AI Resources</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_2.png" class="img-fluid figure-img"></p>
<figcaption>Slide 2</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=6s">Timestamp: 00:06</a>)</p>
<p>This slide provides a QR code and a GitHub URL, directing the audience to the code and resources associated with the talk. It emphasizes that the workshop is practical, with code examples available for attendees to replicate the evaluation techniques discussed.</p>
<p>Rajiv encourages the audience to access these resources to follow along with the technical implementations of the concepts, such as building LLM judges and creating unit tests, which will be covered later in the presentation.</p>
</section>
<section id="customer-support-use-case" class="level3">
<h3 class="anchored" data-anchor-id="customer-support-use-case">3. Customer Support Use Case</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_3.png" class="img-fluid figure-img"></p>
<figcaption>Slide 3</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=48s">Timestamp: 00:48</a>)</p>
<p>To motivate the need for evaluation, the presentation introduces a common real-world use case: <strong>Customer Support</strong>. Generative AI is frequently deployed to help agents compose emails or chat responses based on user inquiries.</p>
<p>This scenario serves as the baseline example throughout the talk. It represents a high-volume task where automation is desirable, but accuracy and tone are critical for maintaining customer satisfaction and brand reputation.</p>
</section>
<section id="vibe-coding" class="level3">
<h3 class="anchored" data-anchor-id="vibe-coding">4. Vibe Coding</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_4.png" class="img-fluid figure-img"></p>
<figcaption>Slide 4</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=59s">Timestamp: 00:59</a>)</p>
<p>This slide introduces the concept of <strong>“Vibe Coding”</strong>—the initial phase where developers grab a simple prompt, feed it to a model, and get a result that feels right. It highlights the misconception that GenAI is easy because it works “out of the box” for simple demos.</p>
<p>Rajiv notes that while “vibe coding” might work for a quick demo app, it is insufficient for production systems. Relying on a “vibe” that the model is working prevents teams from catching subtle failures that occur at scale.</p>
</section>
<section id="good-response-delayed-order" class="level3">
<h3 class="anchored" data-anchor-id="good-response-delayed-order">5. Good Response: Delayed Order</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_5.png" class="img-fluid figure-img"></p>
<figcaption>Slide 5</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=70s">Timestamp: 01:10</a>)</p>
<p>Here, we see a successful output generated by the LLM. The customer inquired about a delayed order, and the AI generated a polite, relevant response acknowledging the delay and apologizing.</p>
<p>This example reinforces the “Vibe Coding” trap: because the model often produces high-quality, human-sounding text like this, developers can be lulled into a false sense of security regarding the system’s reliability.</p>
</section>
<section id="good-response-damaged-product" class="level3">
<h3 class="anchored" data-anchor-id="good-response-damaged-product">6. Good Response: Damaged Product</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_6.png" class="img-fluid figure-img"></p>
<figcaption>Slide 6</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=72s">Timestamp: 01:12</a>)</p>
<p>This slide provides another example of a “good” response. The AI correctly identifies that the customer received a damaged product and initiates a replacement protocol.</p>
<p>These positive examples establish a baseline of expected behavior. The challenge in evaluation is not just confirming that the model <em>can</em> work, but ensuring it works consistently across all edge cases.</p>
</section>
<section id="bad-response-irrelevance" class="level3">
<h3 class="anchored" data-anchor-id="bad-response-irrelevance">7. Bad Response: Irrelevance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_7.png" class="img-fluid figure-img"></p>
<figcaption>Slide 7</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=86s">Timestamp: 01:26</a>)</p>
<p>The presentation shifts to failure modes. In this example, the user asks about an <strong>“Order Delay,”</strong> but the AI responds with information about a <strong>“New Product Launch.”</strong></p>
<p>This illustrates a complete context mismatch. The model failed to attend to the user’s intent, generating a coherent but completely irrelevant response. This type of failure frustrates users and degrades trust in the automated system.</p>
</section>
<section id="bad-response-hallucination" class="level3">
<h3 class="anchored" data-anchor-id="bad-response-hallucination">8. Bad Response: Hallucination</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_8.png" class="img-fluid figure-img"></p>
<figcaption>Slide 8</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=96s">Timestamp: 01:36</a>)</p>
<p>This slide shows a more dangerous failure: <strong>Hallucination</strong>. The AI apologizes for a defective “espresso machine,” but as the speaker notes, “We don’t actually sell espresso machines.”</p>
<p>This highlights the risk of the model fabricating facts to be helpful. Such errors can lead to logistical nightmares, such as customers expecting replacements for products that do not exist or that the company never sold.</p>
</section>
<section id="risks-of-llm-mistakes" class="level3">
<h3 class="anchored" data-anchor-id="risks-of-llm-mistakes">9. Risks of LLM Mistakes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_9.png" class="img-fluid figure-img"></p>
<figcaption>Slide 9</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=111s">Timestamp: 01:51</a>)</p>
<p>Rajiv categorizes the risks associated with LLM failures into three buckets: <strong>Reputational, Legal, and Financial</strong>. He cites the example of <strong>Cursor</strong>, an IDE company, where a support bot hallucinated a policy restricting users to one device, causing customers to cancel subscriptions.</p>
<p>The slide emphasizes that courts may view AI agents as employees; if a bot makes a promise (like a refund or policy change), the company might be legally bound to honor it. This escalates evaluation from a technical nice-to-have to a business necessity.</p>
</section>
<section id="the-despair-of-gen-ai" class="level3">
<h3 class="anchored" data-anchor-id="the-despair-of-gen-ai">10. The Despair of Gen AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=158s">Timestamp: 02:38</a>)</p>
<p>This visual represents the frustration developers feel when moving from a successful demo to a failing production system. The “despair” comes from the realization that the stochastic nature of LLMs makes them difficult to control.</p>
<p>It serves as an emotional anchor for the audience, acknowledging that while GenAI is exciting, the unpredictability of its failures causes significant stress for engineering teams responsible for deployment.</p>
</section>
<section id="high-failure-rates" class="level3">
<h3 class="anchored" data-anchor-id="high-failure-rates">11. High Failure Rates</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=168s">Timestamp: 02:48</a>)</p>
<p>The slide cites an MIT report stating that <strong>“95% of GenAI pilots are failing.”</strong> While Rajiv notes this number might be overstated, it reflects a trend where executives are demanding ROI and seeing lackluster results.</p>
<p>This shift in 2025 means that evaluation is no longer just for debugging; it is required to prove business value and justify the high costs of running Generative AI infrastructure.</p>
</section>
<section id="evaluation-improves-applications" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-improves-applications">12. Evaluation Improves Applications</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=194s">Timestamp: 03:14</a>)</p>
<p>This slide asserts the core thesis: <strong>Evaluation helps you build better GenAI applications.</strong> It references a previous viral video by the speaker on the same topic, positioning this talk as an updated, condensed version with fresh content.</p>
<p>Rajiv explains that you cannot improve what you cannot measure. Without a robust evaluation framework, developers are essentially guessing whether changes to prompts or models are actually improving performance.</p>
</section>
<section id="why-evaluation-is-necessary" class="level3">
<h3 class="anchored" data-anchor-id="why-evaluation-is-necessary">13. Why Evaluation is Necessary</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=220s">Timestamp: 03:40</a>)</p>
<p>This concentric diagram illustrates the stakeholders involved in evaluation. It starts with <strong>“Things Go Wrong”</strong> (technical reality), moves to <strong>“Buy-in”</strong> (convincing managers/teams), and ends with <strong>“Regulators”</strong> (external compliance).</p>
<p>Evaluation serves multiple audiences: it helps the developer debug, it provides the metrics needed to convince management that the app is production-ready, and it creates the audit trails required by third-party auditors or regulators.</p>
</section>
<section id="evaluation-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-dimensions">14. Evaluation Dimensions</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=258s">Timestamp: 04:18</a>)</p>
<p>Evaluation must cover three dimensions: <strong>Technical</strong> (F1 scores, accuracy), <strong>Business</strong> (ROI, value generated), and <strong>Operational</strong> (Total Cost of Ownership, latency).</p>
<p>Rajiv highlights that data scientists often focus solely on the technical, but ignoring operational costs (like the expense of hosting GPUs vs.&nbsp;using APIs) can kill a project. A comprehensive evaluation strategy considers the cost-to-quality ratio.</p>
</section>
<section id="public-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="public-benchmarks">15. Public Benchmarks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_15.png" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=306s">Timestamp: 05:06</a>)</p>
<p>The slide discusses <strong>Public Benchmarks</strong> (like MMLU, GSM8K). While useful for a general idea of a model’s capabilities (e.g., “Is Llama 3 better than Llama 2?”), they are insufficient for specific applications.</p>
<p>Rajiv warns against using these benchmarks to determine if a model fits <em>your</em> specific use case. Companies promote these numbers for marketing, but they rarely reflect performance on proprietary business data.</p>
</section>
<section id="custom-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="custom-benchmarks">16. Custom Benchmarks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=322s">Timestamp: 05:22</a>)</p>
<p>The solution to the limitations of public benchmarks is <strong>Custom Benchmarks</strong>. This slide defines a benchmark as a combination of a <strong>Task</strong>, a <strong>Dataset</strong>, and an <strong>Evaluation Metric</strong>.</p>
<p>This is a critical definition for the workshop. To “tame” GenAI, you must build a dataset that reflects your specific customer queries and define success metrics that matter to your business logic, rather than relying on generic academic tests.</p>
</section>
<section id="taming-gen-ai" class="level3">
<h3 class="anchored" data-anchor-id="taming-gen-ai">17. Taming Gen AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_17.png" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=328s">Timestamp: 05:28</a>)</p>
<p>This title slide signals a transition into the technical “how-to” section of the talk. “Taming” implies that the default state of GenAI is wild and unpredictable.</p>
<p>The goal of the following sections is to bring structure and control to this chaos through rigorous engineering practices and evaluation workflows.</p>
</section>
<section id="workshop-roadmap" class="level3">
<h3 class="anchored" data-anchor-id="workshop-roadmap">18. Workshop Roadmap</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_18.png" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=331s">Timestamp: 05:31</a>)</p>
<p>The roadmap outlines the four main sections of the talk: 1. <strong>Basics of Gen AI:</strong> Understanding variability and technical nuances. 2. <strong>Evaluation Workflow:</strong> Building the dataset and running the first tests. 3. <strong>More Complexity:</strong> Adding unit tests and conducting error analysis. 4. <strong>Agents:</strong> Evaluating complex, multi-step workflows.</p>
</section>
<section id="variability-in-responses" class="level3">
<h3 class="anchored" data-anchor-id="variability-in-responses">19. Variability in Responses</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_19.png" class="img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=360s">Timestamp: 06:00</a>)</p>
<p>This slide visually demonstrates the <strong>Non-Determinism</strong> of LLMs. It shows two responses to the same prompt generated just minutes apart. While substantively similar, the wording and structure differ slightly.</p>
<p>This variability makes exact string matching (a common software testing technique) impossible for LLMs. It necessitates semantic evaluation techniques, which complicates the testing pipeline.</p>
</section>
<section id="input-model-output-diagram" class="level3">
<h3 class="anchored" data-anchor-id="input-model-output-diagram">20. Input-Model-Output Diagram</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_20.png" class="img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=384s">Timestamp: 06:24</a>)</p>
<p>A simple diagram illustrates the flow: <strong>Prompt -&gt; Model -&gt; Output</strong>. Rajiv uses this to structure the analysis of where variability comes from.</p>
<p>He explains that “chaos” can enter the system at any of these three stages: the input (prompt sensitivity), the model (inference non-determinism), or the output (formatting and evaluation).</p>
</section>
<section id="inconsistent-benchmark-scores" class="level3">
<h3 class="anchored" data-anchor-id="inconsistent-benchmark-scores">21. Inconsistent Benchmark Scores</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_21.png" class="img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=404s">Timestamp: 06:44</a>)</p>
<p>The slide presents a discrepancy between benchmark scores tweeted by Hugging Face and those in the official Llama paper. Both used the same dataset (MMLU), but reported different accuracy numbers.</p>
<p>This introduces the problem of <strong>Evaluation Harness Sensitivity</strong>. Even with standard benchmarks, <em>how</em> you ask the model to take the test changes the score, proving that evaluation is fragile and implementation-dependent.</p>
</section>
<section id="mmlu-overview" class="level3">
<h3 class="anchored" data-anchor-id="mmlu-overview">22. MMLU Overview</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_22.png" class="img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=445s">Timestamp: 07:25</a>)</p>
<p><strong>MMLU (Massive Multitask Language Understanding)</strong> is explained here. It is a multiple-choice test covering 57 tasks across STEM, the humanities, and more.</p>
<p>It is currently the standard for measuring general “intelligence” in models. However, because it is a multiple-choice format, it is susceptible to prompt formatting nuances, as the next slides demonstrate.</p>
</section>
<section id="prompt-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="prompt-sensitivity">23. Prompt Sensitivity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_23.png" class="img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=464s">Timestamp: 07:44</a>)</p>
<p>This slide reveals <em>why</em> the scores in Slide 21 differed. The three evaluation harnesses used slightly different prompt structures (e.g., using the word “Question” vs.&nbsp;just listing the text).</p>
<p>These minor changes resulted in significant accuracy shifts. This proves that LLMs are highly sensitive to syntax, meaning a “better” model might just be one that was prompted more effectively for the test, not one that is actually smarter.</p>
</section>
<section id="formatting-changes" class="level3">
<h3 class="anchored" data-anchor-id="formatting-changes">24. Formatting Changes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_24.png" class="img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=502s">Timestamp: 08:22</a>)</p>
<p>Expanding on sensitivity, this slide references Anthropic’s research showing that changing answer choices from <code>(A)</code> to <code>[A]</code> or <code>(1)</code> affects the output.</p>
<p>This level of fragility is a key takeaway: seemingly cosmetic changes in how inputs are formatted can alter the model’s reasoning capabilities or its ability to output the correct token.</p>
</section>
<section id="gpt-4o-performance-drop" class="level3">
<h3 class="anchored" data-anchor-id="gpt-4o-performance-drop">25. GPT-4o Performance Drop</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_25.png" class="img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=518s">Timestamp: 08:38</a>)</p>
<p>A bar chart demonstrates that this issue persists even in state-of-the-art models like <strong>GPT-4o</strong>. Subtle changes in wording can lead to a 5-10% drop in performance.</p>
<p>This counters the assumption that newer, larger models have “solved” prompt sensitivity. It remains a persistent variable that evaluators must control for.</p>
</section>
<section id="tone-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="tone-sensitivity">26. Tone Sensitivity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_26.png" class="img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=526s">Timestamp: 08:46</a>)</p>
<p>This slide shows that the <strong>tone</strong> of a prompt (e.g., being polite vs.&nbsp;direct) affects accuracy. Rajiv jokes, “I guess this is why mom always said to be polite.”</p>
<p>The graph indicates that prompt engineering strategies, like adding emotional weight or politeness, can statistically alter model performance, adding another layer of complexity to evaluation.</p>
</section>
<section id="persistent-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="persistent-sensitivity">27. Persistent Sensitivity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_27.png" class="img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=540s">Timestamp: 09:00</a>)</p>
<p>The slide reiterates that despite years of progress, models are still sensitive to specific phrases. It shows a “Prompt Engineering” guide suggesting specific words to use.</p>
<p>The takeaway is that developers cannot treat the prompt as a static instruction; it is a hyperparameter that requires optimization and constant testing.</p>
</section>
<section id="falcon-llm-bias" class="level3">
<h3 class="anchored" data-anchor-id="falcon-llm-bias">28. Falcon LLM Bias</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_28.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=558s">Timestamp: 09:18</a>)</p>
<p>This slide introduces a case study with the <strong>Falcon LLM</strong>. A user tweet shows the model recommending <strong>Abu Dhabi</strong> as a technological city with glowing sentiment, which raised suspicions about bias given the model’s origin in the Middle East.</p>
<p>This serves as a detective story: users wondered if the model weights were altered or if specific training data was injected to force this positive association.</p>
</section>
<section id="potential-cover-up" class="level3">
<h3 class="anchored" data-anchor-id="potential-cover-up">29. Potential Cover-up?</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_29.png" class="img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=590s">Timestamp: 09:50</a>)</p>
<p>Another tweet speculates if the model is “covering up human rights abuses” because it provides different answers for Abu Dhabi compared to other cities.</p>
<p>This highlights how model behavior can be misinterpreted as malicious bias or censorship, when the root cause might be something much simpler in the input stack.</p>
</section>
<section id="inspecting-the-system-prompt" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-the-system-prompt">30. Inspecting the System Prompt</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_30.png" class="img-fluid figure-img"></p>
<figcaption>Slide 30</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=600s">Timestamp: 10:00</a>)</p>
<p>The reveal: The bias wasn’t in the weights, but in the <strong>System Prompt</strong>. The slide suggests looking at the hidden instructions given to the model.</p>
<p>In Falcon’s case, the system prompt explicitly told the model, “You are a model built in Abu Dhabi.” This context influenced its generation probabilities, causing it to favor Abu Dhabi in its responses.</p>
</section>
<section id="claude-system-prompt" class="level3">
<h3 class="anchored" data-anchor-id="claude-system-prompt">31. Claude System Prompt</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_31.png" class="img-fluid figure-img"></p>
<figcaption>Slide 31</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=633s">Timestamp: 10:33</a>)</p>
<p>Rajiv points out that most developers never read the system prompts of the models they use. He highlights the <strong>Claude System Prompt</strong>, which is 1700 words long and takes nearly 10 minutes to read.</p>
<p>These extensive instructions define the model’s personality and safety guardrails. Ignoring them means you don’t fully understand the inputs driving your application’s behavior.</p>
</section>
<section id="complexity-of-a-single-response" class="level3">
<h3 class="anchored" data-anchor-id="complexity-of-a-single-response">32. Complexity of a Single Response</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_32.png" class="img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=660s">Timestamp: 11:00</a>)</p>
<p>The diagram is updated to show that a “single response” is actually the result of complex interactions: <strong>Tokenization -&gt; Prompt Styles -&gt; Prompt Engineering -&gt; System Prompt</strong>.</p>
<p>This visual summarizes the “Input” section of the talk, reinforcing that before the model even processes data, multiple layers of text transformation occur that can alter the result.</p>
</section>
<section id="inter-text-similarity" class="level3">
<h3 class="anchored" data-anchor-id="inter-text-similarity">33. Inter-text Similarity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_33.png" class="img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=675s">Timestamp: 11:15</a>)</p>
<p>This heatmap compares <strong>Inter-text similarity</strong> between models. It highlights Llama 70B and Llama 8B. Even though they are from the same family and likely trained on similar data, they are not identical.</p>
<p>This means you cannot swap a smaller model for a larger one (or vice versa) and expect the exact same behavior. Any model change requires a full re-evaluation.</p>
</section>
<section id="sycophantic-models" class="level3">
<h3 class="anchored" data-anchor-id="sycophantic-models">34. Sycophantic Models</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_34.png" class="img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=736s">Timestamp: 12:16</a>)</p>
<p>The slide discusses <strong>Sycophancy</strong>—the tendency of models to agree with the user even when the user is wrong. It mentions how early versions of GPT-4 were sometimes “overly nice.”</p>
<p>This behavior is a specific type of model bias that evaluators must watch for. If a user asks a leading question containing false premises, a sycophantic model might validate the falsehood rather than correct it.</p>
</section>
<section id="model-drift" class="level3">
<h3 class="anchored" data-anchor-id="model-drift">35. Model Drift</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_35.png" class="img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=757s">Timestamp: 12:37</a>)</p>
<p><strong>“Model Drift”</strong> refers to the phenomenon where commercial APIs (like OpenAI or Anthropic) change their model behavior over time without warning.</p>
<p>Because developers do not control the weights of API-based models, the “ground underneath them” can shift. A prompt that worked yesterday might fail today because the provider updated the backend or the inference infrastructure.</p>
</section>
<section id="degraded-responses-timeline" class="level3">
<h3 class="anchored" data-anchor-id="degraded-responses-timeline">36. Degraded Responses Timeline</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_36.png" class="img-fluid figure-img"></p>
<figcaption>Slide 36</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=775s">Timestamp: 12:55</a>)</p>
<p>This slide shows a timeline of <strong>Degraded Responses</strong> from an Anthropic incident. Technical issues like context window routing errors led to corrupted outputs for a period of days.</p>
<p>This illustrates that drift isn’t always about model updates; it can be infrastructure failures. Continuous monitoring is required to detect when an external dependency degrades your application’s performance.</p>
</section>
<section id="hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters">37. Hyperparameters</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_37.png" class="img-fluid figure-img"></p>
<figcaption>Slide 37</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=813s">Timestamp: 13:33</a>)</p>
<p>The slide lists <strong>Hyperparameters</strong> like Temperature, Top-P, and Max Length. Rajiv explains that users can control these “knobs” to influence creativity versus determinism.</p>
<p>Setting temperature to 0 makes the model less random, but as the next slides show, it does not guarantee perfect determinism due to hardware nuances.</p>
</section>
<section id="non-deterministic-inference" class="level3">
<h3 class="anchored" data-anchor-id="non-deterministic-inference">38. Non-Deterministic Inference</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_38.png" class="img-fluid figure-img"></p>
<figcaption>Slide 38</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=843s">Timestamp: 14:03</a>)</p>
<p>This slide tackles <strong>Non-Deterministic Inference</strong>. Unlike traditional ML models (e.g., XGBoost) where a fixed seed guarantees identical output, LLMs on GPUs often produce different results for identical inputs.</p>
<p>Causes include floating-point accumulation errors and the behavior of Mixture of Experts (MoE) models where different batches might activate different experts.</p>
</section>
<section id="addressing-non-determinism" class="level3">
<h3 class="anchored" data-anchor-id="addressing-non-determinism">39. Addressing Non-Determinism</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_39.png" class="img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=911s">Timestamp: 15:11</a>)</p>
<p>Rajiv references recent work by <strong>Thinking Machines</strong> and updates to <strong>vLLM</strong> that attempt to solve the non-determinism problem through correct batching.</p>
<p>While solutions are emerging, the takeaway is that most current setups are non-deterministic by default. Evaluators must design their tests to tolerate this variance rather than expecting bit-wise reproducibility.</p>
</section>
<section id="updated-model-diagram" class="level3">
<h3 class="anchored" data-anchor-id="updated-model-diagram">40. Updated Model Diagram</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_40.png" class="img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=943s">Timestamp: 15:43</a>)</p>
<p>The diagram expands again. The “Model” box now includes <strong>Model Selection, Hyperparameters, Non-deterministic Inference, and Forced Updates</strong>.</p>
<p>This visual summarizes the “Model” section, showing that the “black box” is actually a dynamic system with internal variables (weights/architecture) and external variables (infrastructure/updates) that all add noise to the output.</p>
</section>
<section id="output-format-issues" class="level3">
<h3 class="anchored" data-anchor-id="output-format-issues">41. Output Format Issues</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_41.png" class="img-fluid figure-img"></p>
<figcaption>Slide 41</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=961s">Timestamp: 16:01</a>)</p>
<p>Moving to the “Output” stage, this slide uses MMLU again to show how <strong>Output Formatting</strong> affects evaluation. How do you ask the model to answer a multiple-choice question?</p>
<p>Do you ask it to output just the letter “A”? Or the full text? Or the probability of the token “A”? Different evaluation harnesses use different methods, leading to the score discrepancies seen earlier.</p>
</section>
<section id="evaluation-harness-variations" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-harness-variations">42. Evaluation Harness Variations</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_42.png" class="img-fluid figure-img"></p>
<figcaption>Slide 42</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=995s">Timestamp: 16:35</a>)</p>
<p>This table details the specific differences in implementation between harnesses (e.g., original MMLU vs.&nbsp;HELM vs.&nbsp;EleutherAI).</p>
<p>It reinforces that there is no standard “ruler” for measuring LLMs. The tool you use to measure the model introduces its own bias and variance into the final score.</p>
</section>
<section id="score-comparison-table" class="level3">
<h3 class="anchored" data-anchor-id="score-comparison-table">43. Score Comparison Table</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_43.png" class="img-fluid figure-img"></p>
<figcaption>Slide 43</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1016s">Timestamp: 16:56</a>)</p>
<p>A spreadsheet shows the same models scoring differently across different evaluation implementations. The variance is not trivial; it can be large enough to change the ranking of which model is “best.”</p>
<p>This data drives home the point: You must control your own evaluation pipeline. Relying on reported numbers is risky because you don’t know the implementation details behind them.</p>
</section>
<section id="sentiment-analysis-variance" class="level3">
<h3 class="anchored" data-anchor-id="sentiment-analysis-variance">44. Sentiment Analysis Variance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_44.png" class="img-fluid figure-img"></p>
<figcaption>Slide 44</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1029s">Timestamp: 17:09</a>)</p>
<p>This slide shows varying <strong>Sentiment Analysis</strong> outputs. Different models (or the same model with different prompts) might classify a review as “Positive” while another says “Neutral.”</p>
<p>This introduces the concept that even “simple” classification tasks in GenAI are subject to interpretation and variance, unlike traditional classifiers that have a fixed decision boundary.</p>
</section>
<section id="tool-use-variance" class="level3">
<h3 class="anchored" data-anchor-id="tool-use-variance">45. Tool Use Variance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_45.png" class="img-fluid figure-img"></p>
<figcaption>Slide 45</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1043s">Timestamp: 17:23</a>)</p>
<p>Radar charts illustrate variance in <strong>Tool Use</strong>. Models might be good at using an “Email” tool but fail at “Calendar” or “Terminal” tools.</p>
<p>Furthermore, models exhibit non-determinism in <em>decision making</em>—sometimes they choose to use a tool, and sometimes they try to answer from memory. This adds a layer of logic errors on top of text generation errors.</p>
</section>
<section id="summary-why-responses-differ" class="level3">
<h3 class="anchored" data-anchor-id="summary-why-responses-differ">46. Summary: Why Responses Differ</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_46.png" class="img-fluid figure-img"></p>
<figcaption>Slide 46</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1069s">Timestamp: 17:49</a>)</p>
<p>This comprehensive slide aggregates all the factors discussed: <strong>Inputs</strong> (prompts, system prompts), <strong>Model</strong> (drift, hyperparams), <strong>Outputs</strong> (formatting), and <strong>Infrastructure</strong>.</p>
<p>It serves as a checklist for the audience. If your application is behaving inconsistently, investigate these specific layers of the stack to find the source of the noise.</p>
</section>
<section id="chaos-is-okay" class="level3">
<h3 class="anchored" data-anchor-id="chaos-is-okay">47. Chaos is Okay</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_47.png" class="img-fluid figure-img"></p>
<figcaption>Slide 47</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1097s">Timestamp: 18:17</a>)</p>
<p>Rajiv reassures the audience that <strong>“Chaos is Okay.”</strong> The slide presents a chart of evaluation methods ranging from flexible/expensive (human eval) to rigid/cheap (code assertions).</p>
<p>The message is that while the technology is chaotic, there is a spectrum of tools available to manage it. We don’t need to solve every source of variance; we just need a robust process to measure it.</p>
</section>
<section id="from-chaos-to-control" class="level3">
<h3 class="anchored" data-anchor-id="from-chaos-to-control">48. From Chaos to Control</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_48.png" class="img-fluid figure-img"></p>
<figcaption>Slide 48</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1107s">Timestamp: 18:27</a>)</p>
<p>This transition slide marks the beginning of the <strong>Evaluation Workflow</strong> section. The presentation shifts from describing the problem to prescribing the solution.</p>
<p>The goal here is to move from “Vibe Coding” to a structured engineering discipline where changes are measured against a stable baseline.</p>
</section>
<section id="build-the-evaluation-dataset" class="level3">
<h3 class="anchored" data-anchor-id="build-the-evaluation-dataset">49. Build the Evaluation Dataset</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_49.png" class="img-fluid figure-img"></p>
<figcaption>Slide 49</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1117s">Timestamp: 18:37</a>)</p>
<p>The first step in the workflow is to <strong>Build the Evaluation Dataset</strong>. The slide lists examples of prompts for tasks like summarization, extraction, and translation.</p>
<p>Rajiv emphasizes that this dataset should reflect <em>your</em> actual use case. It is the foundation of the “Custom Benchmark” concept introduced earlier.</p>
</section>
<section id="get-labeled-outputs-gold" class="level3">
<h3 class="anchored" data-anchor-id="get-labeled-outputs-gold">50. Get Labeled Outputs (Gold)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_50.png" class="img-fluid figure-img"></p>
<figcaption>Slide 50</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1126s">Timestamp: 18:46</a>)</p>
<p>Step two is to get <strong>Labeled Outputs</strong>, also known as <strong>Gold Outputs</strong>, Reference, or Ground Truth. The slide adds a column showing the ideal answer for each prompt.</p>
<p>This is the standard against which the model will be judged. While obtaining these labels can be expensive (requiring human effort), they are essential for calculating accuracy.</p>
</section>
<section id="compare-to-model-output" class="level3">
<h3 class="anchored" data-anchor-id="compare-to-model-output">51. Compare to Model Output</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_51.png" class="img-fluid figure-img"></p>
<figcaption>Slide 51</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1140s">Timestamp: 19:00</a>)</p>
<p>Step three is to generate responses from your system and place them alongside the Gold Outputs. The slide adds a <strong>“Model Output”</strong> column.</p>
<p>This visual comparison allows developers (and automated judges) to see the delta between what was expected and what was produced.</p>
</section>
<section id="measure-equivalence" class="level3">
<h3 class="anchored" data-anchor-id="measure-equivalence">52. Measure Equivalence</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_52.png" class="img-fluid figure-img"></p>
<figcaption>Slide 52</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1150s">Timestamp: 19:10</a>)</p>
<p>Step four is to <strong>Measure Equivalence</strong>. Since LLMs rarely produce exact string matches, we use an <strong>LLM Judge</strong> (another model) to determine if the Model Output means the same thing as the Gold Output.</p>
<p>The slide shows a prompt for the judge: “Are these two responses semantically equivalent?” This converts a fuzzy text comparison problem into a binary (Pass/Fail) metric.</p>
</section>
<section id="optimize-using-equivalence" class="level3">
<h3 class="anchored" data-anchor-id="optimize-using-equivalence">53. Optimize Using Equivalence</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_53.png" class="img-fluid figure-img"></p>
<figcaption>Slide 53</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1197s">Timestamp: 19:57</a>)</p>
<p>Once you have an equivalence metric, you can <strong>Optimize</strong>. The slide shows Config A vs.&nbsp;Config B. By changing prompts or models, you can track if your “Equivalence Score” goes up or down.</p>
<p>This treats GenAI engineering like traditional hyperparameter tuning. The goal is to maximize the equivalence score on your custom dataset.</p>
</section>
<section id="why-global-metrics-arent-enough" class="level3">
<h3 class="anchored" data-anchor-id="why-global-metrics-arent-enough">54. Why Global Metrics Aren’t Enough</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_54.png" class="img-fluid figure-img"></p>
<figcaption>Slide 54</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1228s">Timestamp: 20:28</a>)</p>
<p>The slide discusses the limitations of the “Equivalence” approach. While good for a general sense of quality, <strong>Global Metrics</strong> miss nuances.</p>
<p>Sometimes it’s hard to get a Gold Answer for open-ended creative tasks. Furthermore, a simple “Pass/Fail” doesn’t tell you <em>why</em> the model failed (e.g., was it tone, length, or factuality?).</p>
</section>
<section id="from-global-to-targeted-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="from-global-to-targeted-evaluation">55. From Global to Targeted Evaluation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_55.png" class="img-fluid figure-img"></p>
<figcaption>Slide 55</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1255s">Timestamp: 20:55</a>)</p>
<p>This slide argues for <strong>Targeted Evaluation</strong>. To maximize performance, you need to dig deeper into the data and identify specific error modes.</p>
<p>This transitions the talk from “Basic Workflow” to “Advanced Testing,” where we break down “Quality” into specific, testable components like tone, length, and safety.</p>
</section>
<section id="building-tests" class="level3">
<h3 class="anchored" data-anchor-id="building-tests">56. Building Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_56.png" class="img-fluid figure-img"></p>
<figcaption>Slide 56</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1274s">Timestamp: 21:14</a>)</p>
<p>The section title <strong>“Building Tests”</strong> appears. This is where the presentation moves into the “Unit Testing” philosophy for GenAI.</p>
<p>Just as software engineering relies on unit tests to verify specific functions, GenAI engineering should use targeted tests to verify specific attributes of the generated text.</p>
</section>
<section id="good-vs.-bad-examples" class="level3">
<h3 class="anchored" data-anchor-id="good-vs.-bad-examples">57. Good vs.&nbsp;Bad Examples</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_57.png" class="img-fluid figure-img"></p>
<figcaption>Slide 57</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1280s">Timestamp: 21:20</a>)</p>
<p>The slide displays a <strong>Good Example</strong> and a <strong>Bad Example</strong> of a response. The bad example is visibly shorter and less polite.</p>
<p>Rajiv asks the audience to identify <em>why</em> it is bad. This exercise is crucial: you cannot build a test until you can articulate exactly what makes a response a failure.</p>
</section>
<section id="develop-an-evaluation-mindset" class="level3">
<h3 class="anchored" data-anchor-id="develop-an-evaluation-mindset">58. Develop an Evaluation Mindset</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_58.png" class="img-fluid figure-img"></p>
<figcaption>Slide 58</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1306s">Timestamp: 21:46</a>)</p>
<p>To define “Bad,” developers need an <strong>Evaluation Mindset</strong>. This involves observing real-world user interactions and problems.</p>
<p>Data scientists often want to stay in their “chair” and optimize algorithms, but Rajiv argues that effective evaluation requires understanding the user’s pain points.</p>
</section>
<section id="collaborate-with-experts" class="level3">
<h3 class="anchored" data-anchor-id="collaborate-with-experts">59. Collaborate with Experts</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_59.png" class="img-fluid figure-img"></p>
<figcaption>Slide 59</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1318s">Timestamp: 21:58</a>)</p>
<p>The slide stresses <strong>Collaboration</strong>. You must talk to domain experts (e.g., the customer support team) to define what a “good” answer looks like.</p>
<p>Naive bootstrapping—pretending to be a user—is a good start, but long-term success requires input from the people who actually know the business domain.</p>
</section>
<section id="identify-and-categorize-failures" class="level3">
<h3 class="anchored" data-anchor-id="identify-and-categorize-failures">60. Identify and Categorize Failures</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_60.png" class="img-fluid figure-img"></p>
<figcaption>Slide 60</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1372s">Timestamp: 22:52</a>)</p>
<p>Once you understand the domain, you can <strong>Categorize Failure Types</strong>. The slide shows a chart grouping errors into categories like “Harmful Content,” “Bias,” or “Incorrect Info.”</p>
<p>This clustering allows you to see patterns. Instead of just knowing “the model failed 20% of the time,” you know “the model has a specific problem with tone.”</p>
</section>
<section id="define-what-good-looks-like" class="level3">
<h3 class="anchored" data-anchor-id="define-what-good-looks-like">61. Define What Good Looks Like</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_61.png" class="img-fluid figure-img"></p>
<figcaption>Slide 61</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1391s">Timestamp: 23:11</a>)</p>
<p>Using the categorization, you can explicitly <strong>Define What Good Looks Like</strong>. The slide contrasts the good/bad examples again, but now with labels: “Too short,” “Lacks professional tone.”</p>
<p>This transforms a subjective feeling (“this response sucks”) into objective criteria (“response must be &gt;50 words and use polite honorifics”).</p>
</section>
<section id="document-every-issue" class="level3">
<h3 class="anchored" data-anchor-id="document-every-issue">62. Document Every Issue</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_62.png" class="img-fluid figure-img"></p>
<figcaption>Slide 62</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1412s">Timestamp: 23:32</a>)</p>
<p>The slide shows a spreadsheet where humans evaluate responses and <strong>Document Every Issue</strong>. Columns track specific attributes like “Is it helpful?” or “Is the tone right?”</p>
<p>This manual annotation is the training data for your automated tests. You need humans to establish the ground truth before you can automate the checking.</p>
</section>
<section id="evaluation-tooling" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-tooling">63. Evaluation Tooling</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_63.png" class="img-fluid figure-img"></p>
<figcaption>Slide 63</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1433s">Timestamp: 23:53</a>)</p>
<p>Rajiv mentions that <strong>Tooling Can Help</strong>. The slide shows a custom chat viewer designed to make human review easier.</p>
<p>However, he warns against getting sidetracked by building fancy tools. Simple spreadsheets often suffice for the early stages. The goal is the data, not the interface.</p>
</section>
<section id="test-1-length-check" class="level3">
<h3 class="anchored" data-anchor-id="test-1-length-check">64. Test 1: Length Check</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_64.png" class="img-fluid figure-img"></p>
<figcaption>Slide 64</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1445s">Timestamp: 24:05</a>)</p>
<p>Now we build the automated tests. <strong>Test 1 is a Length Check</strong>. The slide shows Python code asserting that the word count is between 8 and 200.</p>
<p>This is a <strong>deterministic test</strong>. You don’t need an LLM to count words. Rajiv encourages using simple Python assertions wherever possible because they are fast, cheap, and reliable.</p>
</section>
<section id="test-2-tone-and-style" class="level3">
<h3 class="anchored" data-anchor-id="test-2-tone-and-style">65. Test 2: Tone and Style</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_65.png" class="img-fluid figure-img"></p>
<figcaption>Slide 65</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1462s">Timestamp: 24:22</a>)</p>
<p><strong>Test 2 checks Tone and Style</strong>. Since “tone” is subjective, we use an <strong>LLM Judge</strong> (OpenAI model) to classify the response.</p>
<p>The prompt asks the judge to identify the style. This allows us to automate the “vibe check” that humans were previously doing manually.</p>
</section>
<section id="adding-metrics-to-documentation" class="level3">
<h3 class="anchored" data-anchor-id="adding-metrics-to-documentation">66. Adding Metrics to Documentation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_66.png" class="img-fluid figure-img"></p>
<figcaption>Slide 66</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1481s">Timestamp: 24:41</a>)</p>
<p>The spreadsheet is updated with new columns: <code>Length_OK</code> and <code>Tone_OK</code>. These are the results of the automated tests.</p>
<p>Now, for every row in the dataset, we have granular pass/fail metrics. This helps pinpoint exactly <em>why</em> a specific response failed, rather than just a generic failure.</p>
</section>
<section id="check-judges-against-humans" class="level3">
<h3 class="anchored" data-anchor-id="check-judges-against-humans">67. Check Judges Against Humans</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_67.png" class="img-fluid figure-img"></p>
<figcaption>Slide 67</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1512s">Timestamp: 25:12</a>)</p>
<p>A critical step: <strong>Check LLM Judges Against Humans</strong>. You must verify that your automated “Tone Judge” agrees with your human experts.</p>
<p>If the human says the tone is rude, but the LLM Judge says it’s polite, your metric is useless. You must iterate on the judge’s prompt until alignment is high.</p>
</section>
<section id="self-evaluation-bias" class="level3">
<h3 class="anchored" data-anchor-id="self-evaluation-bias">68. Self-Evaluation Bias</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_68.png" class="img-fluid figure-img"></p>
<figcaption>Slide 68</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1566s">Timestamp: 26:06</a>)</p>
<p>The slide illustrates <strong>Self-Evaluation Bias</strong>. LLMs tend to rate their own outputs higher than outputs from other models. GPT-4 prefers GPT-4 text.</p>
<p>To mitigate this, Rajiv suggests mixing models—use Claude to judge GPT-4, or Gemini to judge Claude. This helps ensure a more neutral evaluation.</p>
</section>
<section id="alignment-checks" class="level3">
<h3 class="anchored" data-anchor-id="alignment-checks">69. Alignment Checks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_69.png" class="img-fluid figure-img"></p>
<figcaption>Slide 69</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1606s">Timestamp: 26:46</a>)</p>
<p>This slide reinforces the need for <strong>Continuous Alignment</strong>. Just because your judge aligned with humans last month doesn’t mean it still does (due to model drift).</p>
<p>Human spot-checks should be a permanent part of the pipeline to ensure the automated judges haven’t drifted.</p>
</section>
<section id="biases-in-llm-judges" class="level3">
<h3 class="anchored" data-anchor-id="biases-in-llm-judges">70. Biases in LLM Judges</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_70.png" class="img-fluid figure-img"></p>
<figcaption>Slide 70</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1622s">Timestamp: 27:02</a>)</p>
<p>The slide lists known <strong>Biases in LLM Judges</strong>, such as <strong>Position Bias</strong> (favoring the first answer presented) or <strong>Verbosity Bias</strong> (favoring longer answers).</p>
<p>Evaluators must be aware of these. For example, you should shuffle the order of answers when asking a judge to compare two options to cancel out position bias.</p>
</section>
<section id="best-practices-for-llm-judges" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-for-llm-judges">71. Best Practices for LLM Judges</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_71.png" class="img-fluid figure-img"></p>
<figcaption>Slide 71</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1631s">Timestamp: 27:11</a>)</p>
<p>A summary of <strong>Best Practices</strong>: Calibrate with human data, use ensembles (multiple judges), avoid asking for “relevance” (too vague), and use discrete rating scales (1-5) rather than continuous numbers.</p>
<p>These tips help stabilize the inherently noisy process of using AI to evaluate AI.</p>
</section>
<section id="error-analysis-chart" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-chart">72. Error Analysis Chart</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_72.png" class="img-fluid figure-img"></p>
<figcaption>Slide 72</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1666s">Timestamp: 27:46</a>)</p>
<p>With tests in place, we move to <strong>Error Analysis</strong>. The bar chart shows the number of failed cases categorized by error type (Length, Tone, Professional, Context).</p>
<p>This visualization tells you where to focus your efforts. If “Tone” is the biggest bar, you work on the system prompt’s tone instructions. If “Context” is the issue, you might need better Retrieval Augmented Generation (RAG).</p>
</section>
<section id="comparing-prompts" class="level3">
<h3 class="anchored" data-anchor-id="comparing-prompts">73. Comparing Prompts</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_73.png" class="img-fluid figure-img"></p>
<figcaption>Slide 73</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1678s">Timestamp: 27:58</a>)</p>
<p>The chart can compare <strong>Prompt A vs.&nbsp;Prompt B</strong>. This allows for A/B testing of prompt engineering strategies.</p>
<p>You can see if a new prompt improves “Tone” but accidentally degrades “Context.” This tradeoff analysis is impossible with a single global score.</p>
</section>
<section id="explanations-guide-improvement" class="level3">
<h3 class="anchored" data-anchor-id="explanations-guide-improvement">74. Explanations Guide Improvement</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_74.png" class="img-fluid figure-img"></p>
<figcaption>Slide 74</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1694s">Timestamp: 28:14</a>)</p>
<p>Rajiv suggests asking the LLM Judge for <strong>Explanations</strong>. Don’t just ask for a score; ask for “one sentence explaining why.”</p>
<p>These explanations act as metadata that helps developers understand the judge’s reasoning, making it easier to debug discrepancies between human and AI judgments.</p>
</section>
<section id="limits-to-explanations" class="level3">
<h3 class="anchored" data-anchor-id="limits-to-explanations">75. Limits to Explanations</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_75.png" class="img-fluid figure-img"></p>
<figcaption>Slide 75</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1715s">Timestamp: 28:35</a>)</p>
<p>A warning: <strong>Explanations are not causal</strong>. When an LLM explains why it did something, it is generating a plausible justification, not a trace of its actual neural activations.</p>
<p>Treat explanations as a heuristic or a helpful hint, not as absolute truth about the model’s internal state.</p>
</section>
<section id="the-evaluation-flywheel" class="level3">
<h3 class="anchored" data-anchor-id="the-evaluation-flywheel">76. The Evaluation Flywheel</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_76.png" class="img-fluid figure-img"></p>
<figcaption>Slide 76</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1726s">Timestamp: 28:46</a>)</p>
<p>The <strong>Evaluation Flywheel</strong> describes the iterative cycle: Build Eval -&gt; Analyze -&gt; Improve -&gt; Repeat.</p>
<p>This concept, credited to Hamill, emphasizes that evaluation is not a one-time event but a continuous loop that spins faster as you gather more data and build better tests.</p>
</section>
<section id="financial-analyst-agent-example" class="level3">
<h3 class="anchored" data-anchor-id="financial-analyst-agent-example">77. Financial Analyst Agent Example</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_77.png" class="img-fluid figure-img"></p>
<figcaption>Slide 77</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1760s">Timestamp: 29:20</a>)</p>
<p>To demonstrate advanced unit testing, Rajiv introduces a <strong>Financial Analyst Agent</strong>. The goal is to assess the specific “style” of a financial report.</p>
<p>This is a complex domain where “good” is highly specific (regulated, precise, risk-aware), making it a perfect candidate for granular unit tests.</p>
</section>
<section id="use-a-global-test" class="level3">
<h3 class="anchored" data-anchor-id="use-a-global-test">78. Use a Global Test?</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_78.png" class="img-fluid figure-img"></p>
<figcaption>Slide 78</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1783s">Timestamp: 29:43</a>)</p>
<p>You <em>could</em> use a <strong>Global Test</strong>: “Was this explained as a financial analyst would?”</p>
<p>While simple, this test is opaque. If it fails, you don’t know if it was because of compliance issues, lack of clarity, or poor formatting.</p>
</section>
<section id="global-vs.-unit-tests" class="level3">
<h3 class="anchored" data-anchor-id="global-vs.-unit-tests">79. Global vs.&nbsp;Unit Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_79.png" class="img-fluid figure-img"></p>
<figcaption>Slide 79</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1794s">Timestamp: 29:54</a>)</p>
<p>The slide contrasts the Global approach with <strong>Unit Tests</strong>. Instead of one question, we ask six: Context, Clarity, Precision, Compliance, Actionability, and Risks.</p>
<p>This breakdown allows for targeted debugging. You might find the model is great at “Clarity” but terrible at “Compliance.”</p>
</section>
<section id="scoring-radar-chart" class="level3">
<h3 class="anchored" data-anchor-id="scoring-radar-chart">80. Scoring Radar Chart</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_80.png" class="img-fluid figure-img"></p>
<figcaption>Slide 80</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1816s">Timestamp: 30:16</a>)</p>
<p>A <strong>Radar Chart</strong> visualizes the unit test scores. This allows for a quick visual assessment of the model’s profile.</p>
<p>It facilitates comparison: you can overlay the profiles of two different models to see which one has the better balance of attributes for your specific needs.</p>
</section>
<section id="analyzing-failures-with-clusters" class="level3">
<h3 class="anchored" data-anchor-id="analyzing-failures-with-clusters">81. Analyzing Failures with Clusters</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_81.png" class="img-fluid figure-img"></p>
<figcaption>Slide 81</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1837s">Timestamp: 30:37</a>)</p>
<p>With enough unit test data, you can use <strong>Clustering (e.g., K-Means)</strong> to group failures. The slide shows clusters like “Synthesis,” “Context,” and “Hallucination.”</p>
<p>This moves error analysis from reading individual logs to analyzing aggregate trends, helping you prioritize which class of errors to fix first.</p>
</section>
<section id="designing-good-unit-tests" class="level3">
<h3 class="anchored" data-anchor-id="designing-good-unit-tests">82. Designing Good Unit Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_82.png" class="img-fluid figure-img"></p>
<figcaption>Slide 82</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1852s">Timestamp: 30:52</a>)</p>
<p>Advice on <strong>Designing Unit Tests</strong>: Keep them focused (one concept per test), use unambiguous language, and use small rating ranges.</p>
<p>Good unit tests are the building blocks of a reliable evaluation pipeline. If the tests themselves are noisy or vague, the entire system collapses.</p>
</section>
<section id="examples-of-unit-tests" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-unit-tests">83. Examples of Unit Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_83.png" class="img-fluid figure-img"></p>
<figcaption>Slide 83</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1855s">Timestamp: 30:55</a>)</p>
<p>The slide lists specific examples of tests for <strong>Legal</strong> (Compliance, Terminology), <strong>Retrieval</strong> (Relevance, Completeness), and <strong>Bias/Fairness</strong>.</p>
<p>This serves as a menu of options for the audience, showing that unit tests can cover almost any dimension of quality required by the business.</p>
</section>
<section id="evaluating-new-prompts" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-new-prompts">84. Evaluating New Prompts</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_84.png" class="img-fluid figure-img"></p>
<figcaption>Slide 84</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1858s">Timestamp: 30:58</a>)</p>
<p>A bar chart shows how unit tests are used to <strong>Evaluate New Prompts</strong>. By running the full suite of unit tests on a new prompt, you get a “scorecard” of its performance.</p>
<p>This data-driven approach removes the guesswork from prompt engineering.</p>
</section>
<section id="tools---no-silver-bullet" class="level3">
<h3 class="anchored" data-anchor-id="tools---no-silver-bullet">85. Tools - No Silver Bullet</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_85.png" class="img-fluid figure-img"></p>
<figcaption>Slide 85</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1862s">Timestamp: 31:02</a>)</p>
<p>Rajiv reminds the audience that <strong>Tools are No Silver Bullet</strong>. You must master the basics (datasets, metrics) first.</p>
<p>He advises logging traces and experiments and practicing <strong>Dataset Versioning</strong>. Tools facilitate these practices, but they cannot replace the fundamental engineering discipline.</p>
</section>
<section id="forest-and-trees" class="level3">
<h3 class="anchored" data-anchor-id="forest-and-trees">86. Forest and Trees</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_86.png" class="img-fluid figure-img"></p>
<figcaption>Slide 86</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1864s">Timestamp: 31:04</a>)</p>
<p>An analogy helps structure the analysis: <strong>Forest (Global/Integration)</strong> vs.&nbsp;<strong>Trees (Test Case/Unit Tests)</strong>.</p>
<p>You need to look at both. The forest tells you the overall health of the app, while the trees tell you specifically what needs pruning or fixing.</p>
</section>
<section id="change-one-thing-at-a-time" class="level3">
<h3 class="anchored" data-anchor-id="change-one-thing-at-a-time">87. Change One Thing at a Time</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_87.png" class="img-fluid figure-img"></p>
<figcaption>Slide 87</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1877s">Timestamp: 31:17</a>)</p>
<p>A crucial scientific principle: <strong>Change One Thing at a Time</strong>. With so many knobs (prompt, temp, model, RAG settings), changing multiple variables simultaneously makes it impossible to know what caused the improvement (or regression).</p>
<p>Isolate your variables to conduct valid experiments.</p>
</section>
<section id="error-analysis-tips" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-tips">88. Error Analysis Tips</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_88.png" class="img-fluid figure-img"></p>
<figcaption>Slide 88</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1892s">Timestamp: 31:32</a>)</p>
<p>A summary of <strong>Error Analysis Tips</strong>: Use ablation studies (removing parts to see impact), categorize failures, save interesting examples, and leverage logs/traces.</p>
<p>These are the daily habits of successful GenAI engineers.</p>
</section>
<section id="the-evaluation-story" class="level3">
<h3 class="anchored" data-anchor-id="the-evaluation-story">89. The Evaluation Story</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_89.png" class="img-fluid figure-img"></p>
<figcaption>Slide 89</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1928s">Timestamp: 32:08</a>)</p>
<p>The slide shows the “Story We Tell”—a linear graph of improvement over time. This is the idealized version of progress often presented in case studies.</p>
<p>It suggests a smooth journey from “Out of the box” to “Specialized” to “User Feedback.”</p>
</section>
<section id="the-reality-of-progress" class="level3">
<h3 class="anchored" data-anchor-id="the-reality-of-progress">90. The Reality of Progress</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_90.png" class="img-fluid figure-img"></p>
<figcaption>Slide 90</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1944s">Timestamp: 32:24</a>)</p>
<p><strong>The Reality</strong> is a messy, non-linear graph. You take two steps forward, one step back. Sometimes an “improvement” breaks the model.</p>
<p>Rajiv encourages resilience. Experienced practitioners know that this messy graph is normal and that sticking to the process eventually yields results.</p>
</section>
<section id="continual-process" class="level3">
<h3 class="anchored" data-anchor-id="continual-process">91. Continual Process</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_91.png" class="img-fluid figure-img"></p>
<figcaption>Slide 91</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1981s">Timestamp: 33:01</a>)</p>
<p><strong>Evaluation is a Continual Process</strong>. It involves Problem ID, Data Collection, Optimization, User Acceptance Testing (UAT), and Updates.</p>
<p>Crucially, <strong>UAT</strong> is your holdout set. Since you don’t have a traditional test set in GenAI, your real users act as the final validation layer.</p>
</section>
<section id="eating-the-elephant" class="level3">
<h3 class="anchored" data-anchor-id="eating-the-elephant">92. Eating the Elephant</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_92.png" class="img-fluid figure-img"></p>
<figcaption>Slide 92</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2043s">Timestamp: 34:03</a>)</p>
<p>The metaphor <strong>“How do you eat an elephant?”</strong> addresses the overwhelming nature of building a comprehensive evaluation suite.</p>
<p>The answer, of course, is “one bite at a time.” You don’t need 100 tests on day one.</p>
</section>
<section id="adding-tests-over-time" class="level3">
<h3 class="anchored" data-anchor-id="adding-tests-over-time">93. Adding Tests Over Time</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_93.png" class="img-fluid figure-img"></p>
<figcaption>Slide 93</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2050s">Timestamp: 34:10</a>)</p>
<p>The slide visualizes the “elephant” being broken down into bites. You start with a few critical tests. As the app matures and you discover new failure modes, you add more tests.</p>
<p>Six months in, you might have 100 tests, but you built them incrementally. This makes the task manageable.</p>
</section>
<section id="doing-evaluation-the-right-way" class="level3">
<h3 class="anchored" data-anchor-id="doing-evaluation-the-right-way">94. Doing Evaluation the Right Way</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_94.png" class="img-fluid figure-img"></p>
<figcaption>Slide 94</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2079s">Timestamp: 34:39</a>)</p>
<p>A summary slide listing best practices: <strong>Annotated Examples</strong>, <strong>Systematic Documentation</strong>, <strong>Continuous Error Analysis</strong>, <strong>Collaboration</strong>, and awareness of <strong>Generalization</strong>.</p>
<p>This concludes the core methodology section of the talk.</p>
</section>
<section id="agentic-use-cases" class="level3">
<h3 class="anchored" data-anchor-id="agentic-use-cases">95. Agentic Use Cases</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_95.png" class="img-fluid figure-img"></p>
<figcaption>Slide 95</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2090s">Timestamp: 34:50</a>)</p>
<p>The final section covers <strong>Agentic Use Cases</strong>, symbolized by a dragon. Agents add a layer of complexity because the model is now making decisions (routing, tool use) rather than just generating text.</p>
<p>This “agency” makes the system harder to track and evaluate.</p>
</section>
<section id="crossing-the-river" class="level3">
<h3 class="anchored" data-anchor-id="crossing-the-river">96. Crossing the River</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_96.png" class="img-fluid figure-img"></p>
<figcaption>Slide 96</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2106s">Timestamp: 35:06</a>)</p>
<p>A conceptual slide asking, <strong>“How should it cross the river?”</strong> (Fly, Swim, Bridge?). This represents the decision-making step in an agent.</p>
<p>Evaluating an agent requires evaluating <em>how</em> it made the decision (the router) separately from <em>how well</em> it executed the action.</p>
</section>
<section id="chat-to-purchase-router" class="level3">
<h3 class="anchored" data-anchor-id="chat-to-purchase-router">97. Chat-to-Purchase Router</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_97.png" class="img-fluid figure-img"></p>
<figcaption>Slide 97</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2122s">Timestamp: 35:22</a>)</p>
<p>A complex flowchart shows a <strong>Chat-to-Purchase Router</strong>. The agent must decide if the user wants to search for a product, get support, or track a package.</p>
<p>Rajiv suggests breaking this down: evaluate the <strong>Router</strong> component first (did it pick the right path?), then evaluate the specific workflow (did it track the package correctly?).</p>
</section>
<section id="text-to-sql-agent" class="level3">
<h3 class="anchored" data-anchor-id="text-to-sql-agent">98. Text to SQL Agent</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_98.png" class="img-fluid figure-img"></p>
<figcaption>Slide 98</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2177s">Timestamp: 36:17</a>)</p>
<p>Another example: <strong>Text to SQL Agent</strong>. This workflow involves classification, feature extraction, and SQL generation.</p>
<p>You can isolate the “Classification” step (is this a valid SQL question?) and build a test just for that, before testing the actual SQL generation.</p>
</section>
<section id="evaluating-office-style-agents" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-office-style-agents">99. Evaluating Office-Style Agents</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_99.png" class="img-fluid figure-img"></p>
<figcaption>Slide 99</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2206s">Timestamp: 36:46</a>)</p>
<p>The slide discusses <strong>OdysseyBench</strong>, a benchmark for office tasks. It highlights failure modes like “Failed to create folder” or “Failed to use tool.”</p>
<p>Evaluating agents involves checking if they successfully manipulated the environment (files, APIs), which is a functional test rather than a text similarity test.</p>
</section>
<section id="error-analysis-for-agents" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-for-agents">100. Error Analysis for Agents</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_100.png" class="img-fluid figure-img"></p>
<figcaption>Slide 100</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2220s">Timestamp: 37:00</a>)</p>
<p><strong>Error Analysis for Agentic Workflows</strong> requires assessing the overall performance, the routing decisions, and the individual steps.</p>
<p>It is the same “action error analysis” process but applied recursively to every node in the agent’s decision tree.</p>
</section>
<section id="evaluating-workflow-vs.-response" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-workflow-vs.-response">101. Evaluating Workflow vs.&nbsp;Response</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_101.png" class="img-fluid figure-img"></p>
<figcaption>Slide 101</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2239s">Timestamp: 37:19</a>)</p>
<p>This slide distinguishes between evaluating a <strong>Response</strong> (text) and a <strong>Workflow</strong> (process). The flowchart shows a conversational flow.</p>
<p>Evaluating a workflow might mean checking if the agent successfully moved the user from “Greeting” to “Resolution,” regardless of the exact words used.</p>
</section>
<section id="agentic-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="agentic-frameworks">102. Agentic Frameworks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_102.png" class="img-fluid figure-img"></p>
<figcaption>Slide 102</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2268s">Timestamp: 37:48</a>)</p>
<p>Rajiv warns that <strong>“Agentic Frameworks Help – Until They Don’t.”</strong> Frameworks (like LangChain or AutoGen) are great for demos because they abstract complexity.</p>
<p>However, in production, these abstractions can break or become outdated. He often recommends using straight Python for production agents to maintain control and reliability.</p>
</section>
<section id="abstraction-for-workflows" class="level3">
<h3 class="anchored" data-anchor-id="abstraction-for-workflows">103. Abstraction for Workflows</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_103.png" class="img-fluid figure-img"></p>
<figcaption>Slide 103</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2312s">Timestamp: 38:32</a>)</p>
<p>The slide illustrates the trade-off in <strong>Abstraction</strong>. You can build rigid workflows (orchestration) where you control every step, or use general agents where the LLM decides.</p>
<p>Orchestration is more reliable but rigid. General agents are flexible but prone to non-deterministic errors.</p>
</section>
<section id="when-abstractions-break" class="level3">
<h3 class="anchored" data-anchor-id="when-abstractions-break">104. When Abstractions Break</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_104.png" class="img-fluid figure-img"></p>
<figcaption>Slide 104</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2333s">Timestamp: 38:53</a>)</p>
<p>Model providers are training models to handle workflows internally (removing the need for external orchestration).</p>
<p>However, until models are perfect, developers often need to break tasks down into specific pieces to ensure reliability. The choice between “letting the model do it” and “scripting the flow” depends on the application’s risk tolerance.</p>
</section>
<section id="lessons-from-agent-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="lessons-from-agent-benchmarks">105. Lessons from Agent Benchmarks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_105.png" class="img-fluid figure-img"></p>
<figcaption>Slide 105</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2355s">Timestamp: 39:15</a>)</p>
<p>The slide lists <strong>Lessons from Reproducing Agent Benchmarks</strong>: Standardize evaluation, measure efficiency, detect shortcuts, and log real behavior.</p>
<p>These are advanced tips for those pushing the boundaries of what agents can do.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">106. Conclusion</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://projects.rajivshah.com/images/genai-evaluation-guide/slide_106.png" class="img-fluid figure-img"></p>
<figcaption>Slide 106</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2367s">Timestamp: 39:27</a>)</p>
<p>The final slide, <strong>“We did it!”</strong>, concludes the presentation. Rajiv thanks the audience and provides the QR code again.</p>
<p>His final message is one of empowerment: he hopes the audience now has the confidence to go out, build their own evaluation datasets, and start “hill climbing” their own applications.</p>
<hr>
<p><em>This annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations.</em></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/rajivshah\.com\/blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 @rajistics - Rajiv Shah
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.rajivshah.com">
<p><u>About Me</u></p>
</a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="./index.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/rajistics/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/rajistics">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/rajistics/">
      <i class="bi bi-instagram" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.tiktok.com/@rajistics">
      <i class="bi bi-tiktok" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCu9fxVjTz5AJO7FR1upY02w">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rajshah4">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>