<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-09-15">

<title>Practical Lessons in Building Generative AI: RAG and Text to SQL – Rajiv Shah - rajistics blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ddb7102b129bb408a3919432018bab43.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-475ad4fe1e4ce2c827a237f0e4cf2c17.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-ddb7102b129bb408a3919432018bab43.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Rajiv Shah - rajistics blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://www.rajivshah.com"> 
<span class="menu-text"><u>About Me</u></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/rajistics/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/rajistics"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/rajistics/"> <i class="bi bi-instagram" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.tiktok.com/@rajistics"> <i class="bi bi-tiktok" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCu9fxVjTz5AJO7FR1upY02w"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rajshah4"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Practical Lessons in Building Generative AI: RAG and Text to SQL</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">RAG</div>
                <div class="quarto-category">Text-to-SQL</div>
                <div class="quarto-category">AI</div>
                <div class="quarto-category">Generative AI</div>
                <div class="quarto-category">Annotated Talk</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 15, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#video" id="toc-video" class="nav-link active" data-scroll-target="#video">Video</a></li>
  <li><a href="#annotated-presentation" id="toc-annotated-presentation" class="nav-link" data-scroll-target="#annotated-presentation">Annotated Presentation</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="practical-rag-text-to-sql.ipynb" download="practical-rag-text-to-sql.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<section id="video" class="level2">
<h2 class="anchored" data-anchor-id="video">Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OyY4uxUShys" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Watch the <a href="https://youtu.be/OyY4uxUShys">full video</a></p>
<hr>
</section>
<section id="annotated-presentation" class="level2">
<h2 class="anchored" data-anchor-id="annotated-presentation">Annotated Presentation</h2>
<p>Below is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.</p>
<p>Here is the annotated presentation based on the video transcript and slide summaries.</p>
<section id="title-slide-a-practical-perspective-on-generative-ai" class="level3">
<h3 class="anchored" data-anchor-id="title-slide-a-practical-perspective-on-generative-ai">1. Title Slide: A Practical Perspective on Generative AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_1.png" class="img-fluid figure-img"></p>
<figcaption>Slide 1</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1s">Timestamp: 00:01</a>)</p>
<p>This presentation begins with an introduction by Rajiv Shah from Snowflake. The talk focuses on distinguishing “what’s easy to do with LLMs, what’s hard to do with LLMs, and where that boundary is for generative AI.” The content is framed as a practical guide for enterprises navigating the hype versus the reality of implementing these technologies.</p>
<p>The speaker sets the stage for a narrative-driven presentation that will move away from abstract theory and into concrete examples. The goal is to walk through the basics of <strong>Large Language Models (LLMs)</strong> and <strong>Retrieval Augmented Generation (RAG)</strong> before applying them to real-world scenarios involving legal research and enterprise data analysis.</p>
</section>
<section id="presentation-goals" class="level3">
<h3 class="anchored" data-anchor-id="presentation-goals">2. Presentation Goals</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_2.png" class="img-fluid figure-img"></p>
<figcaption>Slide 2</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=31s">Timestamp: 00:31</a>)</p>
<p>The agenda for the talk is outlined here. The speaker intends to cover the foundational mechanisms of how to use LLMs effectively, specifically focusing on RAG. To make the concepts relatable, the presentation uses two storytelling devices: a fictional law firm (“Dewey, Cheatham, and Howe”) and a hypothetical company (“Frosty”).</p>
<p>These two stories serve to illustrate how people are currently using Generative AI, the specific limitations they encounter, and the engineering required to build a robust application. The speaker emphasizes that the talk will explore “what does it take to actually develop a generative AI application” beyond just simple prompting.</p>
</section>
<section id="the-avianca-case" class="level3">
<h3 class="anchored" data-anchor-id="the-avianca-case">3. The Avianca Case</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_3.png" class="img-fluid figure-img"></p>
<figcaption>Slide 3</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=60s">Timestamp: 01:00</a>)</p>
<p>The speaker introduces the concept of <strong>hallucinations</strong> through a famous real-world example involving the airline Avianca. A lawyer, attempting to speed up his work on a brief regarding a personal injury case, used ChatGPT for legal research. The AI “found some cases that were unpublished,” which the lawyer cited in court.</p>
<p>However, ChatGPT had “made up those cases.” The lawyer was admonished by the bar for submitting fictitious legal precedents. This slide serves as a warning: while LLMs are powerful tools, they cannot be blindly trusted for factual research because they are prone to fabricating information when they don’t know the answer.</p>
</section>
<section id="generative-ai-in-action" class="level3">
<h3 class="anchored" data-anchor-id="generative-ai-in-action">4. Generative AI in Action</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_4.png" class="img-fluid figure-img"></p>
<figcaption>Slide 4</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=134s">Timestamp: 02:14</a>)</p>
<p>To demonstrate the variability of LLMs, the speaker presents a side-by-side comparison of two models (Google Gemma and a “Woflesh” model) answering the same prompt: “How many vehicles will Rivian manufacture in Normal, Illinois?” The models provide different answers.</p>
<p>This illustrates a key characteristic of Generative AI: “Two different manufacturers, two different methods for training these models are probably going to lead to two different results.” It highlights that out-of-the-box models rely on their specific training data, which may be outdated or weighted differently, leading to inconsistent factual accuracy.</p>
</section>
<section id="next-token-prediction" class="level3">
<h3 class="anchored" data-anchor-id="next-token-prediction">5. Next Token Prediction</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_5.png" class="img-fluid figure-img"></p>
<figcaption>Slide 5</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=182s">Timestamp: 03:02</a>)</p>
<p>This technical diagram explains <em>why</em> models hallucinate. The speaker clarifies that LLMs function by trying to <strong>predict the next word or token</strong> based on statistical likelihood. They are not databases of facts; they are engines designed to construct coherent sentences.</p>
<p>“They’re not worried about truth and false; they’re really trying to tell what the most cohesive, coherent story is.” Because the model is optimizing for the most probable next word to complete a pattern, it will confidently generate plausible-sounding but factually incorrect information if that sequence of words is statistically likely.</p>
</section>
<section id="llm-mistakes" class="level3">
<h3 class="anchored" data-anchor-id="llm-mistakes">6. LLM Mistakes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_6.png" class="img-fluid figure-img"></p>
<figcaption>Slide 6</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=210s">Timestamp: 03:30</a>)</p>
<p>Here, the speaker provides examples of the “Next Token Prediction” logic failing to provide truth. If asked for the “Capital of Mars,” the model doesn’t know Mars has no capital; it simply tries to “complete that story” by inventing a name. Similarly, when asked to perform math, the model isn’t calculating; it is predicting the next characters in a math-like sequence.</p>
<p>The slide shows the model failing at basic arithmetic because “it looks like it’s read too many release notes, not actually enough math.” This reinforces that LLMs are linguistic tools, not calculators or knowledge bases, and they lack an internal concept of “fictional” versus “factual.”</p>
</section>
<section id="risks-for-enterprises" class="level3">
<h3 class="anchored" data-anchor-id="risks-for-enterprises">7. Risks for Enterprises</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_7.png" class="img-fluid figure-img"></p>
<figcaption>Slide 7</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=243s">Timestamp: 04:03</a>)</p>
<p>This slide highlights the liability risks for companies, citing the <strong>Air Canada chatbot case</strong>. In this instance, a chatbot invented a refund policy that did not exist. When the customer sued, the airline argued the chatbot was responsible, but the tribunal ruled the company was liable for its agent’s statements.</p>
<p>The speaker notes, “We’re going to treat this chatbot just like one of your employees… you’re responsible for what this model says.” This legal precedent explains why enterprises are hesitant to deploy Gen AI and why “Gen AI committees” are forming to manage governance and risk before public deployment.</p>
</section>
<section id="retrieval-augmented-generation-rag" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-augmented-generation-rag">8. Retrieval-Augmented Generation (RAG)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_8.png" class="img-fluid figure-img"></p>
<figcaption>Slide 8</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=291s">Timestamp: 04:51</a>)</p>
<p>To solve the hallucination problem, the presentation introduces <strong>Retrieval-Augmented Generation (RAG)</strong>. The speaker describes this as a solution from academia designed to “ground” the model. Instead of relying solely on the model’s internal training data, RAG surrounds the model with external context.</p>
<p>The core idea is simple: “We’re going to ground it with information so it uses that information in answering the question.” This technique attempts to bridge the gap between the model’s linguistic capabilities and the need for factual accuracy in enterprise applications.</p>
</section>
<section id="how-rag-works" class="level3">
<h3 class="anchored" data-anchor-id="how-rag-works">9. How RAG Works</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_9.png" class="img-fluid figure-img"></p>
<figcaption>Slide 9</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=320s">Timestamp: 05:20</a>)</p>
<p>This diagram breaks down the RAG architecture. When a user asks a question, the system does not send it directly to the LLM. First, it goes out to “search and look for is there relevant information that’s related to this question.”</p>
<p>Once relevant documents are collected from a knowledge base, they are bundled with the original question and sent to the LLM. The LLM then generates an answer based <em>only</em> on that provided context. This ensures the “final answer is grounded” by factual documents rather than the model’s statistical predictions alone.</p>
</section>
<section id="grounding-with-10-k-forms" class="level3">
<h3 class="anchored" data-anchor-id="grounding-with-10-k-forms">10. Grounding with 10-K Forms</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=351s">Timestamp: 05:51</a>)</p>
<p>The speaker sets up a practical RAG demonstration using <strong>10-K forms</strong> (annual reports filed by public companies). These documents are chosen because “you can trust that they’re factual.”</p>
<p>This slide prepares the audience to see how the previous question about Rivian’s manufacturing capacity—which generated inconsistent answers earlier—can be answered accurately when the model is forced to look at Rivian’s official financial filings.</p>
</section>
<section id="rivian-manufacturing-answer" class="level3">
<h3 class="anchored" data-anchor-id="rivian-manufacturing-answer">11. Rivian Manufacturing Answer</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=367s">Timestamp: 06:07</a>)</p>
<p>The slide shows the output of a RAG application. The question “How many vehicles do you manufacture in Normal?” is asked again. This time, the application provides a specific, fact-based answer derived from the uploaded documents.</p>
<p>This demonstrates the immediate utility of RAG: it turns the LLM from a creative writing engine into a synthesis engine that can read specific enterprise documents and extract the correct answer, mitigating the hallucination issues seen in Slide 4.</p>
</section>
<section id="context-and-citations" class="level3">
<h3 class="anchored" data-anchor-id="context-and-citations">12. Context and Citations</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=381s">Timestamp: 06:21</a>)</p>
<p>A critical feature of RAG is displayed here: <strong>Citations</strong>. The application shows exactly which document the answer came from. The speaker notes, “I can see exactly what’s the document that this answer came from… a nice source.”</p>
<p>This transparency is why RAG is the “number one most popular generative AI application.” It allows users to verify the AI’s work, building trust in the system—something impossible with a standard “black box” LLM response.</p>
</section>
<section id="chatbot-for-legal-research" class="level3">
<h3 class="anchored" data-anchor-id="chatbot-for-legal-research">13. Chatbot for Legal Research</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=420s">Timestamp: 07:00</a>)</p>
<p>The narrative shifts to the fictional law firm “Dewey, Cheatham, and Howe.” The firm wants to use AI to reduce the heavy workload of legal research. The initial thought process is to use raw LLMs because they are knowledgeable.</p>
<p>The speaker introduces a colleague who assumes, “I know it could pass the bar exam… why don’t I just wire it up directly?” This sets up the common misconception that because a model has general knowledge (passing a test), it is suitable for specialized professional work without further engineering.</p>
</section>
<section id="gpt-models-on-the-bar-exam" class="level3">
<h3 class="anchored" data-anchor-id="gpt-models-on-the-bar-exam">14. GPT Models on the Bar Exam</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=442s">Timestamp: 07:22</a>)</p>
<p>This chart reinforces the previous assumption, showing the progression of GPT models on the <strong>Multistate Bar Exam (MBE)</strong>. GPT-4 significantly outperforms its predecessors, achieving a passing score.</p>
<p>While this suggests the model “knows something about the law,” the speaker hints that this is merely a multiple-choice test. Success here does not necessarily translate to the nuance required for actual legal practice, foreshadowing the errors to come in the story.</p>
</section>
<section id="hallucinating-statutes" class="level3">
<h3 class="anchored" data-anchor-id="hallucinating-statutes">15. Hallucinating Statutes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_15.png" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=470s">Timestamp: 07:50</a>)</p>
<p>The first failure of the “raw LLM” approach is revealed. A lawyer asks for statutes regarding “online dating services in Connecticut.” The model confidently provides “Connecticut General Statute § 42-290.”</p>
<p>However, the lawyer discovers “there is no statute; this was entirely hallucinated.” Despite passing the bar exam, the model fabricated a law that sounded plausible but did not exist. This forces the firm to pivot toward a RAG approach to ground the AI in real legal literature.</p>
</section>
<section id="lexis-ai" class="level3">
<h3 class="anchored" data-anchor-id="lexis-ai">16. Lexis+ AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=510s">Timestamp: 08:30</a>)</p>
<p>The firm decides to use professional tools. They turn to <strong>Lexis+ AI</strong>, a commercial product that promises “Hallucination-Free Linked Legal Citations.” This tool uses the RAG approach discussed earlier, retrieving from a database of real case law.</p>
<p>The expectation is that by using a trusted vendor with a RAG architecture, the hallucination problem will be solved, and lawyers will receive accurate, citable information.</p>
</section>
<section id="conceptual-hallucinations" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-hallucinations">17. Conceptual Hallucinations</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_17.png" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=530s">Timestamp: 08:50</a>)</p>
<p>Even with RAG and real citations, a new problem emerges: <strong>Conceptual confusion</strong>. The AI provides a real case but confuses the “Equity Cleanup Doctrine” with the “Doctrine of Clean Hands.” The speaker explains that while the words are similar, the legal concepts are distinct (one is about consolidating claims, the other about a plaintiff’s conduct, illustrated by a joke about P. Diddy).</p>
<p>The model found a document containing the words but failed to understand the <em>meaning</em>. This shows that RAG ensures the <em>document</em> exists, but not necessarily that the <em>reasoning</em> or application of that document is correct.</p>
</section>
<section id="the-fictional-judge" class="level3">
<h3 class="anchored" data-anchor-id="the-fictional-judge">18. The Fictional Judge</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_18.png" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=590s">Timestamp: 09:50</a>)</p>
<p>The model’s failure deepens with an example of an “inside joke.” A lawyer asks for opinions by “Judge Luther A. Wilgarten.” Wilgarten is a fictional judge created as a prank in law reviews.</p>
<p>The AI, treating the law reviews as factual text, retrieves “cases” by this fake judge. It fails to distinguish between a real judicial opinion and a satirical article within its knowledge base. This illustrates the “garbage in, garbage out” risk even within RAG systems if the model cannot discern the nature of the source material.</p>
</section>
<section id="hallucination-rates-in-legal-ai" class="level3">
<h3 class="anchored" data-anchor-id="hallucination-rates-in-legal-ai">19. Hallucination Rates in Legal AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_19.png" class="img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=644s">Timestamp: 10:44</a>)</p>
<p>The speaker references a Stanford paper analyzing hallucination rates across major legal AI tools (Lexis, Westlaw, GPT-4). The chart shows that these tools still hallucinate or provide incomplete answers <strong>17% to 33% of the time</strong>.</p>
<p>This data point serves as a reality check: “These models hallucinate using real questions.” Despite marketing claims of being “hallucination-free,” the complexity of the domain means that errors are still frequent, posing significant risks for professional use.</p>
</section>
<section id="limits-of-rag" class="level3">
<h3 class="anchored" data-anchor-id="limits-of-rag">20. Limits of RAG</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_20.png" class="img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=663s">Timestamp: 11:03</a>)</p>
<p>This slide summarizes the limitations discovered in the legal example. RAG works well when documents are “True, Authoritative, and Applicable.” However, in complex domains like law, these attributes are often contested.</p>
<p>“Sometimes all these things are very contested and it gets really hard to separate it.” If the underlying documents contain conflicting information, satire, or outdated facts, the RAG system (which assumes retrieved text is “truth”) will propagate those errors to the user.</p>
</section>
<section id="why-legal-is-hard" class="level3">
<h3 class="anchored" data-anchor-id="why-legal-is-hard">21. Why Legal is Hard</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_21.png" class="img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=688s">Timestamp: 11:28</a>)</p>
<p>The speaker elaborates on the complexity of legal research. It involves navigating different <strong>specialties</strong> (Tort vs.&nbsp;Maritime), <strong>jurisdictions</strong> (Federal vs.&nbsp;State), and <strong>authorities</strong> (Supreme Court vs.&nbsp;Law Reviews). Furthermore, the element of <strong>time</strong> is crucial—knowing if a case has been overturned.</p>
<p>“You really have to have a lot of knowledge to be able to weave everything in and out.” An LLM often lacks the meta-knowledge to weigh these factors, treating a lower court opinion from 1950 with the same weight as a Supreme Court ruling from 2024.</p>
</section>
<section id="conclusion-on-legal-chatbots" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-on-legal-chatbots">22. Conclusion on Legal Chatbots</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_22.png" class="img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=780s">Timestamp: 13:00</a>)</p>
<p>The conclusion for the legal use case is that human expertise remains essential. While AI can “get you a stack of documents,” you still need “facts people to actually tease out the insights.”</p>
<p>The current state of technology is an aid, not a replacement. The speaker transitions away from the legal example to a new story about building a data application, suggesting that while law is hard, structured data might offer different challenges and solutions.</p>
</section>
<section id="building-generative-ai-text-to-sql" class="level3">
<h3 class="anchored" data-anchor-id="building-generative-ai-text-to-sql">23. Building Generative AI (Text-to-SQL)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_23.png" class="img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=800s">Timestamp: 13:20</a>)</p>
<p>The presentation shifts to the story of “Frosty,” a company building a <strong>Text-to-SQL</strong> application. The goal is to turn natural language questions (e.g., “How many orders do I have in each state?”) into SQL code that can query a database.</p>
<p>This is a “very common application” for Gen AI, allowing non-technical users to interact with data. This section will focus on the engineering steps required to build this system, moving beyond the simple RAG implementation discussed previously.</p>
</section>
<section id="evaluating-sql-queries" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-sql-queries">24. Evaluating SQL Queries</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_24.png" class="img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=854s">Timestamp: 14:14</a>)</p>
<p>The first challenge in building this app is <strong>evaluation</strong>. How do you know if the AI’s generated SQL is good? The slide shows a “Gold Standard” query (the correct answer) and a “Candidate SQL” (the AI’s attempt).</p>
<p>In this example, the AI added an extra column (“latitude”) that wasn’t requested. While the query might still work, it isn’t an exact match. The speaker notes, “We really need to have a way to give partial credit,” because simple string matching would mark this helpful addition as a failure.</p>
</section>
<section id="model-based-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="model-based-evaluation">25. Model Based Evaluation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_25.png" class="img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=945s">Timestamp: 15:45</a>)</p>
<p>To solve the grading problem at scale, the speaker introduces <strong>Model-Based Evaluation</strong>. This involves using an LLM (like GPT-4) to act as the “judge” for the output of another model.</p>
<p>Instead of humans manually grading thousands of SQL queries, “we’re going to use a large language model to do this.” This allows for nuanced grading (partial credit) that strict code comparison cannot provide.</p>
</section>
<section id="skepticism-of-model-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="skepticism-of-model-evaluation">26. Skepticism of Model Evaluation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_26.png" class="img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=959s">Timestamp: 15:59</a>)</p>
<p>The speaker acknowledges the common reaction to this technique: “Is that going to work? I mean that’s like the fox guarding the outhouse.” There is a fear of “model collapse” or circular logic when AI evaluates AI.</p>
<p>Despite this intuition, the speaker assures the audience that this is a standard and effective practice in modern AI development, and proceeds to explain how to implement it correctly.</p>
</section>
<section id="the-evaluation-prompt" class="level3">
<h3 class="anchored" data-anchor-id="the-evaluation-prompt">27. The Evaluation Prompt</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_27.png" class="img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=974s">Timestamp: 16:14</a>)</p>
<p>This slide reveals the <strong>system prompt</strong> used for the model-based judge. It instructs the LLM to act as a “data quality analyst” and provides a specific grading rubric (0 to 3 scale).</p>
<p>By explicitly defining what constitutes a “Perfect Match,” “Good Match,” or “No Match,” the engineer can control how the AI judges the output. This turns a subjective assessment into a structured, automated process.</p>
</section>
<section id="the-tx-vs-texas-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-tx-vs-texas-problem">28. The “TX” vs “TEXAS” Problem</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_28.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1010s">Timestamp: 16:50</a>)</p>
<p>A specific example of why strict matching fails. The user asked for data in “Texas.” The database uses the abbreviation ‘TX’, but the AI generated a query looking for ‘TEXAS’.</p>
<p>“It’s a natural mistake here to confuse TX and Texas… but if we go with the strict criteria of that exact match, we don’t get an exact match.” A standard code test would fail this, even though the intent is correct and easily fixable.</p>
</section>
<section id="execution-accuracy-failure" class="level3">
<h3 class="anchored" data-anchor-id="execution-accuracy-failure">29. Execution Accuracy Failure</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_29.png" class="img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1036s">Timestamp: 17:16</a>)</p>
<p>This slide confirms that under “Execution Accuracy” (strict matching), the query is a failure (“No Match”). This metric is too harsh for development because it obscures progress; a model that gets the logic right but misses an abbreviation is much better than one that writes gibberish.</p>
</section>
<section id="execution-score-success" class="level3">
<h3 class="anchored" data-anchor-id="execution-score-success">30. Execution Score Success</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_30.png" class="img-fluid figure-img"></p>
<figcaption>Slide 30</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1050s">Timestamp: 17:30</a>)</p>
<p>Using the <strong>Model-Based Evaluation</strong>, the same ‘TX’ vs ‘TEXAS’ error is graded differently. The “Execution Score” is a “Perfect Match” because the judge recognizes the semantic intent was captured.</p>
<p>“It captures the user’s intent… the user could easily fix this.” This allows developers to optimize the model for logic and reasoning first, handling minor syntax issues separately.</p>
</section>
<section id="correlation-with-other-metrics" class="level3">
<h3 class="anchored" data-anchor-id="correlation-with-other-metrics">31. Correlation with Other Metrics</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_31.png" class="img-fluid figure-img"></p>
<figcaption>Slide 31</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1070s">Timestamp: 17:50</a>)</p>
<p>The speaker presents data showing a <strong>strong correlation</strong> between the model-based scores and other evaluation methods. When the model judge gives a 5/5, other metrics generally agree.</p>
<p>This validation step is crucial. The engineer in the story checked her results and found “80% were the exact same when she scored them.” This high level of agreement gives confidence in automating the evaluation pipeline.</p>
</section>
<section id="research-on-model-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="research-on-model-evaluation">32. Research on Model Evaluation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_32.png" class="img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1099s">Timestamp: 18:19</a>)</p>
<p>Supporting the anecdote, this slide references broader research indicating that LLMs correlate with human judges about <strong>80% of the time</strong> regarding correctness and readability.</p>
<p>“I got tired of adding research sites here… universally we see that often in many contexts that these large language models correlate about 80% of the time to humans.” This establishes model-based evaluation as an industry standard.</p>
</section>
<section id="initial-benchmark-results" class="level3">
<h3 class="anchored" data-anchor-id="initial-benchmark-results">33. Initial Benchmark Results</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_33.png" class="img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1130s">Timestamp: 18:50</a>)</p>
<p>After setting up the evaluation pipeline and creating an <strong>internal enterprise benchmark</strong> (not a public dataset), the initial results are poor: only <strong>33% accuracy</strong>.</p>
<p>The speaker emphasizes the importance of using internal data for benchmarks: “You can’t trust those public data sets… they’re far too easy.” The low score sets the stage for the iterative engineering process required to improve the application.</p>
</section>
<section id="using-multiple-models" class="level3">
<h3 class="anchored" data-anchor-id="using-multiple-models">34. Using Multiple Models</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_34.png" class="img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1155s">Timestamp: 19:15</a>)</p>
<p>The first improvement strategy is <strong>Ensembling</strong>. The engineer noticed different models had different strengths, so she combined them.</p>
<p>“In traditional machine learning, we often Ensemble models… she decided to try the same thing here.” By using multiple Text-to-SQL models and combining their outputs, performance improved.</p>
</section>
<section id="error-correction-self-reflection" class="level3">
<h3 class="anchored" data-anchor-id="error-correction-self-reflection">35. Error Correction (Self-Reflection)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_35.png" class="img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1180s">Timestamp: 19:40</a>)</p>
<p>The next optimization is <strong>Error Correction</strong> via self-reflection. When the model generates an error, the system asks the model to “reflect upon it” or think “step-by-step.”</p>
<p>“That actually makes the model spend more time thinking about it… and actually they can use all of that to get a better answer.” This technique, often called <strong>Chain of Thought</strong>, leverages the model’s ability to debug its own output when prompted correctly.</p>
</section>
<section id="screening-inputs" class="level3">
<h3 class="anchored" data-anchor-id="screening-inputs">36. Screening Inputs</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_36.png" class="img-fluid figure-img"></p>
<figcaption>Slide 36</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1230s">Timestamp: 20:30</a>)</p>
<p>Improving the input data is just as important as improving the model. The engineer adds a <strong>Screening</strong> layer to filter out questions that are ambiguous or irrelevant (non-SQL questions).</p>
<p>“She noticed that a lot of what the users were typing in just didn’t make sense.” By catching bad queries early and asking the user for clarification, the system avoids processing garbage data, thereby increasing overall success rates.</p>
</section>
<section id="feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="feature-extraction">37. Feature Extraction</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_37.png" class="img-fluid figure-img"></p>
<figcaption>Slide 37</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1300s">Timestamp: 21:40</a>)</p>
<p>Recognizing that different questions require different handling, the engineer implements <strong>Feature Extraction</strong>. A time-series question needs different context than a ranking question.</p>
<p>“If I’m cooking macaroni and cheese I need different ingredients than if I’m making tacos.” The system now identifies the <em>type</em> of question and extracts the specific features (metadata, table schemas) relevant to that type before generating SQL.</p>
</section>
<section id="the-semantic-layer" class="level3">
<h3 class="anchored" data-anchor-id="the-semantic-layer">38. The Semantic Layer</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_38.png" class="img-fluid figure-img"></p>
<figcaption>Slide 38</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1370s">Timestamp: 22:50</a>)</p>
<p>To bridge the gap between messy enterprise databases and user language, a <strong>Semantic Layer</strong> is added. This involves human experts defining the data structure in business terms.</p>
<p>“We’re going to use the expertise… to give us details of the data structure in a way that deals with all this confusing structure.” This layer translates business logic (e.g., what defines a “churned customer”) into a schema the AI can understand, significantly boosting accuracy.</p>
</section>
<section id="generative-ai-decision-app" class="level3">
<h3 class="anchored" data-anchor-id="generative-ai-decision-app">39. Generative AI Decision App</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_39.png" class="img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1450s">Timestamp: 24:10</a>)</p>
<p>This flowchart represents the final, production-grade system. It is no longer just a prompt sent to a model. It includes classification, feature extraction, multiple SQL generation agents, error correction, and a semantic layer.</p>
<p>The lesson is that “Generative AI is not about a data scientist sitting out an Island by themselves… instead it’s building a system like this.” It requires a cross-functional team of analysts, engineers, and domain experts to build a reliable application.</p>
</section>
<section id="the-future-of-ai" class="level3">
<h3 class="anchored" data-anchor-id="the-future-of-ai">40. The Future of AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_40.png" class="img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1500s">Timestamp: 25:00</a>)</p>
<p>The speaker pivots to the future, acknowledging the rapid pace of innovation from companies like <strong>OpenAI</strong> and <strong>Google DeepMind</strong>. He addresses the audience’s potential skepticism: “The future is you’re just going to be able to just take all your data cram it into one thing it’s just going to solve it all for you.”</p>
<p>This sets up the final section on <strong>Reasoning and Planning</strong>, moving beyond simple retrieval and text generation.</p>
</section>
<section id="can-llms-reason-and-plan-block-world" class="level3">
<h3 class="anchored" data-anchor-id="can-llms-reason-and-plan-block-world">41. Can LLMs Reason and Plan? (Block World)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_41.png" class="img-fluid figure-img"></p>
<figcaption>Slide 41</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1538s">Timestamp: 25:38</a>)</p>
<p>To test reasoning, the speaker introduces the <strong>Block World</strong> benchmark. The task is to stack colored blocks in a specific order. This requires multi-step planning.</p>
<p>“You have to logically think and plan for maybe five, six, ten, even 20 steps to be able to solve it.” This tests the model’s ability to handle dependencies and sub-tasks, rather than just predicting the next word.</p>
</section>
<section id="gpt-4-planning-performance" class="level3">
<h3 class="anchored" data-anchor-id="gpt-4-planning-performance">42. GPT-4 Planning Performance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_42.png" class="img-fluid figure-img"></p>
<figcaption>Slide 42</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1600s">Timestamp: 26:40</a>)</p>
<p>The results for GPT-4 are shown. While it achieves 34% on the standard Block World, its performance collapses to <strong>3%</strong> in “Mystery World.” Mystery World is the same problem, but the block names are randomized (e.g., obfuscated).</p>
<p>“What you call them doesn’t matter [to a human]… but for a large language model, what you does call them matters a lot.” The collapse in performance proves the model was relying on memorized patterns (approximate reasoning) rather than true logical planning.</p>
</section>
<section id="o1-models-and-progress" class="level3">
<h3 class="anchored" data-anchor-id="o1-models-and-progress">43. o1 Models and Progress</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_43.png" class="img-fluid figure-img"></p>
<figcaption>Slide 43</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1670s">Timestamp: 27:50</a>)</p>
<p>The speaker updates the data with the very latest <strong>OpenAI o1 model</strong> results. This model uses “Chain of Thought on steroids” (reinforcement learning). It shows a massive improvement, jumping to nearly 100% on Block World and significantly higher on Mystery World (around 37-53%).</p>
<p>While this is “solid progress,” the speaker notes it “still has a ways to go.” The models are getting better at <strong>approximate reasoning</strong>, but they are not infallible logic engines yet.</p>
</section>
<section id="be-skeptical-of-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="be-skeptical-of-benchmarks">44. Be Skeptical of Benchmarks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_44.png" class="img-fluid figure-img"></p>
<figcaption>Slide 44</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1776s">Timestamp: 29:36</a>)</p>
<p>A warning accompanies the new capabilities: <strong>Be skeptical</strong>. As models get better at approximating reasoning, their mistakes will become harder to spot. They will sound incredibly convincing even when they are logically flawed.</p>
<p>“You’re going to have to have an expert to be able to tell when this models are going off the rails… because the Baseline for these models is so good.” Just as legal experts were needed for RAG, domain experts are needed to verify AI reasoning.</p>
</section>
<section id="common-gen-ai-use-cases-summary" class="level3">
<h3 class="anchored" data-anchor-id="common-gen-ai-use-cases-summary">45. Common Gen AI Use Cases Summary</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_45.png" class="img-fluid figure-img"></p>
<figcaption>Slide 45</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1813s">Timestamp: 30:13</a>)</p>
<p>The speaker summarizes the key technical concepts covered: <strong>Hallucinations</strong>, <strong>RAG</strong>, <strong>Reasoning</strong>, <strong>Evaluation</strong>, <strong>Model as a Judge</strong>, and <strong>Data Enrichment</strong>.</p>
<p>These pillars form the basis of current Gen AI development. The presentation has moved from the simple idea of “asking a chatbot” to the complex reality of building systems that manage retrieval, evaluation, and reasoning.</p>
</section>
<section id="project-reality" class="level3">
<h3 class="anchored" data-anchor-id="project-reality">46. Project Reality</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_46.png" class="img-fluid figure-img"></p>
<figcaption>Slide 46</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1835s">Timestamp: 30:35</a>)</p>
<p>The final takeaway emphasizes the organizational aspect. “Generative AI is like any other project and doesn’t go as planned.” It is not magic; it is engineering.</p>
<p>Success requires a diverse team (“system of people”) including evaluators, analysts, and technical builders. It is an iterative process that involves handling messy data and managing expectations.</p>
</section>
<section id="closing-title" class="level3">
<h3 class="anchored" data-anchor-id="closing-title">47. Closing Title</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/practical-rag-text-to-sql/slide_47.png" class="img-fluid figure-img"></p>
<figcaption>Slide 47</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/OyY4uxUShys&amp;t=1853s">Timestamp: 30:53</a>)</p>
<p>The presentation concludes. The speaker thanks the audience, hoping the stories of the law firm and the data company provided a realistic “Practical Perspective” on the current state of Generative AI.</p>
<hr>
<p><em>This annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations.</em></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/rajivshah\.com\/blog");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 @rajistics - Rajiv Shah
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.rajivshah.com">
<p><u>About Me</u></p>
</a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="./index.xml">
      <i class="bi bi-rss" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/rajistics/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/rajistics">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/rajistics/">
      <i class="bi bi-instagram" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.tiktok.com/@rajistics">
      <i class="bi bi-tiktok" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCu9fxVjTz5AJO7FR1upY02w">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rajshah4">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>