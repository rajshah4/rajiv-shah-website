<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Rajiv Shah - rajistics blog</title>
<link>https://rajivshah.com/blog/</link>
<atom:link href="https://rajivshah.com/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Rajistics blog</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Fri, 26 Dec 2025 06:00:00 GMT</lastBuildDate>
<item>
  <title>Running Code and Failing Models</title>
  <link>https://rajivshah.com/blog/running-code-failing-models.html</link>
  <description><![CDATA[ 






<p><img src="https://cdn-images-1.medium.com/max/1600/1*jL9fT-oAR6Ki3HOvXpwMLQ.png" class="img-fluid" alt="img"> Source: Yuriy Guts selection from Shutterstock</p>
<p>Machine learning is a glass cannon. When used correctly, it can be a truly transformative technology, but just a small oversight can cause it to become misleading and even actively harmful. Even if all the code runs and the model seems to be spitting out reasonable answers, it’s possible for a model to encode fundamental data science mistakes that invalidate its results. These errors might seem small, but the effects can be disastrous when the model is used to make decisions in the real world.</p>
<p>The promise and power of AI lead many researchers to gloss over the ways in which things can go wrong when building and operationalizing machine learning models. As a data scientist, one of my passions is to reproduce research papers as a learning exercise. Along the way, I have uncovered cases where the research was published with faulty methodologies. My hope is that this analysis can increase awareness about data science mistakes and raise the standards for machine learning in research. For example, last year I shared an analysis of a project by Harvard and Google researchers that contained fundamental errors. The researchers refused to fix their mistake even when confronted with it directly.</p>
<p>Over the holidays, I used DataRobot to reproduce a few machine learning benchmarks. I found many examples of machine learning code that ran without errors but that were built using flawed data science practices. The examples I share in this post come from the world’s best data scientists and affect hundreds of peer-reviewed research publications. As these examples show, errors in machine learning can be subtle. The key to finding these errors is to work with a tool that offers guardrails and insights along the way.</p>
<section id="target-leakage-in-a-fast.ai-example" class="level2">
<h2 class="anchored" data-anchor-id="target-leakage-in-a-fast.ai-example">Target Leakage in a fast.ai Example</h2>
<p><em>Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD</em> by Jeremy Howard and Sylvain Gugger is a hands-on guide that helps people with little math background understand and use deep learning quickly. In the section about tabular datasets, the authors use the Blue Book for Bulldozers problem, the goal of which is to predict the sale price for heavy equipment at auction. I tried to replicate their machine learning model and wasn’t able to beat their model’s predictive performance, which piqued my interest.</p>
<p>After carefully inspecting their code, I found a mistake in their validation dataset. Their code attempted to create a validation test set based on a prediction point of November 1, 2011. The goal was to split the data at this point so that you could train on the data known at prediction time. The performance of the model is then analyzed on a test set, which is located after the prediction point. Unfortunately, the code was not written correctly; there was contamination from the future in the training data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/leakage1.png" class="img-fluid figure-img"></p>
<figcaption>Leakage.png</figcaption>
</figure>
</div>
<p>The code below might at first look like it separates data before and after November 1, 2011, but there’s a subtle mistake that includes future dates. The use of information in the model training process that would not be expected at prediction time is known as <strong>target leakage</strong>, and it led to an over-optimistic accuracy. Because I used DataRobot, which requires and validates a date when creating a validation dataset based on time, I was able to find the mistake in the fast.ai book.</p>
<p>After the target leakage was fixed, the fast.ai scores dropped, and I was able to reproduce the results outside of fast.ai. This simple coding mistake led to a notebook and model that appeared valid. If this model were put into production, the results would have been much worse on new data. After I identified this issue, Jeremy Howard agreed to add a note in the course materials.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/fastai2.png" class="img-fluid figure-img"></p>
<figcaption>fastai2.png</figcaption>
</figure>
</div>
</section>
<section id="sarcos-dataset-failure" class="level2">
<h2 class="anchored" data-anchor-id="sarcos-dataset-failure">SARCOS Dataset Failure</h2>
<p>The SARCOS dataset is a widely used benchmark dataset in machine learning. Based on predicting the movement of a robotic arm, SARCOS appears in more than one hundred academic papers. I tested this dataset because it appears in various benchmarks by Google and fast.ai.</p>
<p>The SARCOS dataset is broken into two parts: a training dataset (sarcos_inv) and a test dataset (sarcos_inv_test). Following common data science practices, DataRobot broke the SARCOS training set into a training partition and a validation partition. I treated the SARCOS test set (sarcos_inv_test) as a holdout. When I looked at the results, I immediately noticed something suspicious. Do you see it?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/sarcos3.png" class="img-fluid figure-img"></p>
<figcaption>sarcos3.png</figcaption>
</figure>
</div>
<p>The large drop between the validation score and the holdout score indicates that something is very different between the validation and holdout datasets. When I examined the holdout dataset (the SARCOS test set), I found that every row in the test set was in the training data too. After some investigation, I discovered that the holdout dataset was built out of the training dataset. Of the 4,449 examples in the test set, 4,445 examples are present in the training set, too. The target leakage here is significant. By overfitting or memorizing the training dataset, it’s possible to get perfect results on the test set. Overfitting, a well-known issue in machine learning, is illustrated in the following figure. The test dataset should have used out-of-sample testing to prevent overfitting.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/overfit4.png" class="img-fluid figure-img"></p>
<figcaption>overfit4.png</figcaption>
</figure>
</div>
<p>Target leakage helped to explain the very low scores of the deep learning models. For comparison, a random forest model achieves 2.38 mean squared error (MSE), while a deep learning model overfits and produces 0.038 MSE. Judging from the suspiciously large difference between the models, it appears that the deep learning model just memorized the training data, which is why it had such low error.</p>
<p>The consequences of this target leakage are far-reaching. More than one hundred journal articles relied on this dataset. Thousands of data scientists have used it to benchmark their machine learning code. Researcher Kai Arulkumaran has already acknowledged this issue and now the research community is dealing with the ramifications of the target leakage.</p>
<p>Why wasn’t this error discovered earlier? When I reproduced the SARCOS benchmarks, I used a tool that includes technical safeguards for proper validation splits and provides transparency in the display of the results of each split. DataRobot’s AutoML was designed by data scientists to prevent these sorts of issues. In contrast, working within code, it was quite easy to overlook this fundamental issue. After all, thousands of data scientists have rerun their code and published their results without a second thought.</p>
</section>
<section id="poker-hand-dataset" class="level2">
<h2 class="anchored" data-anchor-id="poker-hand-dataset">Poker Hand Dataset</h2>
<p>The Poker Hand dataset is another widely used benchmark dataset in machine learning. It’s used to predict poker hands (for example, a full house from five cards). The fast.ai and Google benchmarks for this model use the accuracy metric. Accuracy is a measurement for assessing the predictive performance of a model (basically, the percentage of predictions that are correct). Although it’s easy to get running code with the accuracy metric, it’s not good data science practice for this problem.</p>
<p>When DataRobot builds a model with the Poker Hand dataset, by default, it uses log loss as an optimization metric. Log loss is a measure of error for a model. At DataRobot, we believe that it isn’t good practice to use accuracy as your metric on a classification project with imbalanced classes. With imbalanced data, you can easily build a highly accurate model that’s useless.</p>
<p>To understand why accuracy isn’t the best metric when classifying unbalanced data, consider the following figure. Minesweeper is a popular game where the goal is to identify a few mines that are scattered across a board. Because there are a lot of squares with no mines, you could generate a very accurate model just by predicting that every square is safe. Although a 99% accurate model for Minesweeper sounds impressive, it’s not very useful.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/minesweeper5.png" class="img-fluid figure-img"></p>
<figcaption>minesweeper5.png</figcaption>
</figure>
</div>
<p>Automated feature selection in DataRobot provides a more parsimonious featurelist. In the Poker Hand dataset, DataRobot created a DR Reduced Features list with only six features. The starting feature list for this dataset, Cat+Cont, contained 15 features. The leaderboard below shows that the simpler DR Reduced Features list performs better than the full Cat+Cont feature list. The model below was optimized on log loss, but I am viewing the accuracy metrics for comparison to the existing benchmarks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/DRreduce6.png" class="img-fluid figure-img"></p>
<figcaption>DRreduce6.png</figcaption>
</figure>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I have shared simple examples of how data scientists can have running code, but failed models. After spending a week going through a half dozen datasets, I am even more convinced that automation with technical safeguards is a required part of building trusted AI. The mistakes I’ve shared here are not isolated incidents.</p>
<p>The issues go beyond the reproducibility crisis for machine learning research. It’s a great first step for researchers to publish their code and make the data available, but as these examples show, sharing code isn’t enough to validate models. So, what should you do about this?</p>
<p>In regulated industries, there are processes in place to validate running code (for example, building a challenger model using a different technical framework). For its safeguards and transparency, many organizations use DataRobot to validate models. Just rereading or rerunning a project isn’t enough to identify errors.</p>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<ul>
<li><a href="https://medium.com/data-science/stand-up-for-best-practices-8a8433d3e0e8">Stand Up for Best Practices (Harvard Leakage)</a></li>
<li><a href="https://github.com/fastai/fastbook/issues/325">Fast.AI Issue</a></li>
<li><a href="https://github.com/Kaixhin/SARCOS">SARCOS</a></li>
</ul>


</section>

 ]]></description>
  <category>Leakage</category>
  <category>Earthquake</category>
  <category>SARCOS</category>
  <guid>https://rajivshah.com/blog/running-code-failing-models.html</guid>
  <pubDate>Fri, 26 Dec 2025 06:00:00 GMT</pubDate>
  <media:content url="https://cdn-images-1.medium.com/max/1600/1*jL9fT-oAR6Ki3HOvXpwMLQ.png" medium="image" type="image/png"/>
</item>
<item>
  <title>A Practical Guide to Evaluating Generative AI Applications</title>
  <link>https://rajivshah.com/blog/genai-evaluation-guide.html</link>
  <description><![CDATA[ 






<section id="video" class="level2">
<h2 class="anchored" data-anchor-id="video">Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/qPHsWTZP58U" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Watch the <a href="https://youtu.be/qPHsWTZP58U">full video</a> | <a href="https://youtu.be/qPHsWTZP58U">Slides</a></p>
<hr>
</section>
<section id="annotated-presentation" class="level2">
<h2 class="anchored" data-anchor-id="annotated-presentation">Annotated Presentation</h2>
<p>Below is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.</p>
<p>Here is the annotated presentation for Rajiv Shah’s workshop on “Hill Climbing: Best Practices for Evaluating LLMs.”</p>
<section id="title-slide" class="level3">
<h3 class="anchored" data-anchor-id="title-slide">1. Title Slide</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_1.png" class="img-fluid figure-img"></p>
<figcaption>Slide 1</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=0s">Timestamp: 00:00</a>)</p>
<p>This slide introduces the workshop titled <strong>“Hill Climbing: Best Practices for Evaluating LLMs,”</strong> presented by Rajiv Shah, PhD, at the Open Data Science Conference (ODSC). The presentation focuses on the technical nuances of Generative AI and how to build effective evaluation workflows.</p>
<p>Rajiv sets the stage by outlining his three main goals for the session: understanding the technical differences in GenAI evaluation, learning a basic introductory workflow for building evaluation datasets, and inspiring practitioners to start “learning by doing” rather than just reading papers.</p>
<p>The concept of “Hill Climbing” refers to the iterative process of improving LLM applications—starting with a baseline and continuously optimizing performance through rigorous testing and error analysis.</p>
</section>
<section id="evaluating-for-gen-ai-resources" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-for-gen-ai-resources">2. Evaluating for Gen AI Resources</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_2.png" class="img-fluid figure-img"></p>
<figcaption>Slide 2</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=6s">Timestamp: 00:06</a>)</p>
<p>This slide provides a QR code and a GitHub URL, directing the audience to the code and resources associated with the talk. It emphasizes that the workshop is practical, with code examples available for attendees to replicate the evaluation techniques discussed.</p>
<p>Rajiv encourages the audience to access these resources to follow along with the technical implementations of the concepts, such as building LLM judges and creating unit tests, which will be covered later in the presentation.</p>
</section>
<section id="customer-support-use-case" class="level3">
<h3 class="anchored" data-anchor-id="customer-support-use-case">3. Customer Support Use Case</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_3.png" class="img-fluid figure-img"></p>
<figcaption>Slide 3</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=48s">Timestamp: 00:48</a>)</p>
<p>To motivate the need for evaluation, the presentation introduces a common real-world use case: <strong>Customer Support</strong>. Generative AI is frequently deployed to help agents compose emails or chat responses based on user inquiries.</p>
<p>This scenario serves as the baseline example throughout the talk. It represents a high-volume task where automation is desirable, but accuracy and tone are critical for maintaining customer satisfaction and brand reputation.</p>
</section>
<section id="vibe-coding" class="level3">
<h3 class="anchored" data-anchor-id="vibe-coding">4. Vibe Coding</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_4.png" class="img-fluid figure-img"></p>
<figcaption>Slide 4</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=59s">Timestamp: 00:59</a>)</p>
<p>This slide introduces the concept of <strong>“Vibe Coding”</strong>—the initial phase where developers grab a simple prompt, feed it to a model, and get a result that feels right. It highlights the misconception that GenAI is easy because it works “out of the box” for simple demos.</p>
<p>Rajiv notes that while “vibe coding” might work for a quick demo app, it is insufficient for production systems. Relying on a “vibe” that the model is working prevents teams from catching subtle failures that occur at scale.</p>
</section>
<section id="good-response-delayed-order" class="level3">
<h3 class="anchored" data-anchor-id="good-response-delayed-order">5. Good Response: Delayed Order</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_5.png" class="img-fluid figure-img"></p>
<figcaption>Slide 5</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=70s">Timestamp: 01:10</a>)</p>
<p>Here, we see a successful output generated by the LLM. The customer inquired about a delayed order, and the AI generated a polite, relevant response acknowledging the delay and apologizing.</p>
<p>This example reinforces the “Vibe Coding” trap: because the model often produces high-quality, human-sounding text like this, developers can be lulled into a false sense of security regarding the system’s reliability.</p>
</section>
<section id="good-response-damaged-product" class="level3">
<h3 class="anchored" data-anchor-id="good-response-damaged-product">6. Good Response: Damaged Product</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_6.png" class="img-fluid figure-img"></p>
<figcaption>Slide 6</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=72s">Timestamp: 01:12</a>)</p>
<p>This slide provides another example of a “good” response. The AI correctly identifies that the customer received a damaged product and initiates a replacement protocol.</p>
<p>These positive examples establish a baseline of expected behavior. The challenge in evaluation is not just confirming that the model <em>can</em> work, but ensuring it works consistently across all edge cases.</p>
</section>
<section id="bad-response-irrelevance" class="level3">
<h3 class="anchored" data-anchor-id="bad-response-irrelevance">7. Bad Response: Irrelevance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_7.png" class="img-fluid figure-img"></p>
<figcaption>Slide 7</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=86s">Timestamp: 01:26</a>)</p>
<p>The presentation shifts to failure modes. In this example, the user asks about an <strong>“Order Delay,”</strong> but the AI responds with information about a <strong>“New Product Launch.”</strong></p>
<p>This illustrates a complete context mismatch. The model failed to attend to the user’s intent, generating a coherent but completely irrelevant response. This type of failure frustrates users and degrades trust in the automated system.</p>
</section>
<section id="bad-response-hallucination" class="level3">
<h3 class="anchored" data-anchor-id="bad-response-hallucination">8. Bad Response: Hallucination</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_8.png" class="img-fluid figure-img"></p>
<figcaption>Slide 8</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=96s">Timestamp: 01:36</a>)</p>
<p>This slide shows a more dangerous failure: <strong>Hallucination</strong>. The AI apologizes for a defective “espresso machine,” but as the speaker notes, “We don’t actually sell espresso machines.”</p>
<p>This highlights the risk of the model fabricating facts to be helpful. Such errors can lead to logistical nightmares, such as customers expecting replacements for products that do not exist or that the company never sold.</p>
</section>
<section id="risks-of-llm-mistakes" class="level3">
<h3 class="anchored" data-anchor-id="risks-of-llm-mistakes">9. Risks of LLM Mistakes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_9.png" class="img-fluid figure-img"></p>
<figcaption>Slide 9</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=111s">Timestamp: 01:51</a>)</p>
<p>Rajiv categorizes the risks associated with LLM failures into three buckets: <strong>Reputational, Legal, and Financial</strong>. He cites the example of <strong>Cursor</strong>, an IDE company, where a support bot hallucinated a policy restricting users to one device, causing customers to cancel subscriptions.</p>
<p>The slide emphasizes that courts may view AI agents as employees; if a bot makes a promise (like a refund or policy change), the company might be legally bound to honor it. This escalates evaluation from a technical nice-to-have to a business necessity.</p>
</section>
<section id="the-despair-of-gen-ai" class="level3">
<h3 class="anchored" data-anchor-id="the-despair-of-gen-ai">10. The Despair of Gen AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=158s">Timestamp: 02:38</a>)</p>
<p>This visual represents the frustration developers feel when moving from a successful demo to a failing production system. The “despair” comes from the realization that the stochastic nature of LLMs makes them difficult to control.</p>
<p>It serves as an emotional anchor for the audience, acknowledging that while GenAI is exciting, the unpredictability of its failures causes significant stress for engineering teams responsible for deployment.</p>
</section>
<section id="high-failure-rates" class="level3">
<h3 class="anchored" data-anchor-id="high-failure-rates">11. High Failure Rates</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=168s">Timestamp: 02:48</a>)</p>
<p>The slide cites an MIT report stating that <strong>“95% of GenAI pilots are failing.”</strong> While Rajiv notes this number might be overstated, it reflects a trend where executives are demanding ROI and seeing lackluster results.</p>
<p>This shift in 2025 means that evaluation is no longer just for debugging; it is required to prove business value and justify the high costs of running Generative AI infrastructure.</p>
</section>
<section id="evaluation-improves-applications" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-improves-applications">12. Evaluation Improves Applications</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=194s">Timestamp: 03:14</a>)</p>
<p>This slide asserts the core thesis: <strong>Evaluation helps you build better GenAI applications.</strong> It references a previous viral video by the speaker on the same topic, positioning this talk as an updated, condensed version with fresh content.</p>
<p>Rajiv explains that you cannot improve what you cannot measure. Without a robust evaluation framework, developers are essentially guessing whether changes to prompts or models are actually improving performance.</p>
</section>
<section id="why-evaluation-is-necessary" class="level3">
<h3 class="anchored" data-anchor-id="why-evaluation-is-necessary">13. Why Evaluation is Necessary</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=220s">Timestamp: 03:40</a>)</p>
<p>This concentric diagram illustrates the stakeholders involved in evaluation. It starts with <strong>“Things Go Wrong”</strong> (technical reality), moves to <strong>“Buy-in”</strong> (convincing managers/teams), and ends with <strong>“Regulators”</strong> (external compliance).</p>
<p>Evaluation serves multiple audiences: it helps the developer debug, it provides the metrics needed to convince management that the app is production-ready, and it creates the audit trails required by third-party auditors or regulators.</p>
</section>
<section id="evaluation-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-dimensions">14. Evaluation Dimensions</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=258s">Timestamp: 04:18</a>)</p>
<p>Evaluation must cover three dimensions: <strong>Technical</strong> (F1 scores, accuracy), <strong>Business</strong> (ROI, value generated), and <strong>Operational</strong> (Total Cost of Ownership, latency).</p>
<p>Rajiv highlights that data scientists often focus solely on the technical, but ignoring operational costs (like the expense of hosting GPUs vs.&nbsp;using APIs) can kill a project. A comprehensive evaluation strategy considers the cost-to-quality ratio.</p>
</section>
<section id="public-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="public-benchmarks">15. Public Benchmarks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_15.png" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=306s">Timestamp: 05:06</a>)</p>
<p>The slide discusses <strong>Public Benchmarks</strong> (like MMLU, GSM8K). While useful for a general idea of a model’s capabilities (e.g., “Is Llama 3 better than Llama 2?”), they are insufficient for specific applications.</p>
<p>Rajiv warns against using these benchmarks to determine if a model fits <em>your</em> specific use case. Companies promote these numbers for marketing, but they rarely reflect performance on proprietary business data.</p>
</section>
<section id="custom-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="custom-benchmarks">16. Custom Benchmarks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=322s">Timestamp: 05:22</a>)</p>
<p>The solution to the limitations of public benchmarks is <strong>Custom Benchmarks</strong>. This slide defines a benchmark as a combination of a <strong>Task</strong>, a <strong>Dataset</strong>, and an <strong>Evaluation Metric</strong>.</p>
<p>This is a critical definition for the workshop. To “tame” GenAI, you must build a dataset that reflects your specific customer queries and define success metrics that matter to your business logic, rather than relying on generic academic tests.</p>
</section>
<section id="taming-gen-ai" class="level3">
<h3 class="anchored" data-anchor-id="taming-gen-ai">17. Taming Gen AI</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_17.png" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=328s">Timestamp: 05:28</a>)</p>
<p>This title slide signals a transition into the technical “how-to” section of the talk. “Taming” implies that the default state of GenAI is wild and unpredictable.</p>
<p>The goal of the following sections is to bring structure and control to this chaos through rigorous engineering practices and evaluation workflows.</p>
</section>
<section id="workshop-roadmap" class="level3">
<h3 class="anchored" data-anchor-id="workshop-roadmap">18. Workshop Roadmap</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_18.png" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=331s">Timestamp: 05:31</a>)</p>
<p>The roadmap outlines the four main sections of the talk: 1. <strong>Basics of Gen AI:</strong> Understanding variability and technical nuances. 2. <strong>Evaluation Workflow:</strong> Building the dataset and running the first tests. 3. <strong>More Complexity:</strong> Adding unit tests and conducting error analysis. 4. <strong>Agents:</strong> Evaluating complex, multi-step workflows.</p>
</section>
<section id="variability-in-responses" class="level3">
<h3 class="anchored" data-anchor-id="variability-in-responses">19. Variability in Responses</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_19.png" class="img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=360s">Timestamp: 06:00</a>)</p>
<p>This slide visually demonstrates the <strong>Non-Determinism</strong> of LLMs. It shows two responses to the same prompt generated just minutes apart. While substantively similar, the wording and structure differ slightly.</p>
<p>This variability makes exact string matching (a common software testing technique) impossible for LLMs. It necessitates semantic evaluation techniques, which complicates the testing pipeline.</p>
</section>
<section id="input-model-output-diagram" class="level3">
<h3 class="anchored" data-anchor-id="input-model-output-diagram">20. Input-Model-Output Diagram</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_20.png" class="img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=384s">Timestamp: 06:24</a>)</p>
<p>A simple diagram illustrates the flow: <strong>Prompt -&gt; Model -&gt; Output</strong>. Rajiv uses this to structure the analysis of where variability comes from.</p>
<p>He explains that “chaos” can enter the system at any of these three stages: the input (prompt sensitivity), the model (inference non-determinism), or the output (formatting and evaluation).</p>
</section>
<section id="inconsistent-benchmark-scores" class="level3">
<h3 class="anchored" data-anchor-id="inconsistent-benchmark-scores">21. Inconsistent Benchmark Scores</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_21.png" class="img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=404s">Timestamp: 06:44</a>)</p>
<p>The slide presents a discrepancy between benchmark scores tweeted by Hugging Face and those in the official Llama paper. Both used the same dataset (MMLU), but reported different accuracy numbers.</p>
<p>This introduces the problem of <strong>Evaluation Harness Sensitivity</strong>. Even with standard benchmarks, <em>how</em> you ask the model to take the test changes the score, proving that evaluation is fragile and implementation-dependent.</p>
</section>
<section id="mmlu-overview" class="level3">
<h3 class="anchored" data-anchor-id="mmlu-overview">22. MMLU Overview</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_22.png" class="img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=445s">Timestamp: 07:25</a>)</p>
<p><strong>MMLU (Massive Multitask Language Understanding)</strong> is explained here. It is a multiple-choice test covering 57 tasks across STEM, the humanities, and more.</p>
<p>It is currently the standard for measuring general “intelligence” in models. However, because it is a multiple-choice format, it is susceptible to prompt formatting nuances, as the next slides demonstrate.</p>
</section>
<section id="prompt-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="prompt-sensitivity">23. Prompt Sensitivity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_23.png" class="img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=464s">Timestamp: 07:44</a>)</p>
<p>This slide reveals <em>why</em> the scores in Slide 21 differed. The three evaluation harnesses used slightly different prompt structures (e.g., using the word “Question” vs.&nbsp;just listing the text).</p>
<p>These minor changes resulted in significant accuracy shifts. This proves that LLMs are highly sensitive to syntax, meaning a “better” model might just be one that was prompted more effectively for the test, not one that is actually smarter.</p>
</section>
<section id="formatting-changes" class="level3">
<h3 class="anchored" data-anchor-id="formatting-changes">24. Formatting Changes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_24.png" class="img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=502s">Timestamp: 08:22</a>)</p>
<p>Expanding on sensitivity, this slide references Anthropic’s research showing that changing answer choices from <code>(A)</code> to <code>[A]</code> or <code>(1)</code> affects the output.</p>
<p>This level of fragility is a key takeaway: seemingly cosmetic changes in how inputs are formatted can alter the model’s reasoning capabilities or its ability to output the correct token.</p>
</section>
<section id="gpt-4o-performance-drop" class="level3">
<h3 class="anchored" data-anchor-id="gpt-4o-performance-drop">25. GPT-4o Performance Drop</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_25.png" class="img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=518s">Timestamp: 08:38</a>)</p>
<p>A bar chart demonstrates that this issue persists even in state-of-the-art models like <strong>GPT-4o</strong>. Subtle changes in wording can lead to a 5-10% drop in performance.</p>
<p>This counters the assumption that newer, larger models have “solved” prompt sensitivity. It remains a persistent variable that evaluators must control for.</p>
</section>
<section id="tone-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="tone-sensitivity">26. Tone Sensitivity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_26.png" class="img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=526s">Timestamp: 08:46</a>)</p>
<p>This slide shows that the <strong>tone</strong> of a prompt (e.g., being polite vs.&nbsp;direct) affects accuracy. Rajiv jokes, “I guess this is why mom always said to be polite.”</p>
<p>The graph indicates that prompt engineering strategies, like adding emotional weight or politeness, can statistically alter model performance, adding another layer of complexity to evaluation.</p>
</section>
<section id="persistent-sensitivity" class="level3">
<h3 class="anchored" data-anchor-id="persistent-sensitivity">27. Persistent Sensitivity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_27.png" class="img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=540s">Timestamp: 09:00</a>)</p>
<p>The slide reiterates that despite years of progress, models are still sensitive to specific phrases. It shows a “Prompt Engineering” guide suggesting specific words to use.</p>
<p>The takeaway is that developers cannot treat the prompt as a static instruction; it is a hyperparameter that requires optimization and constant testing.</p>
</section>
<section id="falcon-llm-bias" class="level3">
<h3 class="anchored" data-anchor-id="falcon-llm-bias">28. Falcon LLM Bias</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_28.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=558s">Timestamp: 09:18</a>)</p>
<p>This slide introduces a case study with the <strong>Falcon LLM</strong>. A user tweet shows the model recommending <strong>Abu Dhabi</strong> as a technological city with glowing sentiment, which raised suspicions about bias given the model’s origin in the Middle East.</p>
<p>This serves as a detective story: users wondered if the model weights were altered or if specific training data was injected to force this positive association.</p>
</section>
<section id="potential-cover-up" class="level3">
<h3 class="anchored" data-anchor-id="potential-cover-up">29. Potential Cover-up?</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_29.png" class="img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=590s">Timestamp: 09:50</a>)</p>
<p>Another tweet speculates if the model is “covering up human rights abuses” because it provides different answers for Abu Dhabi compared to other cities.</p>
<p>This highlights how model behavior can be misinterpreted as malicious bias or censorship, when the root cause might be something much simpler in the input stack.</p>
</section>
<section id="inspecting-the-system-prompt" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-the-system-prompt">30. Inspecting the System Prompt</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_30.png" class="img-fluid figure-img"></p>
<figcaption>Slide 30</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=600s">Timestamp: 10:00</a>)</p>
<p>The reveal: The bias wasn’t in the weights, but in the <strong>System Prompt</strong>. The slide suggests looking at the hidden instructions given to the model.</p>
<p>In Falcon’s case, the system prompt explicitly told the model, “You are a model built in Abu Dhabi.” This context influenced its generation probabilities, causing it to favor Abu Dhabi in its responses.</p>
</section>
<section id="claude-system-prompt" class="level3">
<h3 class="anchored" data-anchor-id="claude-system-prompt">31. Claude System Prompt</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_31.png" class="img-fluid figure-img"></p>
<figcaption>Slide 31</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=633s">Timestamp: 10:33</a>)</p>
<p>Rajiv points out that most developers never read the system prompts of the models they use. He highlights the <strong>Claude System Prompt</strong>, which is 1700 words long and takes nearly 10 minutes to read.</p>
<p>These extensive instructions define the model’s personality and safety guardrails. Ignoring them means you don’t fully understand the inputs driving your application’s behavior.</p>
</section>
<section id="complexity-of-a-single-response" class="level3">
<h3 class="anchored" data-anchor-id="complexity-of-a-single-response">32. Complexity of a Single Response</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_32.png" class="img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=660s">Timestamp: 11:00</a>)</p>
<p>The diagram is updated to show that a “single response” is actually the result of complex interactions: <strong>Tokenization -&gt; Prompt Styles -&gt; Prompt Engineering -&gt; System Prompt</strong>.</p>
<p>This visual summarizes the “Input” section of the talk, reinforcing that before the model even processes data, multiple layers of text transformation occur that can alter the result.</p>
</section>
<section id="inter-text-similarity" class="level3">
<h3 class="anchored" data-anchor-id="inter-text-similarity">33. Inter-text Similarity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_33.png" class="img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=675s">Timestamp: 11:15</a>)</p>
<p>This heatmap compares <strong>Inter-text similarity</strong> between models. It highlights Llama 70B and Llama 8B. Even though they are from the same family and likely trained on similar data, they are not identical.</p>
<p>This means you cannot swap a smaller model for a larger one (or vice versa) and expect the exact same behavior. Any model change requires a full re-evaluation.</p>
</section>
<section id="sycophantic-models" class="level3">
<h3 class="anchored" data-anchor-id="sycophantic-models">34. Sycophantic Models</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_34.png" class="img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=736s">Timestamp: 12:16</a>)</p>
<p>The slide discusses <strong>Sycophancy</strong>—the tendency of models to agree with the user even when the user is wrong. It mentions how early versions of GPT-4 were sometimes “overly nice.”</p>
<p>This behavior is a specific type of model bias that evaluators must watch for. If a user asks a leading question containing false premises, a sycophantic model might validate the falsehood rather than correct it.</p>
</section>
<section id="model-drift" class="level3">
<h3 class="anchored" data-anchor-id="model-drift">35. Model Drift</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_35.png" class="img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=757s">Timestamp: 12:37</a>)</p>
<p><strong>“Model Drift”</strong> refers to the phenomenon where commercial APIs (like OpenAI or Anthropic) change their model behavior over time without warning.</p>
<p>Because developers do not control the weights of API-based models, the “ground underneath them” can shift. A prompt that worked yesterday might fail today because the provider updated the backend or the inference infrastructure.</p>
</section>
<section id="degraded-responses-timeline" class="level3">
<h3 class="anchored" data-anchor-id="degraded-responses-timeline">36. Degraded Responses Timeline</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_36.png" class="img-fluid figure-img"></p>
<figcaption>Slide 36</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=775s">Timestamp: 12:55</a>)</p>
<p>This slide shows a timeline of <strong>Degraded Responses</strong> from an Anthropic incident. Technical issues like context window routing errors led to corrupted outputs for a period of days.</p>
<p>This illustrates that drift isn’t always about model updates; it can be infrastructure failures. Continuous monitoring is required to detect when an external dependency degrades your application’s performance.</p>
</section>
<section id="hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters">37. Hyperparameters</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_37.png" class="img-fluid figure-img"></p>
<figcaption>Slide 37</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=813s">Timestamp: 13:33</a>)</p>
<p>The slide lists <strong>Hyperparameters</strong> like Temperature, Top-P, and Max Length. Rajiv explains that users can control these “knobs” to influence creativity versus determinism.</p>
<p>Setting temperature to 0 makes the model less random, but as the next slides show, it does not guarantee perfect determinism due to hardware nuances.</p>
</section>
<section id="non-deterministic-inference" class="level3">
<h3 class="anchored" data-anchor-id="non-deterministic-inference">38. Non-Deterministic Inference</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_38.png" class="img-fluid figure-img"></p>
<figcaption>Slide 38</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=843s">Timestamp: 14:03</a>)</p>
<p>This slide tackles <strong>Non-Deterministic Inference</strong>. Unlike traditional ML models (e.g., XGBoost) where a fixed seed guarantees identical output, LLMs on GPUs often produce different results for identical inputs.</p>
<p>Causes include floating-point accumulation errors and the behavior of Mixture of Experts (MoE) models where different batches might activate different experts.</p>
</section>
<section id="addressing-non-determinism" class="level3">
<h3 class="anchored" data-anchor-id="addressing-non-determinism">39. Addressing Non-Determinism</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_39.png" class="img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=911s">Timestamp: 15:11</a>)</p>
<p>Rajiv references recent work by <strong>Thinking Machines</strong> and updates to <strong>vLLM</strong> that attempt to solve the non-determinism problem through correct batching.</p>
<p>While solutions are emerging, the takeaway is that most current setups are non-deterministic by default. Evaluators must design their tests to tolerate this variance rather than expecting bit-wise reproducibility.</p>
</section>
<section id="updated-model-diagram" class="level3">
<h3 class="anchored" data-anchor-id="updated-model-diagram">40. Updated Model Diagram</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_40.png" class="img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=943s">Timestamp: 15:43</a>)</p>
<p>The diagram expands again. The “Model” box now includes <strong>Model Selection, Hyperparameters, Non-deterministic Inference, and Forced Updates</strong>.</p>
<p>This visual summarizes the “Model” section, showing that the “black box” is actually a dynamic system with internal variables (weights/architecture) and external variables (infrastructure/updates) that all add noise to the output.</p>
</section>
<section id="output-format-issues" class="level3">
<h3 class="anchored" data-anchor-id="output-format-issues">41. Output Format Issues</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_41.png" class="img-fluid figure-img"></p>
<figcaption>Slide 41</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=961s">Timestamp: 16:01</a>)</p>
<p>Moving to the “Output” stage, this slide uses MMLU again to show how <strong>Output Formatting</strong> affects evaluation. How do you ask the model to answer a multiple-choice question?</p>
<p>Do you ask it to output just the letter “A”? Or the full text? Or the probability of the token “A”? Different evaluation harnesses use different methods, leading to the score discrepancies seen earlier.</p>
</section>
<section id="evaluation-harness-variations" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-harness-variations">42. Evaluation Harness Variations</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_42.png" class="img-fluid figure-img"></p>
<figcaption>Slide 42</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=995s">Timestamp: 16:35</a>)</p>
<p>This table details the specific differences in implementation between harnesses (e.g., original MMLU vs.&nbsp;HELM vs.&nbsp;EleutherAI).</p>
<p>It reinforces that there is no standard “ruler” for measuring LLMs. The tool you use to measure the model introduces its own bias and variance into the final score.</p>
</section>
<section id="score-comparison-table" class="level3">
<h3 class="anchored" data-anchor-id="score-comparison-table">43. Score Comparison Table</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_43.png" class="img-fluid figure-img"></p>
<figcaption>Slide 43</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1016s">Timestamp: 16:56</a>)</p>
<p>A spreadsheet shows the same models scoring differently across different evaluation implementations. The variance is not trivial; it can be large enough to change the ranking of which model is “best.”</p>
<p>This data drives home the point: You must control your own evaluation pipeline. Relying on reported numbers is risky because you don’t know the implementation details behind them.</p>
</section>
<section id="sentiment-analysis-variance" class="level3">
<h3 class="anchored" data-anchor-id="sentiment-analysis-variance">44. Sentiment Analysis Variance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_44.png" class="img-fluid figure-img"></p>
<figcaption>Slide 44</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1029s">Timestamp: 17:09</a>)</p>
<p>This slide shows varying <strong>Sentiment Analysis</strong> outputs. Different models (or the same model with different prompts) might classify a review as “Positive” while another says “Neutral.”</p>
<p>This introduces the concept that even “simple” classification tasks in GenAI are subject to interpretation and variance, unlike traditional classifiers that have a fixed decision boundary.</p>
</section>
<section id="tool-use-variance" class="level3">
<h3 class="anchored" data-anchor-id="tool-use-variance">45. Tool Use Variance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_45.png" class="img-fluid figure-img"></p>
<figcaption>Slide 45</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1043s">Timestamp: 17:23</a>)</p>
<p>Radar charts illustrate variance in <strong>Tool Use</strong>. Models might be good at using an “Email” tool but fail at “Calendar” or “Terminal” tools.</p>
<p>Furthermore, models exhibit non-determinism in <em>decision making</em>—sometimes they choose to use a tool, and sometimes they try to answer from memory. This adds a layer of logic errors on top of text generation errors.</p>
</section>
<section id="summary-why-responses-differ" class="level3">
<h3 class="anchored" data-anchor-id="summary-why-responses-differ">46. Summary: Why Responses Differ</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_46.png" class="img-fluid figure-img"></p>
<figcaption>Slide 46</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1069s">Timestamp: 17:49</a>)</p>
<p>This comprehensive slide aggregates all the factors discussed: <strong>Inputs</strong> (prompts, system prompts), <strong>Model</strong> (drift, hyperparams), <strong>Outputs</strong> (formatting), and <strong>Infrastructure</strong>.</p>
<p>It serves as a checklist for the audience. If your application is behaving inconsistently, investigate these specific layers of the stack to find the source of the noise.</p>
</section>
<section id="chaos-is-okay" class="level3">
<h3 class="anchored" data-anchor-id="chaos-is-okay">47. Chaos is Okay</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_47.png" class="img-fluid figure-img"></p>
<figcaption>Slide 47</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1097s">Timestamp: 18:17</a>)</p>
<p>Rajiv reassures the audience that <strong>“Chaos is Okay.”</strong> The slide presents a chart of evaluation methods ranging from flexible/expensive (human eval) to rigid/cheap (code assertions).</p>
<p>The message is that while the technology is chaotic, there is a spectrum of tools available to manage it. We don’t need to solve every source of variance; we just need a robust process to measure it.</p>
</section>
<section id="from-chaos-to-control" class="level3">
<h3 class="anchored" data-anchor-id="from-chaos-to-control">48. From Chaos to Control</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_48.png" class="img-fluid figure-img"></p>
<figcaption>Slide 48</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1107s">Timestamp: 18:27</a>)</p>
<p>This transition slide marks the beginning of the <strong>Evaluation Workflow</strong> section. The presentation shifts from describing the problem to prescribing the solution.</p>
<p>The goal here is to move from “Vibe Coding” to a structured engineering discipline where changes are measured against a stable baseline.</p>
</section>
<section id="build-the-evaluation-dataset" class="level3">
<h3 class="anchored" data-anchor-id="build-the-evaluation-dataset">49. Build the Evaluation Dataset</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_49.png" class="img-fluid figure-img"></p>
<figcaption>Slide 49</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1117s">Timestamp: 18:37</a>)</p>
<p>The first step in the workflow is to <strong>Build the Evaluation Dataset</strong>. The slide lists examples of prompts for tasks like summarization, extraction, and translation.</p>
<p>Rajiv emphasizes that this dataset should reflect <em>your</em> actual use case. It is the foundation of the “Custom Benchmark” concept introduced earlier.</p>
</section>
<section id="get-labeled-outputs-gold" class="level3">
<h3 class="anchored" data-anchor-id="get-labeled-outputs-gold">50. Get Labeled Outputs (Gold)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_50.png" class="img-fluid figure-img"></p>
<figcaption>Slide 50</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1126s">Timestamp: 18:46</a>)</p>
<p>Step two is to get <strong>Labeled Outputs</strong>, also known as <strong>Gold Outputs</strong>, Reference, or Ground Truth. The slide adds a column showing the ideal answer for each prompt.</p>
<p>This is the standard against which the model will be judged. While obtaining these labels can be expensive (requiring human effort), they are essential for calculating accuracy.</p>
</section>
<section id="compare-to-model-output" class="level3">
<h3 class="anchored" data-anchor-id="compare-to-model-output">51. Compare to Model Output</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_51.png" class="img-fluid figure-img"></p>
<figcaption>Slide 51</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1140s">Timestamp: 19:00</a>)</p>
<p>Step three is to generate responses from your system and place them alongside the Gold Outputs. The slide adds a <strong>“Model Output”</strong> column.</p>
<p>This visual comparison allows developers (and automated judges) to see the delta between what was expected and what was produced.</p>
</section>
<section id="measure-equivalence" class="level3">
<h3 class="anchored" data-anchor-id="measure-equivalence">52. Measure Equivalence</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_52.png" class="img-fluid figure-img"></p>
<figcaption>Slide 52</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1150s">Timestamp: 19:10</a>)</p>
<p>Step four is to <strong>Measure Equivalence</strong>. Since LLMs rarely produce exact string matches, we use an <strong>LLM Judge</strong> (another model) to determine if the Model Output means the same thing as the Gold Output.</p>
<p>The slide shows a prompt for the judge: “Are these two responses semantically equivalent?” This converts a fuzzy text comparison problem into a binary (Pass/Fail) metric.</p>
</section>
<section id="optimize-using-equivalence" class="level3">
<h3 class="anchored" data-anchor-id="optimize-using-equivalence">53. Optimize Using Equivalence</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_53.png" class="img-fluid figure-img"></p>
<figcaption>Slide 53</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1197s">Timestamp: 19:57</a>)</p>
<p>Once you have an equivalence metric, you can <strong>Optimize</strong>. The slide shows Config A vs.&nbsp;Config B. By changing prompts or models, you can track if your “Equivalence Score” goes up or down.</p>
<p>This treats GenAI engineering like traditional hyperparameter tuning. The goal is to maximize the equivalence score on your custom dataset.</p>
</section>
<section id="why-global-metrics-arent-enough" class="level3">
<h3 class="anchored" data-anchor-id="why-global-metrics-arent-enough">54. Why Global Metrics Aren’t Enough</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_54.png" class="img-fluid figure-img"></p>
<figcaption>Slide 54</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1228s">Timestamp: 20:28</a>)</p>
<p>The slide discusses the limitations of the “Equivalence” approach. While good for a general sense of quality, <strong>Global Metrics</strong> miss nuances.</p>
<p>Sometimes it’s hard to get a Gold Answer for open-ended creative tasks. Furthermore, a simple “Pass/Fail” doesn’t tell you <em>why</em> the model failed (e.g., was it tone, length, or factuality?).</p>
</section>
<section id="from-global-to-targeted-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="from-global-to-targeted-evaluation">55. From Global to Targeted Evaluation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_55.png" class="img-fluid figure-img"></p>
<figcaption>Slide 55</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1255s">Timestamp: 20:55</a>)</p>
<p>This slide argues for <strong>Targeted Evaluation</strong>. To maximize performance, you need to dig deeper into the data and identify specific error modes.</p>
<p>This transitions the talk from “Basic Workflow” to “Advanced Testing,” where we break down “Quality” into specific, testable components like tone, length, and safety.</p>
</section>
<section id="building-tests" class="level3">
<h3 class="anchored" data-anchor-id="building-tests">56. Building Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_56.png" class="img-fluid figure-img"></p>
<figcaption>Slide 56</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1274s">Timestamp: 21:14</a>)</p>
<p>The section title <strong>“Building Tests”</strong> appears. This is where the presentation moves into the “Unit Testing” philosophy for GenAI.</p>
<p>Just as software engineering relies on unit tests to verify specific functions, GenAI engineering should use targeted tests to verify specific attributes of the generated text.</p>
</section>
<section id="good-vs.-bad-examples" class="level3">
<h3 class="anchored" data-anchor-id="good-vs.-bad-examples">57. Good vs.&nbsp;Bad Examples</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_57.png" class="img-fluid figure-img"></p>
<figcaption>Slide 57</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1280s">Timestamp: 21:20</a>)</p>
<p>The slide displays a <strong>Good Example</strong> and a <strong>Bad Example</strong> of a response. The bad example is visibly shorter and less polite.</p>
<p>Rajiv asks the audience to identify <em>why</em> it is bad. This exercise is crucial: you cannot build a test until you can articulate exactly what makes a response a failure.</p>
</section>
<section id="develop-an-evaluation-mindset" class="level3">
<h3 class="anchored" data-anchor-id="develop-an-evaluation-mindset">58. Develop an Evaluation Mindset</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_58.png" class="img-fluid figure-img"></p>
<figcaption>Slide 58</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1306s">Timestamp: 21:46</a>)</p>
<p>To define “Bad,” developers need an <strong>Evaluation Mindset</strong>. This involves observing real-world user interactions and problems.</p>
<p>Data scientists often want to stay in their “chair” and optimize algorithms, but Rajiv argues that effective evaluation requires understanding the user’s pain points.</p>
</section>
<section id="collaborate-with-experts" class="level3">
<h3 class="anchored" data-anchor-id="collaborate-with-experts">59. Collaborate with Experts</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_59.png" class="img-fluid figure-img"></p>
<figcaption>Slide 59</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1318s">Timestamp: 21:58</a>)</p>
<p>The slide stresses <strong>Collaboration</strong>. You must talk to domain experts (e.g., the customer support team) to define what a “good” answer looks like.</p>
<p>Naive bootstrapping—pretending to be a user—is a good start, but long-term success requires input from the people who actually know the business domain.</p>
</section>
<section id="identify-and-categorize-failures" class="level3">
<h3 class="anchored" data-anchor-id="identify-and-categorize-failures">60. Identify and Categorize Failures</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_60.png" class="img-fluid figure-img"></p>
<figcaption>Slide 60</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1372s">Timestamp: 22:52</a>)</p>
<p>Once you understand the domain, you can <strong>Categorize Failure Types</strong>. The slide shows a chart grouping errors into categories like “Harmful Content,” “Bias,” or “Incorrect Info.”</p>
<p>This clustering allows you to see patterns. Instead of just knowing “the model failed 20% of the time,” you know “the model has a specific problem with tone.”</p>
</section>
<section id="define-what-good-looks-like" class="level3">
<h3 class="anchored" data-anchor-id="define-what-good-looks-like">61. Define What Good Looks Like</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_61.png" class="img-fluid figure-img"></p>
<figcaption>Slide 61</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1391s">Timestamp: 23:11</a>)</p>
<p>Using the categorization, you can explicitly <strong>Define What Good Looks Like</strong>. The slide contrasts the good/bad examples again, but now with labels: “Too short,” “Lacks professional tone.”</p>
<p>This transforms a subjective feeling (“this response sucks”) into objective criteria (“response must be &gt;50 words and use polite honorifics”).</p>
</section>
<section id="document-every-issue" class="level3">
<h3 class="anchored" data-anchor-id="document-every-issue">62. Document Every Issue</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_62.png" class="img-fluid figure-img"></p>
<figcaption>Slide 62</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1412s">Timestamp: 23:32</a>)</p>
<p>The slide shows a spreadsheet where humans evaluate responses and <strong>Document Every Issue</strong>. Columns track specific attributes like “Is it helpful?” or “Is the tone right?”</p>
<p>This manual annotation is the training data for your automated tests. You need humans to establish the ground truth before you can automate the checking.</p>
</section>
<section id="evaluation-tooling" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-tooling">63. Evaluation Tooling</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_63.png" class="img-fluid figure-img"></p>
<figcaption>Slide 63</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1433s">Timestamp: 23:53</a>)</p>
<p>Rajiv mentions that <strong>Tooling Can Help</strong>. The slide shows a custom chat viewer designed to make human review easier.</p>
<p>However, he warns against getting sidetracked by building fancy tools. Simple spreadsheets often suffice for the early stages. The goal is the data, not the interface.</p>
</section>
<section id="test-1-length-check" class="level3">
<h3 class="anchored" data-anchor-id="test-1-length-check">64. Test 1: Length Check</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_64.png" class="img-fluid figure-img"></p>
<figcaption>Slide 64</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1445s">Timestamp: 24:05</a>)</p>
<p>Now we build the automated tests. <strong>Test 1 is a Length Check</strong>. The slide shows Python code asserting that the word count is between 8 and 200.</p>
<p>This is a <strong>deterministic test</strong>. You don’t need an LLM to count words. Rajiv encourages using simple Python assertions wherever possible because they are fast, cheap, and reliable.</p>
</section>
<section id="test-2-tone-and-style" class="level3">
<h3 class="anchored" data-anchor-id="test-2-tone-and-style">65. Test 2: Tone and Style</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_65.png" class="img-fluid figure-img"></p>
<figcaption>Slide 65</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1462s">Timestamp: 24:22</a>)</p>
<p><strong>Test 2 checks Tone and Style</strong>. Since “tone” is subjective, we use an <strong>LLM Judge</strong> (OpenAI model) to classify the response.</p>
<p>The prompt asks the judge to identify the style. This allows us to automate the “vibe check” that humans were previously doing manually.</p>
</section>
<section id="adding-metrics-to-documentation" class="level3">
<h3 class="anchored" data-anchor-id="adding-metrics-to-documentation">66. Adding Metrics to Documentation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_66.png" class="img-fluid figure-img"></p>
<figcaption>Slide 66</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1481s">Timestamp: 24:41</a>)</p>
<p>The spreadsheet is updated with new columns: <code>Length_OK</code> and <code>Tone_OK</code>. These are the results of the automated tests.</p>
<p>Now, for every row in the dataset, we have granular pass/fail metrics. This helps pinpoint exactly <em>why</em> a specific response failed, rather than just a generic failure.</p>
</section>
<section id="check-judges-against-humans" class="level3">
<h3 class="anchored" data-anchor-id="check-judges-against-humans">67. Check Judges Against Humans</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_67.png" class="img-fluid figure-img"></p>
<figcaption>Slide 67</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1512s">Timestamp: 25:12</a>)</p>
<p>A critical step: <strong>Check LLM Judges Against Humans</strong>. You must verify that your automated “Tone Judge” agrees with your human experts.</p>
<p>If the human says the tone is rude, but the LLM Judge says it’s polite, your metric is useless. You must iterate on the judge’s prompt until alignment is high.</p>
</section>
<section id="self-evaluation-bias" class="level3">
<h3 class="anchored" data-anchor-id="self-evaluation-bias">68. Self-Evaluation Bias</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_68.png" class="img-fluid figure-img"></p>
<figcaption>Slide 68</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1566s">Timestamp: 26:06</a>)</p>
<p>The slide illustrates <strong>Self-Evaluation Bias</strong>. LLMs tend to rate their own outputs higher than outputs from other models. GPT-4 prefers GPT-4 text.</p>
<p>To mitigate this, Rajiv suggests mixing models—use Claude to judge GPT-4, or Gemini to judge Claude. This helps ensure a more neutral evaluation.</p>
</section>
<section id="alignment-checks" class="level3">
<h3 class="anchored" data-anchor-id="alignment-checks">69. Alignment Checks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_69.png" class="img-fluid figure-img"></p>
<figcaption>Slide 69</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1606s">Timestamp: 26:46</a>)</p>
<p>This slide reinforces the need for <strong>Continuous Alignment</strong>. Just because your judge aligned with humans last month doesn’t mean it still does (due to model drift).</p>
<p>Human spot-checks should be a permanent part of the pipeline to ensure the automated judges haven’t drifted.</p>
</section>
<section id="biases-in-llm-judges" class="level3">
<h3 class="anchored" data-anchor-id="biases-in-llm-judges">70. Biases in LLM Judges</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_70.png" class="img-fluid figure-img"></p>
<figcaption>Slide 70</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1622s">Timestamp: 27:02</a>)</p>
<p>The slide lists known <strong>Biases in LLM Judges</strong>, such as <strong>Position Bias</strong> (favoring the first answer presented) or <strong>Verbosity Bias</strong> (favoring longer answers).</p>
<p>Evaluators must be aware of these. For example, you should shuffle the order of answers when asking a judge to compare two options to cancel out position bias.</p>
</section>
<section id="best-practices-for-llm-judges" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-for-llm-judges">71. Best Practices for LLM Judges</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_71.png" class="img-fluid figure-img"></p>
<figcaption>Slide 71</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1631s">Timestamp: 27:11</a>)</p>
<p>A summary of <strong>Best Practices</strong>: Calibrate with human data, use ensembles (multiple judges), avoid asking for “relevance” (too vague), and use discrete rating scales (1-5) rather than continuous numbers.</p>
<p>These tips help stabilize the inherently noisy process of using AI to evaluate AI.</p>
</section>
<section id="error-analysis-chart" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-chart">72. Error Analysis Chart</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_72.png" class="img-fluid figure-img"></p>
<figcaption>Slide 72</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1666s">Timestamp: 27:46</a>)</p>
<p>With tests in place, we move to <strong>Error Analysis</strong>. The bar chart shows the number of failed cases categorized by error type (Length, Tone, Professional, Context).</p>
<p>This visualization tells you where to focus your efforts. If “Tone” is the biggest bar, you work on the system prompt’s tone instructions. If “Context” is the issue, you might need better Retrieval Augmented Generation (RAG).</p>
</section>
<section id="comparing-prompts" class="level3">
<h3 class="anchored" data-anchor-id="comparing-prompts">73. Comparing Prompts</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_73.png" class="img-fluid figure-img"></p>
<figcaption>Slide 73</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1678s">Timestamp: 27:58</a>)</p>
<p>The chart can compare <strong>Prompt A vs.&nbsp;Prompt B</strong>. This allows for A/B testing of prompt engineering strategies.</p>
<p>You can see if a new prompt improves “Tone” but accidentally degrades “Context.” This tradeoff analysis is impossible with a single global score.</p>
</section>
<section id="explanations-guide-improvement" class="level3">
<h3 class="anchored" data-anchor-id="explanations-guide-improvement">74. Explanations Guide Improvement</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_74.png" class="img-fluid figure-img"></p>
<figcaption>Slide 74</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1694s">Timestamp: 28:14</a>)</p>
<p>Rajiv suggests asking the LLM Judge for <strong>Explanations</strong>. Don’t just ask for a score; ask for “one sentence explaining why.”</p>
<p>These explanations act as metadata that helps developers understand the judge’s reasoning, making it easier to debug discrepancies between human and AI judgments.</p>
</section>
<section id="limits-to-explanations" class="level3">
<h3 class="anchored" data-anchor-id="limits-to-explanations">75. Limits to Explanations</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_75.png" class="img-fluid figure-img"></p>
<figcaption>Slide 75</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1715s">Timestamp: 28:35</a>)</p>
<p>A warning: <strong>Explanations are not causal</strong>. When an LLM explains why it did something, it is generating a plausible justification, not a trace of its actual neural activations.</p>
<p>Treat explanations as a heuristic or a helpful hint, not as absolute truth about the model’s internal state.</p>
</section>
<section id="the-evaluation-flywheel" class="level3">
<h3 class="anchored" data-anchor-id="the-evaluation-flywheel">76. The Evaluation Flywheel</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_76.png" class="img-fluid figure-img"></p>
<figcaption>Slide 76</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1726s">Timestamp: 28:46</a>)</p>
<p>The <strong>Evaluation Flywheel</strong> describes the iterative cycle: Build Eval -&gt; Analyze -&gt; Improve -&gt; Repeat.</p>
<p>This concept, credited to Hamill, emphasizes that evaluation is not a one-time event but a continuous loop that spins faster as you gather more data and build better tests.</p>
</section>
<section id="financial-analyst-agent-example" class="level3">
<h3 class="anchored" data-anchor-id="financial-analyst-agent-example">77. Financial Analyst Agent Example</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_77.png" class="img-fluid figure-img"></p>
<figcaption>Slide 77</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1760s">Timestamp: 29:20</a>)</p>
<p>To demonstrate advanced unit testing, Rajiv introduces a <strong>Financial Analyst Agent</strong>. The goal is to assess the specific “style” of a financial report.</p>
<p>This is a complex domain where “good” is highly specific (regulated, precise, risk-aware), making it a perfect candidate for granular unit tests.</p>
</section>
<section id="use-a-global-test" class="level3">
<h3 class="anchored" data-anchor-id="use-a-global-test">78. Use a Global Test?</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_78.png" class="img-fluid figure-img"></p>
<figcaption>Slide 78</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1783s">Timestamp: 29:43</a>)</p>
<p>You <em>could</em> use a <strong>Global Test</strong>: “Was this explained as a financial analyst would?”</p>
<p>While simple, this test is opaque. If it fails, you don’t know if it was because of compliance issues, lack of clarity, or poor formatting.</p>
</section>
<section id="global-vs.-unit-tests" class="level3">
<h3 class="anchored" data-anchor-id="global-vs.-unit-tests">79. Global vs.&nbsp;Unit Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_79.png" class="img-fluid figure-img"></p>
<figcaption>Slide 79</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1794s">Timestamp: 29:54</a>)</p>
<p>The slide contrasts the Global approach with <strong>Unit Tests</strong>. Instead of one question, we ask six: Context, Clarity, Precision, Compliance, Actionability, and Risks.</p>
<p>This breakdown allows for targeted debugging. You might find the model is great at “Clarity” but terrible at “Compliance.”</p>
</section>
<section id="scoring-radar-chart" class="level3">
<h3 class="anchored" data-anchor-id="scoring-radar-chart">80. Scoring Radar Chart</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_80.png" class="img-fluid figure-img"></p>
<figcaption>Slide 80</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1816s">Timestamp: 30:16</a>)</p>
<p>A <strong>Radar Chart</strong> visualizes the unit test scores. This allows for a quick visual assessment of the model’s profile.</p>
<p>It facilitates comparison: you can overlay the profiles of two different models to see which one has the better balance of attributes for your specific needs.</p>
</section>
<section id="analyzing-failures-with-clusters" class="level3">
<h3 class="anchored" data-anchor-id="analyzing-failures-with-clusters">81. Analyzing Failures with Clusters</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_81.png" class="img-fluid figure-img"></p>
<figcaption>Slide 81</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1837s">Timestamp: 30:37</a>)</p>
<p>With enough unit test data, you can use <strong>Clustering (e.g., K-Means)</strong> to group failures. The slide shows clusters like “Synthesis,” “Context,” and “Hallucination.”</p>
<p>This moves error analysis from reading individual logs to analyzing aggregate trends, helping you prioritize which class of errors to fix first.</p>
</section>
<section id="designing-good-unit-tests" class="level3">
<h3 class="anchored" data-anchor-id="designing-good-unit-tests">82. Designing Good Unit Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_82.png" class="img-fluid figure-img"></p>
<figcaption>Slide 82</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1852s">Timestamp: 30:52</a>)</p>
<p>Advice on <strong>Designing Unit Tests</strong>: Keep them focused (one concept per test), use unambiguous language, and use small rating ranges.</p>
<p>Good unit tests are the building blocks of a reliable evaluation pipeline. If the tests themselves are noisy or vague, the entire system collapses.</p>
</section>
<section id="examples-of-unit-tests" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-unit-tests">83. Examples of Unit Tests</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_83.png" class="img-fluid figure-img"></p>
<figcaption>Slide 83</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1855s">Timestamp: 30:55</a>)</p>
<p>The slide lists specific examples of tests for <strong>Legal</strong> (Compliance, Terminology), <strong>Retrieval</strong> (Relevance, Completeness), and <strong>Bias/Fairness</strong>.</p>
<p>This serves as a menu of options for the audience, showing that unit tests can cover almost any dimension of quality required by the business.</p>
</section>
<section id="evaluating-new-prompts" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-new-prompts">84. Evaluating New Prompts</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_84.png" class="img-fluid figure-img"></p>
<figcaption>Slide 84</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1858s">Timestamp: 30:58</a>)</p>
<p>A bar chart shows how unit tests are used to <strong>Evaluate New Prompts</strong>. By running the full suite of unit tests on a new prompt, you get a “scorecard” of its performance.</p>
<p>This data-driven approach removes the guesswork from prompt engineering.</p>
</section>
<section id="tools---no-silver-bullet" class="level3">
<h3 class="anchored" data-anchor-id="tools---no-silver-bullet">85. Tools - No Silver Bullet</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_85.png" class="img-fluid figure-img"></p>
<figcaption>Slide 85</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1862s">Timestamp: 31:02</a>)</p>
<p>Rajiv reminds the audience that <strong>Tools are No Silver Bullet</strong>. You must master the basics (datasets, metrics) first.</p>
<p>He advises logging traces and experiments and practicing <strong>Dataset Versioning</strong>. Tools facilitate these practices, but they cannot replace the fundamental engineering discipline.</p>
</section>
<section id="forest-and-trees" class="level3">
<h3 class="anchored" data-anchor-id="forest-and-trees">86. Forest and Trees</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_86.png" class="img-fluid figure-img"></p>
<figcaption>Slide 86</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1864s">Timestamp: 31:04</a>)</p>
<p>An analogy helps structure the analysis: <strong>Forest (Global/Integration)</strong> vs.&nbsp;<strong>Trees (Test Case/Unit Tests)</strong>.</p>
<p>You need to look at both. The forest tells you the overall health of the app, while the trees tell you specifically what needs pruning or fixing.</p>
</section>
<section id="change-one-thing-at-a-time" class="level3">
<h3 class="anchored" data-anchor-id="change-one-thing-at-a-time">87. Change One Thing at a Time</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_87.png" class="img-fluid figure-img"></p>
<figcaption>Slide 87</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1877s">Timestamp: 31:17</a>)</p>
<p>A crucial scientific principle: <strong>Change One Thing at a Time</strong>. With so many knobs (prompt, temp, model, RAG settings), changing multiple variables simultaneously makes it impossible to know what caused the improvement (or regression).</p>
<p>Isolate your variables to conduct valid experiments.</p>
</section>
<section id="error-analysis-tips" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-tips">88. Error Analysis Tips</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_88.png" class="img-fluid figure-img"></p>
<figcaption>Slide 88</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1892s">Timestamp: 31:32</a>)</p>
<p>A summary of <strong>Error Analysis Tips</strong>: Use ablation studies (removing parts to see impact), categorize failures, save interesting examples, and leverage logs/traces.</p>
<p>These are the daily habits of successful GenAI engineers.</p>
</section>
<section id="the-evaluation-story" class="level3">
<h3 class="anchored" data-anchor-id="the-evaluation-story">89. The Evaluation Story</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_89.png" class="img-fluid figure-img"></p>
<figcaption>Slide 89</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1928s">Timestamp: 32:08</a>)</p>
<p>The slide shows the “Story We Tell”—a linear graph of improvement over time. This is the idealized version of progress often presented in case studies.</p>
<p>It suggests a smooth journey from “Out of the box” to “Specialized” to “User Feedback.”</p>
</section>
<section id="the-reality-of-progress" class="level3">
<h3 class="anchored" data-anchor-id="the-reality-of-progress">90. The Reality of Progress</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_90.png" class="img-fluid figure-img"></p>
<figcaption>Slide 90</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1944s">Timestamp: 32:24</a>)</p>
<p><strong>The Reality</strong> is a messy, non-linear graph. You take two steps forward, one step back. Sometimes an “improvement” breaks the model.</p>
<p>Rajiv encourages resilience. Experienced practitioners know that this messy graph is normal and that sticking to the process eventually yields results.</p>
</section>
<section id="continual-process" class="level3">
<h3 class="anchored" data-anchor-id="continual-process">91. Continual Process</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_91.png" class="img-fluid figure-img"></p>
<figcaption>Slide 91</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=1981s">Timestamp: 33:01</a>)</p>
<p><strong>Evaluation is a Continual Process</strong>. It involves Problem ID, Data Collection, Optimization, User Acceptance Testing (UAT), and Updates.</p>
<p>Crucially, <strong>UAT</strong> is your holdout set. Since you don’t have a traditional test set in GenAI, your real users act as the final validation layer.</p>
</section>
<section id="eating-the-elephant" class="level3">
<h3 class="anchored" data-anchor-id="eating-the-elephant">92. Eating the Elephant</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_92.png" class="img-fluid figure-img"></p>
<figcaption>Slide 92</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2043s">Timestamp: 34:03</a>)</p>
<p>The metaphor <strong>“How do you eat an elephant?”</strong> addresses the overwhelming nature of building a comprehensive evaluation suite.</p>
<p>The answer, of course, is “one bite at a time.” You don’t need 100 tests on day one.</p>
</section>
<section id="adding-tests-over-time" class="level3">
<h3 class="anchored" data-anchor-id="adding-tests-over-time">93. Adding Tests Over Time</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_93.png" class="img-fluid figure-img"></p>
<figcaption>Slide 93</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2050s">Timestamp: 34:10</a>)</p>
<p>The slide visualizes the “elephant” being broken down into bites. You start with a few critical tests. As the app matures and you discover new failure modes, you add more tests.</p>
<p>Six months in, you might have 100 tests, but you built them incrementally. This makes the task manageable.</p>
</section>
<section id="doing-evaluation-the-right-way" class="level3">
<h3 class="anchored" data-anchor-id="doing-evaluation-the-right-way">94. Doing Evaluation the Right Way</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_94.png" class="img-fluid figure-img"></p>
<figcaption>Slide 94</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2079s">Timestamp: 34:39</a>)</p>
<p>A summary slide listing best practices: <strong>Annotated Examples</strong>, <strong>Systematic Documentation</strong>, <strong>Continuous Error Analysis</strong>, <strong>Collaboration</strong>, and awareness of <strong>Generalization</strong>.</p>
<p>This concludes the core methodology section of the talk.</p>
</section>
<section id="agentic-use-cases" class="level3">
<h3 class="anchored" data-anchor-id="agentic-use-cases">95. Agentic Use Cases</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_95.png" class="img-fluid figure-img"></p>
<figcaption>Slide 95</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2090s">Timestamp: 34:50</a>)</p>
<p>The final section covers <strong>Agentic Use Cases</strong>, symbolized by a dragon. Agents add a layer of complexity because the model is now making decisions (routing, tool use) rather than just generating text.</p>
<p>This “agency” makes the system harder to track and evaluate.</p>
</section>
<section id="crossing-the-river" class="level3">
<h3 class="anchored" data-anchor-id="crossing-the-river">96. Crossing the River</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_96.png" class="img-fluid figure-img"></p>
<figcaption>Slide 96</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2106s">Timestamp: 35:06</a>)</p>
<p>A conceptual slide asking, <strong>“How should it cross the river?”</strong> (Fly, Swim, Bridge?). This represents the decision-making step in an agent.</p>
<p>Evaluating an agent requires evaluating <em>how</em> it made the decision (the router) separately from <em>how well</em> it executed the action.</p>
</section>
<section id="chat-to-purchase-router" class="level3">
<h3 class="anchored" data-anchor-id="chat-to-purchase-router">97. Chat-to-Purchase Router</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_97.png" class="img-fluid figure-img"></p>
<figcaption>Slide 97</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2122s">Timestamp: 35:22</a>)</p>
<p>A complex flowchart shows a <strong>Chat-to-Purchase Router</strong>. The agent must decide if the user wants to search for a product, get support, or track a package.</p>
<p>Rajiv suggests breaking this down: evaluate the <strong>Router</strong> component first (did it pick the right path?), then evaluate the specific workflow (did it track the package correctly?).</p>
</section>
<section id="text-to-sql-agent" class="level3">
<h3 class="anchored" data-anchor-id="text-to-sql-agent">98. Text to SQL Agent</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_98.png" class="img-fluid figure-img"></p>
<figcaption>Slide 98</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2177s">Timestamp: 36:17</a>)</p>
<p>Another example: <strong>Text to SQL Agent</strong>. This workflow involves classification, feature extraction, and SQL generation.</p>
<p>You can isolate the “Classification” step (is this a valid SQL question?) and build a test just for that, before testing the actual SQL generation.</p>
</section>
<section id="evaluating-office-style-agents" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-office-style-agents">99. Evaluating Office-Style Agents</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_99.png" class="img-fluid figure-img"></p>
<figcaption>Slide 99</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2206s">Timestamp: 36:46</a>)</p>
<p>The slide discusses <strong>OdysseyBench</strong>, a benchmark for office tasks. It highlights failure modes like “Failed to create folder” or “Failed to use tool.”</p>
<p>Evaluating agents involves checking if they successfully manipulated the environment (files, APIs), which is a functional test rather than a text similarity test.</p>
</section>
<section id="error-analysis-for-agents" class="level3">
<h3 class="anchored" data-anchor-id="error-analysis-for-agents">100. Error Analysis for Agents</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_100.png" class="img-fluid figure-img"></p>
<figcaption>Slide 100</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2220s">Timestamp: 37:00</a>)</p>
<p><strong>Error Analysis for Agentic Workflows</strong> requires assessing the overall performance, the routing decisions, and the individual steps.</p>
<p>It is the same “action error analysis” process but applied recursively to every node in the agent’s decision tree.</p>
</section>
<section id="evaluating-workflow-vs.-response" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-workflow-vs.-response">101. Evaluating Workflow vs.&nbsp;Response</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_101.png" class="img-fluid figure-img"></p>
<figcaption>Slide 101</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2239s">Timestamp: 37:19</a>)</p>
<p>This slide distinguishes between evaluating a <strong>Response</strong> (text) and a <strong>Workflow</strong> (process). The flowchart shows a conversational flow.</p>
<p>Evaluating a workflow might mean checking if the agent successfully moved the user from “Greeting” to “Resolution,” regardless of the exact words used.</p>
</section>
<section id="agentic-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="agentic-frameworks">102. Agentic Frameworks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_102.png" class="img-fluid figure-img"></p>
<figcaption>Slide 102</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2268s">Timestamp: 37:48</a>)</p>
<p>Rajiv warns that <strong>“Agentic Frameworks Help – Until They Don’t.”</strong> Frameworks (like LangChain or AutoGen) are great for demos because they abstract complexity.</p>
<p>However, in production, these abstractions can break or become outdated. He often recommends using straight Python for production agents to maintain control and reliability.</p>
</section>
<section id="abstraction-for-workflows" class="level3">
<h3 class="anchored" data-anchor-id="abstraction-for-workflows">103. Abstraction for Workflows</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_103.png" class="img-fluid figure-img"></p>
<figcaption>Slide 103</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2312s">Timestamp: 38:32</a>)</p>
<p>The slide illustrates the trade-off in <strong>Abstraction</strong>. You can build rigid workflows (orchestration) where you control every step, or use general agents where the LLM decides.</p>
<p>Orchestration is more reliable but rigid. General agents are flexible but prone to non-deterministic errors.</p>
</section>
<section id="when-abstractions-break" class="level3">
<h3 class="anchored" data-anchor-id="when-abstractions-break">104. When Abstractions Break</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_104.png" class="img-fluid figure-img"></p>
<figcaption>Slide 104</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2333s">Timestamp: 38:53</a>)</p>
<p>Model providers are training models to handle workflows internally (removing the need for external orchestration).</p>
<p>However, until models are perfect, developers often need to break tasks down into specific pieces to ensure reliability. The choice between “letting the model do it” and “scripting the flow” depends on the application’s risk tolerance.</p>
</section>
<section id="lessons-from-agent-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="lessons-from-agent-benchmarks">105. Lessons from Agent Benchmarks</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_105.png" class="img-fluid figure-img"></p>
<figcaption>Slide 105</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2355s">Timestamp: 39:15</a>)</p>
<p>The slide lists <strong>Lessons from Reproducing Agent Benchmarks</strong>: Standardize evaluation, measure efficiency, detect shortcuts, and log real behavior.</p>
<p>These are advanced tips for those pushing the boundaries of what agents can do.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">106. Conclusion</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_106.png" class="img-fluid figure-img"></p>
<figcaption>Slide 106</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/qPHsWTZP58U&amp;t=2367s">Timestamp: 39:27</a>)</p>
<p>The final slide, <strong>“We did it!”</strong>, concludes the presentation. Rajiv thanks the audience and provides the QR code again.</p>
<p>His final message is one of empowerment: he hopes the audience now has the confidence to go out, build their own evaluation datasets, and start “hill climbing” their own applications.</p>
<hr>
<p><em>This annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations.</em></p>


</section>
</section>

 ]]></description>
  <category>GenAI</category>
  <category>Evaluation</category>
  <category>LLM</category>
  <category>Testing</category>
  <category>Annotated Talk</category>
  <guid>https://rajivshah.com/blog/genai-evaluation-guide.html</guid>
  <pubDate>Sat, 01 Nov 2025 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/genai-evaluation-guide/slide_1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>From Vectors to Agents: Managing RAG in an Agentic World</title>
  <link>https://rajivshah.com/blog/rag-agentic-world.html</link>
  <description><![CDATA[ 






<section id="video" class="level2">
<h2 class="anchored" data-anchor-id="video">Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AS_HlJbJjH8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Watch the <a href="https://youtu.be/AS_HlJbJjH8">full video</a> | <a href="https://youtu.be/AS_HlJbJjH8">Slides</a></p>
<hr>
</section>
<section id="annotated-presentation" class="level2">
<h2 class="anchored" data-anchor-id="annotated-presentation">Annotated Presentation</h2>
<p>Below is an annotated version of the presentation, with timestamped links to the relevant parts of the video for each slide.</p>
<p>Here is the slide-by-slide annotated presentation based on the video “From Vectors to Agents: Managing RAG in an Agentic World” by Rajiv Shah.</p>
<hr>
<section id="title-slide" class="level3">
<h3 class="anchored" data-anchor-id="title-slide">1. Title Slide</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_1.png" class="img-fluid figure-img"></p>
<figcaption>Slide 1</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=0s">Timestamp: 00:00</a>)</p>
<p>The presentation begins with the title slide, introducing the core theme: <strong>“From Vectors to Agents: Managing RAG in an Agentic World.”</strong> The speaker, Rajiv Shah from Contextual, sets the stage for a technical deep dive into Retrieval-Augmented Generation (RAG).</p>
<p>He outlines the agenda, promising to move beyond basic RAG concepts to focus specifically on <strong>retrieval approaches</strong>. The talk is designed to cover the spectrum from traditional methods like BM25 and Language Models to the emerging field of Agentic Search.</p>
</section>
<section id="acme-gpt" class="level3">
<h3 class="anchored" data-anchor-id="acme-gpt">2. ACME GPT</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_2.png" class="img-fluid figure-img"></p>
<figcaption>Slide 2</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=40s">Timestamp: 00:40</a>)</p>
<p>This slide displays a stylized logo for “ACME GPT,” representing the typical enterprise aspiration. Companies see tools like ChatGPT and immediately want to apply that capability to their internal data, asking questions like, “Can I get the list of board of directors?”</p>
<p>However, the speaker notes a common hurdle: generic models don’t know enterprise-specific knowledge. This sets up the necessity for RAG—injecting private data into the model—rather than relying solely on the model’s pre-trained knowledge.</p>
</section>
<section id="building-rag-is-easy" class="level3">
<h3 class="anchored" data-anchor-id="building-rag-is-easy">3. Building RAG is Easy</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_3.png" class="img-fluid figure-img"></p>
<figcaption>Slide 3</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=70s">Timestamp: 01:10</a>)</p>
<p>The speaker illustrates the deceptively simple workflow of a basic RAG demo. The diagram shows the standard path: a user query is converted to vectors, matched against a database, and sent to an LLM.</p>
<p>Shah acknowledges that building a “hello world” version of this is trivial. He notes, “You can build a very easy RAG demo out of the box by just grabbing some data, using an embedding model, creating vectors, doing the similarity.”</p>
</section>
<section id="building-rag-is-easy-code-example" class="level3">
<h3 class="anchored" data-anchor-id="building-rag-is-easy-code-example">4. Building RAG is Easy (Code Example)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_4.png" class="img-fluid figure-img"></p>
<figcaption>Slide 4</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=82s">Timestamp: 01:22</a>)</p>
<p>A Python code snippet using <strong>LangChain</strong> is displayed to reinforce how accessible basic RAG has become. The code demonstrates loading a document, chunking it, and setting up a retrieval chain in just a few lines.</p>
<p>This slide serves as a foil for the upcoming reality check. While the code works for a demo, it hides the immense complexity required to make such a system robust, accurate, and scalable in a real-world production environment.</p>
</section>
<section id="rag-reality-check" class="level3">
<h3 class="anchored" data-anchor-id="rag-reality-check">5. RAG Reality Check</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_5.png" class="img-fluid figure-img"></p>
<figcaption>Slide 5</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=95s">Timestamp: 01:35</a>)</p>
<p>The tone shifts to the challenges of production. The slide highlights a sobering statistic: <strong>95% of Gen AI projects fail to reach production</strong>. The speaker details the specific reasons why demos fail when scaled: poor accuracy, unbearable latency, scaling issues with millions of documents, and ballooning costs.</p>
<p>He emphasizes a critical, often overlooked factor: <strong>Compliance</strong>. “Inside an enterprise, not everybody gets to read every document.” A demo ignores entitlements, but a production system cannot.</p>
</section>
<section id="maybe-try-a-different-rag" class="level3">
<h3 class="anchored" data-anchor-id="maybe-try-a-different-rag">6. Maybe try a different RAG?</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_6.png" class="img-fluid figure-img"></p>
<figcaption>Slide 6</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=180s">Timestamp: 03:00</a>)</p>
<p>This slide lists a dizzying array of RAG variants (GraphRAG, RAPTOR, CRAG, etc.) and retrieval techniques. It represents the “analysis paralysis” developers face when scouring arXiv papers for a solution to their accuracy problems.</p>
<p>Shah warns against blindly chasing the latest academic paper to fix fundamental system issues. “The answer is not in here of pulling together like a bunch of archive papers.” Instead, he advocates for a structured framework to make decisions.</p>
</section>
<section id="ultimate-rag-solution" class="level3">
<h3 class="anchored" data-anchor-id="ultimate-rag-solution">7. Ultimate RAG Solution</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_7.png" class="img-fluid figure-img"></p>
<figcaption>Slide 7</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=210s">Timestamp: 03:30</a>)</p>
<p>A humorous cartoon depicts a “Rube Goldberg” machine, representing the <strong>“Ultimate RAG Solution.”</strong> It mocks the tendency to over-engineer systems with too many interconnected, fragile components in the pursuit of performance.</p>
<p>The speaker uses this visual to argue for simplicity and deliberate design. The goal is to avoid building a monstrosity that is impossible to maintain, urging the audience to think about trade-offs before complexity.</p>
</section>
<section id="rag-as-a-system" class="level3">
<h3 class="anchored" data-anchor-id="rag-as-a-system">8. RAG as a system</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_8.png" class="img-fluid figure-img"></p>
<figcaption>Slide 8</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=215s">Timestamp: 03:35</a>)</p>
<p>The speaker introduces a clean system architecture for RAG, broken into four distinct stages: <strong>Parsing, Querying, Retrieving, and Generation</strong>. This framework serves as the mental map for the rest of the presentation.</p>
<p>He highlights that “Parsing” is vastly overlooked—getting information out of complex documents cleanly is a prerequisite for success. Today’s talk, however, will zoom in specifically on the <strong>Retrieving</strong> and <strong>Querying</strong> components.</p>
</section>
<section id="designing-a-rag-solution" class="level3">
<h3 class="anchored" data-anchor-id="designing-a-rag-solution">9. Designing a RAG Solution</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_9.png" class="img-fluid figure-img"></p>
<figcaption>Slide 9</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=250s">Timestamp: 04:10</a>)</p>
<p>This slide presents a “Tradeoff Triangle” for RAG, balancing <strong>Problem Complexity, Latency, and Cost</strong>. The speaker advises having a serious conversation with stakeholders about these constraints before writing code.</p>
<p>A key concept introduced here is the <strong>“Cost of a mistake.”</strong> In coding assistants, a mistake is low-cost (the developer fixes it). In medical RAG systems, the cost of a mistake is high (life or death), which dictates a completely different architectural approach.</p>
</section>
<section id="rag-considerations" class="level3">
<h3 class="anchored" data-anchor-id="rag-considerations">10. RAG Considerations</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_10.png" class="img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=330s">Timestamp: 05:30</a>)</p>
<p>A detailed table breaks down specific considerations that influence RAG design, such as domain difficulty, multilingual requirements, and data quality. This slide was originally created for sales teams to help scope customer problems.</p>
<p>Shah emphasizes that understanding the <strong>nuances</strong> of the use case upfront saves heartache later. For instance, knowing if users will ask simple questions or require complex reasoning changes the retrieval strategy entirely.</p>
</section>
<section id="consider-query-complexity" class="level3">
<h3 class="anchored" data-anchor-id="consider-query-complexity">11. Consider Query Complexity</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_11.png" class="img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=375s">Timestamp: 06:15</a>)</p>
<p>The speaker categorizes queries by complexity, ranging from simple <strong>Keywords</strong> (“Total Revenue”) to <strong>Semantic</strong> variations (“How much bank?”), to <strong>Multi-hop</strong> reasoning, and finally <strong>Agentic</strong> scenarios.</p>
<p>He points out a common failure mode: “The answers aren’t in the documents… all of a sudden they’re asking for knowledge that’s outside.” Recognizing the query complexity determines whether you need a simple search engine or a complex agentic workflow.</p>
</section>
<section id="retrieval-highlighted" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-highlighted">12. Retrieval (Highlighted)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_12.png" class="img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=452s">Timestamp: 07:32</a>)</p>
<p>The presentation zooms back into the system diagram, highlighting the <strong>“Retrieving”</strong> box. This signals the start of the deep technical dive into retrieval algorithms.</p>
<p>Shah notes that this area causes the most confusion due to the sheer number of model choices and architectures available. He aims to provide a practical guide to selecting the right retrieval tool.</p>
</section>
<section id="retrieval-approaches" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-approaches">13. Retrieval Approaches</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_13.png" class="img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=496s">Timestamp: 08:16</a>)</p>
<p>Three primary retrieval pillars are introduced: 1. <strong>BM25:</strong> The lexical, keyword-based standard. 2. <strong>Language Models:</strong> Semantic embeddings and vector search. 3. <strong>Agentic Search:</strong> The new frontier of iterative reasoning.</p>
<p>The speaker emphasizes that documents must be broken into pieces (<strong>chunking</strong>) because no single model context window is efficient enough to hold all enterprise data for every query.</p>
</section>
<section id="building-rag-is-easy-code-highlight" class="level3">
<h3 class="anchored" data-anchor-id="building-rag-is-easy-code-highlight">14. Building RAG is Easy (Code Highlight)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_14.png" class="img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=530s">Timestamp: 08:50</a>)</p>
<p>Returning to the initial code snippet, the speaker highlights the <code>vectorstore</code> and <code>retriever</code> initialization lines. This pinpoints exactly where the upcoming concepts fit into the implementation.</p>
<p>This visual anchor helps developers map the theoretical concepts of BM25 and Embeddings back to the actual lines of code they write in libraries like LangChain or LlamaIndex.</p>
</section>
<section id="bm25" class="level3">
<h3 class="anchored" data-anchor-id="bm25">15. BM25</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_15.png" class="img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=558s">Timestamp: 09:18</a>)</p>
<p><strong>BM25 (Best Match 25)</strong> is explained as a probabilistic lexical ranking function. The slide visualizes an <strong>inverted index</strong>, mapping words (like “butterfly”) to the specific documents containing them.</p>
<p>Shah explains that this is the 25th iteration of the formula, designed to score documents based on word frequency and saturation. It remains a powerful, fast baseline for retrieval.</p>
</section>
<section id="bm25-performance" class="level3">
<h3 class="anchored" data-anchor-id="bm25-performance">16. BM25 Performance</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_16.png" class="img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=595s">Timestamp: 09:55</a>)</p>
<p>A table compares the speed of a <strong>Linear Scan</strong> (Ctrl+F style) versus an <strong>Inverted Index (BM25)</strong> as the document count grows from 1,000 to 9,000.</p>
<p>The data shows that linear search becomes exponentially slower (taking 3,000 seconds for 1k documents in this synthetic test), while BM25 remains orders of magnitude faster. This efficiency is why lexical search is still widely used in production.</p>
</section>
<section id="bm25-failure-cases" class="level3">
<h3 class="anchored" data-anchor-id="bm25-failure-cases">17. BM25 Failure Cases</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_17.png" class="img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=668s">Timestamp: 11:08</a>)</p>
<p>The limitations of BM25 are exposed. Because it relies on exact word matches, it fails when users use synonyms. If a user searches for <strong>“Physician”</strong> but the documents only contain <strong>“Doctor,”</strong> BM25 will return zero results.</p>
<p>Similarly, it struggles with acronyms like <strong>“IBM”</strong> vs <strong>“International Business Machines.”</strong> Despite this, Shah argues BM25 is a “very strong baseline” that often beats complex neural models on specific keyword-heavy datasets.</p>
</section>
<section id="hands-on-bm25s" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-bm25s">18. Hands on: BM25s</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_18.png" class="img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=734s">Timestamp: 12:14</a>)</p>
<p>For developers wanting to implement this, the slide points to a library called <code>bm25s</code>, a high-performance Python implementation available on Hugging Face.</p>
<p>This reinforces the practical nature of the talk—BM25 isn’t just a legacy concept; it is an active, installable tool that developers should consider using alongside vector search.</p>
</section>
<section id="enter-language-models" class="level3">
<h3 class="anchored" data-anchor-id="enter-language-models">19. Enter Language Models</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_19.png" class="img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=744s">Timestamp: 12:24</a>)</p>
<p>The talk transitions to <strong>Language Models (Embeddings)</strong>. The slide explains how an encoder model turns text into a dense vector (a list of numbers) that captures semantic meaning.</p>
<p>Because these models are trained on vast amounts of data, they “have an idea of these similar concepts.” This solves the synonym problem that plagues BM25.</p>
</section>
<section id="embeddings-visualized" class="level3">
<h3 class="anchored" data-anchor-id="embeddings-visualized">20. Embeddings Visualized</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_20.png" class="img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=770s">Timestamp: 12:50</a>)</p>
<p>A 2D visualization demonstrates how embeddings group related concepts in <strong>latent space</strong>. The word “Doctor” and “Physician” would be located very close to each other mathematically.</p>
<p>This spatial proximity allows for <strong>Semantic Search</strong>: finding documents that mean the same thing as the query, even if they don’t share a single word.</p>
</section>
<section id="semantic-search-is-widely-used" class="level3">
<h3 class="anchored" data-anchor-id="semantic-search-is-widely-used">21. Semantic search is widely used</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_21.png" class="img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=795s">Timestamp: 13:15</a>)</p>
<p>The speaker validates the importance of semantic search by showing a tweet from Google’s SearchLiaison regarding BERT, and a screenshot of Hugging Face’s model repository.</p>
<p>This confirms that semantic search is the industry standard for modern information retrieval, having been deployed at massive scale by tech giants to improve result relevance.</p>
</section>
<section id="which-language-model" class="level3">
<h3 class="anchored" data-anchor-id="which-language-model">22. Which language model?</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_22.png" class="img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=810s">Timestamp: 13:30</a>)</p>
<p>A scatter plot compares various models based on <strong>Inference Speed</strong> (X-axis) and <strong>NDCG@10</strong> (Y-axis, a measure of retrieval quality).</p>
<p>Shah places <strong>BM25</strong> on the right (fast but lower accuracy) to orient the audience. He points out that there is a massive variety of models with different trade-offs between compute cost and retrieval quality.</p>
</section>
<section id="static-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="static-embeddings">23. Static Embeddings</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_23.png" class="img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=883s">Timestamp: 14:43</a>)</p>
<p>The speaker introduces <strong>Static Embeddings</strong> (like Word2Vec or GloVe) which are located on the far right of the previous scatter plot—extremely fast, even on CPUs.</p>
<p>These models assign a fixed vector to every word. While efficient, they lack context. The word “bank” has the same vector whether referring to a river bank or a financial bank, which limits their accuracy.</p>
</section>
<section id="why-context-matters" class="level3">
<h3 class="anchored" data-anchor-id="why-context-matters">24. Why Context Matters</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_24.png" class="img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=916s">Timestamp: 15:16</a>)</p>
<p>A cartoon illustrates the difference between Static Embeddings and Transformers. The Transformer can distinguish between “Model” in a data science context versus “Model” in a fashion context.</p>
<p>This contextual awareness is why modern Transformer-based embeddings (like BERT) generally outperform static embeddings and BM25 in complex retrieval tasks, despite being slower.</p>
</section>
<section id="many-more-models" class="level3">
<h3 class="anchored" data-anchor-id="many-more-models">25. Many more models!</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_25.png" class="img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=955s">Timestamp: 15:55</a>)</p>
<p>Returning to the scatter plot, a red arrow points toward the top-left quadrant—models that are slower but achieve higher accuracy.</p>
<p>The speaker notes that the field is constantly evolving, with “newer generations of models” pushing the boundary of what is possible in terms of retrieval quality.</p>
</section>
<section id="mtebrteb" class="level3">
<h3 class="anchored" data-anchor-id="mtebrteb">26. MTEB/RTEB</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_26.png" class="img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=995s">Timestamp: 16:35</a>)</p>
<p>To help developers choose, Shah introduces the <strong>MTEB (Massive Text Embedding Benchmark)</strong> and <strong>RTEB (Retrieval Text Embedding Benchmark)</strong>. These are leaderboards hosted on Hugging Face.</p>
<p>He highlights a key distinction: MTEB uses public datasets, while RTEB uses <strong>private, held-out datasets</strong>. This is crucial for avoiding “data contamination,” where models perform well simply because they were trained on the test data.</p>
</section>
<section id="selecting-an-embedding-model" class="level3">
<h3 class="anchored" data-anchor-id="selecting-an-embedding-model">27. Selecting an embedding model</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_27.png" class="img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1008s">Timestamp: 16:48</a>)</p>
<p>The speaker switches to a live browser view (captured in the slide) of the leaderboard. He discusses the bubble chart visualization where size often correlates with parameter count.</p>
<p>He points out an interesting trend: “You’ll see that there’s a bunch of models here that are all the same size… but the performance differs.” This indicates improvements in training strategies and architecture rather than just throwing more compute at the problem.</p>
</section>
<section id="selecting-an-embedding-model-other-considerations" class="level3">
<h3 class="anchored" data-anchor-id="selecting-an-embedding-model-other-considerations">28. Selecting an embedding model (Other Considerations)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_28.png" class="img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1147s">Timestamp: 19:07</a>)</p>
<p>Beyond the leaderboard score, Shah lists practical selection criteria: <strong>Model Size</strong> (can it fit in memory?), <strong>Architecture</strong> (CPU vs GPU), <strong>Embedding Dimension</strong> (storage costs), and <strong>Training Data</strong> (multilingual support).</p>
<p>He advises checking if a model is open source and quantizable, as this can significantly reduce latency without a major hit to accuracy.</p>
</section>
<section id="matryoshka-embedding-models" class="level3">
<h3 class="anchored" data-anchor-id="matryoshka-embedding-models">29. Matryoshka Embedding Models</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_29.png" class="img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1253s">Timestamp: 20:53</a>)</p>
<p>A specific innovation is highlighted: <strong>Matryoshka Embeddings</strong>. These models allow developers to truncate vectors (e.g., from 768 dimensions down to 64) while retaining most of the performance.</p>
<p>This is a “neat kind of innovation” for optimizing storage and search speed. OpenAI’s newer models also support this feature, offering flexibility between cost and accuracy.</p>
</section>
<section id="sentence-transformer" class="level3">
<h3 class="anchored" data-anchor-id="sentence-transformer">30. Sentence Transformer</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_30.png" class="img-fluid figure-img"></p>
<figcaption>Slide 30</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1302s">Timestamp: 21:42</a>)</p>
<p>The <strong>Sentence Transformer</strong> architecture is described as the dominant approach for RAG. Unlike standard BERT which works on tokens, these are fine-tuned to understand full sentences and paragraphs.</p>
<p>This architecture uses Siamese networks to ensure that semantically similar sentences are close in vector space, making them ideal for the “chunk-level” retrieval required in RAG.</p>
</section>
<section id="cross-encoder-reranker" class="level3">
<h3 class="anchored" data-anchor-id="cross-encoder-reranker">31. Cross Encoder / Reranker</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_31.png" class="img-fluid figure-img"></p>
<figcaption>Slide 31</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1336s">Timestamp: 22:16</a>)</p>
<p>The concept of a <strong>Cross Encoder (or Reranker)</strong> is introduced. Unlike the bi-encoder (retriever) which processes query and document separately, the cross-encoder processes them <em>together</em>.</p>
<p>This allows for a much deeper calculation of relevance. It is typically used as a second stage: retrieve 50 documents quickly with vectors, then use the slow but accurate Cross Encoder to rank the top 5.</p>
</section>
<section id="cross-encoder-reranker-duplicate" class="level3">
<h3 class="anchored" data-anchor-id="cross-encoder-reranker-duplicate">32. Cross Encoder / Reranker (Duplicate)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_32.png" class="img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1336s">Timestamp: 22:16</a>)</p>
<p>(This slide reinforces the previous diagram, emphasizing the “crossing” of the query and document in the model architecture.)</p>
</section>
<section id="cross-encoder-reranker-accuracy-boost" class="level3">
<h3 class="anchored" data-anchor-id="cross-encoder-reranker-accuracy-boost">33. Cross Encoder / Reranker (Accuracy Boost)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_33.png" class="img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1387s">Timestamp: 23:07</a>)</p>
<p>A bar chart quantifies the value of reranking. It shows a significant boost in <strong>NDCG (accuracy)</strong> when a reranker is added to the pipeline.</p>
<p>The speaker notes that while you get a “bump” in quality, it “doesn’t come for free.” The trade-off is increased latency, as the cross-encoder is computationally expensive.</p>
</section>
<section id="cross-encoder-reranker-execution-flow" class="level3">
<h3 class="anchored" data-anchor-id="cross-encoder-reranker-execution-flow">34. Cross Encoder / Reranker (Execution Flow)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_34.png" class="img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1395s">Timestamp: 23:15</a>)</p>
<p>The execution flow diagram highlights the reranker’s position in the pipeline. It sits between the Vector Store retrieval and the LLM generation.</p>
<p>This visual reinforces the latency implication: the user has to wait for both the initial search <em>and</em> the reranking pass before the LLM even starts generating an answer.</p>
</section>
<section id="hands-on-retriever-reranker" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-retriever-reranker">35. Hands On: Retriever &amp; Reranker</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_35.png" class="img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1410s">Timestamp: 23:30</a>)</p>
<p>A screenshot of a Google Colab notebook is shown, demonstrating a practical implementation of the Retrieve and Re-rank strategy using the <code>SentenceTransformer</code> and <code>CrossEncoder</code> libraries.</p>
<p>This provides a concrete resource for the audience to test the accuracy vs.&nbsp;speed trade-offs themselves on simple datasets like Wikipedia.</p>
</section>
<section id="instruction-following-reranker" class="level3">
<h3 class="anchored" data-anchor-id="instruction-following-reranker">36. Instruction Following Reranker</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_36.png" class="img-fluid figure-img"></p>
<figcaption>Slide 36</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1428s">Timestamp: 23:48</a>)</p>
<p>Shah mentions a specific advancement: <strong>Instruction Following Rerankers</strong> (developed by his company, Contextual). These allow developers to pass a prompt to the reranker, such as “Prioritize safety notices.”</p>
<p>This adds a “knob” for developers to tune retrieval based on business logic without retraining the model.</p>
</section>
<section id="combine-multiple-retrievers" class="level3">
<h3 class="anchored" data-anchor-id="combine-multiple-retrievers">37. Combine Multiple Retrievers</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_37.png" class="img-fluid figure-img"></p>
<figcaption>Slide 37</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1459s">Timestamp: 24:19</a>)</p>
<p>The presentation suggests that you don’t have to pick just one method. You can combine BM25, various embedding models (E5, BGE), and rerankers.</p>
<p>While combining them (Ensemble Retrieval) often yields better recall, Shah warns that “you got to engineer this.” Managing multiple indexes and fusion logic increases operational complexity and compute costs.</p>
</section>
<section id="cascading-rerankers-in-kaggle" class="level3">
<h3 class="anchored" data-anchor-id="cascading-rerankers-in-kaggle">38. Cascading Rerankers in Kaggle</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_38.png" class="img-fluid figure-img"></p>
<figcaption>Slide 38</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1496s">Timestamp: 24:56</a>)</p>
<p>A complex diagram from a Kaggle competition winner illustrates a <strong>Cascade Strategy</strong>. The solution used three different rerankers, filtering from 64 documents down to 8, and then to 5.</p>
<p>This shows the extreme end of retrieval engineering, where multiple models are chained to squeeze out every percentage point of accuracy.</p>
</section>
<section id="best-practices" class="level3">
<h3 class="anchored" data-anchor-id="best-practices">39. Best practices</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_39.png" class="img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1516s">Timestamp: 25:16</a>)</p>
<p>Shah distills the complexity into a recommended <strong>Best Practice</strong>: 1. <strong>Hybrid Search:</strong> Combine Semantic Search (Vectors) and Lexical Search (BM25). 2. <strong>Reciprocal Rank Fusion:</strong> Merge the results. 3. <strong>Reranker:</strong> Pass the top results through a cross-encoder.</p>
<p>This setup provides a “pretty good standard performance out of the box” and should be the default baseline before trying exotic methods.</p>
</section>
<section id="families-of-embedding-models" class="level3">
<h3 class="anchored" data-anchor-id="families-of-embedding-models">40. Families of Embedding Models</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_40.png" class="img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1542s">Timestamp: 25:42</a>)</p>
<p>A taxonomy slide categorizes the models discussed: <strong>Static</strong> (Fastest/Low Accuracy), <strong>Bi-Encoders</strong> (Fast/Good Accuracy), and <strong>Cross-Encoders</strong> (Slow/Best Accuracy).</p>
<p>This summary helps the audience mentally organize the tools available in their toolbox.</p>
</section>
<section id="lots-of-new-models" class="level3">
<h3 class="anchored" data-anchor-id="lots-of-new-models">41. Lots of New Models</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_41.png" class="img-fluid figure-img"></p>
<figcaption>Slide 41</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1550s">Timestamp: 25:50</a>)</p>
<p>Logos for IBM Granite, Google EmbeddingGemma, and others appear. The speaker notes that while new models from major players appear weekly, the improvements are often “incremental.”</p>
<p>He advises against “ripping up” a working system just to switch to a model that is 1% better on a leaderboard.</p>
</section>
<section id="other-retrieval-methods" class="level3">
<h3 class="anchored" data-anchor-id="other-retrieval-methods">42. Other retrieval methods</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_42.png" class="img-fluid figure-img"></p>
<figcaption>Slide 42</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1578s">Timestamp: 26:18</a>)</p>
<p>Alternative methods are briefly listed: <strong>SPLADE</strong> (Sparse retrieval), <strong>ColBERT</strong> (Late interaction), and <strong>GraphRAG</strong>.</p>
<p>Shah acknowledges these exist and may fit specific niches, but warns against chasing the “flavor of the week” before establishing a solid baseline with hybrid search.</p>
</section>
<section id="operational-concerns" class="level3">
<h3 class="anchored" data-anchor-id="operational-concerns">43. Operational Concerns</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_43.png" class="img-fluid figure-img"></p>
<figcaption>Slide 43</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1650s">Timestamp: 27:30</a>)</p>
<p>The talk shifts to operations. Libraries like <strong>FAISS</strong> are mentioned for efficient vector similarity search.</p>
<p>A key point is that for many use cases, you can simply store embeddings <strong>in memory</strong>. You don’t always need a complex vector database if your dataset fits in RAM.</p>
</section>
<section id="vector-database-options" class="level3">
<h3 class="anchored" data-anchor-id="vector-database-options">44. Vector Database Options</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_44.png" class="img-fluid figure-img"></p>
<figcaption>Slide 44</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1675s">Timestamp: 27:55</a>)</p>
<p>A diagram categorizes storage into <strong>Hot (In-Memory)</strong>, <strong>Warm (SSD/Disk)</strong>, and <strong>Cold</strong> tiers.</p>
<p>Shah notes there are “tons of vector database options” (Snowflake, Pinecone, etc.). The choice should be governed by <strong>latency requirements</strong>. If you need sub-millisecond retrieval, you need in-memory storage.</p>
</section>
<section id="operational-concerns-datastore-size" class="level3">
<h3 class="anchored" data-anchor-id="operational-concerns-datastore-size">45. Operational Concerns (Datastore Size)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_45.png" class="img-fluid figure-img"></p>
<figcaption>Slide 45</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1720s">Timestamp: 28:40</a>)</p>
<p>A graph shows that as <strong>Datastore Size</strong> increases (X-axis), retrieval performance naturally degrades (Y-axis).</p>
<p>To combat this, the speaker strongly recommends using <strong>Metadata Filtering</strong>. “If you’re not using something like metadata… it’s going to be very tough.” Narrowing the search scope is essential for scaling to millions of documents.</p>
</section>
<section id="search-strategy-comparison" class="level3">
<h3 class="anchored" data-anchor-id="search-strategy-comparison">46. Search Strategy Comparison</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_46.png" class="img-fluid figure-img"></p>
<figcaption>Slide 46</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1762s">Timestamp: 29:22</a>)</p>
<p>The presentation pivots to the “exciting part”: <strong>Agentic RAG</strong>. A visual compares “Traditional RAG” (a linear path) with “Agentic RAG” (a winding, exploratory path).</p>
<p>This represents the shift from a “one-shot” retrieval attempt to an iterative system that can explore, backtrack, and reason.</p>
</section>
<section id="tools-use-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="tools-use-reasoning">47. Tools use / Reasoning</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_47.png" class="img-fluid figure-img"></p>
<figcaption>Slide 47</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1780s">Timestamp: 29:40</a>)</p>
<p>Reasoning models (like o1 or DeepSeek R1) enable LLMs to use tools effectively. A code snippet shows an agent loop: query -&gt; generate -&gt; <strong>“Did it answer the question?”</strong></p>
<p>If the answer is no, the model can “rewrite the query… try to find that missing information, feed that back into the loop.” This self-correction is the core of Agentic RAG.</p>
</section>
<section id="agentic-rag-workflow" class="level3">
<h3 class="anchored" data-anchor-id="agentic-rag-workflow">48. Agentic RAG (Workflow)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_48.png" class="img-fluid figure-img"></p>
<figcaption>Slide 48</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1832s">Timestamp: 30:32</a>)</p>
<p>A flowchart details the Agentic RAG lifecycle. The model thinks through steps: “Oh, this is the query I need to make… based on those results… maybe we should do it a different way.”</p>
<p>This workflow allows the system to synthesize answers from multiple sources or clarify ambiguous queries automatically.</p>
</section>
<section id="tools-use-reasoning-detailed-example" class="level3">
<h3 class="anchored" data-anchor-id="tools-use-reasoning-detailed-example">49. Tools use / Reasoning (Detailed Example)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_49.png" class="img-fluid figure-img"></p>
<figcaption>Slide 49</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1835s">Timestamp: 30:35</a>)</p>
<p>A specific example of a complex query is shown. The agent breaks the problem down, calls tools, and iterates.</p>
<p>This demonstrates that the “Thinking” time is where the value is generated, allowing for a depth of research that a single retrieval pass cannot match.</p>
</section>
<section id="open-deep-research" class="level3">
<h3 class="anchored" data-anchor-id="open-deep-research">50. Open Deep Research</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_50.png" class="img-fluid figure-img"></p>
<figcaption>Slide 50</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1862s">Timestamp: 31:02</a>)</p>
<p>Shah references <strong>“Open Deep Research”</strong> by LangChain, an open-source framework where sub-agents go out, perform research, and report back.</p>
<p>This is a specific category of Agentic RAG focused on generating comprehensive reports rather than quick answers.</p>
</section>
<section id="deepresearch-bench" class="level3">
<h3 class="anchored" data-anchor-id="deepresearch-bench">51. DeepResearch Bench</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_51.png" class="img-fluid figure-img"></p>
<figcaption>Slide 51</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1890s">Timestamp: 31:30</a>)</p>
<p>A leaderboard for <strong>DeepResearch Bench</strong> is shown, testing models on “100 PhD level research tasks.”</p>
<p>The speaker warns that this approach “can get very expensive.” Solving a single complex query might cost significant money due to the number of tokens and iterative steps required.</p>
</section>
<section id="westlaw-ai-deep-research" class="level3">
<h3 class="anchored" data-anchor-id="westlaw-ai-deep-research">52. Westlaw AI Deep Research</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_52.png" class="img-fluid figure-img"></p>
<figcaption>Slide 52</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1915s">Timestamp: 31:55</a>)</p>
<p>A real-world application is highlighted: <strong>Westlaw AI</strong>. In the legal field, thoroughness is worth the latency and cost.</p>
<p>This proves that Agentic RAG isn’t just a toy; it is being commercialized in high-value verticals where accuracy is paramount.</p>
</section>
<section id="agentic-rag-self-rag" class="level3">
<h3 class="anchored" data-anchor-id="agentic-rag-self-rag">53. Agentic RAG (Self-RAG)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_53.png" class="img-fluid figure-img"></p>
<figcaption>Slide 53</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1931s">Timestamp: 32:11</a>)</p>
<p>The concept of <strong>Self-RAG</strong> is introduced, emphasizing the “Reflection” step. The model critiques its own retrieved documents and generation quality.</p>
<p>Shah notes that this isn’t brand new, but has become practical due to better reasoning models.</p>
</section>
<section id="agentic-rag-langchain-reddit" class="level3">
<h3 class="anchored" data-anchor-id="agentic-rag-langchain-reddit">54. Agentic RAG (LangChain Reddit)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_54.png" class="img-fluid figure-img"></p>
<figcaption>Slide 54</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2044s">Timestamp: 34:04</a>)</p>
<p>A Reddit post is shown where a developer discusses building a self-reflection RAG system. This highlights the community’s active experimentation with these loops.</p>
</section>
<section id="agentic-rag-efficiency-concerns" class="level3">
<h3 class="anchored" data-anchor-id="agentic-rag-efficiency-concerns">55. Agentic RAG (Efficiency Concerns)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_55.png" class="img-fluid figure-img"></p>
<figcaption>Slide 55</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2055s">Timestamp: 34:15</a>)</p>
<p>The discussion turns to the “Rub”: <strong>Inefficiency</strong>. Agentic loops can be slow and wasteful, re-retrieving data unnecessarily.</p>
<p>This sets up the trade-off conversation again: Is the extra time and compute worth the accuracy gain?</p>
</section>
<section id="research-bright" class="level3">
<h3 class="anchored" data-anchor-id="research-bright">56. Research: BRIGHT</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_56.png" class="img-fluid figure-img"></p>
<figcaption>Slide 56</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1931s">Timestamp: 32:11</a>)</p>
<p><em>Note: The speaker introduces the BRIGHT benchmark around 32:11, slightly out of slide order in the transcript flow, but connects it here.</em></p>
<p><strong>BRIGHT</strong> is a benchmark specifically designed for <strong>Retrieval Reasoning</strong>. Unlike standard benchmarks that test keyword matching, BRIGHT tests questions that require thinking, logic, and multi-step deduction to find the correct document.</p>
</section>
<section id="bright-1-diver" class="level3">
<h3 class="anchored" data-anchor-id="bright-1-diver">57. BRIGHT #1: DIVER</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_57.png" class="img-fluid figure-img"></p>
<figcaption>Slide 57</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=1968s">Timestamp: 32:48</a>)</p>
<p>The top-performing system on BRIGHT is <strong>DIVER</strong>. The diagram shows it uses the exact components discussed earlier: Chunking, Retrieving, and Reranking, but wrapped in an iterative loop.</p>
<p>Shah points out, “It probably doesn’t look that crazy to you if you’re used to RAG.” The innovation is in the process, not necessarily a magical new model architecture.</p>
</section>
<section id="bright-1-diver-llm-instructions" class="level3">
<h3 class="anchored" data-anchor-id="bright-1-diver-llm-instructions">58. BRIGHT #1: DIVER (LLM Instructions)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_58.png" class="img-fluid figure-img"></p>
<figcaption>Slide 58</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2011s">Timestamp: 33:31</a>)</p>
<p>The specific prompts used in DIVER are shown. The system asks the LLM: “Given a query… what do you think would be possibly helpful to do?”</p>
<p>This <strong>Query Expansion</strong> allows the system to generate new search terms that the user didn’t think of, bridging the semantic gap through reasoning.</p>
</section>
<section id="agentic-rag-on-wixqa" class="level3">
<h3 class="anchored" data-anchor-id="agentic-rag-on-wixqa">59. Agentic RAG on WixQA</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_59.png" class="img-fluid figure-img"></p>
<figcaption>Slide 59</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2076s">Timestamp: 34:36</a>)</p>
<p>Shah shares his own experiment results on the <strong>WixQA</strong> dataset (technical support). * <strong>One Shot RAG:</strong> 5 seconds latency, <strong>76%</strong> Factuality. * <strong>Agentic RAG:</strong> Slower latency, <strong>93%</strong> Factuality.</p>
<p>This massive jump in accuracy (0.76 to 0.93) is the key takeaway. “That has a ton of implications.” It suggests that the limitation of RAG often isn’t the data, but the lack of reasoning applied to the retrieval process.</p>
</section>
<section id="rethink-your-assumptions" class="level3">
<h3 class="anchored" data-anchor-id="rethink-your-assumptions">60. Rethink your Assumptions</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_60.png" class="img-fluid figure-img"></p>
<figcaption>Slide 60</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2230s">Timestamp: 37:10</a>)</p>
<p><strong>This is the climax of the technical argument.</strong> A graph from the BRIGHT paper shows that <strong>BM25 (lexical search)</strong> combined with an Agentic loop (GPT-4) outperforms advanced embedding models (Qwen).</p>
<p>“This is crazy,” Shah exclaims. Because the LLM can rewrite queries into many variations, it mitigates BM25’s weakness (synonyms). This implies you might not need complex vector databases if you have a smart agent.</p>
</section>
<section id="agentic-rag-with-bm25" class="level3">
<h3 class="anchored" data-anchor-id="agentic-rag-with-bm25">61. Agentic RAG with BM25</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_61.png" class="img-fluid figure-img"></p>
<figcaption>Slide 61</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2300s">Timestamp: 38:20</a>)</p>
<p>Shah validates the paper’s finding with his own internal data (Financial 10Ks). <strong>Agentic RAG with BM25</strong> performed nearly as well as Agentic RAG with Embeddings.</p>
<p>He suggests a radical possibility: “I could throw all that away [vector DBs]… just stick this in a text-only database and use BM25.”</p>
</section>
<section id="agentic-rag-for-code-search" class="level3">
<h3 class="anchored" data-anchor-id="agentic-rag-for-code-search">62. Agentic RAG for Code Search</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_62.png" class="img-fluid figure-img"></p>
<figcaption>Slide 62</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2386s">Timestamp: 39:46</a>)</p>
<p>He connects this finding to <strong>Claude Code</strong>, which uses a lexical approach (like <code>grep</code>) rather than vectors for code search.</p>
<p>Since code doesn’t have the same semantic ambiguity as natural language, and agents can iterate rapidly, lexical search is proving to be superior for coding assistants.</p>
</section>
<section id="combine-retrieval-approaches" class="level3">
<h3 class="anchored" data-anchor-id="combine-retrieval-approaches">63. Combine Retrieval Approaches</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_63.png" class="img-fluid figure-img"></p>
<figcaption>Slide 63</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2415s">Timestamp: 40:15</a>)</p>
<p>A <strong>DoorDash</strong> case study illustrates a two-tier guardrail system. They use simple text similarity first (fast/cheap). If that fails or is uncertain, they kick it to an LLM (slow/expensive).</p>
<p>This “Tiered” approach optimizes the trade-off between cost and accuracy in production.</p>
</section>
<section id="hands-on-agentic-rag-smolagents" class="level3">
<h3 class="anchored" data-anchor-id="hands-on-agentic-rag-smolagents">64. Hands on: Agentic RAG (Smolagents)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_64.png" class="img-fluid figure-img"></p>
<figcaption>Slide 64</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2467s">Timestamp: 41:07</a>)</p>
<p>The speaker points to <strong>Smolagents</strong>, a Hugging Face library, as a way to get hands-on with these concepts. A Colab notebook is provided for the audience to build their own agentic retrieval loops.</p>
</section>
<section id="solutions-for-a-rag-solution" class="level3">
<h3 class="anchored" data-anchor-id="solutions-for-a-rag-solution">65. Solutions for a RAG Solution</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_65.png" class="img-fluid figure-img"></p>
<figcaption>Slide 65</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2478s">Timestamp: 41:18</a>)</p>
<p>Shah updates the “Problem Complexity” framework from the beginning of the talk with specific recommendations: * <strong>Low Latency (&lt;5s):</strong> Use BM25 or Static Embeddings. * <strong>High Cost of Mistake:</strong> Add a Reranker. * <strong>Complex Multi-hop:</strong> Use Agentic RAG.</p>
</section>
<section id="retriever-checklist" class="level3">
<h3 class="anchored" data-anchor-id="retriever-checklist">66. Retriever Checklist</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_66.png" class="img-fluid figure-img"></p>
<figcaption>Slide 66</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2512s">Timestamp: 41:52</a>)</p>
<p>A final checklist summarizes the retrieval hierarchy: 1. <strong>Keyword/BM25</strong> (The baseline). 2. <strong>Semantic Search</strong> (The standard). 3. <strong>Agentic/Reasoning</strong> (The problem solver).</p>
<p>This provides the audience with a mental menu to choose from based on their specific constraints.</p>
</section>
<section id="rag-as-a-system-retrieval-with-instruction-following-reranker" class="level3">
<h3 class="anchored" data-anchor-id="rag-as-a-system-retrieval-with-instruction-following-reranker">67. RAG as a system (Retrieval with Instruction Following Reranker)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_67.png" class="img-fluid figure-img"></p>
<figcaption>Slide 67</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2520s">Timestamp: 42:00</a>)</p>
<p>The system diagram is shown one last time, updated to include the <strong>Instruction Following Reranker</strong> in the retrieval box, solidifying the modern RAG architecture.</p>
</section>
<section id="rag---generation" class="level3">
<h3 class="anchored" data-anchor-id="rag---generation">68. RAG - Generation</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_68.png" class="img-fluid figure-img"></p>
<figcaption>Slide 68</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2530s">Timestamp: 42:10</a>)</p>
<p><em>Note: The speaker concludes the talk at 42:10, stating “I’m going to end it here.” Slides 68-70 regarding the Generation stage were included in the deck but skipped in the video recording due to time constraints.</em></p>
<p>This slide would have covered the final stage of RAG: generating the answer. The focus here is typically on reducing hallucinations and ensuring the tone matches the user’s needs.</p>
</section>
<section id="rag---generation-model-selection" class="level3">
<h3 class="anchored" data-anchor-id="rag---generation-model-selection">69. RAG - Generation (Model Selection)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_69.png" class="img-fluid figure-img"></p>
<figcaption>Slide 69</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2530s">Timestamp: 42:10</a>)</p>
<p><em>Skipped in video.</em> This slide illustrates the choice of LLM for generation (e.g., GPT-4 vs Llama 3 vs Claude). The choice depends on the “Cost/Latency budget” and specific domain requirements.</p>
</section>
<section id="chunking-approaches" class="level3">
<h3 class="anchored" data-anchor-id="chunking-approaches">70. Chunking approaches</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_70.png" class="img-fluid figure-img"></p>
<figcaption>Slide 70</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2530s">Timestamp: 42:10</a>)</p>
<p><em>Skipped in video.</em> This slide compares <strong>Original Chunking</strong> (cutting text at fixed intervals) with <strong>Contextual Chunking</strong> (adding a summary prefix to every chunk). Contextual chunking significantly improves retrieval because every chunk carries the context of the parent document.</p>
</section>
<section id="title-slide-duplicate" class="level3">
<h3 class="anchored" data-anchor-id="title-slide-duplicate">71. Title Slide (Duplicate)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/rag-talk/slide_71.png" class="img-fluid figure-img"></p>
<figcaption>Slide 71</figcaption>
</figure>
</div>
<p>(<a href="https://youtu.be/AS_HlJbJjH8&amp;t=2530s">Timestamp: 42:10</a>)</p>
<p>The presentation concludes with the title slide. Rajiv Shah thanks the audience, encouraging them to think about trade-offs rather than just chasing the latest models. “Hopefully I’ve given you a sense of thinking about these trade-offs… thank you all.”</p>
<hr>
<p><em>This annotated presentation was generated from the talk using AI-assisted tools. Each slide includes timestamps and detailed explanations.</em></p>


</section>
</section>

 ]]></description>
  <category>RAG</category>
  <category>AI</category>
  <category>Retrieval</category>
  <category>Agentic</category>
  <category>Annotated Talk</category>
  <guid>https://rajivshah.com/blog/rag-agentic-world.html</guid>
  <pubDate>Mon, 27 Oct 2025 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/rag-talk/slide_1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Understanding Sparse Matrices through Interactive Visualizations</title>
  <link>https://rajivshah.com/blog/sparsedataframe.html</link>
  <description><![CDATA[ 






<p>When working with machine learning models, preparing data properly is essential. One common preprocessing technique is one-hot encoding, which transforms categorical data into a format algorithms can understand. However, this transformation often creates sparse matrices - dataframes where most values are zero.</p>
<section id="basic-one-hot-encoding" class="level2">
<h2 class="anchored" data-anchor-id="basic-one-hot-encoding">Basic One-Hot Encoding</h2>
<p>The first animation illustrates the fundamental concept of one-hot encoding. This transformation converts a single categorical column (like “city”) into multiple binary columns, where each column represents one possible category value.</p>
<p><a href="./sparse-1.html">View the basic one-hot encoding animation</a></p>
<p>This visualization walks through the transformation step-by-step:</p>
<ol type="1">
<li>Starting with the original dataset containing categorical values</li>
<li>Adding binary indicator columns for each category</li>
<li>Showing how the dataset becomes wider but sparse (mostly filled with zeros)</li>
<li>Demonstrating how the original categorical column becomes redundant</li>
</ol>
<p>In traditional tabular data processing, we often don’t see this sparsity visually. The animation makes it clear how one-hot encoding dramatically changes the structure of our data.</p>
</section>
<section id="the-curse-of-dimensionality" class="level2">
<h2 class="anchored" data-anchor-id="the-curse-of-dimensionality">The Curse of Dimensionality</h2>
<p>The second animation takes the concept further by demonstrating what happens with high-cardinality categorical features - those with many possible values.</p>
<p><a href="./sparse-2.html">View the curse of dimensionality animation</a></p>
<p>This more advanced visualization shows how one-hot encoding can lead to the “curse of dimensionality”:</p>
<ol type="1">
<li>Starting with a modest 4-column dataset</li>
<li>Expanding to over 150 columns when encoding a categorical feature with many values</li>
<li>Creating an extremely sparse matrix where 99% of values are zeros</li>
<li>Illustrating the practical challenges this presents for machine learning</li>
</ol>
</section>
<section id="why-it-matters" class="level2">
<h2 class="anchored" data-anchor-id="why-it-matters">Why It Matters</h2>
<p>Understanding the sparsity that results from one-hot encoding is crucial for several reasons:</p>
<ul>
<li><strong>Memory usage</strong>: Sparse matrices can consume excessive memory if not properly handled</li>
<li><strong>Computational efficiency</strong>: Processing mostly-zero matrices is inefficient</li>
<li><strong>Model performance</strong>: Many algorithms struggle with extremely sparse data</li>
<li><strong>Feature selection</strong>: With hundreds of binary columns, feature selection becomes critical</li>
</ul>
<p>For high-cardinality features, consider alternatives like feature hashing, target encoding, or embeddings to avoid the dimensionality explosion shown in the second animation.</p>
<p>These visualizations help build intuition about what’s happening “under the hood” when we preprocess data - something that’s often hidden when we use high-level libraries that handle these transformations automatically.</p>
<p>Related videos: <a href="https://youtube.com/shorts/M3AhBvaSSvY">Sparsity in AI</a> or <a href="https://www.tiktok.com/@rajistics/video/7470265095654739230">Curse of Dimensionality</a> or <a href="https://www.instagram.com/reel/DGlW4V1A0Ri/">Reality of Models</a></p>


</section>

 ]]></description>
  <category>Sparse</category>
  <category>Dataframes</category>
  <category>Machine Learning</category>
  <category>Data Preprocessing</category>
  <guid>https://rajivshah.com/blog/sparsedataframe.html</guid>
  <pubDate>Fri, 07 Mar 2025 06:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/sparsedataframe.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Reasoning in Large Language Models</title>
  <link>https://rajivshah.com/blog/HF-Reasoning.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/hf/r-title.png" class="img-fluid figure-img"></p>
<figcaption>Reasoning</figcaption>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>I was wowed by ChatGPT. While I understood tasks like text generation and summarization, something was different with ChatGPT. When I looked at the literature, I saw this work exploring reasoning. Models reasoning, c’mon. As a very skeptical data scientist, that seemed far-fetched to me. But I had to explore.</p>
<p>I came upon the <a href="https://github.com/google/BIG-bench">Big Bench Benchmark</a>, composed of more than 200 reasoning tasks. The tasks include playing chess, describing code, guessing the perpetrator of a crime in a short story, identifying sarcasm, and even recognizing self-awareness. A common benchmark to test models is the Big Bench Hard (BBH), a subset of 23 tasks from Big Bench. Early models like OpenAI’s text-ada-00 struggle to reach a random score of 25. However, several newer models reach and surpass the average human rater score of 67.7. You can see results for these models in these publications: <a href="https://arxiv.org/pdf/2301.13688.pdf">1</a>, <a href="https://arxiv.org/pdf/2210.09261.pdf">2</a>, and <a href="https://arxiv.org/pdf/2210.11416.pdf">3</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/hf/BBH.png" class="img-fluid figure-img"></p>
<figcaption>Big Bench Hard (23 Tasks) (1).png</figcaption>
</figure>
</div>
<p>A <a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec09.pdf">survey of the research</a> pointed out some common starting points for evaluating reasoning in models, including Arithmetic Reasoning, Symbolic Reasoning, and Commonsense Reasoning. This blog post provides examples of reasoning, but you should try out all these examples yourself. Hugging Face has a <a href="https://huggingface.co/spaces/osanseviero/i-like-flan">space where you can try</a> to test a Flan T5 model yourself.</p>
</section>
<section id="arithmetic-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="arithmetic-reasoning">Arithmetic <strong>Reasoning</strong></h3>
<p>Let’s start with the following problem.</p>
<pre><code>Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: The answer is 5</code></pre>
<p>If you ask an older text generation model like GPT-2 to complete this, it doesn’t understand the question and instead continues to write a story like this.</p>
<p><img src="https://rajivshah.com/blog/images/hf/R-Cars-GPT2.png" alt="R-Cars-GPT2.png" style="zoom:50%;"></p>
<p>While I don’t have access to PalM - 540B parameter model in the Big Bench, I was able to work with the Flan-T5 XXL using this publicly available space. I entered the problem and got this answer!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/hf/R-Cars-Flan.png" class="img-fluid figure-img"></p>
<figcaption>R-Cars-Flan.png</figcaption>
</figure>
</div>
<p>It solved it! I tried messing with it and changing the words, but it still answered correctly. To my untrained eye, it is trying to take the numbers and perform a calculation using the surrounding information. This is an elementary problem, but this is more sophisticated than the GPT-2 response. I next wanted to do a more challenging problem like this:</p>
<pre><code>Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?</code></pre>
<p>The model gave an answer of 8, which isn’t correct. Recent research has found using chain-of-thought prompting can improve the ability of models. This involves providing intermediate reasoning to help the model determine the answer.</p>
<pre><code>Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is </code></pre>
<p>The model correctly answers 11. To solve the juggling problem, I used this chain-of-thought prompt as an example. Giving the model some examples is known as few-shot learning. The new combined prompt using chain-of-thought and few-shot learning is:</p>
<pre><code>Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?
A:</code></pre>
<p>Try it, it works! Giving it an example and making it think everything through step by step was beneficial. This was fascinating for me. We don’t train the model in the sense of updating it’s weights. Instead, we are guiding it purely by the inference process.</p>
</section>
<section id="symbolic-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="symbolic-reasoning"><strong>Symbolic Reasoning</strong></h3>
<p>The first symbolic reasoning was doing a reversal and the Flan-T5 worked very well on this type of problem.</p>
<pre><code>Reverse the sequence "glasses, pen, alarm, license".</code></pre>
<p>A more complex problem on coin flipping was more interesting for me.</p>
<pre><code>Q: A coin is heads up. Tom does not flip the coin. Mike does not flip the coin. Is the coin still heads up?
A:</code></pre>
<p>For this one, I played around with different combinations of people flipping and showing the coin and the model, and it answered correctly. It was following the logic that was going through.</p>
</section>
<section id="common-sense-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="common-sense-reasoning"><strong>Common sense reasoning</strong></h3>
<p>The last category was common sense reasoning and much less obvious to me how models know how to solve these problems correctly.</p>
<pre><code>Q: What home entertainment equipment requires cable?
Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet
A: The answer is</code></pre>
<p>I was amazed at how well the model did, even when I changed the order.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/hf/Rcommon2.gif" class="img-fluid figure-img"></p>
<figcaption>Reasongif</figcaption>
</figure>
</div>
<p>Another common reasoning example goes like this:</p>
<pre><code>Q: Can Barack Obama have a conversation with George Washington? Give the rationale before answering.</code></pre>
<p>I changed around people to someone currently living, and it still works well.</p>
</section>
<section id="thoughts" class="level3">
<h3 class="anchored" data-anchor-id="thoughts"><strong>Thoughts</strong></h3>
<p>As the first step, please, go try out these models for yourself. <a href="https://huggingface.co/google/flan-t5-xxl">Google’s Flan-T5 is available</a> with an Apache 2.0 license. Hugging Face has a <a href="https://huggingface.co/spaces/osanseviero/i-like-flan">space where you can try</a> all these reasoning examples yourself. You can also replicate this using OpenAI’s GPT or other language models. I have a <a href="https://youtu.be/teRu-ZT9XJs">short video on the reasoning</a> that also shows several examples.</p>
<p>The current language models have many known limitations. The next generation of models will likely be able to retrieve relevant information before answering. Additionally, language models will likely be able to delegate tasks to other services. You can see a demo of this integrating <a href="https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain">ChatGPT with Wolfram’s scientific API</a>. By letting language models offload other tasks, the role of language models will emphasize communication and reasoning.</p>
<p>The current generation of models is starting to solve some reasoning tasks and match average human raters. It also appears that performance can still keep increasing. What happens when there are a set of reasoning tasks that computers are better than humans? While plenty of academic literature highlights the limitations, the overall trajectory is clear and has extraordinary implications.</p>


</section>

 ]]></description>
  <category>LLM</category>
  <category>NLP</category>
  <guid>https://rajivshah.com/blog/HF-Reasoning.html</guid>
  <pubDate>Wed, 08 Feb 2023 06:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/hf/r-title.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Text style transfer in a spreadsheet using Hugging Face Inference Endpoints</title>
  <link>https://rajivshah.com/blog/HF-Endpoint.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/hf/informal_endpoint_cover.png" class="img-fluid figure-img"></p>
<figcaption>SetFit</figcaption>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>We change our conversational style from informal to formal speech. We often do this without thinking when talking to our friends compared to addressing a judge. Computers now have this capability! I use <a href="https://blog.fastforwardlabs.com/2022/03/22/an-introduction-to-text-style-transfer.html">textual style transfer</a> in this post to convert informal text to formal text. To make this easy to use, we do it in a spreadsheet.</p>
</section>
<section id="step-1" class="level3">
<h3 class="anchored" data-anchor-id="step-1">Step 1</h3>
<p>The first step is identifying an <a href="https://huggingface.co/rajistics/informal_formal_style_transfer">informal to formal text style model</a>. Next, we deploy the model using <a href="https://ui.endpoints.huggingface.co/endpoints">Hugging Face Inference endpoints</a>. <a href="https://huggingface.co/docs/inference-endpoints/index">Inference endpoints</a> is a production-grade solution for model deployment.</p>
<p><img src="https://rajivshah.com/blog/images/hf/model-inference.png" class="img-fluid"></p>
</section>
<section id="step-2" class="level3">
<h3 class="anchored" data-anchor-id="step-2">Step 2</h3>
<p>Let’s incorporate the endpoint into Google Sheets custom function to make the model easy to use.</p>
<p><img src="https://rajivshah.com/blog/images/hf/HF_IE_examples-min.png" class="img-fluid"></p>
<p>I added the code to Google Sheets through the Apps Script extension. Grab it <a href="https://gist.github.com/rajshah4/6cde451b7f126aeaa67d89503cba5b93">here as a gist</a>. Once that is saved, you can use the new function as a formula. Now, I can use one simple command if I want to do textual style transfer!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/hf/informal_endpoints.png" class="img-fluid figure-img"></p>
<figcaption>Alt Text</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level3">
<h3 class="anchored" data-anchor-id="resources">Resources</h3>
<p>I created a Youtube 🎥 <a href="https://youtu.be/jA6VDKO7XfA">video</a> for a more detailed walkthrough.</p>
<p>Go try this out with your favorite model! For another example, check out the <a href="https://huggingface.co/RamAnanth1/positive-reframing">positive style textual model</a> in a <a href="https://www.tiktok.com/@rajistics/video/7161954065243508014">Tik Tok video</a>.</p>


</section>

 ]]></description>
  <category>NLP</category>
  <category>Huggingface</category>
  <category>Finetuning</category>
  <guid>https://rajivshah.com/blog/HF-Endpoint.html</guid>
  <pubDate>Mon, 07 Nov 2022 06:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/hf/informal_endpoint_cover.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Few shot text classification with SetFit</title>
  <link>https://rajivshah.com/blog/setfit.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/setfit.png" class="img-fluid figure-img"></p>
<figcaption>SetFit</figcaption>
</figure>
</div>
<section id="introduction" class="level4">
<h4 class="anchored" data-anchor-id="introduction">Introduction</h4>
<p>Data scientists often do not have large amounts of labeled data. This issue is even graver when dealing with problems with tens or hundreds of classes. The reality is very few text classification problems get to the point where adding more labeled data isn’t improving performance.</p>
<p>SetFit offers a few-shot learning approach for text classification. The <a href="https://arxiv.org/abs/2209.11055">paper’s results</a> show across many datasets, it’s possible to get better performance with less labeled data. This technique uses contrastive learning to build a larger dataset for fine-tuning a text classification model. This approach was new to me and was why I did a video explaining how contrastive learning helps with text classification.</p>
<p>I have created a Colab 📓 companion notebook at <a href="https://bit.ly/raj_setfit">https://bit.ly/raj_setfit</a>, and the Youtube 🎥 <a href="https://youtu.be/Pg-smN4fUy0">video</a> that provides a detailed explanation. I walk through a simple churn example to give the intuition behind SetFit. The notebook trains the CR (customer review dataset) highlighted in the SetFit paper.</p>
<p>The <a href="https://github.com/huggingface/setfit">SetFit github</a> contains the code, and a great deep dive for text classification is found on <a href="https://www.philschmid.de/getting-started-setfit">Philipp’s blog</a>. For those looking to productionize a SetFit model, Philipp has also documented how to create the <a href="https://huggingface.co/philschmid/setfit-ag-news-endpoint">Hugging Face endpoint</a> for a SetFit model.</p>
<p>So grab your favorite text classification dataset and give it a try!</p>


</section>

 ]]></description>
  <category>Setfit</category>
  <category>Classification</category>
  <category>NLP</category>
  <guid>https://rajivshah.com/blog/setfit.html</guid>
  <pubDate>Thu, 27 Oct 2022 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/setfit.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Getting predictions intervals with conformal inference</title>
  <link>https://rajivshah.com/blog/conformal_predictions.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/conformal_inference.png" class="img-fluid figure-img"></p>
<figcaption>Conformal</figcaption>
</figure>
</div>
<section id="introduction" class="level4">
<h4 class="anchored" data-anchor-id="introduction">Introduction</h4>
<p>Data scientists often overstate the certainty of their predictions. I have had engineers laugh at my point predictions and point out several types of errors in my model that create uncertainty. Prediction intervals are an excellent counterbalance for communicating the uncertainty of predictions.</p>
<p>Conformal inference offers a model agnostic technique for prediction intervals. It’s well known within statistics but not as well established in machine learning. This post focuses on a straightforward conformal inference technique, but there are more sophisticated techniques that provide more adaptable prediction intervals.</p>
<p>I have created a Colab 📓 companion notebook at <a href="https://bit.ly/raj_conf">https://bit.ly/raj_conf</a>, and the Youtube 🎥 <a href="https://youtu.be/ZUK4zR0IeLU">video</a> that provides a detailed explanation. This explanation is a toy example to learn how conformal inference works. Typical applications will use a more sophisticated methodology along with implementations found within the resources below.</p>
<p>For python folks, a great package to start using conformal inference is <a href="https://mapie.readthedocs.io/en/latest/index.html">MAPIE - Model Agnostic Prediction Interval Estimator</a>. It works for tabular and time series problems.</p>
</section>
<section id="further-resources" class="level4">
<h4 class="anchored" data-anchor-id="further-resources">Further Resources:</h4>
<p>Quick intro to conformal prediction using MAPIE in <a href="https://towardsdatascience.com/mapie-explained-exactly-how-you-wished-someone-explained-to-you-78fb8ce81ff3">medium</a></p>
<p>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification, <a href="https://people.eecs.berkeley.edu/~angelopoulos/publications/downloads/gentle_intro_conformal_dfuq.pdf">paper link</a></p>
<p><a href="https://github.com/valeman/awesome-conformal-prediction">Awesome Conformal Prediction</a> (lots of resources)</p>


</section>

 ]]></description>
  <category>Conformal</category>
  <category>MLOps</category>
  <category>MAPIE</category>
  <guid>https://rajivshah.com/blog/conformal_predictions.html</guid>
  <pubDate>Sat, 24 Sep 2022 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/conformal_inference.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Explaining predictions from 🤗 transformer models</title>
  <link>https://rajivshah.com/blog/explaining_transformers.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/transformers/banner.png" class="img-fluid figure-img"></p>
<figcaption>Banner</figcaption>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>This post covers 3 easy-to-use 📦 packages to get started. You can also check out the Colab 📓 companion notebook at https://bit.ly/raj_explain and the Youtube 🎥 <a href="https://youtu.be/j6WbCS0GLuY">video</a> for a deeper treatment.</p>
<p>Explanations are useful for explaining predictions. In the case of text, they highlight how the text influenced the prediction. They are helpful for 🩺 diagnosing model issues, 👀 showing stakeholders understand how a model is working, and 🧑‍⚖️ meeting regulatory requirements. Here is an explanation 👇 using shap. For more on explanations, check out the <a href="https://youtu.be/SVfrxFdJNB4">explanations in machine learning video</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/transformers/shap.png" class="img-fluid figure-img"></p>
<figcaption>Screen Shot 2022-08-12 at 9.25.07 AM</figcaption>
</figure>
</div>
<p>Let’s review 3 packages you can use to get explanations. All of these work with transformers, provide visualizations, and only require a few lines of code.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/transformers/code.png" class="img-fluid figure-img"></p>
<figcaption>Red and Purple Real Estate Soft Gradients Twitter Ad (1)</figcaption>
</figure>
</div>
</section>
<section id="shap" class="level3">
<h3 class="anchored" data-anchor-id="shap">Shap</h3>
<ol type="1">
<li><a href="https://github.com/slundberg/shap">SHAP</a> is a well-known, well-regarded, and robust package for explanations. In working with text, SHAP typically defers to using a Partition Shap explainer. This method makes the shap computation tractable by using hierarchical clustering and Owens values. The image here shows the clustering for a simple phrase. If you want to learn more about Shapley values, I have a <a href="https://youtu.be/DYA5SA0edb0">video on shapley values</a> and a deep dive on <a href="https://towardsdatascience.com/shaps-partition-explainer-for-language-models-ec2e7a6c1b77">Partition Shap explainer is here</a>.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/transformers/cluster.png" class="img-fluid figure-img"></p>
<figcaption>Screen Shot 2022-08-12 at 9.35.34 AM</figcaption>
</figure>
</div>
</section>
<section id="transformers-interpret" class="level3">
<h3 class="anchored" data-anchor-id="transformers-interpret">Transformers Interpret</h3>
<ol start="2" type="1">
<li><a href="https://github.com/cdpierse/transformers-interpret">Transformers Interpret</a> uses Integrated Gradients from <a href="https://captum.ai/">Captum</a> to calculate the explanations. This approach is 🐇 quicker than shap! Check out <a href="https://huggingface.co/spaces/rajistics/interpet_transformers">this space</a> to see a demo.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/transformers/ti.png" class="img-fluid figure-img"></p>
<figcaption>Screen Shot 2022-08-12 at 9.27.04 AM</figcaption>
</figure>
</div>
</section>
<section id="ferret" class="level3">
<h3 class="anchored" data-anchor-id="ferret">Ferret</h3>
<ol start="3" type="1">
<li><p><a href="https://github.com/g8a9/ferret">Ferret</a> is built for benchmarking interpretability techniques and includes multiple explanation methodologies (including Partition Shap and Integrated Gradients). A spaces <a href="https://huggingface.co/spaces/g8a9/ferret">demo for ferret is here</a> along with <a href="https://arxiv.org/abs/2208.01575">a paper</a> that explains the various metrics incorporated in ferret.</p>
<p>You can see below how explanations can differ when using different explanation methods. A great reminder that explanations for text are complicated and need to be appropriately caveated.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/transformers/ferret.png" class="img-fluid figure-img"></p>
<figcaption>Screen Shot 2022-08-11 at 1.19.05 PM</figcaption>
</figure>
</div>
<p>Ready to dive in? 🟢</p>
<p>For a longer walkthrough of all the 📦 packages with code snippets, web-based demos, and links to documentation/papers, check out:</p>
<p>👉 Colab notebook: https://bit.ly/raj_explain</p>
<p>🎥 https://youtu.be/j6WbCS0GLuY</p></li>
</ol>


</section>

 ]]></description>
  <category>MLOps</category>
  <category>NLP</category>
  <category>Huggingface</category>
  <category>Explainability</category>
  <guid>https://rajivshah.com/blog/explaining_transformers.html</guid>
  <pubDate>Sun, 14 Aug 2022 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/transformers/banner.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Dynamic Adversarial Data Collection</title>
  <link>https://rajivshah.com/blog/DADC.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/DADC.png" class="img-fluid figure-img"></p>
<figcaption>img</figcaption>
</figure>
</div>
<p>Are you looking for better training data for your models? Let me tell you about dynamic adversarial data collection!</p>
<p>I had a large enterprise customer asking me to incorporate this workflow into a <a href="https://www.linkedin.com/company/huggingface/">Hugging Face</a> private hub demo. Here are some resources I found useful: <a href="https://www.linkedin.com/in/ACoAADC2ZecBGpOHE1kqHIx4NINercY4WG0IkJs">Chris Emezue</a> put together a blog post: “<a href="https://huggingface.co/blog/mnist-adversarial">How to train your model dynamically using adversarial data</a>” and a real-life example using <a href="https://huggingface.co/spaces/chrisjay/mnist-adversarial">MNIST using Spaces</a>.</p>
<p>If you want an academic paper that details this process, check out: <a href="https://arxiv.org/abs/2110.08514">Analyzing Dynamic Adversarial Training Data in the Limit</a>. By using this approach, this paper found models made 26% fewer errors on the expert-curated test set.</p>
<p>And if you prefer a video — check out my Tik Tok:</p>
<p>https://www.tiktok.com/<span class="citation" data-cites="rajistics/video/7123667796453592366?is_from_webapp">@rajistics/video/7123667796453592366?is_from_webapp</span>=1&amp;sender_device=pc&amp;web_id=7106277315414181422</p>



 ]]></description>
  <category>Dataset</category>
  <category>Adversarial</category>
  <category>MNIST</category>
  <guid>https://rajivshah.com/blog/DADC.html</guid>
  <pubDate>Thu, 11 Aug 2022 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/DADC.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Stand Up for Best Practices</title>
  <link>https://rajivshah.com/blog/standup.html</link>
  <description><![CDATA[ 






<p><img src="https://cdn-images-1.medium.com/max/1600/1*jL9fT-oAR6Ki3HOvXpwMLQ.png" class="img-fluid" alt="img"> Source: Yuriy Guts selection from Shutterstock</p>
<section id="stand-up-for-best-practices" class="level3">
<h3 class="anchored" data-anchor-id="stand-up-for-best-practices"><strong>Stand Up for Best Practices:</strong></h3>
</section>
<section id="misuse-of-deep-learning-in-natures-earthquake-aftershock-paper" class="level3">
<h3 class="anchored" data-anchor-id="misuse-of-deep-learning-in-natures-earthquake-aftershock-paper"><strong>Misuse of Deep Learning in Nature’s Earthquake Aftershock Paper</strong></h3>
</section>
<section id="the-dangers-of-machine-learning-hype" class="level3">
<h3 class="anchored" data-anchor-id="the-dangers-of-machine-learning-hype">The Dangers of Machine Learning Hype</h3>
<p>Practitioners of AI, machine learning, predictive modeling, and data science have grown enormously over the last few years. What was once a niche field defined by its blend of knowledge is becoming a rapidly growing profession. As the excitement around AI continues to grow, the new wave of ML augmentation, automation, and GUI tools will lead to even more growth in the number of people trying to build predictive models.</p>
<p>But here’s the rub: While it becomes easier to use the tools of predictive modeling, predictive modeling knowledge is not yet a widespread commodity. Errors can be counterintuitive and subtle, and they can easily lead you to the wrong conclusions if you’re not careful.</p>
<p>I’m a data scientist who works with dozens of expert data science teams for a living. In my day job, I see these teams striving to build high-quality models. The best teams work together to review their models to detect problems. There are many hard-to-detect-ways that lead to problematic models (say, by allowing <a href="https://www.datarobot.com/wiki/target-leakage/">target leakage</a> into their training data).</p>
<p>Identifying issues is not fun. This requires admitting that exciting results are “too good to be true” or that their methods were not the right approach. In other words, <strong>it’s less about the sexy data science hype that gets headlines and more about a rigorous scientific discipline</strong>.</p>
</section>
<section id="bad-methods-create-bad-results" class="level3">
<h3 class="anchored" data-anchor-id="bad-methods-create-bad-results">Bad Methods Create Bad Results</h3>
<p>Almost a year ago, I read an article in Nature that claimed unprecedented accuracy in <a href="https://www.nature.com/articles/s41586-018-0438-y">predicting earthquake aftershocks by using deep learning</a>. Reading the article, my internal radar became deeply suspicious of their results. <strong>Their methods simply didn’t carry many of the hallmarks of careful predicting modeling.</strong></p>
<p>I started to dig deeper. In the meantime, this article blew up and became <a href="https://blog.google/technology/ai/forecasting-earthquake-aftershock-locations-ai-assisted-science/">widely recognized</a>! It was even included in the <a href="https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8">release notes for Tensorflow</a> as an example of what deep learning could do. However, in my digging, I found major flaws in the paper. Namely, data leakage which leads to unrealistic accuracy scores and a lack of attention to model selection (you don’t build a 6 layer neural network when a simpler model provides the same level of accuracy).</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*CPPVFzHd4GXlBSI4EILWZw.png" class="img-fluid" alt="img">The testing dataset had a much higher AUC than the training set . . . this is not normal</p>
<p>To my earlier point: these are subtle, <strong>but incredibly basic</strong> predictive modeling errors that can invalidate the entire results of an experiment. Data scientists are trained to recognize and avoid these issues in their work. I assumed that this was simply overlooked by the author, so I contacted her and let her know so that she could improve her analysis. Although we had previously communicated, she did not respond to my email over concerns with the paper.</p>
</section>
<section id="falling-on-deaf-ears" class="level3">
<h3 class="anchored" data-anchor-id="falling-on-deaf-ears">Falling On Deaf Ears</h3>
<p>So, what was I to do? My coworkers told me to just tweet it and let it go, but I wanted to stand up for good modeling practices. I thought reason and best practices would prevail, so I started a 6-month process of writing up my results and shared them with Nature.</p>
<p>Upon sharing my results, I received a note from Nature in January 2019 that despite serious concerns about data leakage and model selection that invalidate their experiment, they saw no need to correct the errors, because “<strong>Devries et al.&nbsp;are concerned primarily with using machine learning as [a] tool to extract insight into the natural world, and not with details of the algorithm design</strong>”. The authors provided a much harsher response.</p>
<p>You can read the entire exchange <a href="https://github.com/rajshah4/aftershocks_issues">on my github</a>.</p>
<p>It’s not enough to say that I was disappointed. This was a major paper (<em>it’s Nature!</em>) that bought into AI hype and published a paper despite it using flawed methods.</p>
<p>Then, just this week, I ran across <a href="https://link.springer.com/chapter/10.1007/978-3-030-20521-8_1">articles by Arnaud Mignan and Marco Broccardo</a> on <a href="https://arxiv.org/abs/1904.01983">shortcomings</a> that they found in the aftershocks article. Here are two more data scientists with expertise in earthquake analysis who also noticed flaws in the paper. I also have placed my analysis and reproducible code <a href="https://github.com/rajshah4/aftershocks_issues">on github</a>.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*Op19T2cR7gG60fbQLWS5cA.png" class="img-fluid" alt="img">Go run the analysis yourself and see the issue</p>
</section>
<section id="standing-up-for-predictive-modeling-methods" class="level3">
<h3 class="anchored" data-anchor-id="standing-up-for-predictive-modeling-methods">Standing Up For Predictive Modeling Methods</h3>
<p>I want to make it clear: my goal is not to villainize the authors of the aftershocks paper. I don’t believe that they were malicious, and I think that they would argue their goal was to just show how machine learning could be applied to aftershocks. Devries is an accomplished earthquake scientist who wanted to use the latest methods for her field of study and found exciting results from it.</p>
<p>But here’s the problem: their insights and results were based on fundamentally flawed methods. It’s not enough to say, “This isn’t a machine learning paper, it’s an earthquake paper.” <strong>If you use predictive modeling, then the quality of your results are determined by the quality of your modeling.</strong> Your work becomes data science work, and you are on the hook for your scientific rigor.</p>
<p>There is a huge appetite for papers that use the latest technologies and approaches. It becomes very difficult to push back on these papers.</p>
<p><strong>But if we allow papers or projects with fundamental issues to advance, it hurts all of us. It undermines the field of predictive modeling.</strong></p>
<p>Please push back on bad data science. Report bad findings to papers. And if they don’t take action, go to twitter, post about it, share your results and make noise. This type of collective action worked to raise awareness of p-values and combat the epidemic of p-hacking. We need good machine learning practices if we want our field to continue to grow and maintain credibility.</p>
<p><strong>Acknowledgments:</strong> I want to thank all the great data scientists at <a href="http://www.datarobot.com">DataRobot</a> that collaborated and supported me this past year, a few of these include: Lukas Innig, Amanda Schierz, Jett Oristaglio, Thomas Stearns, and Taylor Larkin.</p>
<p><strong>This article was orignally posted on <a href="https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8">Medium</a> and featured on <a href="https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/">Reddit</a></strong></p>


</section>

 ]]></description>
  <category>Leakage</category>
  <category>Earthquake</category>
  <guid>https://rajivshah.com/blog/standup.html</guid>
  <pubDate>Thu, 15 Aug 2019 05:00:00 GMT</pubDate>
  <media:content url="https://cdn-images-1.medium.com/max/1600/1*jL9fT-oAR6Ki3HOvXpwMLQ.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Optimization Strategies</title>
  <link>https://rajivshah.com/blog/optimization.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/fanduel2.jpg" class="img-fluid figure-img"></p>
<figcaption>fanduel</figcaption>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>As a data scientist, you spend a lot of your time helping to make better decisions. You build predictive models to provide improved insights. You might be predicting whether an image is a cat or dog, store sales for the next month, or the likelihood if a part will fail. In this post, I won’t help you with making better predictions, but instead how to make the <strong>best</strong> decision.</p>
<p>The post strives to give you some background on optimization. It starts with a simply toy example show you the math behind an optimization calculation. After that, this post tackles a more sophisticated optimization problem, trying to pick the best team for fantasy football. The FanDuel image below is a very common sort of game that is widely played (ask your inlaws). The optimization strategies in this post were shown to consistently win! Along the way, I will show a few code snippets and provide links to working code in R, Python, and Julia. And if you do win money, feel free to share it :)</p>
</section>
<section id="simple-optimization-example" class="level3">
<h3 class="anchored" data-anchor-id="simple-optimization-example">Simple Optimization Example</h3>
<p>A simple example, which I found <a href="http://melaniewingard.weebly.com/uploads/3/7/5/5/37554047/09-30-16_section_3.5_linear_programming_and_optimization_continued.pdf">online</a>, starts with a carpenter making bookcases in two sizes, large and small. It takes 6 hours to make a large bookcase and 2 hours to make a small one. The profit on a large bookcase is $50, and the profit on a small bookcase is $20. The carpenter can spend only 24 hours per week making bookcases and must make at least 2 of each size per week. Your job as a data scientist is to help your carpenter maximize her revenue.</p>
<p>Your initial inclination could be that since the large bookcase is the most profitable, why not focus on them. In that case, you would profit (2*$20) + (3*$50) which is $190. That is a pretty good baseline, but not the best possible answer. It is time to get the algebra out and create equations that define the problem. First, we start with the constraints:</p>
<pre><code>x&gt;=2    ## large bookcases

y&gt;=2    ## small bookcases

6x + 2y &lt;= 24  (labor constraint)</code></pre>
<p>Our objective function which we are trying to maximize is:</p>
<pre><code>P = 50x + 20y</code></pre>
<p>If we do the algebra by hand, we can convert out constraints to <code>y &lt;= 12 - 3x</code>. Then we graph all the constraints and find the feasible area for the portion of making small and large bookcases:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/graph.png" class="img-fluid figure-img"></p>
<figcaption>graph</figcaption>
</figure>
</div>
<p>The next step is figuring out the optimal point. Using the corner-point principle of linear programming, the maximum and minimum values of the objective function each occur at one of the vertices of the feasible region. Looking here, the maximum values (2,6) is when we make 2 large bookcases and 6 small bookcases, which results in an income of $220.</p>
<p>This is a very simple toy problem, typically there are many more constraints and the objective functions can get complicated. There are lots of classic problems in optimization such as routing algorithms to find the best path, scheduling algorithms to optimize staffing, or trying to find the best way to allocate a group of people to set of tasks. As a data scientist, you need to dissect what you are trying to maximize and identify the constraints in the form of equations. Once you can do this, we can hand this over to a computer to solve. So lets next walk through a bit more complicated example.</p>
</section>
<section id="fantasy-football" class="level3">
<h3 class="anchored" data-anchor-id="fantasy-football">Fantasy Football</h3>
<p>Over the last few years, fantasy sports have increasingly grown in popularity. One game is to pick a set of football players to make the best possible team. Each football player has a price and there is a salary cap limit. The challenge is to optimize your team to produce the highest total points while staying within a salary cap limit. This type of optimization problem is known as the knapsack problem or an assignment problem.</p>
</section>
<section id="simple-linear-optimization" class="level3">
<h3 class="anchored" data-anchor-id="simple-linear-optimization">Simple Linear Optimization</h3>
<p>So for this problem, let’s start by loading a dataset and taking a look at the raw data. You need to know both the salary as well as the expected points. Most football fans spend a lot of time trying to predict how many points a player will score. If you want to build a model for predicting the expected performance of a player, take a look at Ben’s blog post.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/qbs.png" class="img-fluid figure-img"></p>
<figcaption>QB points</figcaption>
</figure>
</div>
<p>The goal here is to build the best possible team for a salary cap, let’s say $50,000. A team consists of a quarterback, running backs, wide receivers, tight ends, and a defense. We can use the <code>lpSolve</code> package in R to set up the problem. Here is a code snippet for setting up the constraints.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/constraints.png" class="img-fluid figure-img"></p>
<figcaption>constraints</figcaption>
</figure>
</div>
<p>If you parse through this, you can see we have set a minimum and maximum for QB of 1 player. However, for the RB, we have allowed a maximum of 3 and a minimum of 2. This is not unusual in fantasy football, be because there is a role called a flex player, which anyone can choose and they can either be a RB, WR, or TE. Now let’s look at the code for the objective:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/objective.png" class="img-fluid figure-img"></p>
<figcaption>objective</figcaption>
</figure>
</div>
<p>The code shows that we have set up the problem to maximize the objective of the most points and include our constraints. Once the code is run, it outputs an optimal team! I forked an existing repo and have made the R code and dataset are <a href="https://github.com/rajshah4/linear-optimization-fantasy-football">available here.</a> A more sophisticated <a href="https://github.com/mattbrondum/Fantasy-Football-Optimization">python</a> optimization repo is also available.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/finalteam.png" class="img-fluid figure-img"></p>
<figcaption>finalteam</figcaption>
</figure>
</div>
</section>
<section id="advanced-steps" class="level3">
<h3 class="anchored" data-anchor-id="advanced-steps">Advanced steps</h3>
<p>So far, we have built a very simple optimization to solve the problem. There are several other strategies to further improve the optimizer. First, the variance of our teams can be increased by using a strategy called <strong>stacking</strong>, where you make sure your QB and WR are on the same team. A simple optimization is a constraint for selecting a QB and WR from the same team. Another strategy is using an <strong>overlap</strong> constraint for selecting multiple lineups. An overlap constraint ensures a diversity of players and not the same set of players for each optimized team. This strategy is particularly effective when submitting multiple lineups. You can read more about these <a href="https://arxiv.org/pdf/1604.01455v2.pdf">strategies here</a> and run the code in Julia <a href="https://github.com/dscotthunter/Fantasy-Hockey-IP-Code">here</a>. An code snippet of the stacking constraint (this is for a hockey optimization):</p>
<p><img src="https://rajivshah.com/blog/images/goalie.png" class="img-fluid" alt="goalie">.</p>
<p>Last year, at Sloan sports conference, <a href="http://www.sloansportsconference.com/wp-content/uploads/2018/02/1001.pdf">Haugh and Sighal</a> , presented a paper with additional optimization constraints. They include what an <strong>opponents team</strong> is likely to look like. After all, there are some players that are much more popular. Using this knowledge, you can predict the likely teams that will oppose your team. The approach here used Dirichlet regressions for modeling players. The result was a much-improved optimizer that was capable of consistently winning!</p>
<p>I hope this post has shown you how optimization strategies can help you find the best possible solution.</p>
<p>​</p>


</section>

 ]]></description>
  <category>Optimization</category>
  <category>sport</category>
  <guid>https://rajivshah.com/blog/optimization.html</guid>
  <pubDate>Mon, 30 Jul 2018 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/fanduel2.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Using Unlabeled Data to Label Data</title>
  <link>https://rajivshah.com/blog/semi_sup.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg" class="img-fluid figure-img"></p>
<figcaption>https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg</figcaption>
</figure>
</div>
<p>Your boss hands you a pile of a 100,000 unlabeled images and asks you to categorize whether they are sandals, pants, boots, etc.</p>
<p>So now you have a massive set of unlabeled data and you need labels. <strong>What should you do?</strong></p>
<p>This problem is commonplace. Lots of companies are swimming with data, whether its transactional, IoT sensors, security logs, images, voice, or more, and its all unlabeled. With so little labeled data, it is a tedious and slow process for data scientists to build machine learning models in <del>most</del> all enterprises.</p>
<p>Take Google’s street view data. <a href="https://ai.stanford.edu/~tgebru/papers/pnas.pdf">Gebru had to figure out how to label cars</a> in 50 million images with very little labeled data. Over at Facebook, they used algorithms <a href="https://arxiv.org/abs/1712.09374">to label half a million videos</a>, a task that would have otherwise taken 16 years.</p>
<p>This post shows you how to <strong>label hundreds of thousands of images in an afternoon</strong>. You can use the same approach whether you are labeling images or labeling traditional tabular data (e.g, identifying cyber security atacks or potential part failures.)</p>
<section id="the-manual-method" class="level3">
<h3 class="anchored" data-anchor-id="the-manual-method">The Manual Method</h3>
<p>For most data scientists when asked to do something, the first step is to calculate who else should do this.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="http://vni.s3.amazonaws.com/130905163633604.jpg" class="img-fluid figure-img"></p>
<figcaption>http://vni.s3.amazonaws.com/130905163633604.jpg</figcaption>
</figure>
</div>
<p>But 100,000 images could cost you at least $30,000 on <a href="https://cloudacademy.com/blog/machine-learning-datasets-mechanical-turk/">Mechanical Turk</a> or some other competitor. Your boss expects this done cheaply, since after all, they hired you because you use free software. Now, she doesn’t budget for anything other than your salary (if you don’t believe me, ask to go to <a href="https://pydata.org/">pydata</a>).</p>
<p>You take a deep breath and figure you can probably label 200 images in an hour. So that means in three weeks of non stop work, you can get this done!! Yikes!</p>
</section>
<section id="just-build-a-model" class="level3">
<h3 class="anchored" data-anchor-id="just-build-a-model">Just Build a Model</h3>
<p>The first idea is to label a handful of the images, train a machine learning algorithm, and then predict the remaining set of labels. For this exercise, I am using the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> dataset (you could also make your own <a href="https://rajivshah.com/blog/blog/2017/07/14/QuickDraw/">using quickdraw</a>). There are ten classes of images to identify and here is a sample of what they look like:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://pbs.twimg.com/media/DJNuE7BWAAAo3eC.jpg" class="img-fluid figure-img"></p>
<figcaption>https://pbs.twimg.com/media/DJNuE7BWAAAo3eC.jpg</figcaption>
</figure>
</div>
<p>I like this dataset, because each image is 28 by 28 pixels, which means it contains 784 unique features/variables. For a blog post this works great, but its also not like any datasets you see in the real world, which are often either much narrower (traditional tabluar business problem datasets) or much wider (real images are much bigger and include color).</p>
<p>I built models using the most common data science algorithms: logistic regression, support vector machines (SVM), random forest and gradient boosted machines (GBM).</p>
<p>I evaluated the performance based on labeling 100, 200, 500, 1000, and 2000 images.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/All.png" class="img-fluid figure-img"></p>
<figcaption>All</figcaption>
</figure>
</div>
<p>At this point in the post, if you are still with me, slow down and mull this graph over. There is a lot of good stuff here. <strong>Which algorithm does the best?</strong> (If you a data scientist, you shouldn’t fall for that question.) It really depends on the context.</p>
<p>You want something quick and dependable out of the box, you could go for the logistic regression. While the random forest starts way ahead, the SVM is coming on fast. If we had more labeled data the SVM would pass the random forest. And the GBM works great, but can take a bit of work to perform their best. The scores here are using out of the box implementations in R (e1071, randomForest, gbm, nnet).</p>
<p>If our benchmark is 80% accuracy for ten classes of images, we could get there by building a Random Forest model with 1000 images. But 1000 images is still a lot of data to label, 5 hours by my estimate. Lets think about ways we can improve.</p>
</section>
<section id="lets-think-about-data" class="level3">
<h3 class="anchored" data-anchor-id="lets-think-about-data">Let’s Think About Data</h3>
<p>After a little reflection, you remember what you often tell others — <strong>that data isn’t random, but has patterns</strong>. By taking advantage of these patterns we can get insight in our data.</p>
<p>Lets start with an autoencoder (AE). An autoencoder squeezes and compresses your data, kind of like turning soup into a bouillon cube. Autoencoders are the hipster’s Principle Component Analysis (PCA) , since they support nonlinear transformations.</p>
<p>Effectively this means we are taking our wide data (784 features/variables) reducing it down to 128 features. We then take this new compressed data and train our machine learning algorithm (SVM in this case). The graph below shows the difference in performance between an SVM fed with an autoencoder (AE_SVM) versus the SVM on the raw data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/AE.png" class="img-fluid figure-img"></p>
<figcaption>AE</figcaption>
</figure>
</div>
<p>By squeezing the information down to 128 features, we were able to actually improve the performance of the SVM algorithm at the low end. At the 100 labels mark, accuracy went from 44% to 59%. At the 1000 labels mark, the autoencoder was still helping, we see an improvement from 74% to 78%. So we are on to something here. We just need think a bit more about the distribution and patterns in our data that we can take advantage of.</p>
</section>
<section id="thinking-deeper-about-your-data" class="level3">
<h3 class="anchored" data-anchor-id="thinking-deeper-about-your-data">Thinking Deeper About Your Data</h3>
<p>We know that our data are images and since 2012, the <strong>hammer for images is a convolutional neural network (CNN)</strong>. There are a couple of ways we could use a CNN, from a pretrained network or as a simple model to pre-process the images. For this post, I am going to use a Convolutional Variational Autoencoder as a path towards the technique by <a href="https://arxiv.org/abs/1406.5298">Kingma for semi-supervised learning</a>.</p>
<p>So lets build a Convolutional Variational Autoencoder (CVAE). The leap here is twofold. First, “variational” means the autoenconder compress the information down into a probability distribution. Second is the addition of using a convolutional neural networks as an encoder. This is a bit of deep learning, but the emphasis here is on how we are solving the problem, not the latest shiny toy.</p>
<p>For coding my CVAE, I used the example CVAE from the list of <a href="https://keras.rstudio.com/articles/examples/variational_autoencoder_deconv.html">examples over at RStudio’s Keras page</a>. Like the previous autoencoder, we design the latent space to reduce the data to 128 features. We then use this new data to train an SVM model. Below is a plot of the performance of the CVAE as compared to the SVM and RandomForest on the raw data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/CVAE.png" class="img-fluid figure-img"></p>
<figcaption>CVAE</figcaption>
</figure>
</div>
<p>Wow! The new model is much more accurate. We can <strong>get well past 80% accuracy with just 500 labels</strong>. By using these techniques we get better performance and require less labelled images! At the top end, we can also do much better than the RandomForest or SVM model.</p>
</section>
<section id="next-steps" class="level3">
<h3 class="anchored" data-anchor-id="next-steps">Next Steps</h3>
<p>By using some very simple semi-supervised techniques with autoencoders, its possible to quickly and accurately label data. But the takeaway is not to use deep learning auto encoders! Instead, I hope you understand the methodology here of starting very simple and then trying gradually more complex solutions. Don’t fall for the latest shiny toy — pratical data science is not about using the latest approaches found in arxiv.</p>
<p>If this idea of semi-supervised learning inspires you, this post is the logistic regression of semi-supervised learning. If you want to dig further into Semi-Supervised Learning and Domain Adaptation, check out Brian Keng’s <a href="http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/">great walkthrough of using variational autoencoders</a> (which goes beyond what we have done here) or the work of <a href="https://thecuriousaicompany.com/">Curious AI</a>, which has been advancing semi-supervised learning using deep learning and <a href="https://github.com/CuriousAI">sharing their code</a>. But at the very least, don’t reflexively think all your data has to be hand labeled.</p>


</section>

 ]]></description>
  <category>Semi-supervised</category>
  <category>Label</category>
  <guid>https://rajivshah.com/blog/semi_sup.html</guid>
  <pubDate>Tue, 16 Jan 2018 06:00:00 GMT</pubDate>
  <media:content url="https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Using Google’s Quickdraw to create an MNIST style dataset!</title>
  <link>https://rajivshah.com/blog/QuickDraw.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.tensorflow.org/images/MNIST.png" class="img-fluid figure-img"></p>
<figcaption>https://www.tensorflow.org/images/MNIST.png</figcaption>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>For those running deep learning models, MNIST is ubiquotuous. This dataset of handwritten digits serves many purposes from benchmarking numerous algorithms (its referenced in thousands of papers) and as a visualization, its even more prevelant than Napoleon’s 1812 March. The digits look like this:</p>
<p>There are many reasons for its enduring use, but much of it is the lack of an alternative. In this post, I want to introduce an alternative, the Google QuickDraw dataset. The quickdraw dataset was captured in 2017 by Google’s drawing game, <a href="https://quickdraw.withgoogle.com/">Quick, Draw!</a>. The dataset consists of 50 million drawings across 345 categories. The drawings look like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/preview.jpg" class="img-fluid figure-img"></p>
<figcaption>https://github.com/googlecreativelab/quickdraw-dataset/blob/master/preview.jpg</figcaption>
</figure>
</div>
</section>
<section id="build-your-own-quickdraw-dataset" class="level3">
<h3 class="anchored" data-anchor-id="build-your-own-quickdraw-dataset">Build your own Quickdraw dataset</h3>
<p>I want to walk through how you can use this drawings and create your own MNIST like dataset. Google has made available 28x28 grayscale bitmap files of each drawing. These can serve as drop in replacements for the MNIST 28x28 grayscale bitmap images.</p>
<p>As a starting point, Google has graciously made the dataset publicly available with <a href="https://github.com/googlecreativelab/quickdraw-dataset">documentation on the dataset</a>. All the data is sitting in Google’s <a href="https://console.cloud.google.com/storage/browser/quickdraw_dataset/?pli=1">Cloud Console</a>, but for the images, you want this link of the <a href="https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap/?pli=1">numpy_bitmaps</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/GCP2.png" class="img-fluid figure-img"></p>
<figcaption>GCP2</figcaption>
</figure>
</div>
<p>You should arrive on a page that allows you to download all the images for any category. So this is when you have fun! Go ahead and pick your own categories. I started with eyeglasses, face, pencil, and television. As I learned from the face, the drawings that have fine points can be more difficult to learn. But you should play around and pick fun categories.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/shortqd.png" class="img-fluid figure-img"></p>
<figcaption>shortqd</figcaption>
</figure>
</div>
<p>The next challenge is taking these .npy files and using them. Here is a short <a href="https://gist.github.com/rajshah4/903d086adb4e5075415381e1f6038a88">python gist</a> that I used to read the .npy files and combine them to create a 80,000 images dataset that I could use in place of MNIST. They are saved in a hdf5 format that is cross platform and often used in deep learning.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/gist.png" class="img-fluid figure-img"></p>
<figcaption>gist</figcaption>
</figure>
</div>
</section>
<section id="using-quickdraw-instead-of-mnist" class="level3">
<h3 class="anchored" data-anchor-id="using-quickdraw-instead-of-mnist">Using Quickdraw instead of MNIST</h3>
<p>The next thing is to go have fun with it. I used this dataset in place of MNIST for some work playing around with autoencoders in Python from the <a href="https://blog.keras.io/building-autoencoders-in-keras.html">Keras tutorials</a>. The below picture represents the original images at the top and reconstructed ones at the bottom, using an autoencoder.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/vae10.png" class="img-fluid figure-img"></p>
<figcaption>vae10</figcaption>
</figure>
</div>
<p>I next used this dataset with a <a href="https://rstudio.github.io/keras/articles/examples/variational_autoencoder.html">variational autoencoder in R</a>. Here is the code snippet to import the data:</p>
<pre><code>library(rhdf5)
x_test &lt;- t(h5read("x_test.h5", "name-of-dataset"))
x_train &lt;- t(h5read("x_train.h5", "name-of-dataset"))
y_test &lt;- (h5read("y_test.h5", "name-of-dataset"))
y_train &lt;- (h5read("y_train.h5", "name-of-dataset"))</code></pre>
<p>Here is a visualization of its latent space using my custom quickdraw dataset. For me, this was a nice fresh alternative to always staring at the MNIST dataset. So next time you see MNIST listed . . . go build your own!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/VAEgw.gif" class="img-fluid figure-img"></p>
<figcaption>VAE15</figcaption>
</figure>
</div>


</section>

 ]]></description>
  <category>Quickdraw</category>
  <category>MNIST</category>
  <category>Dataset</category>
  <guid>https://rajivshah.com/blog/QuickDraw.html</guid>
  <pubDate>Fri, 14 Jul 2017 05:00:00 GMT</pubDate>
  <media:content url="https://www.tensorflow.org/images/MNIST.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Deep Learning with R</title>
  <link>https://rajivshah.com/blog/deeplearningR.html</link>
  <description><![CDATA[ 






<p>For R users, there hasn’t been a production grade solution for deep learning (sorry <a href="http://mxnet.io/api/r/index.html">MXNET</a>). This post introduces the Keras interface for R and how it can be used to perform image classification. The post ends by providing some code snippets that show Keras is intuitive and powerful 💪🏽.</p>
<section id="tensorflow" class="level3">
<h3 class="anchored" data-anchor-id="tensorflow">Tensorflow</h3>
<p>Last January, <a href="https://github.com/rstudio/tensorflow">Tensorflow for R</a> was released, which provided access to the Tensorflow API from R. This was signficant, as Tensorflow is the most popular library for deep learning. However, for most R users, the Tensorflow for R interface was not very R like. 🤢 Take a look at this code chunk for training a model:</p>
<pre><code>cross_entropy &lt;- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))
train_step &lt;- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)
correct_prediction &lt;- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))
accuracy &lt;- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
sess$run(tf$global_variables_initializer())

for (i in 1:20000) {
  batch &lt;- mnist$train$next_batch(50L)
  if (i %% 100 == 0) {
    train_accuracy &lt;- accuracy$eval(feed_dict = dict(
        x = batch[[1]], y_ = batch[[2]], keep_prob = 1.0))
    cat(sprintf("step %d, training accuracy %g\n", i, train_accuracy))
  }
  train_step$run(feed_dict = dict(
    x = batch[[1]], y_ = batch[[2]], keep_prob = 0.5))
}

test_accuracy &lt;- accuracy$eval(feed_dict = dict(
     x = mnist$test$images, y_ = mnist$test$labels, keep_prob = 1.0))
cat(sprintf("test accuracy %g", test_accuracy))</code></pre>
<p>Yikes!</p>
<p>Unless you are familiar with tensorflow, it’s not readily apparent what is going on. A quick search on Github finds less than a 100 code results using tensorflow for R. 😔</p>
</section>
<section id="keras" class="level3">
<h3 class="anchored" data-anchor-id="keras">Keras</h3>
<p>All this is going to change with Keras and R! ☺️</p>
<p>For background, <a href="https://keras.io/index.html">Keras</a> is a high-level neural network API that is designed for experimentation and can run on top of Tensorflow. Keras is what data scientists like to use. 🤓 Keras has grown in popularity and supported on a wide set of platforms including Tensorflow, CNTK, Apple’s CoreML, and Theano. It is becoming the de factor language for deep learning.</p>
<p>As a simple example, here is the code to train a model in Keras:</p>
<pre><code>model_top %&gt;% fit(
        x = train_x, y = train_y,
        epochs=epochs, 
        batch_size=batch_size,
        validation_data=valid)</code></pre>
</section>
<section id="image-classification-with-keras" class="level3">
<h3 class="anchored" data-anchor-id="image-classification-with-keras">Image Classification with Keras</h3>
<p>So if you are still with me, let me show you how to build deep learning models using R, Keras, and Tensorflow together. You will find a Github repo at <a href="https://github.com/rajshah4/image_keras/">https://github.com/rajshah4/image_keras/</a> that contains the code and data you will need. Included is an <a href="http://htmlpreview.github.io/?https://github.com/rajshah4/image_keras/blob/master/Rnotebook.nb.html">R notebook</a> (and Python notebooks) that walks through building an image classifier (telling 🐱 from 🐶), but can easily be generalized to other images. The walk through includes advanced methods that are commonly used for production deep learning work including:</p>
<ul>
<li>augmenting data</li>
<li>using the bottleneck features of a pre-trained network</li>
<li>fine-tuning the top layers of a pre-trained network</li>
<li>saving weights of models</li>
</ul>
</section>
<section id="code-snippets-of-keras" class="level3">
<h3 class="anchored" data-anchor-id="code-snippets-of-keras">Code Snippets of Keras</h3>
<p>The R interface to Keras truly makes it easy to build deep learning models in R. Here are some code snippets based on my example of building an image classifier to illustrate how intuitive and useful Keras for R is:</p>
<p>To load 🖼 from a folder:</p>
<pre><code>train_generator &lt;- flow_images_from_directory(train_directory, generator = image_data_generator(), target_size = c(img_width, img_height), color_mode = "rgb",
  class_mode = "binary", batch_size = batch_size, shuffle = TRUE,
  seed = 123)</code></pre>
<p>To define a simple convolutional neural network:</p>
<pre><code>model &lt;- keras_model_sequential()

model %&gt;%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), input_shape = c(img_width, img_height, 3)) %&gt;%
  layer_activation("relu") %&gt;%
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% 
  
  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %&gt;%
  layer_activation("relu") %&gt;%
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%
  
  layer_conv_2d(filter = 64, kernel_size = c(3,3)) %&gt;%
  layer_activation("relu") %&gt;%
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%
  
  layer_flatten() %&gt;%
  layer_dense(64) %&gt;%
  layer_activation("relu") %&gt;%
  layer_dropout(0.5) %&gt;%
  layer_dense(1) %&gt;%
  layer_activation("sigmoid")</code></pre>
<p>To augment data:</p>
<pre><code>augment &lt;- image_data_generator(rescale=1./255,
                               shear_range=0.2,
                               zoom_range=0.2,
                               horizontal_flip=TRUE)</code></pre>
<p>To load a pretrained network:</p>
<pre><code>model_vgg &lt;- application_vgg16(include_top = FALSE, weights = "imagenet")</code></pre>
<p>To save model weights:</p>
<pre><code>save_model_weights_hdf5(model_ft, 'finetuning_30epochs_vggR.h5', overwrite = TRUE)</code></pre>
<p>The Keras for R interface makes it much easier for R users to build and refine deep learning models. Its no longer necessary to force everyone to use Python to build, refine, and test deep learning models. I really think this will open up deep learning to a wider audience that was a bit apprehensive on using python.</p>
<p>To start with, you can grab my <a href="https://github.com/rajshah4/image_keras/">repo</a>, fire up RStudio (or your IDE of choice), and go build a simple classifier using Keras. There are also a wealth of <a href="https://rstudio.github.io/keras/articles/examples/index.html">other examples</a> such as <a href="https://rstudio.github.io/keras/articles/examples/lstm_text_generation.html">generating text</a> from Nietzsche’s writings, <a href="https://rstudio.github.io/keras/articles/examples/deep_dream.html">deep dreaming</a>, or creating a <a href="https://rstudio.github.io/keras/articles/examples/variational_autoencoder.html">variational encoder</a>.</p>
<p>So for now, give it a spin!</p>
<p>An earlier version of this post was posted at <a href="https://datascienceplus.com/deep-learning-with-r/">Datascience+</a>.</p>


</section>

 ]]></description>
  <category>Tensorflow</category>
  <category>R</category>
  <category>Keras</category>
  <guid>https://rajivshah.com/blog/deeplearningR.html</guid>
  <pubDate>Sun, 04 Jun 2017 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Building Worlds for Reinforcement Learning</title>
  <link>https://rajivshah.com/blog/openai_mod.html</link>
  <description><![CDATA[ 






<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p><a href="https://gym.openai.com/">OpenAI’s Gym</a> places reinforcement learning into the masses. It comes with a wealth of environments from the classic cart pole, board games, Atari, and now the new <a href="https://universe.openai.com/">Universe</a> which adds flash games and PC Games like GTA or Portal. This is great news, but for someone starting out, working on some of these games is overkill. You can learn a lot more in a shorter time, by playing around with some smaller toy environments.</p>
<p>One area I like within the gym environments are the classic control problems (besides the fun of <a href="http://tinyurl.com/eatmelon">eating melon and poop</a>). These are great problems for understanding the basics of reinforcement learning because we intutiively understand the rewards and they run really fast. Its not like pong that can take several days to train, instead, you can train these environments within minutes!</p>
<p>If you aren’t happy with the current environments, it is possible to modify and even add more environments. In this post, I will highlight other environments and share how I modified an Acrobot-v1 environment.</p>
</section>
<section id="rlpy" class="level3">
<h3 class="anchored" data-anchor-id="rlpy">RLPy</h3>
<p>To begin, grab the <a href="https://github.com/openai/gym">repo for the OpenAI gym</a>. Inside the repo, navigate to <code>gym/envs/classic_control</code>where you will see the scripts that define the class control environments. If you open one of the scripts, you will see a heading on the top that says:</p>
<p><code>__copyright__ = "Copyright 2013, RLPy http://acl.mit.edu/RLPy"</code></p>
<p>Ahh! In the spirit of open source, OpenAI stands on the shoulders of another reinforcement library, <a href="http://rlpy.readthedocs.io/en/latest/">RLPy</a>. You can learn a lot more about them at the RLPy site or take a look at their <a href="https://github.com/rlpy/rlpy">github</a>. If you browse here, you can find the original script that was used in OpenAI under <code>rlpy /rlpy/Domains</code>. The interesting thing here is that there are a ton more interesting reinforcement problems!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/RLlisting.png" class="img-fluid figure-img"></p>
<figcaption>RLlisting</figcaption>
</figure>
</div>
<p>You can run these using RLPy or you can try and hack this into OpenAI.</p>
</section>
<section id="modifying-openai-environments" class="level3">
<h3 class="anchored" data-anchor-id="modifying-openai-environments">Modifying OpenAI Environments</h3>
<p>I decided to modify the <a href="https://gym.openai.com/envs/Acrobot-v1">Acrobot</a> environment. Acrobot is a 2-link pendulum with only the second joint actuated (it has three states, left, right, and no movement). The goal is to swing the end to a height of at least one link above the base. If you look at the leaderboard on OpenAIs site, they meet that criterion, but its not very impressive. Here is the current <a href="https://gym.openai.com/evaluations/eval_Ig1wrPzQlGipmBAhZ5Tw">highest scoring entry</a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/training.gif" class="img-fluid figure-img"></p>
<figcaption>training</figcaption>
</figure>
</div>
<p>This is way boring compared to what Hardmaru shows in his <a href="http://otoro.net/ml/pendulum-cne/">demo</a>, where a pendulum is capable of balancing for a short time.<img src="https://rajivshah.com/blog/images/hardmaru.gif" class="img-fluid" alt="hardmaru"></p>
<p>So I decided to try and modify the Acrobot demo to make this task a little more interesting, <a href="https://gist.github.com/rajshah4/677698166b860bc11d34507ee18b2d41">Acrobot gist here</a>. The main change was to the reward system. I added a variable <code>steps_beyond_done</code> that would keep track of successes when the end was swung high. I also changed the reward structure, so it would gradually be rewarded as it swung higher. I also changed g to 0, this removes gravity’s effect.</p>
<pre><code>self.rewardx = (-np.cos(s[0]) - np.cos(s[1] + s[0])) ##Swung height is calculated 
if self.rewardx &lt; .5:
    reward = -1.
    self.steps_beyond_done = 0
if (self.rewardx &gt; .5 and self.rewardx &lt; .8):
    reward = -0.8
    self.steps_beyond_done = 0  
if self.rewardx &gt; .8:
    reward = -0.6 
if self.rewardx &gt; 1:
    reward = -0.4
    self.steps_beyond_done += 1 
if self.steps_beyond_done &gt; 4:
    reward = -0.2
if self.steps_beyond_done &gt; 8:
    reward = -0.1
if self.steps_beyond_done &gt; 12:
    reward = 0.</code></pre>
<p>Another important file to be aware of is where the benchmarks are kept for each environment. You can navigate to this at <code>gym/gym/benchmarks/__init__.py</code>Within this file, you will see the following:</p>
<pre><code>{'env_id': 'Acrobot-v1',
         'trials': 3,
         'max_timesteps': 100000,
         'reward_floor': -500.0,
         'reward_ceiling': 0.0,
        },</code></pre>
<p>I then ran an <a href="https://github.com/arnomoonens/DeepRL">implementation of Asynchronous Advantage Actor Critic A3C)</a> by Arno Moonens. After running for a half hour, you can see the improvement in the algorithm:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/training1.gif" class="img-fluid figure-img"></p>
<figcaption>training1</figcaption>
</figure>
</div>
<p>Now a half hour later:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/training2.gif" class="img-fluid figure-img"></p>
<figcaption>training2</figcaption>
</figure>
</div>
<p>The result is teaching the pendulum to stay up for an extended time! This is much more interesting and what I was looking for. I hope this will inspire others to build new and interesting environments.</p>


</section>

 ]]></description>
  <category>OpenAI</category>
  <category>RL</category>
  <guid>https://rajivshah.com/blog/openai_mod.html</guid>
  <pubDate>Tue, 24 Jan 2017 06:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/training.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Taking an H2O Model to Production</title>
  <link>https://rajivshah.com/blog/H2O_prod.html</link>
  <description><![CDATA[ 






<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>One of the best feelings as a Data Scientist is when the model you have poured your heart and soul into, moves into production. Your model is now <em>grown-up</em> and you get to watch it mature.</p>
<p>This post shows how to take a H2O model and move it into a production environment. In this post, we will develop a simple H2O based predictive model, convert it into a Plain Old Java Object (POJO), compile it along with other Java packages, and package the compiled class files into a deployable JAR file so that it can readily be deployed onto any Java based application servers. This model will accept the input data set in the form of CSV file and return the predicted output in CSV format.</p>
<p>H2O is one of my favorite tools for building models because it is well designed from an algorithm perspective, easy to use, and can scale to larger datasets. However, <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html">H2O’s documentation</a>, though voluminous, doesn’t have clear instructions for moving a POJO model into production. This post will discuss this approach in greater detail besides providing code for how to do this. (H2O does have a post on doing <a href="https://github.com/h2oai/h2o-training/tree/master/tutorials/streaming/storm/README.md">real time predictions with storm</a>). Special thanks to <a href="https://www.linkedin.com/in/socrates-krishnamurthy-420b892">Socrates Krishnamurthy</a> who co-wrote this post with me.</p>
</section>
<section id="building-h2o-model" class="level3">
<h3 class="anchored" data-anchor-id="building-h2o-model">Building H2O Model</h3>
<p>As a starting point, lets use our favorite ice cream dataset to create a toy model in H2O:</p>
<pre><code>  library(h2o)  
  library(Ecdat)  
  data(Icecream)  
  h2o.init()  
  train.h2o &lt;- as.h2o(Icecream)  
  rf &lt;- h2o.randomForest(x=2:4, y=1, ntrees=2, training_frame=train.h2o)   </code></pre>
<p>Once you have developed your model in H2O, then the next step is downloading the POJO:</p>
<pre><code>h2o.download_pojo(rf, getjar=TRUE, path="~/Code/h2o-3.9.1.3459/test/")
# you must give a path to download a file</code></pre>
<p>This will save two files, a H2O jar file about the model and an actual model file (that begins with <code>DRF</code> and ends with <code>.java</code>). Go ahead and open the model file in a text editor if you want to have a look at it.</p>
</section>
<section id="compiling-the-h2o-model" class="level3">
<h3 class="anchored" data-anchor-id="compiling-the-h2o-model">Compiling the H2O Model</h3>
<p>The next step is to compile and run the model (say, the downloaded model name is <code>DRF_model_R_1470416938086_15.java</code>), then type:</p>
<pre><code>&gt; javac -cp h2o-genmodel.jar -J-Xmx2g DRF_model_R_1470416938086_15.java  </code></pre>
<p>This creates a bunch of java class files.</p>
</section>
<section id="scoring-the-input-data" class="level3">
<h3 class="anchored" data-anchor-id="scoring-the-input-data">Scoring the Input Data</h3>
<p>The final step is scoring some input data. Prior to running the model, it is necessary to have files created for the input and output. For the input, the default setting is to read the first row as a header. The assumption is that the csv is well formed (this approach is not using the H2O parser). Once that is done, run:</p>
<pre><code>&gt; java -cp .:h2o-genmodel.jar hex.genmodel.tools.PredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv</code></pre>
<p>If you open the <code>output.csv</code> file, it can be noticed that the predicted values are in Hexadecimal and not in Numeric format. For example, the output will be something like this:</p>
<pre><code>0x1.a24dd2p-2</code></pre>
</section>
<section id="fixing-the-hexadecimal-issue" class="level3">
<h3 class="anchored" data-anchor-id="fixing-the-hexadecimal-issue">Fixing the Hexadecimal Issue</h3>
<p>The model is now predicting, but the predictions are in the wrong format. Yikes! To fix this issue requires some hacking of the java code. The rest of this post will show you how to hack the java code in PredictCsv, which can fix this issue and other unexpected issues with PredictCsv (for example, if your input comes tab separated).</p>
<p>If we take a deeper look at the PredictCsv java file located in the <a href="https://github.com/h2oai/h2o-3/blob/master/h2o-genmodel/src/main/java/hex/genmodel/tools/PredictCsv.java">h2o github</a>, the <code>myDoubleToString</code> method returns Hexadecimal string. But the challenge is this method being <code>static</code> in nature, cannot be overridden in a subclass or cannot be updated directly since it was provided by H2O jar file, to return regular numeric value in String format.</p>
<p>This can be fixed by creating a new java file (say, <code>NewPredictCsv.java</code>) by copying the entire content of <code>PredictCsv.java</code> from the above location and saving it locally. You then need to:</p>
<ul>
<li>comment out the first line, so it should be <code>//package hex.genmodel.tools;</code><br>
</li>
<li>change the name of the class name (~line 20) to read: <code>public class NewPredictCsv {</code><br>
</li>
<li>correct the hexadecimal issue by changing the return statement of <code>myDoubleToString</code> method to <code>.toString()</code> in lieu of <code>.toHexString()</code> (~line 131).</li>
</ul>
<p>After creating <code>NewPredictCsv.java</code>, compile it using the following command:</p>
<pre><code>&gt; javac -cp h2o-genmodel.jar -J-Xmx2g NewPredictCsv.java DRF_model_R_1470416938086_15.java</code></pre>
<p>Run the compiled file by providing input and output CSV files using the following command (Ensure that the <code>input.csv</code> file is in the current folder where you will run this):</p>
<pre><code>&gt; java -cp .:h2o-genmodel.jar NewPredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv</code></pre>
<p>If you open the <code>output.csv</code> file now, it will be in the proper numeric format as follows:</p>
<pre><code>0.40849998593330383</code></pre>
</section>
<section id="deploying-the-solution-into-production" class="level3">
<h3 class="anchored" data-anchor-id="deploying-the-solution-into-production">Deploying the Solution into Production:</h3>
<p>At this point, we have a workable flow for using our model to score new data. But we can clean up the code to make it a little friendlier for our data engineers. First, create a jar file out of the class files created in previous steps. To do that, issue the following command:</p>
<pre><code>&gt; jar cf my-RF-model.jar *.class</code></pre>
<p>This will place all the class files and our NewPredictCsv inside the jar. This is helpful when we have a model with say 500 trees. Now all we need is three files to run our scorer. So copy the above two jar files along with <code>input.csv</code> file in any folder/directory from where the program has to be executed. After copying, the folder should contain following files:</p>
<pre><code>&gt; my-RF-model.jar  
&gt; h2o-genmodel.jar  
&gt; input.csv  </code></pre>
<p>The above <code>input.csv</code> file contains the dataset for which the dependent variable has to be predicted. To compute/ predict the values, run the <code>java</code> command as below:</p>
<pre><code>&gt; java -cp .:my-RF-model.jar:h2o-genmodel.jar NewPredictCsv --header --model DRF_model_R_1470416938086_15 --input input.csv --output output.csv</code></pre>
</section>
<section id="note" class="level3">
<h3 class="anchored" data-anchor-id="note">Note:</h3>
<p>Replace <code>:</code> with <code>;</code> in above commands if you are working in Windows (yuck).</p>


</section>

 ]]></description>
  <category>H2O</category>
  <category>MLOps</category>
  <guid>https://rajivshah.com/blog/H2O_prod.html</guid>
  <pubDate>Mon, 22 Aug 2016 05:00:00 GMT</pubDate>
  <media:content url="https://www.imda.gov.sg/-/media/imda/images/content/for-industry/technology-and-innovation/innovative-tech-companies/accreditation/h2o.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Using xgbfi for revealing feature interactions</title>
  <link>https://rajivshah.com/blog/xgbfi.html</link>
  <description><![CDATA[ 






<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Tree based methods excel in using feature or variable interactions. As a tree is built, it picks up on the <a href="https://datamining.bus.utk.edu/Documents/Decision-Trees-for-Predictive-Modeling-(Neville).pdf">interaction of features</a>. For example, buying ice cream may not be affected by having extra money unless the weather is hot. It is the interaction of both of these features that can affect whether ice cream will be consumed.</p>
<p>The traditional manner for examining interactions is relying on measures of <a href="https://www.quora.com/How-do-I-find-variable-importance-in-random-forest/answer/Rajiv-Shah-6">variable importance</a>. However, these measures don’t provide insights into second or third order interactions. Identifying these interactions are important in building better models, especially when finding features to use within linear models.</p>
<p>In this post, I show how to find higher order interactions using <a href="https://github.com/Far0n/xgbfi">XGBoost Feature Interactions &amp; Importance</a>. This tool has been available for a while, but outside of <a href="https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/18754/feature-engineering/107518">kagglers</a>, it has received relatively little attention.</p>
<p>As a starting point, I used the Ice Cream dataset to illustrate using xgbfi. This walkthrough is in R, but python instructions are also available at the repo. I am going to break the code into three sections, the initial build of the model, exporting the files necessary for xgbfi, and running xgbi.</p>
<section id="building-the-model" class="level4">
<h4 class="anchored" data-anchor-id="building-the-model">Building the model</h4>
<p>Lets start by loading the data:</p>
<pre><code>library(xgboost)
library(Ecdat)
data(Icecream)
train.data &lt;- data.matrix(Icecream[,-1])</code></pre>
<p>The next step is running xgboost:</p>
<pre><code>bst &lt;- xgboost(data = train.data, label = Icecream$cons, max.depth = 3, eta = 1, nthread = 2, nround = 2, objective = "reg:linear")</code></pre>
<p>To better understand how the model is working, lets go ahead and look at the trees:</p>
<pre><code>xgb.plot.tree(feature_names = names((Icecream[,-1])), model = bst)</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/xgplot.png" class="img-fluid figure-img"></p>
<figcaption>xg tree plot</figcaption>
</figure>
</div>
<p>The results here line up with our intution. Hot days seems to be the biggest variable by just eyeing the plot. This lines up with the results of a variable importance calculation:</p>
<pre><code>&gt; xgb.importance(colnames(train.data, do.NULL = TRUE, prefix = "col"), model = bst)
   Feature       Gain      Cover Frequency
1:    temp 0.75047187 0.66896552 0.4444444
2:  income 0.18846270 0.27586207 0.4444444
3:   price 0.06106542 0.05517241 0.1111111</code></pre>
<p>All of this should be very familiar to anyone who has used decision trees for modeling. <strong>But what are the second order interactions? Third order interactions? Can you rank them?</strong></p>
</section>
<section id="exporting-the-tree" class="level4">
<h4 class="anchored" data-anchor-id="exporting-the-tree">Exporting the tree</h4>
<p>The next step involves saving the tree and moving it outside of R so xgbfi can parse the tree. The code below will help to create two files that are needed:<code>xgb.dump</code> and <code>fmap.text</code>.</p>
<pre><code>featureList &lt;- names(Icecream[,-1])
featureVector &lt;- c() 
for (i in 1:length(featureList)) { 
  featureVector[i] &lt;- paste(i-1, featureList[i], "q", sep="\t") 
}
write.table(featureVector, "fmap.txt", row.names=FALSE, quote = FALSE, col.names = FALSE)
xgb.dump(model = bst, fname = 'xgb.dump', fmap = "fmap.txt", with.stats = TRUE)</code></pre>
</section>
<section id="running-xgbfi" class="level4">
<h4 class="anchored" data-anchor-id="running-xgbfi">Running xgbfi</h4>
<p>The first step is to clone the <a href="https://github.com/Far0n/xgbfi">xgbfi repository</a> onto your computer. Then copy the files <code>xgb.dump</code> and <code>fmap.text</code> to the bin directory.</p>
<p>Go to your terminal or command line and run: <code>XgbFeatureInteractions.exe</code> application. On a mac, <a href="http://www.mono-project.com/download/">download mono</a> and then run the command: <code>mono XgbFeatureInteractions.exe</code>. There is also a <code>XgbFeatureInteractions.exe.config</code> file that contains configuration settings in the bin directory.</p>
<p>After the application runs, it will write out an excel spreadsheet titled: <code>XgbFeatureInteractions.xlsx</code>. This spreadsheet has the good stuff! Open up the spreadsheet and you should see:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/firstinteraction.png" class="img-fluid figure-img"></p>
<figcaption>interaction depth 0</figcaption>
</figure>
</div>
<p>This tab of the spreadsheet shows the first order interactions. These results are similar to what variable importance showed. The good stuff is when you click on the tab for Interaction Depth 1 or Interaction Depth 2.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/secondinteraction.png" class="img-fluid figure-img"></p>
<figcaption>interaction depth 1</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/thirdinteraction.png" class="img-fluid figure-img"></p>
<figcaption>interaction depth 2</figcaption>
</figure>
</div>
<p>It is now possible to rank the higher order interactions. With the simple dataset, you can see that the results out of xgbfi match what is happening in the tree. The real value of this tool is for much larger datasets, where its difficult to examine the trees for the interactions.</p>


</section>
</section>

 ]]></description>
  <category>Xgboost</category>
  <category>MLOps</category>
  <category>Explainability</category>
  <guid>https://rajivshah.com/blog/xgbfi.html</guid>
  <pubDate>Mon, 01 Aug 2016 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/firstinteraction.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Outlier App</title>
  <link>https://rajivshah.com/blog/outlier_app.html</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/algorithms.gif" class="img-fluid figure-img"></p>
<figcaption>algorithms</figcaption>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>I was recently trying various outlier detection algorithms. For me, the best way to understand an algorithm is to tinker with it. I wanted to share my recent work on a shiny app that allows you to play around with various outlier algorithms.</p>
<p>The <a href="https://rajivshah.com/blog/shiny/outlier/">shiny app is available on my site</a>, but even better, the <a href="https://github.com/rajshah4/outliers_shiny">code is on github</a> for you to run locally or improve! I also <a href="https://www.youtube.com/watch?v=1zPuRAgr1F4">posted a video</a> that provides background on the app. Let me give you a quick tour of the app:</p>
</section>
<section id="algorithms" class="level3">
<h3 class="anchored" data-anchor-id="algorithms">Algorithms</h3>
<p>The available algorithms include:</p>
<ul>
<li>Hierarchical Clustering (DMwR)</li>
<li>Kmeans (distance metrics from proxy)
<ul>
<li>Kmeans Euclidean Distance</li>
<li>Kmeans Mahalanobis</li>
<li>Kmeans Manhattan</li>
</ul></li>
<li>Fuzzy kmeans (all from fclust)
<ul>
<li>Fuzzy kmeans - Gustafson and Kessel</li>
<li>Fuzzy k-medoids</li>
<li>Fuzzy k-means with polynomial fuzzifier</li>
</ul></li>
<li>Local Outlier Factor (dbscan)</li>
<li>RandomForest (proximity from randomForest)
<ul>
<li>Isolation Forest (IsolationForest)</li>
</ul></li>
<li>Autoencoder (Autoencoder)</li>
<li>FBOD and SOD (HighDimOut)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/algorithms.gif" class="img-fluid figure-img"></p>
<figcaption>algorithms</figcaption>
</figure>
</div>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<p>There are also a wide range of datasets to try as well:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/datasets.png" class="img-fluid figure-img"></p>
<figcaption>datasets</figcaption>
</figure>
</div>
<p>Once the data is loaded, you can start exploring. One thing you can do is look at the effect scaling can have. In this example, you can see how outliers differ when scaling is used. The values on the far right no longer dominate the distance measurements, and there are now outliers from other areas:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/scaling.gif" class="img-fluid figure-img"></p>
<figcaption>scaling</figcaption>
</figure>
</div>
<p>By trying different algorithms, you can see how different algorithms will select outliers. In this case, you see a difference between the outliers selected using an autoencoder versus isolation forest.<img src="https://rajivshah.com/blog/images/auto_iso.gif" class="img-fluid" alt="auto_iso"></p>
<p>Another example here is the difference between kmeans and fuzzy kmeans as show below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/fuzzy.gif" class="img-fluid figure-img"></p>
<figcaption>fuzzy</figcaption>
</figure>
</div>
<p>A density based algorithm can also select different outliers versus a distance based algorithm. This example nicely shows the difference between kmeans and lof (local outlier factor from dbscan)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/density.gif" class="img-fluid figure-img"></p>
<figcaption>density</figcaption>
</figure>
</div>
<p>An important part of using this visualization is studying the distance numbers that are calculated. Are these numbers meshing with your intuition? How big of a quantitative difference is there between outliers and other points?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/outlier_table.png" class="img-fluid figure-img"></p>
<figcaption>outlier_table</figcaption>
</figure>
</div>
<p>So that is the 2D app. Please send me bug fixes, additional algorithms, or tighter code!</p>
<p><strong>3D+ App?</strong></p>
<p>The next thing is whether to expand this to larger datasets. This is something that you would run locally (large datasets take too long to run for my shiny server). The downside of larger datasets is that it gets tricker to visualize them. For now, I am using a TSNE plot. I am open to suggestions, but the intent here is a way to evaluate outlier algorithms on a variety of datasets.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://rajivshah.com/blog/images/multiD.gif" class="img-fluid figure-img"></p>
<figcaption>datasets</figcaption>
</figure>
</div>


</section>

 ]]></description>
  <category>Outlier</category>
  <category>Anomaly</category>
  <guid>https://rajivshah.com/blog/outlier_app.html</guid>
  <pubDate>Mon, 27 Jun 2016 05:00:00 GMT</pubDate>
  <media:content url="https://rajivshah.com/blog/images/algorithms.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>RNN Addition (1st Grade)</title>
  <link>https://rajivshah.com/blog/rnn_addition.html</link>
  <description><![CDATA[ 






<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Ever since I ran across RNNs, they have intrigued me with their ability to learn. The best background is <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Denny Britz’s tutorial</a>, Karpathy’s totally accessible and fun post on <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">character-level language models</a>, and Colah’s detailed <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">descriptions of LSTMs</a>. Besides all the fun examples of generating content with RNNs, other people have been applying them and winning <a href="http://blog.kaggle.com/2016/01/04/how-much-did-it-rain-ii-winners-interview-1st-place-pupa-aka-aaron-sim/">Kaggle competitions</a> and the ECML/PKDD challenge.</p>
<p>I am still blown away by how RNN’s can learn to add. RNNs are trained through thousands of examples and can learn how to sum numbers. For example, the <a href="https://github.com/fchollet/keras/blob/master/examples/addition_rnn.py">Keras addition example</a> show how to add two sets of numbers up to 5 digital long each (e.g., 54678 + 78967). It achieves 99% train/test accuracy in 30 epochs with a one layer LSTM (128 HN) and 550k training examples.</p>
<p>My eventual goal is to use RNNs to study various sequenced data (such as the NBA SportVu), so I thought I should start simple. I wanted to teach a RNN to add a series of numbers. For example: 5+7+9. The rest of the post discusses this journey.</p>
</section>
<section id="st-grade-model" class="level3">
<h3 class="anchored" data-anchor-id="st-grade-model">1st Grade Model</h3>
<p>My first model was teaching an RNN to add between 5 to 15 single digit numbers. This would be at the level of a first grader in the US. For example, using a 2 layer LSTM network with 100 hidden units, a batch of 50 training examples, and 5000 epochs, the RNN summed up:</p>
<p>8+6+4+4+0+9+1+1+7+3+9+2+8 as 66.2154007</p>
<p>This isn’t too far from the actual answer of 62. The Keras addition example show that with even more examples/training, the RNN can get much better. The <a href="https://gist.github.com/rajshah4/aa6c67944f4a43a7c9a1204301788e0c">code for this RNN is available as a gist using tensorflow</a>. I made this in a notebook format so its easy to play with.</p>
<p>There are lots of parameters to tweak with RNN models, such as the number of hidden units, epochs, batch size, dropout, and training rate. Each of these has different sorts of effects on the model. For example, increasing the number of hidden units will provide more space for learning, but consequently take longer to learn/train. The chart below shows the effect of different choices. Please take the time to really study/investigate the role of hidden units. Its a dynamic plot so you can zoom in and examine each series individually by clicking on the legend.</p>
<div>
<a href="https://plot.ly/~rshah/6/" target="_blank" title="Cost by epoch for varying hidden units" style="display: block; text-align: center;"><img src="https://plot.ly/~rshah/6.png" alt="Cost by epoch" style="max-width: 100%;width: 600px;" width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';"></a>
<script data-plotly="rshah:6" src="https://plot.ly/embed.js" async=""></script>
</div>


</section>

 ]]></description>
  <category>RNN</category>
  <category>Keras</category>
  <guid>https://rajivshah.com/blog/rnn_addition.html</guid>
  <pubDate>Tue, 05 Apr 2016 05:00:00 GMT</pubDate>
</item>
</channel>
</rss>
