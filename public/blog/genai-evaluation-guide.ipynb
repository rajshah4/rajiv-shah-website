{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A Practical Guide to Evaluating Generative AI Applications\n",
        "\n",
        "## Video\n",
        "\n",
        "<https://youtu.be/qPHsWTZP58U>\n",
        "\n",
        "Watch the [full video](https://youtu.be/qPHsWTZP58U)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Annotated Presentation\n",
        "\n",
        "Below is an annotated version of the presentation, with timestamped\n",
        "links to the relevant parts of the video for each slide.\n",
        "\n",
        "Here is the annotated presentation for Rajiv Shah’s workshop on “Hill\n",
        "Climbing: Best Practices for Evaluating LLMs.”\n",
        "\n",
        "### 1. Title Slide\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_1.png\"\n",
        "alt=\"Slide 1\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 1</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:00](https://youtu.be/qPHsWTZP58U&t=0s))\n",
        "\n",
        "This slide introduces the workshop titled **“Hill Climbing: Best\n",
        "Practices for Evaluating LLMs,”** presented by Rajiv Shah, PhD, at the\n",
        "Open Data Science Conference (ODSC). The presentation focuses on the\n",
        "technical nuances of Generative AI and how to build effective evaluation\n",
        "workflows.\n",
        "\n",
        "Rajiv sets the stage by outlining his three main goals for the session:\n",
        "understanding the technical differences in GenAI evaluation, learning a\n",
        "basic introductory workflow for building evaluation datasets, and\n",
        "inspiring practitioners to start “learning by doing” rather than just\n",
        "reading papers.\n",
        "\n",
        "The concept of “Hill Climbing” refers to the iterative process of\n",
        "improving LLM applications—starting with a baseline and continuously\n",
        "optimizing performance through rigorous testing and error analysis.\n",
        "\n",
        "### 2. Evaluating for Gen AI Resources\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_2.png\"\n",
        "alt=\"Slide 2\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 2</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:06](https://youtu.be/qPHsWTZP58U&t=6s))\n",
        "\n",
        "This slide provides a QR code and a GitHub URL, directing the audience\n",
        "to the code and resources associated with the talk. It emphasizes that\n",
        "the workshop is practical, with code examples available for attendees to\n",
        "replicate the evaluation techniques discussed.\n",
        "\n",
        "Rajiv encourages the audience to access these resources to follow along\n",
        "with the technical implementations of the concepts, such as building LLM\n",
        "judges and creating unit tests, which will be covered later in the\n",
        "presentation.\n",
        "\n",
        "### 3. Customer Support Use Case\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_3.png\"\n",
        "alt=\"Slide 3\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 3</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:48](https://youtu.be/qPHsWTZP58U&t=48s))\n",
        "\n",
        "To motivate the need for evaluation, the presentation introduces a\n",
        "common real-world use case: **Customer Support**. Generative AI is\n",
        "frequently deployed to help agents compose emails or chat responses\n",
        "based on user inquiries.\n",
        "\n",
        "This scenario serves as the baseline example throughout the talk. It\n",
        "represents a high-volume task where automation is desirable, but\n",
        "accuracy and tone are critical for maintaining customer satisfaction and\n",
        "brand reputation.\n",
        "\n",
        "### 4. Vibe Coding\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_4.png\"\n",
        "alt=\"Slide 4\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 4</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:59](https://youtu.be/qPHsWTZP58U&t=59s))\n",
        "\n",
        "This slide introduces the concept of **“Vibe Coding”**—the initial phase\n",
        "where developers grab a simple prompt, feed it to a model, and get a\n",
        "result that feels right. It highlights the misconception that GenAI is\n",
        "easy because it works “out of the box” for simple demos.\n",
        "\n",
        "Rajiv notes that while “vibe coding” might work for a quick demo app, it\n",
        "is insufficient for production systems. Relying on a “vibe” that the\n",
        "model is working prevents teams from catching subtle failures that occur\n",
        "at scale.\n",
        "\n",
        "### 5. Good Response: Delayed Order\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_5.png\"\n",
        "alt=\"Slide 5\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 5</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:10](https://youtu.be/qPHsWTZP58U&t=70s))\n",
        "\n",
        "Here, we see a successful output generated by the LLM. The customer\n",
        "inquired about a delayed order, and the AI generated a polite, relevant\n",
        "response acknowledging the delay and apologizing.\n",
        "\n",
        "This example reinforces the “Vibe Coding” trap: because the model often\n",
        "produces high-quality, human-sounding text like this, developers can be\n",
        "lulled into a false sense of security regarding the system’s\n",
        "reliability.\n",
        "\n",
        "### 6. Good Response: Damaged Product\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_6.png\"\n",
        "alt=\"Slide 6\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 6</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:12](https://youtu.be/qPHsWTZP58U&t=72s))\n",
        "\n",
        "This slide provides another example of a “good” response. The AI\n",
        "correctly identifies that the customer received a damaged product and\n",
        "initiates a replacement protocol.\n",
        "\n",
        "These positive examples establish a baseline of expected behavior. The\n",
        "challenge in evaluation is not just confirming that the model *can*\n",
        "work, but ensuring it works consistently across all edge cases.\n",
        "\n",
        "### 7. Bad Response: Irrelevance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_7.png\"\n",
        "alt=\"Slide 7\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 7</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:26](https://youtu.be/qPHsWTZP58U&t=86s))\n",
        "\n",
        "The presentation shifts to failure modes. In this example, the user asks\n",
        "about an **“Order Delay,”** but the AI responds with information about a\n",
        "**“New Product Launch.”**\n",
        "\n",
        "This illustrates a complete context mismatch. The model failed to attend\n",
        "to the user’s intent, generating a coherent but completely irrelevant\n",
        "response. This type of failure frustrates users and degrades trust in\n",
        "the automated system.\n",
        "\n",
        "### 8. Bad Response: Hallucination\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_8.png\"\n",
        "alt=\"Slide 8\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 8</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:36](https://youtu.be/qPHsWTZP58U&t=96s))\n",
        "\n",
        "This slide shows a more dangerous failure: **Hallucination**. The AI\n",
        "apologizes for a defective “espresso machine,” but as the speaker notes,\n",
        "“We don’t actually sell espresso machines.”\n",
        "\n",
        "This highlights the risk of the model fabricating facts to be helpful.\n",
        "Such errors can lead to logistical nightmares, such as customers\n",
        "expecting replacements for products that do not exist or that the\n",
        "company never sold.\n",
        "\n",
        "### 9. Risks of LLM Mistakes\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_9.png\"\n",
        "alt=\"Slide 9\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 9</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:51](https://youtu.be/qPHsWTZP58U&t=111s))\n",
        "\n",
        "Rajiv categorizes the risks associated with LLM failures into three\n",
        "buckets: **Reputational, Legal, and Financial**. He cites the example of\n",
        "**Cursor**, an IDE company, where a support bot hallucinated a policy\n",
        "restricting users to one device, causing customers to cancel\n",
        "subscriptions.\n",
        "\n",
        "The slide emphasizes that courts may view AI agents as employees; if a\n",
        "bot makes a promise (like a refund or policy change), the company might\n",
        "be legally bound to honor it. This escalates evaluation from a technical\n",
        "nice-to-have to a business necessity.\n",
        "\n",
        "### 10. The Despair of Gen AI\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_10.png\"\n",
        "alt=\"Slide 10\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 10</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 02:38](https://youtu.be/qPHsWTZP58U&t=158s))\n",
        "\n",
        "This visual represents the frustration developers feel when moving from\n",
        "a successful demo to a failing production system. The “despair” comes\n",
        "from the realization that the stochastic nature of LLMs makes them\n",
        "difficult to control.\n",
        "\n",
        "It serves as an emotional anchor for the audience, acknowledging that\n",
        "while GenAI is exciting, the unpredictability of its failures causes\n",
        "significant stress for engineering teams responsible for deployment.\n",
        "\n",
        "### 11. High Failure Rates\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_11.png\"\n",
        "alt=\"Slide 11\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 11</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 02:48](https://youtu.be/qPHsWTZP58U&t=168s))\n",
        "\n",
        "The slide cites an MIT report stating that **“95% of GenAI pilots are\n",
        "failing.”** While Rajiv notes this number might be overstated, it\n",
        "reflects a trend where executives are demanding ROI and seeing\n",
        "lackluster results.\n",
        "\n",
        "This shift in 2025 means that evaluation is no longer just for\n",
        "debugging; it is required to prove business value and justify the high\n",
        "costs of running Generative AI infrastructure.\n",
        "\n",
        "### 12. Evaluation Improves Applications\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_12.png\"\n",
        "alt=\"Slide 12\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 12</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:14](https://youtu.be/qPHsWTZP58U&t=194s))\n",
        "\n",
        "This slide asserts the core thesis: **Evaluation helps you build better\n",
        "GenAI applications.** It references a previous viral video by the\n",
        "speaker on the same topic, positioning this talk as an updated,\n",
        "condensed version with fresh content.\n",
        "\n",
        "Rajiv explains that you cannot improve what you cannot measure. Without\n",
        "a robust evaluation framework, developers are essentially guessing\n",
        "whether changes to prompts or models are actually improving performance.\n",
        "\n",
        "### 13. Why Evaluation is Necessary\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_13.png\"\n",
        "alt=\"Slide 13\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 13</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 03:40](https://youtu.be/qPHsWTZP58U&t=220s))\n",
        "\n",
        "This concentric diagram illustrates the stakeholders involved in\n",
        "evaluation. It starts with **“Things Go Wrong”** (technical reality),\n",
        "moves to **“Buy-in”** (convincing managers/teams), and ends with\n",
        "**“Regulators”** (external compliance).\n",
        "\n",
        "Evaluation serves multiple audiences: it helps the developer debug, it\n",
        "provides the metrics needed to convince management that the app is\n",
        "production-ready, and it creates the audit trails required by\n",
        "third-party auditors or regulators.\n",
        "\n",
        "### 14. Evaluation Dimensions\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_14.png\"\n",
        "alt=\"Slide 14\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 14</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 04:18](https://youtu.be/qPHsWTZP58U&t=258s))\n",
        "\n",
        "Evaluation must cover three dimensions: **Technical** (F1 scores,\n",
        "accuracy), **Business** (ROI, value generated), and **Operational**\n",
        "(Total Cost of Ownership, latency).\n",
        "\n",
        "Rajiv highlights that data scientists often focus solely on the\n",
        "technical, but ignoring operational costs (like the expense of hosting\n",
        "GPUs vs. using APIs) can kill a project. A comprehensive evaluation\n",
        "strategy considers the cost-to-quality ratio.\n",
        "\n",
        "### 15. Public Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_15.png\"\n",
        "alt=\"Slide 15\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 15</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 05:06](https://youtu.be/qPHsWTZP58U&t=306s))\n",
        "\n",
        "The slide discusses **Public Benchmarks** (like MMLU, GSM8K). While\n",
        "useful for a general idea of a model’s capabilities (e.g., “Is Llama 3\n",
        "better than Llama 2?”), they are insufficient for specific applications.\n",
        "\n",
        "Rajiv warns against using these benchmarks to determine if a model fits\n",
        "*your* specific use case. Companies promote these numbers for marketing,\n",
        "but they rarely reflect performance on proprietary business data.\n",
        "\n",
        "### 16. Custom Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_16.png\"\n",
        "alt=\"Slide 16\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 16</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 05:22](https://youtu.be/qPHsWTZP58U&t=322s))\n",
        "\n",
        "The solution to the limitations of public benchmarks is **Custom\n",
        "Benchmarks**. This slide defines a benchmark as a combination of a\n",
        "**Task**, a **Dataset**, and an **Evaluation Metric**.\n",
        "\n",
        "This is a critical definition for the workshop. To “tame” GenAI, you\n",
        "must build a dataset that reflects your specific customer queries and\n",
        "define success metrics that matter to your business logic, rather than\n",
        "relying on generic academic tests.\n",
        "\n",
        "### 17. Taming Gen AI\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_17.png\"\n",
        "alt=\"Slide 17\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 17</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 05:28](https://youtu.be/qPHsWTZP58U&t=328s))\n",
        "\n",
        "This title slide signals a transition into the technical “how-to”\n",
        "section of the talk. “Taming” implies that the default state of GenAI is\n",
        "wild and unpredictable.\n",
        "\n",
        "The goal of the following sections is to bring structure and control to\n",
        "this chaos through rigorous engineering practices and evaluation\n",
        "workflows.\n",
        "\n",
        "### 18. Workshop Roadmap\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_18.png\"\n",
        "alt=\"Slide 18\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 18</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 05:31](https://youtu.be/qPHsWTZP58U&t=331s))\n",
        "\n",
        "The roadmap outlines the four main sections of the talk: 1. **Basics of\n",
        "Gen AI:** Understanding variability and technical nuances. 2.\n",
        "**Evaluation Workflow:** Building the dataset and running the first\n",
        "tests. 3. **More Complexity:** Adding unit tests and conducting error\n",
        "analysis. 4. **Agents:** Evaluating complex, multi-step workflows.\n",
        "\n",
        "### 19. Variability in Responses\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_19.png\"\n",
        "alt=\"Slide 19\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 19</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 06:00](https://youtu.be/qPHsWTZP58U&t=360s))\n",
        "\n",
        "This slide visually demonstrates the **Non-Determinism** of LLMs. It\n",
        "shows two responses to the same prompt generated just minutes apart.\n",
        "While substantively similar, the wording and structure differ slightly.\n",
        "\n",
        "This variability makes exact string matching (a common software testing\n",
        "technique) impossible for LLMs. It necessitates semantic evaluation\n",
        "techniques, which complicates the testing pipeline.\n",
        "\n",
        "### 20. Input-Model-Output Diagram\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_20.png\"\n",
        "alt=\"Slide 20\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 20</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 06:24](https://youtu.be/qPHsWTZP58U&t=384s))\n",
        "\n",
        "A simple diagram illustrates the flow: **Prompt -\\> Model -\\> Output**.\n",
        "Rajiv uses this to structure the analysis of where variability comes\n",
        "from.\n",
        "\n",
        "He explains that “chaos” can enter the system at any of these three\n",
        "stages: the input (prompt sensitivity), the model (inference\n",
        "non-determinism), or the output (formatting and evaluation).\n",
        "\n",
        "### 21. Inconsistent Benchmark Scores\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_21.png\"\n",
        "alt=\"Slide 21\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 21</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 06:44](https://youtu.be/qPHsWTZP58U&t=404s))\n",
        "\n",
        "The slide presents a discrepancy between benchmark scores tweeted by\n",
        "Hugging Face and those in the official Llama paper. Both used the same\n",
        "dataset (MMLU), but reported different accuracy numbers.\n",
        "\n",
        "This introduces the problem of **Evaluation Harness Sensitivity**. Even\n",
        "with standard benchmarks, *how* you ask the model to take the test\n",
        "changes the score, proving that evaluation is fragile and\n",
        "implementation-dependent.\n",
        "\n",
        "### 22. MMLU Overview\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_22.png\"\n",
        "alt=\"Slide 22\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 22</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 07:25](https://youtu.be/qPHsWTZP58U&t=445s))\n",
        "\n",
        "**MMLU (Massive Multitask Language Understanding)** is explained here.\n",
        "It is a multiple-choice test covering 57 tasks across STEM, the\n",
        "humanities, and more.\n",
        "\n",
        "It is currently the standard for measuring general “intelligence” in\n",
        "models. However, because it is a multiple-choice format, it is\n",
        "susceptible to prompt formatting nuances, as the next slides\n",
        "demonstrate.\n",
        "\n",
        "### 23. Prompt Sensitivity\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_23.png\"\n",
        "alt=\"Slide 23\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 23</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 07:44](https://youtu.be/qPHsWTZP58U&t=464s))\n",
        "\n",
        "This slide reveals *why* the scores in Slide 21 differed. The three\n",
        "evaluation harnesses used slightly different prompt structures (e.g.,\n",
        "using the word “Question” vs. just listing the text).\n",
        "\n",
        "These minor changes resulted in significant accuracy shifts. This proves\n",
        "that LLMs are highly sensitive to syntax, meaning a “better” model might\n",
        "just be one that was prompted more effectively for the test, not one\n",
        "that is actually smarter.\n",
        "\n",
        "### 24. Formatting Changes\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_24.png\"\n",
        "alt=\"Slide 24\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 24</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 08:22](https://youtu.be/qPHsWTZP58U&t=502s))\n",
        "\n",
        "Expanding on sensitivity, this slide references Anthropic’s research\n",
        "showing that changing answer choices from `(A)` to `[A]` or `(1)`\n",
        "affects the output.\n",
        "\n",
        "This level of fragility is a key takeaway: seemingly cosmetic changes in\n",
        "how inputs are formatted can alter the model’s reasoning capabilities or\n",
        "its ability to output the correct token.\n",
        "\n",
        "### 25. GPT-4o Performance Drop\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_25.png\"\n",
        "alt=\"Slide 25\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 25</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 08:38](https://youtu.be/qPHsWTZP58U&t=518s))\n",
        "\n",
        "A bar chart demonstrates that this issue persists even in\n",
        "state-of-the-art models like **GPT-4o**. Subtle changes in wording can\n",
        "lead to a 5-10% drop in performance.\n",
        "\n",
        "This counters the assumption that newer, larger models have “solved”\n",
        "prompt sensitivity. It remains a persistent variable that evaluators\n",
        "must control for.\n",
        "\n",
        "### 26. Tone Sensitivity\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_26.png\"\n",
        "alt=\"Slide 26\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 26</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 08:46](https://youtu.be/qPHsWTZP58U&t=526s))\n",
        "\n",
        "This slide shows that the **tone** of a prompt (e.g., being polite\n",
        "vs. direct) affects accuracy. Rajiv jokes, “I guess this is why mom\n",
        "always said to be polite.”\n",
        "\n",
        "The graph indicates that prompt engineering strategies, like adding\n",
        "emotional weight or politeness, can statistically alter model\n",
        "performance, adding another layer of complexity to evaluation.\n",
        "\n",
        "### 27. Persistent Sensitivity\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_27.png\"\n",
        "alt=\"Slide 27\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 27</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:00](https://youtu.be/qPHsWTZP58U&t=540s))\n",
        "\n",
        "The slide reiterates that despite years of progress, models are still\n",
        "sensitive to specific phrases. It shows a “Prompt Engineering” guide\n",
        "suggesting specific words to use.\n",
        "\n",
        "The takeaway is that developers cannot treat the prompt as a static\n",
        "instruction; it is a hyperparameter that requires optimization and\n",
        "constant testing.\n",
        "\n",
        "### 28. Falcon LLM Bias\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_28.png\"\n",
        "alt=\"Slide 28\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 28</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:18](https://youtu.be/qPHsWTZP58U&t=558s))\n",
        "\n",
        "This slide introduces a case study with the **Falcon LLM**. A user tweet\n",
        "shows the model recommending **Abu Dhabi** as a technological city with\n",
        "glowing sentiment, which raised suspicions about bias given the model’s\n",
        "origin in the Middle East.\n",
        "\n",
        "This serves as a detective story: users wondered if the model weights\n",
        "were altered or if specific training data was injected to force this\n",
        "positive association.\n",
        "\n",
        "### 29. Potential Cover-up?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_29.png\"\n",
        "alt=\"Slide 29\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 29</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:50](https://youtu.be/qPHsWTZP58U&t=590s))\n",
        "\n",
        "Another tweet speculates if the model is “covering up human rights\n",
        "abuses” because it provides different answers for Abu Dhabi compared to\n",
        "other cities.\n",
        "\n",
        "This highlights how model behavior can be misinterpreted as malicious\n",
        "bias or censorship, when the root cause might be something much simpler\n",
        "in the input stack.\n",
        "\n",
        "### 30. Inspecting the System Prompt\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_30.png\"\n",
        "alt=\"Slide 30\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 30</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 10:00](https://youtu.be/qPHsWTZP58U&t=600s))\n",
        "\n",
        "The reveal: The bias wasn’t in the weights, but in the **System\n",
        "Prompt**. The slide suggests looking at the hidden instructions given to\n",
        "the model.\n",
        "\n",
        "In Falcon’s case, the system prompt explicitly told the model, “You are\n",
        "a model built in Abu Dhabi.” This context influenced its generation\n",
        "probabilities, causing it to favor Abu Dhabi in its responses.\n",
        "\n",
        "### 31. Claude System Prompt\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_31.png\"\n",
        "alt=\"Slide 31\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 31</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 10:33](https://youtu.be/qPHsWTZP58U&t=633s))\n",
        "\n",
        "Rajiv points out that most developers never read the system prompts of\n",
        "the models they use. He highlights the **Claude System Prompt**, which\n",
        "is 1700 words long and takes nearly 10 minutes to read.\n",
        "\n",
        "These extensive instructions define the model’s personality and safety\n",
        "guardrails. Ignoring them means you don’t fully understand the inputs\n",
        "driving your application’s behavior.\n",
        "\n",
        "### 32. Complexity of a Single Response\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_32.png\"\n",
        "alt=\"Slide 32\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 32</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 11:00](https://youtu.be/qPHsWTZP58U&t=660s))\n",
        "\n",
        "The diagram is updated to show that a “single response” is actually the\n",
        "result of complex interactions: **Tokenization -\\> Prompt Styles -\\>\n",
        "Prompt Engineering -\\> System Prompt**.\n",
        "\n",
        "This visual summarizes the “Input” section of the talk, reinforcing that\n",
        "before the model even processes data, multiple layers of text\n",
        "transformation occur that can alter the result.\n",
        "\n",
        "### 33. Inter-text Similarity\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_33.png\"\n",
        "alt=\"Slide 33\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 33</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 11:15](https://youtu.be/qPHsWTZP58U&t=675s))\n",
        "\n",
        "This heatmap compares **Inter-text similarity** between models. It\n",
        "highlights Llama 70B and Llama 8B. Even though they are from the same\n",
        "family and likely trained on similar data, they are not identical.\n",
        "\n",
        "This means you cannot swap a smaller model for a larger one (or vice\n",
        "versa) and expect the exact same behavior. Any model change requires a\n",
        "full re-evaluation.\n",
        "\n",
        "### 34. Sycophantic Models\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_34.png\"\n",
        "alt=\"Slide 34\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 34</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 12:16](https://youtu.be/qPHsWTZP58U&t=736s))\n",
        "\n",
        "The slide discusses **Sycophancy**—the tendency of models to agree with\n",
        "the user even when the user is wrong. It mentions how early versions of\n",
        "GPT-4 were sometimes “overly nice.”\n",
        "\n",
        "This behavior is a specific type of model bias that evaluators must\n",
        "watch for. If a user asks a leading question containing false premises,\n",
        "a sycophantic model might validate the falsehood rather than correct it.\n",
        "\n",
        "### 35. Model Drift\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_35.png\"\n",
        "alt=\"Slide 35\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 35</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 12:37](https://youtu.be/qPHsWTZP58U&t=757s))\n",
        "\n",
        "**“Model Drift”** refers to the phenomenon where commercial APIs (like\n",
        "OpenAI or Anthropic) change their model behavior over time without\n",
        "warning.\n",
        "\n",
        "Because developers do not control the weights of API-based models, the\n",
        "“ground underneath them” can shift. A prompt that worked yesterday might\n",
        "fail today because the provider updated the backend or the inference\n",
        "infrastructure.\n",
        "\n",
        "### 36. Degraded Responses Timeline\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_36.png\"\n",
        "alt=\"Slide 36\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 36</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 12:55](https://youtu.be/qPHsWTZP58U&t=775s))\n",
        "\n",
        "This slide shows a timeline of **Degraded Responses** from an Anthropic\n",
        "incident. Technical issues like context window routing errors led to\n",
        "corrupted outputs for a period of days.\n",
        "\n",
        "This illustrates that drift isn’t always about model updates; it can be\n",
        "infrastructure failures. Continuous monitoring is required to detect\n",
        "when an external dependency degrades your application’s performance.\n",
        "\n",
        "### 37. Hyperparameters\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_37.png\"\n",
        "alt=\"Slide 37\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 37</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 13:33](https://youtu.be/qPHsWTZP58U&t=813s))\n",
        "\n",
        "The slide lists **Hyperparameters** like Temperature, Top-P, and Max\n",
        "Length. Rajiv explains that users can control these “knobs” to influence\n",
        "creativity versus determinism.\n",
        "\n",
        "Setting temperature to 0 makes the model less random, but as the next\n",
        "slides show, it does not guarantee perfect determinism due to hardware\n",
        "nuances.\n",
        "\n",
        "### 38. Non-Deterministic Inference\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_38.png\"\n",
        "alt=\"Slide 38\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 38</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 14:03](https://youtu.be/qPHsWTZP58U&t=843s))\n",
        "\n",
        "This slide tackles **Non-Deterministic Inference**. Unlike traditional\n",
        "ML models (e.g., XGBoost) where a fixed seed guarantees identical\n",
        "output, LLMs on GPUs often produce different results for identical\n",
        "inputs.\n",
        "\n",
        "Causes include floating-point accumulation errors and the behavior of\n",
        "Mixture of Experts (MoE) models where different batches might activate\n",
        "different experts.\n",
        "\n",
        "### 39. Addressing Non-Determinism\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_39.png\"\n",
        "alt=\"Slide 39\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 39</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 15:11](https://youtu.be/qPHsWTZP58U&t=911s))\n",
        "\n",
        "Rajiv references recent work by **Thinking Machines** and updates to\n",
        "**vLLM** that attempt to solve the non-determinism problem through\n",
        "correct batching.\n",
        "\n",
        "While solutions are emerging, the takeaway is that most current setups\n",
        "are non-deterministic by default. Evaluators must design their tests to\n",
        "tolerate this variance rather than expecting bit-wise reproducibility.\n",
        "\n",
        "### 40. Updated Model Diagram\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_40.png\"\n",
        "alt=\"Slide 40\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 40</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 15:43](https://youtu.be/qPHsWTZP58U&t=943s))\n",
        "\n",
        "The diagram expands again. The “Model” box now includes **Model\n",
        "Selection, Hyperparameters, Non-deterministic Inference, and Forced\n",
        "Updates**.\n",
        "\n",
        "This visual summarizes the “Model” section, showing that the “black box”\n",
        "is actually a dynamic system with internal variables\n",
        "(weights/architecture) and external variables (infrastructure/updates)\n",
        "that all add noise to the output.\n",
        "\n",
        "### 41. Output Format Issues\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_41.png\"\n",
        "alt=\"Slide 41\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 41</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:01](https://youtu.be/qPHsWTZP58U&t=961s))\n",
        "\n",
        "Moving to the “Output” stage, this slide uses MMLU again to show how\n",
        "**Output Formatting** affects evaluation. How do you ask the model to\n",
        "answer a multiple-choice question?\n",
        "\n",
        "Do you ask it to output just the letter “A”? Or the full text? Or the\n",
        "probability of the token “A”? Different evaluation harnesses use\n",
        "different methods, leading to the score discrepancies seen earlier.\n",
        "\n",
        "### 42. Evaluation Harness Variations\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_42.png\"\n",
        "alt=\"Slide 42\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 42</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:35](https://youtu.be/qPHsWTZP58U&t=995s))\n",
        "\n",
        "This table details the specific differences in implementation between\n",
        "harnesses (e.g., original MMLU vs. HELM vs. EleutherAI).\n",
        "\n",
        "It reinforces that there is no standard “ruler” for measuring LLMs. The\n",
        "tool you use to measure the model introduces its own bias and variance\n",
        "into the final score.\n",
        "\n",
        "### 43. Score Comparison Table\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_43.png\"\n",
        "alt=\"Slide 43\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 43</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:56](https://youtu.be/qPHsWTZP58U&t=1016s))\n",
        "\n",
        "A spreadsheet shows the same models scoring differently across different\n",
        "evaluation implementations. The variance is not trivial; it can be large\n",
        "enough to change the ranking of which model is “best.”\n",
        "\n",
        "This data drives home the point: You must control your own evaluation\n",
        "pipeline. Relying on reported numbers is risky because you don’t know\n",
        "the implementation details behind them.\n",
        "\n",
        "### 44. Sentiment Analysis Variance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_44.png\"\n",
        "alt=\"Slide 44\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 44</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 17:09](https://youtu.be/qPHsWTZP58U&t=1029s))\n",
        "\n",
        "This slide shows varying **Sentiment Analysis** outputs. Different\n",
        "models (or the same model with different prompts) might classify a\n",
        "review as “Positive” while another says “Neutral.”\n",
        "\n",
        "This introduces the concept that even “simple” classification tasks in\n",
        "GenAI are subject to interpretation and variance, unlike traditional\n",
        "classifiers that have a fixed decision boundary.\n",
        "\n",
        "### 45. Tool Use Variance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_45.png\"\n",
        "alt=\"Slide 45\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 45</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 17:23](https://youtu.be/qPHsWTZP58U&t=1043s))\n",
        "\n",
        "Radar charts illustrate variance in **Tool Use**. Models might be good\n",
        "at using an “Email” tool but fail at “Calendar” or “Terminal” tools.\n",
        "\n",
        "Furthermore, models exhibit non-determinism in *decision\n",
        "making*—sometimes they choose to use a tool, and sometimes they try to\n",
        "answer from memory. This adds a layer of logic errors on top of text\n",
        "generation errors.\n",
        "\n",
        "### 46. Summary: Why Responses Differ\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_46.png\"\n",
        "alt=\"Slide 46\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 46</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 17:49](https://youtu.be/qPHsWTZP58U&t=1069s))\n",
        "\n",
        "This comprehensive slide aggregates all the factors discussed:\n",
        "**Inputs** (prompts, system prompts), **Model** (drift, hyperparams),\n",
        "**Outputs** (formatting), and **Infrastructure**.\n",
        "\n",
        "It serves as a checklist for the audience. If your application is\n",
        "behaving inconsistently, investigate these specific layers of the stack\n",
        "to find the source of the noise.\n",
        "\n",
        "### 47. Chaos is Okay\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_47.png\"\n",
        "alt=\"Slide 47\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 47</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 18:17](https://youtu.be/qPHsWTZP58U&t=1097s))\n",
        "\n",
        "Rajiv reassures the audience that **“Chaos is Okay.”** The slide\n",
        "presents a chart of evaluation methods ranging from flexible/expensive\n",
        "(human eval) to rigid/cheap (code assertions).\n",
        "\n",
        "The message is that while the technology is chaotic, there is a spectrum\n",
        "of tools available to manage it. We don’t need to solve every source of\n",
        "variance; we just need a robust process to measure it.\n",
        "\n",
        "### 48. From Chaos to Control\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_48.png\"\n",
        "alt=\"Slide 48\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 48</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 18:27](https://youtu.be/qPHsWTZP58U&t=1107s))\n",
        "\n",
        "This transition slide marks the beginning of the **Evaluation Workflow**\n",
        "section. The presentation shifts from describing the problem to\n",
        "prescribing the solution.\n",
        "\n",
        "The goal here is to move from “Vibe Coding” to a structured engineering\n",
        "discipline where changes are measured against a stable baseline.\n",
        "\n",
        "### 49. Build the Evaluation Dataset\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_49.png\"\n",
        "alt=\"Slide 49\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 49</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 18:37](https://youtu.be/qPHsWTZP58U&t=1117s))\n",
        "\n",
        "The first step in the workflow is to **Build the Evaluation Dataset**.\n",
        "The slide lists examples of prompts for tasks like summarization,\n",
        "extraction, and translation.\n",
        "\n",
        "Rajiv emphasizes that this dataset should reflect *your* actual use\n",
        "case. It is the foundation of the “Custom Benchmark” concept introduced\n",
        "earlier.\n",
        "\n",
        "### 50. Get Labeled Outputs (Gold)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_50.png\"\n",
        "alt=\"Slide 50\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 50</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 18:46](https://youtu.be/qPHsWTZP58U&t=1126s))\n",
        "\n",
        "Step two is to get **Labeled Outputs**, also known as **Gold Outputs**,\n",
        "Reference, or Ground Truth. The slide adds a column showing the ideal\n",
        "answer for each prompt.\n",
        "\n",
        "This is the standard against which the model will be judged. While\n",
        "obtaining these labels can be expensive (requiring human effort), they\n",
        "are essential for calculating accuracy.\n",
        "\n",
        "### 51. Compare to Model Output\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_51.png\"\n",
        "alt=\"Slide 51\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 51</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 19:00](https://youtu.be/qPHsWTZP58U&t=1140s))\n",
        "\n",
        "Step three is to generate responses from your system and place them\n",
        "alongside the Gold Outputs. The slide adds a **“Model Output”** column.\n",
        "\n",
        "This visual comparison allows developers (and automated judges) to see\n",
        "the delta between what was expected and what was produced.\n",
        "\n",
        "### 52. Measure Equivalence\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_52.png\"\n",
        "alt=\"Slide 52\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 52</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 19:10](https://youtu.be/qPHsWTZP58U&t=1150s))\n",
        "\n",
        "Step four is to **Measure Equivalence**. Since LLMs rarely produce exact\n",
        "string matches, we use an **LLM Judge** (another model) to determine if\n",
        "the Model Output means the same thing as the Gold Output.\n",
        "\n",
        "The slide shows a prompt for the judge: “Are these two responses\n",
        "semantically equivalent?” This converts a fuzzy text comparison problem\n",
        "into a binary (Pass/Fail) metric.\n",
        "\n",
        "### 53. Optimize Using Equivalence\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_53.png\"\n",
        "alt=\"Slide 53\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 53</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 19:57](https://youtu.be/qPHsWTZP58U&t=1197s))\n",
        "\n",
        "Once you have an equivalence metric, you can **Optimize**. The slide\n",
        "shows Config A vs. Config B. By changing prompts or models, you can\n",
        "track if your “Equivalence Score” goes up or down.\n",
        "\n",
        "This treats GenAI engineering like traditional hyperparameter tuning.\n",
        "The goal is to maximize the equivalence score on your custom dataset.\n",
        "\n",
        "### 54. Why Global Metrics Aren’t Enough\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_54.png\"\n",
        "alt=\"Slide 54\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 54</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 20:28](https://youtu.be/qPHsWTZP58U&t=1228s))\n",
        "\n",
        "The slide discusses the limitations of the “Equivalence” approach. While\n",
        "good for a general sense of quality, **Global Metrics** miss nuances.\n",
        "\n",
        "Sometimes it’s hard to get a Gold Answer for open-ended creative tasks.\n",
        "Furthermore, a simple “Pass/Fail” doesn’t tell you *why* the model\n",
        "failed (e.g., was it tone, length, or factuality?).\n",
        "\n",
        "### 55. From Global to Targeted Evaluation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_55.png\"\n",
        "alt=\"Slide 55\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 55</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 20:55](https://youtu.be/qPHsWTZP58U&t=1255s))\n",
        "\n",
        "This slide argues for **Targeted Evaluation**. To maximize performance,\n",
        "you need to dig deeper into the data and identify specific error modes.\n",
        "\n",
        "This transitions the talk from “Basic Workflow” to “Advanced Testing,”\n",
        "where we break down “Quality” into specific, testable components like\n",
        "tone, length, and safety.\n",
        "\n",
        "### 56. Building Tests\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_56.png\"\n",
        "alt=\"Slide 56\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 56</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:14](https://youtu.be/qPHsWTZP58U&t=1274s))\n",
        "\n",
        "The section title **“Building Tests”** appears. This is where the\n",
        "presentation moves into the “Unit Testing” philosophy for GenAI.\n",
        "\n",
        "Just as software engineering relies on unit tests to verify specific\n",
        "functions, GenAI engineering should use targeted tests to verify\n",
        "specific attributes of the generated text.\n",
        "\n",
        "### 57. Good vs. Bad Examples\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_57.png\"\n",
        "alt=\"Slide 57\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 57</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:20](https://youtu.be/qPHsWTZP58U&t=1280s))\n",
        "\n",
        "The slide displays a **Good Example** and a **Bad Example** of a\n",
        "response. The bad example is visibly shorter and less polite.\n",
        "\n",
        "Rajiv asks the audience to identify *why* it is bad. This exercise is\n",
        "crucial: you cannot build a test until you can articulate exactly what\n",
        "makes a response a failure.\n",
        "\n",
        "### 58. Develop an Evaluation Mindset\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_58.png\"\n",
        "alt=\"Slide 58\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 58</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:46](https://youtu.be/qPHsWTZP58U&t=1306s))\n",
        "\n",
        "To define “Bad,” developers need an **Evaluation Mindset**. This\n",
        "involves observing real-world user interactions and problems.\n",
        "\n",
        "Data scientists often want to stay in their “chair” and optimize\n",
        "algorithms, but Rajiv argues that effective evaluation requires\n",
        "understanding the user’s pain points.\n",
        "\n",
        "### 59. Collaborate with Experts\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_59.png\"\n",
        "alt=\"Slide 59\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 59</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:58](https://youtu.be/qPHsWTZP58U&t=1318s))\n",
        "\n",
        "The slide stresses **Collaboration**. You must talk to domain experts\n",
        "(e.g., the customer support team) to define what a “good” answer looks\n",
        "like.\n",
        "\n",
        "Naive bootstrapping—pretending to be a user—is a good start, but\n",
        "long-term success requires input from the people who actually know the\n",
        "business domain.\n",
        "\n",
        "### 60. Identify and Categorize Failures\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_60.png\"\n",
        "alt=\"Slide 60\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 60</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 22:52](https://youtu.be/qPHsWTZP58U&t=1372s))\n",
        "\n",
        "Once you understand the domain, you can **Categorize Failure Types**.\n",
        "The slide shows a chart grouping errors into categories like “Harmful\n",
        "Content,” “Bias,” or “Incorrect Info.”\n",
        "\n",
        "This clustering allows you to see patterns. Instead of just knowing “the\n",
        "model failed 20% of the time,” you know “the model has a specific\n",
        "problem with tone.”\n",
        "\n",
        "### 61. Define What Good Looks Like\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_61.png\"\n",
        "alt=\"Slide 61\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 61</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:11](https://youtu.be/qPHsWTZP58U&t=1391s))\n",
        "\n",
        "Using the categorization, you can explicitly **Define What Good Looks\n",
        "Like**. The slide contrasts the good/bad examples again, but now with\n",
        "labels: “Too short,” “Lacks professional tone.”\n",
        "\n",
        "This transforms a subjective feeling (“this response sucks”) into\n",
        "objective criteria (“response must be \\>50 words and use polite\n",
        "honorifics”).\n",
        "\n",
        "### 62. Document Every Issue\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_62.png\"\n",
        "alt=\"Slide 62\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 62</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:32](https://youtu.be/qPHsWTZP58U&t=1412s))\n",
        "\n",
        "The slide shows a spreadsheet where humans evaluate responses and\n",
        "**Document Every Issue**. Columns track specific attributes like “Is it\n",
        "helpful?” or “Is the tone right?”\n",
        "\n",
        "This manual annotation is the training data for your automated tests.\n",
        "You need humans to establish the ground truth before you can automate\n",
        "the checking.\n",
        "\n",
        "### 63. Evaluation Tooling\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_63.png\"\n",
        "alt=\"Slide 63\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 63</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:53](https://youtu.be/qPHsWTZP58U&t=1433s))\n",
        "\n",
        "Rajiv mentions that **Tooling Can Help**. The slide shows a custom chat\n",
        "viewer designed to make human review easier.\n",
        "\n",
        "However, he warns against getting sidetracked by building fancy tools.\n",
        "Simple spreadsheets often suffice for the early stages. The goal is the\n",
        "data, not the interface.\n",
        "\n",
        "### 64. Test 1: Length Check\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_64.png\"\n",
        "alt=\"Slide 64\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 64</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:05](https://youtu.be/qPHsWTZP58U&t=1445s))\n",
        "\n",
        "Now we build the automated tests. **Test 1 is a Length Check**. The\n",
        "slide shows Python code asserting that the word count is between 8 and\n",
        "200.\n",
        "\n",
        "This is a **deterministic test**. You don’t need an LLM to count words.\n",
        "Rajiv encourages using simple Python assertions wherever possible\n",
        "because they are fast, cheap, and reliable.\n",
        "\n",
        "### 65. Test 2: Tone and Style\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_65.png\"\n",
        "alt=\"Slide 65\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 65</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:22](https://youtu.be/qPHsWTZP58U&t=1462s))\n",
        "\n",
        "**Test 2 checks Tone and Style**. Since “tone” is subjective, we use an\n",
        "**LLM Judge** (OpenAI model) to classify the response.\n",
        "\n",
        "The prompt asks the judge to identify the style. This allows us to\n",
        "automate the “vibe check” that humans were previously doing manually.\n",
        "\n",
        "### 66. Adding Metrics to Documentation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_66.png\"\n",
        "alt=\"Slide 66\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 66</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 24:41](https://youtu.be/qPHsWTZP58U&t=1481s))\n",
        "\n",
        "The spreadsheet is updated with new columns: `Length_OK` and `Tone_OK`.\n",
        "These are the results of the automated tests.\n",
        "\n",
        "Now, for every row in the dataset, we have granular pass/fail metrics.\n",
        "This helps pinpoint exactly *why* a specific response failed, rather\n",
        "than just a generic failure.\n",
        "\n",
        "### 67. Check Judges Against Humans\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_67.png\"\n",
        "alt=\"Slide 67\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 67</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 25:12](https://youtu.be/qPHsWTZP58U&t=1512s))\n",
        "\n",
        "A critical step: **Check LLM Judges Against Humans**. You must verify\n",
        "that your automated “Tone Judge” agrees with your human experts.\n",
        "\n",
        "If the human says the tone is rude, but the LLM Judge says it’s polite,\n",
        "your metric is useless. You must iterate on the judge’s prompt until\n",
        "alignment is high.\n",
        "\n",
        "### 68. Self-Evaluation Bias\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_68.png\"\n",
        "alt=\"Slide 68\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 68</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:06](https://youtu.be/qPHsWTZP58U&t=1566s))\n",
        "\n",
        "The slide illustrates **Self-Evaluation Bias**. LLMs tend to rate their\n",
        "own outputs higher than outputs from other models. GPT-4 prefers GPT-4\n",
        "text.\n",
        "\n",
        "To mitigate this, Rajiv suggests mixing models—use Claude to judge\n",
        "GPT-4, or Gemini to judge Claude. This helps ensure a more neutral\n",
        "evaluation.\n",
        "\n",
        "### 69. Alignment Checks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_69.png\"\n",
        "alt=\"Slide 69\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 69</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:46](https://youtu.be/qPHsWTZP58U&t=1606s))\n",
        "\n",
        "This slide reinforces the need for **Continuous Alignment**. Just\n",
        "because your judge aligned with humans last month doesn’t mean it still\n",
        "does (due to model drift).\n",
        "\n",
        "Human spot-checks should be a permanent part of the pipeline to ensure\n",
        "the automated judges haven’t drifted.\n",
        "\n",
        "### 70. Biases in LLM Judges\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_70.png\"\n",
        "alt=\"Slide 70\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 70</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:02](https://youtu.be/qPHsWTZP58U&t=1622s))\n",
        "\n",
        "The slide lists known **Biases in LLM Judges**, such as **Position\n",
        "Bias** (favoring the first answer presented) or **Verbosity Bias**\n",
        "(favoring longer answers).\n",
        "\n",
        "Evaluators must be aware of these. For example, you should shuffle the\n",
        "order of answers when asking a judge to compare two options to cancel\n",
        "out position bias.\n",
        "\n",
        "### 71. Best Practices for LLM Judges\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_71.png\"\n",
        "alt=\"Slide 71\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 71</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:11](https://youtu.be/qPHsWTZP58U&t=1631s))\n",
        "\n",
        "A summary of **Best Practices**: Calibrate with human data, use\n",
        "ensembles (multiple judges), avoid asking for “relevance” (too vague),\n",
        "and use discrete rating scales (1-5) rather than continuous numbers.\n",
        "\n",
        "These tips help stabilize the inherently noisy process of using AI to\n",
        "evaluate AI.\n",
        "\n",
        "### 72. Error Analysis Chart\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_72.png\"\n",
        "alt=\"Slide 72\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 72</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:46](https://youtu.be/qPHsWTZP58U&t=1666s))\n",
        "\n",
        "With tests in place, we move to **Error Analysis**. The bar chart shows\n",
        "the number of failed cases categorized by error type (Length, Tone,\n",
        "Professional, Context).\n",
        "\n",
        "This visualization tells you where to focus your efforts. If “Tone” is\n",
        "the biggest bar, you work on the system prompt’s tone instructions. If\n",
        "“Context” is the issue, you might need better Retrieval Augmented\n",
        "Generation (RAG).\n",
        "\n",
        "### 73. Comparing Prompts\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_73.png\"\n",
        "alt=\"Slide 73\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 73</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:58](https://youtu.be/qPHsWTZP58U&t=1678s))\n",
        "\n",
        "The chart can compare **Prompt A vs. Prompt B**. This allows for A/B\n",
        "testing of prompt engineering strategies.\n",
        "\n",
        "You can see if a new prompt improves “Tone” but accidentally degrades\n",
        "“Context.” This tradeoff analysis is impossible with a single global\n",
        "score.\n",
        "\n",
        "### 74. Explanations Guide Improvement\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_74.png\"\n",
        "alt=\"Slide 74\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 74</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:14](https://youtu.be/qPHsWTZP58U&t=1694s))\n",
        "\n",
        "Rajiv suggests asking the LLM Judge for **Explanations**. Don’t just ask\n",
        "for a score; ask for “one sentence explaining why.”\n",
        "\n",
        "These explanations act as metadata that helps developers understand the\n",
        "judge’s reasoning, making it easier to debug discrepancies between human\n",
        "and AI judgments.\n",
        "\n",
        "### 75. Limits to Explanations\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_75.png\"\n",
        "alt=\"Slide 75\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 75</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:35](https://youtu.be/qPHsWTZP58U&t=1715s))\n",
        "\n",
        "A warning: **Explanations are not causal**. When an LLM explains why it\n",
        "did something, it is generating a plausible justification, not a trace\n",
        "of its actual neural activations.\n",
        "\n",
        "Treat explanations as a heuristic or a helpful hint, not as absolute\n",
        "truth about the model’s internal state.\n",
        "\n",
        "### 76. The Evaluation Flywheel\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_76.png\"\n",
        "alt=\"Slide 76\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 76</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:46](https://youtu.be/qPHsWTZP58U&t=1726s))\n",
        "\n",
        "The **Evaluation Flywheel** describes the iterative cycle: Build Eval\n",
        "-\\> Analyze -\\> Improve -\\> Repeat.\n",
        "\n",
        "This concept, credited to Hamill, emphasizes that evaluation is not a\n",
        "one-time event but a continuous loop that spins faster as you gather\n",
        "more data and build better tests.\n",
        "\n",
        "### 77. Financial Analyst Agent Example\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_77.png\"\n",
        "alt=\"Slide 77\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 77</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:20](https://youtu.be/qPHsWTZP58U&t=1760s))\n",
        "\n",
        "To demonstrate advanced unit testing, Rajiv introduces a **Financial\n",
        "Analyst Agent**. The goal is to assess the specific “style” of a\n",
        "financial report.\n",
        "\n",
        "This is a complex domain where “good” is highly specific (regulated,\n",
        "precise, risk-aware), making it a perfect candidate for granular unit\n",
        "tests.\n",
        "\n",
        "### 78. Use a Global Test?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_78.png\"\n",
        "alt=\"Slide 78\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 78</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:43](https://youtu.be/qPHsWTZP58U&t=1783s))\n",
        "\n",
        "You *could* use a **Global Test**: “Was this explained as a financial\n",
        "analyst would?”\n",
        "\n",
        "While simple, this test is opaque. If it fails, you don’t know if it was\n",
        "because of compliance issues, lack of clarity, or poor formatting.\n",
        "\n",
        "### 79. Global vs. Unit Tests\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_79.png\"\n",
        "alt=\"Slide 79\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 79</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:54](https://youtu.be/qPHsWTZP58U&t=1794s))\n",
        "\n",
        "The slide contrasts the Global approach with **Unit Tests**. Instead of\n",
        "one question, we ask six: Context, Clarity, Precision, Compliance,\n",
        "Actionability, and Risks.\n",
        "\n",
        "This breakdown allows for targeted debugging. You might find the model\n",
        "is great at “Clarity” but terrible at “Compliance.”\n",
        "\n",
        "### 80. Scoring Radar Chart\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_80.png\"\n",
        "alt=\"Slide 80\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 80</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:16](https://youtu.be/qPHsWTZP58U&t=1816s))\n",
        "\n",
        "A **Radar Chart** visualizes the unit test scores. This allows for a\n",
        "quick visual assessment of the model’s profile.\n",
        "\n",
        "It facilitates comparison: you can overlay the profiles of two different\n",
        "models to see which one has the better balance of attributes for your\n",
        "specific needs.\n",
        "\n",
        "### 81. Analyzing Failures with Clusters\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_81.png\"\n",
        "alt=\"Slide 81\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 81</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:37](https://youtu.be/qPHsWTZP58U&t=1837s))\n",
        "\n",
        "With enough unit test data, you can use **Clustering (e.g., K-Means)**\n",
        "to group failures. The slide shows clusters like “Synthesis,” “Context,”\n",
        "and “Hallucination.”\n",
        "\n",
        "This moves error analysis from reading individual logs to analyzing\n",
        "aggregate trends, helping you prioritize which class of errors to fix\n",
        "first.\n",
        "\n",
        "### 82. Designing Good Unit Tests\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_82.png\"\n",
        "alt=\"Slide 82\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 82</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:52](https://youtu.be/qPHsWTZP58U&t=1852s))\n",
        "\n",
        "Advice on **Designing Unit Tests**: Keep them focused (one concept per\n",
        "test), use unambiguous language, and use small rating ranges.\n",
        "\n",
        "Good unit tests are the building blocks of a reliable evaluation\n",
        "pipeline. If the tests themselves are noisy or vague, the entire system\n",
        "collapses.\n",
        "\n",
        "### 83. Examples of Unit Tests\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_83.png\"\n",
        "alt=\"Slide 83\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 83</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:55](https://youtu.be/qPHsWTZP58U&t=1855s))\n",
        "\n",
        "The slide lists specific examples of tests for **Legal** (Compliance,\n",
        "Terminology), **Retrieval** (Relevance, Completeness), and\n",
        "**Bias/Fairness**.\n",
        "\n",
        "This serves as a menu of options for the audience, showing that unit\n",
        "tests can cover almost any dimension of quality required by the\n",
        "business.\n",
        "\n",
        "### 84. Evaluating New Prompts\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_84.png\"\n",
        "alt=\"Slide 84\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 84</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:58](https://youtu.be/qPHsWTZP58U&t=1858s))\n",
        "\n",
        "A bar chart shows how unit tests are used to **Evaluate New Prompts**.\n",
        "By running the full suite of unit tests on a new prompt, you get a\n",
        "“scorecard” of its performance.\n",
        "\n",
        "This data-driven approach removes the guesswork from prompt engineering.\n",
        "\n",
        "### 85. Tools - No Silver Bullet\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_85.png\"\n",
        "alt=\"Slide 85\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 85</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:02](https://youtu.be/qPHsWTZP58U&t=1862s))\n",
        "\n",
        "Rajiv reminds the audience that **Tools are No Silver Bullet**. You must\n",
        "master the basics (datasets, metrics) first.\n",
        "\n",
        "He advises logging traces and experiments and practicing **Dataset\n",
        "Versioning**. Tools facilitate these practices, but they cannot replace\n",
        "the fundamental engineering discipline.\n",
        "\n",
        "### 86. Forest and Trees\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_86.png\"\n",
        "alt=\"Slide 86\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 86</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:04](https://youtu.be/qPHsWTZP58U&t=1864s))\n",
        "\n",
        "An analogy helps structure the analysis: **Forest (Global/Integration)**\n",
        "vs. **Trees (Test Case/Unit Tests)**.\n",
        "\n",
        "You need to look at both. The forest tells you the overall health of the\n",
        "app, while the trees tell you specifically what needs pruning or fixing.\n",
        "\n",
        "### 87. Change One Thing at a Time\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_87.png\"\n",
        "alt=\"Slide 87\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 87</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:17](https://youtu.be/qPHsWTZP58U&t=1877s))\n",
        "\n",
        "A crucial scientific principle: **Change One Thing at a Time**. With so\n",
        "many knobs (prompt, temp, model, RAG settings), changing multiple\n",
        "variables simultaneously makes it impossible to know what caused the\n",
        "improvement (or regression).\n",
        "\n",
        "Isolate your variables to conduct valid experiments.\n",
        "\n",
        "### 88. Error Analysis Tips\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_88.png\"\n",
        "alt=\"Slide 88\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 88</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 31:32](https://youtu.be/qPHsWTZP58U&t=1892s))\n",
        "\n",
        "A summary of **Error Analysis Tips**: Use ablation studies (removing\n",
        "parts to see impact), categorize failures, save interesting examples,\n",
        "and leverage logs/traces.\n",
        "\n",
        "These are the daily habits of successful GenAI engineers.\n",
        "\n",
        "### 89. The Evaluation Story\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_89.png\"\n",
        "alt=\"Slide 89\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 89</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:08](https://youtu.be/qPHsWTZP58U&t=1928s))\n",
        "\n",
        "The slide shows the “Story We Tell”—a linear graph of improvement over\n",
        "time. This is the idealized version of progress often presented in case\n",
        "studies.\n",
        "\n",
        "It suggests a smooth journey from “Out of the box” to “Specialized” to\n",
        "“User Feedback.”\n",
        "\n",
        "### 90. The Reality of Progress\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_90.png\"\n",
        "alt=\"Slide 90\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 90</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 32:24](https://youtu.be/qPHsWTZP58U&t=1944s))\n",
        "\n",
        "**The Reality** is a messy, non-linear graph. You take two steps\n",
        "forward, one step back. Sometimes an “improvement” breaks the model.\n",
        "\n",
        "Rajiv encourages resilience. Experienced practitioners know that this\n",
        "messy graph is normal and that sticking to the process eventually yields\n",
        "results.\n",
        "\n",
        "### 91. Continual Process\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_91.png\"\n",
        "alt=\"Slide 91\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 91</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 33:01](https://youtu.be/qPHsWTZP58U&t=1981s))\n",
        "\n",
        "**Evaluation is a Continual Process**. It involves Problem ID, Data\n",
        "Collection, Optimization, User Acceptance Testing (UAT), and Updates.\n",
        "\n",
        "Crucially, **UAT** is your holdout set. Since you don’t have a\n",
        "traditional test set in GenAI, your real users act as the final\n",
        "validation layer.\n",
        "\n",
        "### 92. Eating the Elephant\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_92.png\"\n",
        "alt=\"Slide 92\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 92</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:03](https://youtu.be/qPHsWTZP58U&t=2043s))\n",
        "\n",
        "The metaphor **“How do you eat an elephant?”** addresses the\n",
        "overwhelming nature of building a comprehensive evaluation suite.\n",
        "\n",
        "The answer, of course, is “one bite at a time.” You don’t need 100 tests\n",
        "on day one.\n",
        "\n",
        "### 93. Adding Tests Over Time\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_93.png\"\n",
        "alt=\"Slide 93\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 93</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:10](https://youtu.be/qPHsWTZP58U&t=2050s))\n",
        "\n",
        "The slide visualizes the “elephant” being broken down into bites. You\n",
        "start with a few critical tests. As the app matures and you discover new\n",
        "failure modes, you add more tests.\n",
        "\n",
        "Six months in, you might have 100 tests, but you built them\n",
        "incrementally. This makes the task manageable.\n",
        "\n",
        "### 94. Doing Evaluation the Right Way\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_94.png\"\n",
        "alt=\"Slide 94\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 94</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:39](https://youtu.be/qPHsWTZP58U&t=2079s))\n",
        "\n",
        "A summary slide listing best practices: **Annotated Examples**,\n",
        "**Systematic Documentation**, **Continuous Error Analysis**,\n",
        "**Collaboration**, and awareness of **Generalization**.\n",
        "\n",
        "This concludes the core methodology section of the talk.\n",
        "\n",
        "### 95. Agentic Use Cases\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_95.png\"\n",
        "alt=\"Slide 95\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 95</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:50](https://youtu.be/qPHsWTZP58U&t=2090s))\n",
        "\n",
        "The final section covers **Agentic Use Cases**, symbolized by a dragon.\n",
        "Agents add a layer of complexity because the model is now making\n",
        "decisions (routing, tool use) rather than just generating text.\n",
        "\n",
        "This “agency” makes the system harder to track and evaluate.\n",
        "\n",
        "### 96. Crossing the River\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_96.png\"\n",
        "alt=\"Slide 96\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 96</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 35:06](https://youtu.be/qPHsWTZP58U&t=2106s))\n",
        "\n",
        "A conceptual slide asking, **“How should it cross the river?”** (Fly,\n",
        "Swim, Bridge?). This represents the decision-making step in an agent.\n",
        "\n",
        "Evaluating an agent requires evaluating *how* it made the decision (the\n",
        "router) separately from *how well* it executed the action.\n",
        "\n",
        "### 97. Chat-to-Purchase Router\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_97.png\"\n",
        "alt=\"Slide 97\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 97</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 35:22](https://youtu.be/qPHsWTZP58U&t=2122s))\n",
        "\n",
        "A complex flowchart shows a **Chat-to-Purchase Router**. The agent must\n",
        "decide if the user wants to search for a product, get support, or track\n",
        "a package.\n",
        "\n",
        "Rajiv suggests breaking this down: evaluate the **Router** component\n",
        "first (did it pick the right path?), then evaluate the specific workflow\n",
        "(did it track the package correctly?).\n",
        "\n",
        "### 98. Text to SQL Agent\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_98.png\"\n",
        "alt=\"Slide 98\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 98</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 36:17](https://youtu.be/qPHsWTZP58U&t=2177s))\n",
        "\n",
        "Another example: **Text to SQL Agent**. This workflow involves\n",
        "classification, feature extraction, and SQL generation.\n",
        "\n",
        "You can isolate the “Classification” step (is this a valid SQL\n",
        "question?) and build a test just for that, before testing the actual SQL\n",
        "generation.\n",
        "\n",
        "### 99. Evaluating Office-Style Agents\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_99.png\"\n",
        "alt=\"Slide 99\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 99</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 36:46](https://youtu.be/qPHsWTZP58U&t=2206s))\n",
        "\n",
        "The slide discusses **OdysseyBench**, a benchmark for office tasks. It\n",
        "highlights failure modes like “Failed to create folder” or “Failed to\n",
        "use tool.”\n",
        "\n",
        "Evaluating agents involves checking if they successfully manipulated the\n",
        "environment (files, APIs), which is a functional test rather than a text\n",
        "similarity test.\n",
        "\n",
        "### 100. Error Analysis for Agents\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_100.png\"\n",
        "alt=\"Slide 100\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 100</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 37:00](https://youtu.be/qPHsWTZP58U&t=2220s))\n",
        "\n",
        "**Error Analysis for Agentic Workflows** requires assessing the overall\n",
        "performance, the routing decisions, and the individual steps.\n",
        "\n",
        "It is the same “action error analysis” process but applied recursively\n",
        "to every node in the agent’s decision tree.\n",
        "\n",
        "### 101. Evaluating Workflow vs. Response\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_101.png\"\n",
        "alt=\"Slide 101\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 101</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 37:19](https://youtu.be/qPHsWTZP58U&t=2239s))\n",
        "\n",
        "This slide distinguishes between evaluating a **Response** (text) and a\n",
        "**Workflow** (process). The flowchart shows a conversational flow.\n",
        "\n",
        "Evaluating a workflow might mean checking if the agent successfully\n",
        "moved the user from “Greeting” to “Resolution,” regardless of the exact\n",
        "words used.\n",
        "\n",
        "### 102. Agentic Frameworks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_102.png\"\n",
        "alt=\"Slide 102\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 102</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 37:48](https://youtu.be/qPHsWTZP58U&t=2268s))\n",
        "\n",
        "Rajiv warns that **“Agentic Frameworks Help – Until They Don’t.”**\n",
        "Frameworks (like LangChain or AutoGen) are great for demos because they\n",
        "abstract complexity.\n",
        "\n",
        "However, in production, these abstractions can break or become outdated.\n",
        "He often recommends using straight Python for production agents to\n",
        "maintain control and reliability.\n",
        "\n",
        "### 103. Abstraction for Workflows\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_103.png\"\n",
        "alt=\"Slide 103\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 103</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 38:32](https://youtu.be/qPHsWTZP58U&t=2312s))\n",
        "\n",
        "The slide illustrates the trade-off in **Abstraction**. You can build\n",
        "rigid workflows (orchestration) where you control every step, or use\n",
        "general agents where the LLM decides.\n",
        "\n",
        "Orchestration is more reliable but rigid. General agents are flexible\n",
        "but prone to non-deterministic errors.\n",
        "\n",
        "### 104. When Abstractions Break\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_104.png\"\n",
        "alt=\"Slide 104\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 104</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 38:53](https://youtu.be/qPHsWTZP58U&t=2333s))\n",
        "\n",
        "Model providers are training models to handle workflows internally\n",
        "(removing the need for external orchestration).\n",
        "\n",
        "However, until models are perfect, developers often need to break tasks\n",
        "down into specific pieces to ensure reliability. The choice between\n",
        "“letting the model do it” and “scripting the flow” depends on the\n",
        "application’s risk tolerance.\n",
        "\n",
        "### 105. Lessons from Agent Benchmarks\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_105.png\"\n",
        "alt=\"Slide 105\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 105</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 39:15](https://youtu.be/qPHsWTZP58U&t=2355s))\n",
        "\n",
        "The slide lists **Lessons from Reproducing Agent Benchmarks**:\n",
        "Standardize evaluation, measure efficiency, detect shortcuts, and log\n",
        "real behavior.\n",
        "\n",
        "These are advanced tips for those pushing the boundaries of what agents\n",
        "can do.\n",
        "\n",
        "### 106. Conclusion\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/genai-evaluation-guide/slide_106.png\"\n",
        "alt=\"Slide 106\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 106</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 39:27](https://youtu.be/qPHsWTZP58U&t=2367s))\n",
        "\n",
        "The final slide, **“We did it!”**, concludes the presentation. Rajiv\n",
        "thanks the audience and provides the QR code again.\n",
        "\n",
        "His final message is one of empowerment: he hopes the audience now has\n",
        "the confidence to go out, build their own evaluation datasets, and start\n",
        "“hill climbing” their own applications.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "*This annotated presentation was generated from the talk using\n",
        "AI-assisted tools. Each slide includes timestamps and detailed\n",
        "explanations.*"
      ],
      "id": "b6fefd00-a085-48f7-9543-b4229dc70156"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}