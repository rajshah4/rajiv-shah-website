{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Unlabeled Data to Label Data\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg\"\n",
        "alt=\"https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg\" />\n",
        "<figcaption\n",
        "aria-hidden=\"true\">https://hips.hearstapps.com/cosmouk.cdnds.net/14/38/nrm_1410777104-jul12-coveteurclueless.jpg</figcaption>\n",
        "</figure>\n",
        "\n",
        "Your boss hands you a pile of a 100,000 unlabeled images and asks you to\n",
        "categorize whether they are sandals, pants, boots, etc.\n",
        "\n",
        "So now you have a massive set of unlabeled data and you need labels.\n",
        "**What should you do?**\n",
        "\n",
        "This problem is commonplace. Lots of companies are swimming with data,\n",
        "whether its transactional, IoT sensors, security logs, images, voice, or\n",
        "more, and its all unlabeled. With so little labeled data, it is a\n",
        "tedious and slow process for data scientists to build machine learning\n",
        "models in ~~most~~ all enterprises.\n",
        "\n",
        "Take Google’s street view data. [Gebru had to figure out how to label\n",
        "cars](https://ai.stanford.edu/~tgebru/papers/pnas.pdf) in 50 million\n",
        "images with very little labeled data. Over at Facebook, they used\n",
        "algorithms [to label half a million\n",
        "videos](https://arxiv.org/abs/1712.09374), a task that would have\n",
        "otherwise taken 16 years.\n",
        "\n",
        "This post shows you how to **label hundreds of thousands of images in an\n",
        "afternoon**. You can use the same approach whether you are labeling\n",
        "images or labeling traditional tabular data (e.g, identifying cyber\n",
        "security atacks or potential part failures.)\n",
        "\n",
        "### The Manual Method\n",
        "\n",
        "For most data scientists when asked to do something, the first step is\n",
        "to calculate who else should do this.\n",
        "\n",
        "<figure>\n",
        "<img src=\"http://vni.s3.amazonaws.com/130905163633604.jpg\"\n",
        "alt=\"http://vni.s3.amazonaws.com/130905163633604.jpg\" />\n",
        "<figcaption\n",
        "aria-hidden=\"true\">http://vni.s3.amazonaws.com/130905163633604.jpg</figcaption>\n",
        "</figure>\n",
        "\n",
        "But 100,000 images could cost you at least \\$30,000 on [Mechanical\n",
        "Turk](https://cloudacademy.com/blog/machine-learning-datasets-mechanical-turk/)\n",
        "or some other competitor. Your boss expects this done cheaply, since\n",
        "after all, they hired you because you use free software. Now, she\n",
        "doesn’t budget for anything other than your salary (if you don’t believe\n",
        "me, ask to go to [pydata](https://pydata.org/)).\n",
        "\n",
        "You take a deep breath and figure you can probably label 200 images in\n",
        "an hour. So that means in three weeks of non stop work, you can get this\n",
        "done!! Yikes!\n",
        "\n",
        "### Just Build a Model\n",
        "\n",
        "The first idea is to label a handful of the images, train a machine\n",
        "learning algorithm, and then predict the remaining set of labels. For\n",
        "this exercise, I am using the\n",
        "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
        "dataset (you could also make your own [using\n",
        "quickdraw](https://rajivshah.com/blog/blog/2017/07/14/QuickDraw/)).\n",
        "There are ten classes of images to identify and here is a sample of what\n",
        "they look like:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://pbs.twimg.com/media/DJNuE7BWAAAo3eC.jpg\"\n",
        "alt=\"https://pbs.twimg.com/media/DJNuE7BWAAAo3eC.jpg\" />\n",
        "<figcaption\n",
        "aria-hidden=\"true\">https://pbs.twimg.com/media/DJNuE7BWAAAo3eC.jpg</figcaption>\n",
        "</figure>\n",
        "\n",
        "I like this dataset, because each image is 28 by 28 pixels, which means\n",
        "it contains 784 unique features/variables. For a blog post this works\n",
        "great, but its also not like any datasets you see in the real world,\n",
        "which are often either much narrower (traditional tabluar business\n",
        "problem datasets) or much wider (real images are much bigger and include\n",
        "color).\n",
        "\n",
        "I built models using the most common data science algorithms: logistic\n",
        "regression, support vector machines (SVM), random forest and gradient\n",
        "boosted machines (GBM).\n",
        "\n",
        "I evaluated the performance based on labeling 100, 200, 500, 1000, and\n",
        "2000 images.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/All.png\" alt=\"All\" />\n",
        "<figcaption aria-hidden=\"true\">All</figcaption>\n",
        "</figure>\n",
        "\n",
        "At this point in the post, if you are still with me, slow down and mull\n",
        "this graph over. There is a lot of good stuff here. **Which algorithm\n",
        "does the best?** (If you a data scientist, you shouldn’t fall for that\n",
        "question.) It really depends on the context.\n",
        "\n",
        "You want something quick and dependable out of the box, you could go for\n",
        "the logistic regression. While the random forest starts way ahead, the\n",
        "SVM is coming on fast. If we had more labeled data the SVM would pass\n",
        "the random forest. And the GBM works great, but can take a bit of work\n",
        "to perform their best. The scores here are using out of the box\n",
        "implementations in R (e1071, randomForest, gbm, nnet).\n",
        "\n",
        "If our benchmark is 80% accuracy for ten classes of images, we could get\n",
        "there by building a Random Forest model with 1000 images. But 1000\n",
        "images is still a lot of data to label, 5 hours by my estimate. Lets\n",
        "think about ways we can improve.\n",
        "\n",
        "### Let’s Think About Data\n",
        "\n",
        "After a little reflection, you remember what you often tell others —\n",
        "**that data isn’t random, but has patterns**. By taking advantage of\n",
        "these patterns we can get insight in our data.\n",
        "\n",
        "Lets start with an autoencoder (AE). An autoencoder squeezes and\n",
        "compresses your data, kind of like turning soup into a bouillon cube.\n",
        "Autoencoders are the hipster’s Principle Component Analysis (PCA) ,\n",
        "since they support nonlinear transformations.\n",
        "\n",
        "Effectively this means we are taking our wide data (784\n",
        "features/variables) reducing it down to 128 features. We then take this\n",
        "new compressed data and train our machine learning algorithm (SVM in\n",
        "this case). The graph below shows the difference in performance between\n",
        "an SVM fed with an autoencoder (AE_SVM) versus the SVM on the raw data.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/AE.png\" alt=\"AE\" />\n",
        "<figcaption aria-hidden=\"true\">AE</figcaption>\n",
        "</figure>\n",
        "\n",
        "By squeezing the information down to 128 features, we were able to\n",
        "actually improve the performance of the SVM algorithm at the low end. At\n",
        "the 100 labels mark, accuracy went from 44% to 59%. At the 1000 labels\n",
        "mark, the autoencoder was still helping, we see an improvement from 74%\n",
        "to 78%. So we are on to something here. We just need think a bit more\n",
        "about the distribution and patterns in our data that we can take\n",
        "advantage of.\n",
        "\n",
        "### Thinking Deeper About Your Data\n",
        "\n",
        "We know that our data are images and since 2012, the **hammer for images\n",
        "is a convolutional neural network (CNN)**. There are a couple of ways we\n",
        "could use a CNN, from a pretrained network or as a simple model to\n",
        "pre-process the images. For this post, I am going to use a Convolutional\n",
        "Variational Autoencoder as a path towards the technique by [Kingma for\n",
        "semi-supervised learning](https://arxiv.org/abs/1406.5298).\n",
        "\n",
        "So lets build a Convolutional Variational Autoencoder (CVAE). The leap\n",
        "here is twofold. First, “variational” means the autoenconder compress\n",
        "the information down into a probability distribution. Second is the\n",
        "addition of using a convolutional neural networks as an encoder. This is\n",
        "a bit of deep learning, but the emphasis here is on how we are solving\n",
        "the problem, not the latest shiny toy.\n",
        "\n",
        "For coding my CVAE, I used the example CVAE from the list of [examples\n",
        "over at RStudio’s Keras\n",
        "page](https://keras.rstudio.com/articles/examples/variational_autoencoder_deconv.html).\n",
        "Like the previous autoencoder, we design the latent space to reduce the\n",
        "data to 128 features. We then use this new data to train an SVM model.\n",
        "Below is a plot of the performance of the CVAE as compared to the SVM\n",
        "and RandomForest on the raw data.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/CVAE.png\" alt=\"CVAE\" />\n",
        "<figcaption aria-hidden=\"true\">CVAE</figcaption>\n",
        "</figure>\n",
        "\n",
        "Wow! The new model is much more accurate. We can **get well past 80%\n",
        "accuracy with just 500 labels**. By using these techniques we get better\n",
        "performance and require less labelled images! At the top end, we can\n",
        "also do much better than the RandomForest or SVM model.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "By using some very simple semi-supervised techniques with autoencoders,\n",
        "its possible to quickly and accurately label data. But the takeaway is\n",
        "not to use deep learning auto encoders! Instead, I hope you understand\n",
        "the methodology here of starting very simple and then trying gradually\n",
        "more complex solutions. Don’t fall for the latest shiny toy — pratical\n",
        "data science is not about using the latest approaches found in arxiv.\n",
        "\n",
        "If this idea of semi-supervised learning inspires you, this post is the\n",
        "logistic regression of semi-supervised learning. If you want to dig\n",
        "further into Semi-Supervised Learning and Domain Adaptation, check out\n",
        "Brian Keng’s [great walkthrough of using variational\n",
        "autoencoders](http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/)\n",
        "(which goes beyond what we have done here) or the work of [Curious\n",
        "AI](https://thecuriousaicompany.com/), which has been advancing\n",
        "semi-supervised learning using deep learning and [sharing their\n",
        "code](https://github.com/CuriousAI). But at the very least, don’t\n",
        "reflexively think all your data has to be hand labeled."
      ],
      "id": "ba8458de-c261-4e12-9747-e100f539754d"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}