{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Interpretability and Explainability for Machine Learning Models\n",
        "\n",
        "## Video\n",
        "\n",
        "<https://youtu.be/ZRckw_fE56Q>\n",
        "\n",
        "Watch the [full video](https://youtu.be/ZRckw_fE56Q)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Annotated Presentation\n",
        "\n",
        "Below is an annotated version of the presentation, with timestamped\n",
        "links to the relevant parts of the video for each slide.\n",
        "\n",
        "Here is the slide-by-slide annotated presentation based on the technical\n",
        "talk “A Quest for Interpretability.”\n",
        "\n",
        "### 1. A Quest for Interpretability\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_1.png\"\n",
        "alt=\"Slide 1\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 1</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:00](https://youtu.be/ZRckw_fE56Q&t=0s))\n",
        "\n",
        "The presentation opens with the title slide, introducing the core\n",
        "mission of the talk: demystifying machine learning models. The speaker\n",
        "sets the stage for both data science novices and experts, promising to\n",
        "provide methods to “ask any particular machine learning model you see\n",
        "and be able to explain it.”\n",
        "\n",
        "The goal is to move beyond simply generating predictions to\n",
        "understanding the “why” behind them. The speaker emphasizes that whether\n",
        "you are new to the field or comfortable with interpretability, the\n",
        "session will dive deeper into techniques that provide transparency to\n",
        "complex algorithms.\n",
        "\n",
        "### 2. Predictive Model Around Aggression\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_2.png\"\n",
        "alt=\"Slide 2\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 2</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 00:41](https://youtu.be/ZRckw_fE56Q&t=41s))\n",
        "\n",
        "To make the concepts more engaging, the speaker introduces a “Dragon\n",
        "theme” as a visual metaphor. The hypothetical problem presented is\n",
        "building a **Predictive Model Around Aggression**. The objective is\n",
        "practical and dire: “we want to use machine learning to help us figure\n",
        "out which dragons are likely to eat us.”\n",
        "\n",
        "This metaphor serves as a stand-in for real-world risk assessment\n",
        "models. Instead of dry financial or medical data initially, the audience\n",
        "is asked to consider the stakes of a model that must accurately predict\n",
        "danger (getting eaten) based on various dragon attributes.\n",
        "\n",
        "### 3. Trust: The Big Picture\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_3.png\"\n",
        "alt=\"Slide 3\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 3</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 01:28](https://youtu.be/ZRckw_fE56Q&t=88s))\n",
        "\n",
        "The speaker broadens the scope to explain that **interpretability** is\n",
        "just one component of a much larger ecosystem called “Trust.” This slide\n",
        "illustrates that trusting a model involves asking questions about bias,\n",
        "correctness, ethical purposes (like facial recognition debates), and\n",
        "model health over time.\n",
        "\n",
        "While acknowledging these critical factors—such as “is your data biased”\n",
        "or “is your model being used for an ethical purpose”—the speaker\n",
        "clarifies that this specific presentation will focus on the\n",
        "interpretability slice of the pie: “can we explain what’s going on…\n",
        "inside that model.”\n",
        "\n",
        "### 4. Interpretable Predictive Model Around Aggression\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_4.png\"\n",
        "alt=\"Slide 4\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 4</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 02:28](https://youtu.be/ZRckw_fE56Q&t=148s))\n",
        "\n",
        "Returning to the dragon metaphor, this slide reiterates the specific\n",
        "technical goal: building an **Interpretable Predictive Model Around\n",
        "Aggression**. The speaker distinguishes this from simply dumping data\n",
        "into a “black box” like TensorFlow and deploying it based solely on\n",
        "performance metrics.\n",
        "\n",
        "The focus here is on the deliberate choice to build a model that is not\n",
        "just predictive, but understandable. This sets up the central tension of\n",
        "the talk: the trade-off between model complexity (accuracy) and the\n",
        "ability to explain how the model works.\n",
        "\n",
        "### 5. Why Interpretability?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_5.png\"\n",
        "alt=\"Slide 5\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 5</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 02:38](https://youtu.be/ZRckw_fE56Q&t=158s))\n",
        "\n",
        "This slide outlines the three key audiences for interpretability. First,\n",
        "for **Yourself**: debugging is essential because “it’s very easy for\n",
        "things to go wrong.” Second, for **Stakeholders**: managers and bosses\n",
        "will demand to know how a model works, regardless of how high the AUC\n",
        "(Area Under the Curve) is.\n",
        "\n",
        "Third, the speaker highlights **Regulators** in high-risk industries\n",
        "like insurance, finance, and healthcare. In these sectors, there is a\n",
        "“higher standard set” where you must prove you understand the model’s\n",
        "behavior to mitigate risks to the financial system or public health.\n",
        "\n",
        "### 6. An Understandable White Box Model (CLEAR-2)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_6.png\"\n",
        "alt=\"Slide 6\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 6</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 04:21](https://youtu.be/ZRckw_fE56Q&t=261s))\n",
        "\n",
        "The presentation begins with the “simplest, easiest, most interpretable\n",
        "model”: a linear regression for housing prices. This **White Box Model**\n",
        "uses only two features: the number of bathrooms and square footage.\n",
        "\n",
        "The transparency is total; you can see the coefficients directly (e.g.,\n",
        "multiplying bathrooms by a value). The audience is asked to confirm that\n",
        "this is intuitive, and the consensus is that yes, this is an easily\n",
        "explainable model where the inputs have a clear, logical relationship to\n",
        "the output.\n",
        "\n",
        "### 7. White Box Model (CLEAR-8)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_7.png\"\n",
        "alt=\"Slide 7\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 7</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 05:41](https://youtu.be/ZRckw_fE56Q&t=341s))\n",
        "\n",
        "Complexity is introduced by adding more features to improve accuracy.\n",
        "However, this slide reveals a paradox of linear models:\n",
        "**Multicollinearity**. The speaker points out that while the model might\n",
        "be “transparent” (you can see the math), the logic breaks down.\n",
        "\n",
        "Specifically, the model shows that “as the total rooms gets higher, the\n",
        "value of my house goes down.” This counter-intuitive finding occurs\n",
        "because features are not independent. While technically a “white box,”\n",
        "the interpretability suffers because the coefficients no longer align\n",
        "with human intuition due to correlations between variables.\n",
        "\n",
        "### 8. Understandable White Box Model? (Tree - AUC 0.74)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_8.png\"\n",
        "alt=\"Slide 8\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 8</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 09:16](https://youtu.be/ZRckw_fE56Q&t=556s))\n",
        "\n",
        "Moving to **Decision Trees**, the speaker presents a simple tree based\n",
        "on the Titanic dataset (predicting survival). With only two features\n",
        "(gender and age), the logic is stark and easy to follow: “if you’re a\n",
        "male and your age is greater than 10 years old… chance of survival is\n",
        "very low.”\n",
        "\n",
        "This model has an AUC of 0.74. It is highly interpretable, acting as a\n",
        "flowchart that anyone can trace. However, the speaker hints at the\n",
        "limitation: simplicity often comes at the cost of accuracy.\n",
        "\n",
        "### 9. Understandable White Box Model? (Tree - AUC 0.78)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_9.png\"\n",
        "alt=\"Slide 9\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 9</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 11:19](https://youtu.be/ZRckw_fE56Q&t=679s))\n",
        "\n",
        "To improve the model, more features are added, raising the AUC to 0.78.\n",
        "The tree grows branches, becoming visually more cluttered. The speaker\n",
        "notes that “by adding more features or variables… the performance of our\n",
        "model increases.”\n",
        "\n",
        "This slide represents the tipping point where the visual representation\n",
        "of the model starts to become less of a helpful flowchart and more of a\n",
        "complex web, though it is still technically possible to trace a single\n",
        "path.\n",
        "\n",
        "### 10. Understandable White Box Model? (Tree - AUC 0.79)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_10.png\"\n",
        "alt=\"Slide 10\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 10</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 11:38](https://youtu.be/ZRckw_fE56Q&t=698s))\n",
        "\n",
        "The optimization continues, pushing the AUC to 0.79. The tree on the\n",
        "slide is now dense and difficult to read. The question mark in the title\n",
        "“Understandable White Box Model?” becomes more relevant.\n",
        "\n",
        "The speaker emphasizes that data scientists “don’t have to kind of stop\n",
        "there.” The drive for higher accuracy encourages adding more depth and\n",
        "complexity to the tree, sacrificing the immediate “glance-value”\n",
        "interpretability that smaller trees possess.\n",
        "\n",
        "### 11. Better Performance but too much to Comprehend (AUC 0.81)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_11.png\"\n",
        "alt=\"Slide 11\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 11</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 11:44](https://youtu.be/ZRckw_fE56Q&t=704s))\n",
        "\n",
        "This slide shows a massive, unreadable decision tree with an AUC of\n",
        "0.81. The speaker notes, “it gets a little tricky… lot harder to\n",
        "understand what’s going on.” This illustrates the “Black Box” problem\n",
        "even within models considered interpretable.\n",
        "\n",
        "Furthermore, the speaker points out that data scientists rarely stop at\n",
        "one tree; they use **Random Forests** (collections of trees).\n",
        "Interpreting a forest by looking at the trees is impossible,\n",
        "necessitating new tools for explanation.\n",
        "\n",
        "### 12. So Many Algorithms to Try\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_12.png\"\n",
        "alt=\"Slide 12\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 12</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 13:07](https://youtu.be/ZRckw_fE56Q&t=787s))\n",
        "\n",
        "This heatmap, derived from a study by Randy Olssen, visualizes the\n",
        "performance of different algorithms across 165 datasets. It illustrates\n",
        "the **No Free Lunch Theorem**: there is not one single algorithm that\n",
        "always works best.\n",
        "\n",
        "Because of this, data scientists must try various complex algorithms\n",
        "(Gradient Boosting, Neural Networks, Ensembles) to find the best\n",
        "solution. We cannot simply restrict ourselves to linear regression just\n",
        "for the sake of interpretability if it fails to solve the problem.\n",
        "\n",
        "### 13. Algorithms Matter\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_13.png\"\n",
        "alt=\"Slide 13\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 13</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 13:49](https://youtu.be/ZRckw_fE56Q&t=829s))\n",
        "\n",
        "The speaker reinforces that model choice is critical. Using a simple\n",
        "model that yields inaccurate predictions is dangerous: “if we can’t\n",
        "figure out if this model is going to work or not we’re in trouble.”\n",
        "\n",
        "The slide emphasizes that accuracy is paramount (“we are toast” if we\n",
        "are wrong). Therefore, we need methods that allow us to use complex,\n",
        "accurate algorithms without flying blind regarding how they work.\n",
        "\n",
        "### 14. Simple Models != Accurate\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_14.png\"\n",
        "alt=\"Slide 14\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 14</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 14:11](https://youtu.be/ZRckw_fE56Q&t=851s))\n",
        "\n",
        "This slide counters the argument that we should only use simple models.\n",
        "The speaker asserts, “most simple models are just not very accurate.”\n",
        "Real-world problems are complex, and if they could be solved with a few\n",
        "simple rules, machine learning wouldn’t be necessary.\n",
        "\n",
        "Resources are provided on the slide for further reading, including\n",
        "defenses of black box models. The takeaway is that complexity is often a\n",
        "requirement for accuracy, so we must find ways to explain complex models\n",
        "rather than avoiding them.\n",
        "\n",
        "### 15. Tools That Can Explain Any Black Box Model\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_15.png\"\n",
        "alt=\"Slide 15\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 15</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 14:59](https://youtu.be/ZRckw_fE56Q&t=899s))\n",
        "\n",
        "This is the pivot point of the presentation. The speaker introduces the\n",
        "solution: “There are tools here that can explain any blackbox model.”\n",
        "This promises a methodology that is **Model Agnostic**—meaning it works\n",
        "regardless of whether you are using a Random Forest, a Neural Network,\n",
        "or an SVM.\n",
        "\n",
        "### 16. Model Agnostic Explanation Tools\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_16.png\"\n",
        "alt=\"Slide 16\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 16</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 15:07](https://youtu.be/ZRckw_fE56Q&t=907s))\n",
        "\n",
        "The speaker outlines the three specific pillars of interpretability that\n",
        "the rest of the talk will cover: 1. **Feature Importance:**\n",
        "Understanding what variables are most impactful. 2. **Partial\n",
        "Dependence:** Understanding the directionality of features (e.g., does\n",
        "age increase or decrease risk?). 3. **Prediction Explanations:**\n",
        "Explaining why a specific prediction was made for a specific individual\n",
        "(using techniques like SHAP).\n",
        "\n",
        "### 17. Feature Importance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_17.png\"\n",
        "alt=\"Slide 17\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 17</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 16:11](https://youtu.be/ZRckw_fE56Q&t=971s))\n",
        "\n",
        "The first pillar is **Feature Importance**. Returning to the dragon\n",
        "example, the speaker discusses the data collection process: asking\n",
        "domain experts (or watching Game of Thrones) to determine factors like\n",
        "age, weight, or number of children.\n",
        "\n",
        "The goal is to determine which of these collected variables actually\n",
        "drives the model. This is crucial for debugging, feature selection, and\n",
        "explaining the model to stakeholders.\n",
        "\n",
        "### 18. Dragon Reading: Milk vs. Age\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_18.png\"\n",
        "alt=\"Slide 18\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 18</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 17:41](https://youtu.be/ZRckw_fE56Q&t=1061s))\n",
        "\n",
        "To illustrate the pitfalls of feature importance, the speaker introduces\n",
        "a new scenario: “how dragons learn to read.” We intuitively know that\n",
        "**Age** affects reading ability (older children read better).\n",
        "\n",
        "The speaker then asks about **Milk Consumption**. While one might guess\n",
        "milk helps (calcium), the reality is that milk consumption is negatively\n",
        "correlated with age (babies drink milk, teenagers don’t). Therefore,\n",
        "milk consumption appears related to reading ability, but it is a\n",
        "**spurious correlation**. It has “nothing at all to do with the ability\n",
        "to read,” yet the data might suggest otherwise.\n",
        "\n",
        "### 19. Split Based Variable Importance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_19.png\"\n",
        "alt=\"Slide 19\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 19</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 20:39](https://youtu.be/ZRckw_fE56Q&t=1239s))\n",
        "\n",
        "This slide shows what happens when you use the default “Split Based”\n",
        "importance metric in algorithms like LightGBM. The chart shows\n",
        "**milk_consumption** as the *most* important feature, ranking higher\n",
        "than age.\n",
        "\n",
        "This happens because the model uses milk consumption as a proxy for age\n",
        "during the tree-splitting process. The speaker warns that relying on\n",
        "default metrics can lead to incorrect conclusions where spurious\n",
        "correlations mask the true drivers of the model.\n",
        "\n",
        "### 20. Permutation Based Variable Importance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_20.png\"\n",
        "alt=\"Slide 20\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 20</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:11](https://youtu.be/ZRckw_fE56Q&t=1271s))\n",
        "\n",
        "By switching to a **Permutation Based** approach, the chart flips. Now,\n",
        "**Age** is correctly identified as the dominant feature, and milk\n",
        "consumption drops to near zero importance.\n",
        "\n",
        "The speaker emphasizes that this technique “cuts right through” the\n",
        "noise. It correctly identifies that while milk varies with age, it does\n",
        "not actually influence the reading score when age is accounted for.\n",
        "\n",
        "### 21. Spurious Correlations (Nicolas Cage)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_21.png\"\n",
        "alt=\"Slide 21\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 21</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 21:38](https://youtu.be/ZRckw_fE56Q&t=1298s))\n",
        "\n",
        "This slide references the famous spurious correlation between Nicolas\n",
        "Cage films and swimming pool drownings. The speaker uses this to\n",
        "highlight the danger of “Enterprise Data Lakes.”\n",
        "\n",
        "When data scientists grab massive tables of data without domain\n",
        "knowledge, they risk finding these coincidental patterns. Machine\n",
        "learning models are excellent at finding patterns, even ones that are\n",
        "nonsensical, making robust feature importance techniques vital.\n",
        "\n",
        "### 22. Feature Impact Ranking\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_22.png\"\n",
        "alt=\"Slide 22\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 22</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 17:09](https://youtu.be/ZRckw_fE56Q&t=1029s))\n",
        "\n",
        "The presentation shows a ranked list of features for the dragon model.\n",
        "The speaker reiterates that getting this ranking right has “real\n",
        "consequences.”\n",
        "\n",
        "If you tell a business stakeholder that a specific variable is driving\n",
        "the risk, they will make decisions based on that. Understanding the true\n",
        "hierarchy of influence is essential for trust and actionable insight.\n",
        "\n",
        "### 23. If Your Feature Impact is Wrong…\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_23.png\"\n",
        "alt=\"Slide 23\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 23</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 17:17](https://youtu.be/ZRckw_fE56Q&t=1037s))\n",
        "\n",
        "A humorous but serious warning: “If your feature impact is wrong, you\n",
        "are toast.”\n",
        "\n",
        "This underscores the professional risk. If a data scientist attributes a\n",
        "prediction to the wrong cause (like milk instead of age), they lose\n",
        "credibility and potentially cause the business to pull the wrong levers\n",
        "to try and optimize the outcome.\n",
        "\n",
        "### 24. Feature Importance: Ablation Methodology\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_24.png\"\n",
        "alt=\"Slide 24\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 24</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 22:15](https://youtu.be/ZRckw_fE56Q&t=1335s))\n",
        "\n",
        "The speaker explains the logic behind feature importance using an\n",
        "**Ablation Methodology**. He presents three models: 1. Model AB (Both\n",
        "features): R-squared 0.9 2. Model A (Feature A only): R-squared 0.7 3.\n",
        "Model B (Feature B only): R-squared 0.8\n",
        "\n",
        "He asks the audience to intuit which feature is more important based on\n",
        "these scores.\n",
        "\n",
        "### 25. Ablation Methodology Definition\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_25.png\"\n",
        "alt=\"Slide 25\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 25</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 22:57](https://youtu.be/ZRckw_fE56Q&t=1377s))\n",
        "\n",
        "The audience correctly identifies that Feature B is more important\n",
        "because it carries more signal (higher R-squared) on its own.\n",
        "\n",
        "The speaker defines **Ablation** as comparing the model performance with\n",
        "and without specific features. It is a scientific control method: “try\n",
        "something with it and without it,” similar to testing if coffee makes a\n",
        "person happy by withholding it for a day.\n",
        "\n",
        "### 26. ‘Leave it Out’ Feature Importance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_26.png\"\n",
        "alt=\"Slide 26\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 26</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 23:49](https://youtu.be/ZRckw_fE56Q&t=1429s))\n",
        "\n",
        "This slide formalizes the “Leave One Out” approach. By calculating the\n",
        "drop in performance when a feature is removed, we quantify its value. \\*\n",
        "Remove B: Performance drops by 0.2 (0.9 -\\> 0.7). \\* Remove A:\n",
        "Performance drops by 0.1 (0.9 -\\> 0.8).\n",
        "\n",
        "Since removing B causes a larger drop in accuracy, B is the more\n",
        "important feature. However, the speaker notes a problem: with 100\n",
        "features, you would have to build 100 different models, which is\n",
        "computationally expensive.\n",
        "\n",
        "### 27. Permutation Based Feature Importance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_27.png\"\n",
        "alt=\"Slide 27\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 27</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 26:53](https://youtu.be/ZRckw_fE56Q&t=1613s))\n",
        "\n",
        "To solve the computational cost of retraining models, the speaker\n",
        "introduces **Permutation Importance** (attributed to Breiman/Random\n",
        "Forests). Instead of removing a column and retraining, you simply\n",
        "**shuffle** the values of that column (permute them) within the existing\n",
        "test data.\n",
        "\n",
        "By shuffling the data, you break the relationship between that feature\n",
        "and the target, effectively “removing” the signal while keeping the\n",
        "model structure intact. If the model’s error increases significantly\n",
        "after shuffling a feature, that feature was important.\n",
        "\n",
        "### 28. R Package: randomForest\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_28.png\"\n",
        "alt=\"Slide 28\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 28</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:31](https://youtu.be/ZRckw_fE56Q&t=1651s))\n",
        "\n",
        "The speaker highlights that this is a standard technique available in\n",
        "common tools. In the R language, the `randomForest` package has\n",
        "supported permutation-based importance for a long time.\n",
        "\n",
        "This slide serves as a resource pointer for R users, confirming that\n",
        "these advanced interpretability checks are accessible within their\n",
        "standard toolkits.\n",
        "\n",
        "### 29. Python: scikit-learn\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_29.png\"\n",
        "alt=\"Slide 29\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 29</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 27:36](https://youtu.be/ZRckw_fE56Q&t=1656s))\n",
        "\n",
        "Similarly, for Python users, `scikit-learn` has added support for\n",
        "permutation importance. This accessibility reinforces the speaker’s\n",
        "point that there is no excuse for not using these techniques to validate\n",
        "model behavior.\n",
        "\n",
        "### 30. Multicollinearity\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_30.png\"\n",
        "alt=\"Slide 30\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 30</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:10](https://youtu.be/ZRckw_fE56Q&t=1690s))\n",
        "\n",
        "The speaker addresses a complex issue: **Multicollinearity**. The Venn\n",
        "diagrams illustrate that features often share information (variance).\n",
        "\n",
        "When features are highly correlated, they “share the signal.” This makes\n",
        "it difficult for the model (and the interpreter) to assign credit. Does\n",
        "the credit go to Feature A or Feature B if they both describe the same\n",
        "underlying phenomenon?\n",
        "\n",
        "### 31. 10 Different Models, 10 Different Importances\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_31.png\"\n",
        "alt=\"Slide 31\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 31</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 28:25](https://youtu.be/ZRckw_fE56Q&t=1705s))\n",
        "\n",
        "Due to multicollinearity, running the same algorithm on the same data\n",
        "multiple times (with different random seeds or data partitions) can\n",
        "result in different feature rankings.\n",
        "\n",
        "This instability is frustrating. In one run, “Milk” might be important;\n",
        "in another, “Age” takes the lead. This happens because the model\n",
        "arbitrarily chooses one of the correlated features to split on, and this\n",
        "choice changes based on randomness in the training process.\n",
        "\n",
        "### 32. Multicollinearity Affects Interpreting Models\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_32.png\"\n",
        "alt=\"Slide 32\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 32</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 29:08](https://youtu.be/ZRckw_fE56Q&t=1748s))\n",
        "\n",
        "This chart visualizes the “trading off” effect. You can see features\n",
        "swapping positions in importance rankings across different model runs.\n",
        "\n",
        "The speaker notes that you cannot simply remove correlated features\n",
        "without potentially hurting accuracy, as they might contain slight\n",
        "unique signals. This trade-off between accuracy and stable\n",
        "interpretability is a core challenge in data science.\n",
        "\n",
        "### 33. Pro Tip: Aggregate Feature Importance (Same Model)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_33.png\"\n",
        "alt=\"Slide 33\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 33</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:09](https://youtu.be/ZRckw_fE56Q&t=1809s))\n",
        "\n",
        "To handle instability, the speaker suggests a “Pro Tip”: **Aggregate\n",
        "Feature Importance**. Run the feature importance calculation multiple\n",
        "times on the same model and plot the variability (the box plots in the\n",
        "slide).\n",
        "\n",
        "This gives a “richer understanding.” Instead of a single number, you see\n",
        "a range. If the range is huge, you know the feature’s importance is\n",
        "unstable due to correlation or noise.\n",
        "\n",
        "### 34. Aggregate Feature Importance (Different Models)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_34.png\"\n",
        "alt=\"Slide 34\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 34</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:15](https://youtu.be/ZRckw_fE56Q&t=1815s))\n",
        "\n",
        "Expanding on the previous tip, you can also aggregate importance across\n",
        "*different* models (e.g., comparing importance in a Random Forest vs. a\n",
        "Gradient Boosted Machine).\n",
        "\n",
        "If a feature is consistently important across different algorithms and\n",
        "multiple runs, you can be much more confident that it is a true driver\n",
        "of the target variable.\n",
        "\n",
        "### 35. Pro Tips: Add Random Features\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_35.png\"\n",
        "alt=\"Slide 35\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 35</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 30:20](https://youtu.be/ZRckw_fE56Q&t=1820s))\n",
        "\n",
        "Another technique mentioned is adding a **Random Feature** (noise) to\n",
        "the dataset. If a real feature ranks lower in importance than the random\n",
        "noise variable, it is likely not a significant predictor.\n",
        "\n",
        "This serves as a baseline or “sanity check” to distinguish true signal\n",
        "from statistical noise in the feature ranking list.\n",
        "\n",
        "### 36. Permutation Based Importance Conclusion\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_36.png\"\n",
        "alt=\"Slide 36\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 36</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 33:34](https://youtu.be/ZRckw_fE56Q&t=2014s))\n",
        "\n",
        "The section concludes by asserting that **Permutation based importance**\n",
        "is the “best practice.” It offers a “good balance of computation and\n",
        "performance for any model.”\n",
        "\n",
        "References to academic papers (like Strobl) are provided for those who\n",
        "want to dive into the edge cases, but for general application, this is\n",
        "the recommended approach for determining *what* matters in a model.\n",
        "\n",
        "### 37. Partial Dependence\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_37.png\"\n",
        "alt=\"Slide 37\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 37</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:22](https://youtu.be/ZRckw_fE56Q&t=2062s))\n",
        "\n",
        "The second tool introduced is **Partial Dependence**. While feature\n",
        "importance tells us *which* variables matter, Partial Dependence tells\n",
        "us *how* they matter.\n",
        "\n",
        "The slide shows example plots for Age and Weight. The goal is to\n",
        "understand the functional relationship: as age increases, does the\n",
        "predicted aggression go up, down, or follow a complex curve?\n",
        "\n",
        "### 38. Effect of Age on Our Target\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_38.png\"\n",
        "alt=\"Slide 38\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 38</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 34:41](https://youtu.be/ZRckw_fE56Q&t=2081s))\n",
        "\n",
        "The speaker reiterates that in complex “black box” models, we don’t have\n",
        "coefficients (positive or negative signs) like in linear regression. We\n",
        "cannot simply say “age is positive.”\n",
        "\n",
        "Therefore, we need a visualization that maps the input value to the\n",
        "prediction output to understand the behavior of the model across the\n",
        "range of the feature.\n",
        "\n",
        "### 39. Calculating Partial Dependence (Step 1)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_39.png\"\n",
        "alt=\"Slide 39\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 39</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 35:13](https://youtu.be/ZRckw_fE56Q&t=2113s))\n",
        "\n",
        "To explain how Partial Dependence is calculated, the speaker walks\n",
        "through the process. Step 1: Take a single observation (one Dragon).\n",
        "\n",
        "Step 2: Keep all features constant *except* the one we are interested in\n",
        "(Age). Manually force the age to different values (e.g., 5, 10, 15 years\n",
        "old) and ask the model for a prediction at each point. This generates a\n",
        "hypothetical curve for that specific dragon.\n",
        "\n",
        "### 40. Calculating Partial Dependence (Step 2)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_40.png\"\n",
        "alt=\"Slide 40\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 40</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 35:53](https://youtu.be/ZRckw_fE56Q&t=2153s))\n",
        "\n",
        "The process is repeated for a second dragon. Because the other features\n",
        "(weight, color, etc.) are different for this dragon, the curve might\n",
        "look slightly different (higher or lower baseline), but it follows the\n",
        "model’s logic for age.\n",
        "\n",
        "### 41. Calculating Partial Dependence (Step 3)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_41.png\"\n",
        "alt=\"Slide 41\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 41</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 36:06](https://youtu.be/ZRckw_fE56Q&t=2166s))\n",
        "\n",
        "This is repeated for many observations in the dataset. The slide shows\n",
        "multiple data points being generated. This creates a “what-if” scenario\n",
        "for every dragon in the dataset across the spectrum of ages.\n",
        "\n",
        "### 42. Individual Conditional Expectation (ICE) Curves\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_42.png\"\n",
        "alt=\"Slide 42\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 42</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 36:15](https://youtu.be/ZRckw_fE56Q&t=2175s))\n",
        "\n",
        "When you draw lines connecting these predictions for each individual\n",
        "instance, you get **ICE Curves** (Individual Conditional Expectation).\n",
        "\n",
        "This visualizes the relationship between the feature and the prediction\n",
        "for every single data point. It shows the variability: for some dragons,\n",
        "age might have a steep effect; for others, it might be flatter.\n",
        "\n",
        "### 43. Partial Dependence Plots (PDPs)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_43.png\"\n",
        "alt=\"Slide 43\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 43</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 36:27](https://youtu.be/ZRckw_fE56Q&t=2187s))\n",
        "\n",
        "To get the **Partial Dependence Plot (PDP)**, you simply **average** all\n",
        "the ICE curves.\n",
        "\n",
        "This single line represents the *average* effect of the feature on the\n",
        "model’s prediction, holding everything else constant. It distills the\n",
        "complex interactions into a single, interpretable trend line.\n",
        "\n",
        "### 44. Resulting Partial Dependence\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_44.png\"\n",
        "alt=\"Slide 44\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 44</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 37:07](https://youtu.be/ZRckw_fE56Q&t=2227s))\n",
        "\n",
        "The final plot shows the isolated effect of Age. The speaker notes this\n",
        "gives “really good insight.” We can now see if the risk rises linearly\n",
        "with age, or if (as often happens in nonlinear models) it plateaus or\n",
        "dips at certain points.\n",
        "\n",
        "### 45. ICE Plots\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_45.png\"\n",
        "alt=\"Slide 45\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 45</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 40:48](https://youtu.be/ZRckw_fE56Q&t=2448s))\n",
        "\n",
        "This slide formally defines ICE Plots. While the PDP shows the average,\n",
        "ICE plots are useful for seeing heterogeneity. For example, if the model\n",
        "treats males and females differently, the ICE curves might show two\n",
        "distinct clusters of lines that the average PDP would obscure.\n",
        "\n",
        "### 46. Partial Dependence to Show Price Elasticity\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_46.png\"\n",
        "alt=\"Slide 46\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 46</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 37:20](https://youtu.be/ZRckw_fE56Q&t=2240s))\n",
        "\n",
        "The speaker moves to a real-world example: **Orange Juice Sales**. The\n",
        "goal is to understand **Price Elasticity**—if we raise the price, do\n",
        "sales go down?\n",
        "\n",
        "Economics 101 says yes, but the model includes complex factors like\n",
        "store location, coupons, and competitor prices (10 other brands), making\n",
        "it a high-dimensional problem.\n",
        "\n",
        "### 47. Change in Price Affects Sales?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_47.png\"\n",
        "alt=\"Slide 47\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 47</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 38:45](https://youtu.be/ZRckw_fE56Q&t=2325s))\n",
        "\n",
        "This chart shows the raw data (orange line) of Price vs. Sales. It is\n",
        "“all over the place.” There is no clear linear relationship visible\n",
        "because the data is noisy and confounded by other variables (e.g., maybe\n",
        "high prices occurred during a holiday when sales were high anyway).\n",
        "\n",
        "Looking just at the raw data fails to isolate the specific impact of the\n",
        "price change on consumer behavior.\n",
        "\n",
        "### 48. Ahh, Price Does Affect Sales!\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_48.png\"\n",
        "alt=\"Slide 48\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 48</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 39:16](https://youtu.be/ZRckw_fE56Q&t=2356s))\n",
        "\n",
        "By applying **Partial Dependence**, the signal emerges from the noise.\n",
        "The blue line clearly shows that as price increases, sales generally\n",
        "decrease.\n",
        "\n",
        "Crucially, the plot reveals a non-linear drop at exactly **\\$3.50**. The\n",
        "speaker interprets this as a psychological threshold where customers\n",
        "decide “maybe I’ll buy something else.” This insight—a specific price\n",
        "point where demand collapses—is only visible through this\n",
        "interpretability technique.\n",
        "\n",
        "### 49. Distributions and Partial Dependence\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_49.png\"\n",
        "alt=\"Slide 49\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 49</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 40:30](https://youtu.be/ZRckw_fE56Q&t=2430s))\n",
        "\n",
        "A warning is issued regarding **Distributions**. Partial Dependence\n",
        "assumes you can vary a feature independently of others. However, if\n",
        "features are correlated, you might create impossible combinations (like\n",
        "a 5-year-old dragon that weighs 5 tons).\n",
        "\n",
        "Making predictions on these “impossible” data points means extrapolating\n",
        "outside the training distribution, which can lead to unreliable\n",
        "explanations.\n",
        "\n",
        "### 50. Partial Dependence Conclusion\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_50.png\"\n",
        "alt=\"Slide 50\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 50</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 40:25](https://youtu.be/ZRckw_fE56Q&t=2425s))\n",
        "\n",
        "The speaker concludes that Partial Dependence is a “best practice” for\n",
        "understanding feature behavior. References to Goldstein and Friedman\n",
        "(classic papers) are provided.\n",
        "\n",
        "This tool answers the “directionality” question, proving that the model\n",
        "aligns with domain knowledge (e.g., higher prices = lower sales).\n",
        "\n",
        "### 51. Predictions\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_51.png\"\n",
        "alt=\"Slide 51\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 51</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 41:52](https://youtu.be/ZRckw_fE56Q&t=2512s))\n",
        "\n",
        "The final section focuses on **Predictions**. The speaker shows three\n",
        "dragons with their associated risk scores (9.1, 2.4, etc.).\n",
        "\n",
        "While the model successfully identifies the red dragon as high risk, the\n",
        "next logical question from a user is “Why?”\n",
        "\n",
        "### 52. Predictions & Explanations\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_52.png\"\n",
        "alt=\"Slide 52\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 52</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 42:25](https://youtu.be/ZRckw_fE56Q&t=2545s))\n",
        "\n",
        "This slide introduces **Prediction Explanations**. Alongside the score\n",
        "of 9.1, the model provides a list of contributing factors: “Number of\n",
        "past kills” increased the score, while “Gender” might have decreased it.\n",
        "\n",
        "This moves from global interpretability (how the model works generally)\n",
        "to **local interpretability** (why this specific instance was scored\n",
        "this way).\n",
        "\n",
        "### 53. Floor Map with Readmission Probability\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_53.png\"\n",
        "alt=\"Slide 53\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 53</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 43:01](https://youtu.be/ZRckw_fE56Q&t=2581s))\n",
        "\n",
        "A real-world application is shown: a hospital dashboard predicting\n",
        "patient readmission. The interface doesn’t just show a risk score\n",
        "(63.7%); it lists the reasons (e.g., “Abdominal pain,” “Medical\n",
        "specialty unspecified”).\n",
        "\n",
        "The speaker highlights that these explanations build **Trust** with\n",
        "end-users (nurses/doctors) and provide **Context** that helps them\n",
        "decide *how* to intervene, rather than just knowing *that* they should\n",
        "intervene.\n",
        "\n",
        "### 54. Local Interpretable Model-Agnostic Explanations (LIME)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_54.png\"\n",
        "alt=\"Slide 54\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 54</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:01](https://youtu.be/ZRckw_fE56Q&t=2701s))\n",
        "\n",
        "The speaker mentions **LIME**, one of the “traditional” or early\n",
        "techniques for this type of explanation. LIME works by fitting a simple\n",
        "local model around a single prediction to approximate the complex\n",
        "model’s behavior.\n",
        "\n",
        "### 55. LIME Flaw: Explanations Should Be Identical\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_55.png\"\n",
        "alt=\"Slide 55\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 55</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:12](https://youtu.be/ZRckw_fE56Q&t=2712s))\n",
        "\n",
        "The tone shifts to a critique of LIME. The slide asserts a fundamental\n",
        "requirement: **“EXPLANATIONS SHOULD BE IDENTICAL”** for the same data\n",
        "and same model.\n",
        "\n",
        "If you ask the model twice why it predicted a score for the same dragon,\n",
        "the answer should be the same both times.\n",
        "\n",
        "### 56. LIME: Two Different Explanations\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_56.png\"\n",
        "alt=\"Slide 56\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 56</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:15](https://youtu.be/ZRckw_fE56Q&t=2715s))\n",
        "\n",
        "This slide provides code evidence of LIME’s instability. Running LIME\n",
        "twice on the “SAME DATA, SAME MODEL” produces “TWO DIFFERENT\n",
        "EXPLANATIONS.”\n",
        "\n",
        "This occurs because LIME relies on random sampling to build its local\n",
        "approximation. This randomness makes it unreliable for serious\n",
        "applications where consistency is required for trust.\n",
        "\n",
        "### 57. Explanations Should Have Fidelity\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_57.png\"\n",
        "alt=\"Slide 57\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 57</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:20](https://youtu.be/ZRckw_fE56Q&t=2720s))\n",
        "\n",
        "The speaker argues that explanations must have **Fidelity** to the data.\n",
        "If two data points are very similar, their explanations should be\n",
        "similar. LIME often fails this test, producing vastly different\n",
        "explanations for minor changes in input.\n",
        "\n",
        "### 58. LIME Isn’t Responsive to Data\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_58.png\"\n",
        "alt=\"Slide 58\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 58</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:22](https://youtu.be/ZRckw_fE56Q&t=2722s))\n",
        "\n",
        "Further criticism of LIME. The slide suggests that LIME explanations\n",
        "sometimes lack “local fidelity,” meaning the explanation doesn’t\n",
        "accurately reflect the model’s behavior in that specific region of the\n",
        "data.\n",
        "\n",
        "### 59. Anyone Relying on LIME is Toast\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_59.png\"\n",
        "alt=\"Slide 59\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 59</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:25](https://youtu.be/ZRckw_fE56Q&t=2725s))\n",
        "\n",
        "A blunt conclusion: “Anyone relying on LIME is toast.” The speaker\n",
        "strongly advises against using LIME due to these flaws, suggesting that\n",
        "while it was a pioneering method, it is no longer the standard for\n",
        "reliable interpretability.\n",
        "\n",
        "### 60. What Can We Learn From This?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_60.png\"\n",
        "alt=\"Slide 60\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 60</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:30](https://youtu.be/ZRckw_fE56Q&t=2730s))\n",
        "\n",
        "This slide summarizes the requirements for a good explanation method\n",
        "derived from LIME’s failures: consistency, accuracy, and fidelity. It\n",
        "sets the stage for introducing the superior method: Shapley values.\n",
        "\n",
        "### 61. Your Model or a Surrogate Model?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_61.png\"\n",
        "alt=\"Slide 61\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 61</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:30](https://youtu.be/ZRckw_fE56Q&t=2730s))\n",
        "\n",
        "The speaker questions whether we are explaining the *actual* model or a\n",
        "*surrogate* (approximation). LIME explains a surrogate. Ideally, we want\n",
        "to explain the actual model directly.\n",
        "\n",
        "### 62. What is Local?\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_62.png\"\n",
        "alt=\"Slide 62\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 62</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:30](https://youtu.be/ZRckw_fE56Q&t=2730s))\n",
        "\n",
        "Another critique of LIME involves the definition of “local.” The “kernel\n",
        "width” is a hyperparameter that changes the explanation. If the\n",
        "explanation depends on how you tune the explainer, rather than just the\n",
        "data, it is problematic.\n",
        "\n",
        "### 63. Explanations Should Be Model Agnostic\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_63.png\"\n",
        "alt=\"Slide 63\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 63</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:06](https://youtu.be/ZRckw_fE56Q&t=2706s))\n",
        "\n",
        "The speaker reiterates the requirement that the method must work for any\n",
        "model type (Trees, Neural Nets, SVMs). This is a strength of LIME, but\n",
        "also a requirement for its replacement.\n",
        "\n",
        "### 64. Explanations Should Be Fast\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_64.png\"\n",
        "alt=\"Slide 64\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 64</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:33](https://youtu.be/ZRckw_fE56Q&t=2733s))\n",
        "\n",
        "Speed is critical. The slide compares LIME’s speed across datasets. If\n",
        "an explanation takes too long to generate, it cannot be used in\n",
        "real-time applications (like the hospital dashboard).\n",
        "\n",
        "### 65. Shapley Values for Explanations\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_65.png\"\n",
        "alt=\"Slide 65\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 65</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 45:36](https://youtu.be/ZRckw_fE56Q&t=2736s))\n",
        "\n",
        "The speaker introduces **Shapley Values** as the modern standard.\n",
        "Originating from Game Theory (and Nobel Prize-winning economics), this\n",
        "method provides a mathematically sound way to attribute the “marginal\n",
        "effect” of features to a prediction.\n",
        "\n",
        "### 66. Shapley Values Metaphor: Pushing a Car\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_66.png\"\n",
        "alt=\"Slide 66\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 66</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 46:00](https://youtu.be/ZRckw_fE56Q&t=2760s))\n",
        "\n",
        "To explain the concept, the speaker uses a metaphor: **Pushing a car\n",
        "stuck in the snow**. It’s a cooperative game. Several people (features)\n",
        "are pushing to achieve an outcome (moving the car/making a prediction).\n",
        "\n",
        "The goal is to determine how much each person contributed. Did the\n",
        "teenager actually push, or just stand there?\n",
        "\n",
        "### 67. Intuition of Shapley Values\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_67.png\"\n",
        "alt=\"Slide 67\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 67</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 47:25](https://youtu.be/ZRckw_fE56Q&t=2845s))\n",
        "\n",
        "The speaker expands the metaphor. If “The Rock” joins the pushing, he\n",
        "might only need to add a small amount of force (10 units) to get the car\n",
        "moving because the others are already pushing.\n",
        "\n",
        "However, if The Rock was pushing alone, he would contribute much more.\n",
        "Shapley values calculate the average contribution across all possible\n",
        "“coalitions” (combinations of people pushing).\n",
        "\n",
        "### 68. Calculating Average Contribution\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_68.png\"\n",
        "alt=\"Slide 68\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 68</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 46:50](https://youtu.be/ZRckw_fE56Q&t=2810s))\n",
        "\n",
        "The slide visually represents different scenarios (orders of arrival).\n",
        "The contribution of a person depends on who is already there. Shapley\n",
        "values “unpack” this by averaging the marginal contribution of a feature\n",
        "across all possible permutations of features.\n",
        "\n",
        "### 69. Calculating Shapley Values: Subsets\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_69.png\"\n",
        "alt=\"Slide 69\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 69</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 48:43](https://youtu.be/ZRckw_fE56Q&t=2923s))\n",
        "\n",
        "Mathematically, this means looking at all possible subsets of features.\n",
        "The slide lists the combinations (A alone, B alone, A+B, etc.) and the\n",
        "model output (“Force”) for each.\n",
        "\n",
        "### 70. Calculating Shapley Values: Marginal Contributions\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_70.png\"\n",
        "alt=\"Slide 70\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 70</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 48:51](https://youtu.be/ZRckw_fE56Q&t=2931s))\n",
        "\n",
        "By comparing the output of a subset *with* a feature to the subset\n",
        "*without* it, we find the **marginal contribution** for that specific\n",
        "scenario.\n",
        "\n",
        "### 71. Calculating Shapley Values: The Average\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_71.png\"\n",
        "alt=\"Slide 71\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 71</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 48:54](https://youtu.be/ZRckw_fE56Q&t=2934s))\n",
        "\n",
        "The final Shapley value is the **average** of these marginal\n",
        "contributions. This provides a fair distribution of credit among the\n",
        "features that sums up to the total prediction.\n",
        "\n",
        "### 72. Shapley Values Formula\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_72.png\"\n",
        "alt=\"Slide 72\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 72</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 48:40](https://youtu.be/ZRckw_fE56Q&t=2920s))\n",
        "\n",
        "The slide presents the formal mathematical formula. It is defined as the\n",
        "“average marginal contribution of a feature with respect to all subsets\n",
        "of other features.” While complex, it guarantees unique properties like\n",
        "consistency that LIME lacks.\n",
        "\n",
        "### 73. Shapley Values for Feature Attribution\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_73.png\"\n",
        "alt=\"Slide 73\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 73</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 49:05](https://youtu.be/ZRckw_fE56Q&t=2945s))\n",
        "\n",
        "Applying this to Machine Learning: The “Game” is the prediction task.\n",
        "The “Players” are the features. The “Payout” is the prediction score.\n",
        "\n",
        "The slide shows a Boston Housing prediction. The Shapley values tell us\n",
        "that for this specific house, the “LSTAT” feature pushed the price down,\n",
        "while “RM” (rooms) pushed it up, relative to the average house price.\n",
        "\n",
        "### 74. So Many Methods for Shapley Values\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_74.png\"\n",
        "alt=\"Slide 74\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 74</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:35](https://youtu.be/ZRckw_fE56Q&t=3035s))\n",
        "\n",
        "The speaker notes that calculating exact Shapley values is\n",
        "computationally expensive (2^N combinations). Therefore, many\n",
        "approximation methods exist. The slide lists implementations in R\n",
        "(`iml`, `fastshap`) and Python (`shap`).\n",
        "\n",
        "### 75. Calculating Shapley Values - Linear Model\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_75.png\"\n",
        "alt=\"Slide 75\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 75</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:10](https://youtu.be/ZRckw_fE56Q&t=3010s))\n",
        "\n",
        "For a simple **Linear Model**, Shapley values are easy to calculate.\n",
        "Because features in a linear model are additive and independent\n",
        "(conceptually), the coefficient \\* value roughly equals the\n",
        "contribution.\n",
        "\n",
        "### 76. Linear Model Example\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_76.png\"\n",
        "alt=\"Slide 76\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 76</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:12](https://youtu.be/ZRckw_fE56Q&t=3012s))\n",
        "\n",
        "The slide shows that if you change the Age, the prediction changes by a\n",
        "specific amount. In linear models, the difference between the prediction\n",
        "and the baseline is simply the sum of these changes.\n",
        "\n",
        "### 77. Simple to Get Shapley Values for Linear Model\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_77.png\"\n",
        "alt=\"Slide 77\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 77</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:15](https://youtu.be/ZRckw_fE56Q&t=3015s))\n",
        "\n",
        "This reinforces that for linear models, we don’t need complex\n",
        "approximations. The structure of the model allows for exact calculation\n",
        "easily.\n",
        "\n",
        "### 78. Shapley Values for Trees: Tree Shap\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_78.png\"\n",
        "alt=\"Slide 78\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 78</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:45](https://youtu.be/ZRckw_fE56Q&t=3045s))\n",
        "\n",
        "For tree-based models (Random Forest, XGBoost, LightGBM), there is a\n",
        "specific, fast algorithm called **Tree SHAP** (developed by Scott\n",
        "Lundberg). It computes exact Shapley values in polynomial time by\n",
        "leveraging the tree structure, making it feasible for large models.\n",
        "\n",
        "### 79. Tree Shap Calculation\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_79.png\"\n",
        "alt=\"Slide 79\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 79</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:45](https://youtu.be/ZRckw_fE56Q&t=3045s))\n",
        "\n",
        "This slide visualizes how Tree SHAP works by tracing paths down the\n",
        "decision tree to calculate expectations. This efficiency is why SHAP has\n",
        "become the industry standard for boosting models.\n",
        "\n",
        "### 80. Approximating Shapley Values\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_80.png\"\n",
        "alt=\"Slide 80\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 80</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 50:55](https://youtu.be/ZRckw_fE56Q&t=3055s))\n",
        "\n",
        "For other “Black Box” models (like Neural Networks or SVMs) where exact\n",
        "calculation is intractable due to the combinatorial explosion (100\n",
        "features = impossible to compute all subsets), we must use\n",
        "approximations.\n",
        "\n",
        "### 81. Approximating Shapley Values: Strumbelj\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_81.png\"\n",
        "alt=\"Slide 81\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 81</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:04](https://youtu.be/ZRckw_fE56Q&t=3064s))\n",
        "\n",
        "One method is **Strumbelj’s algorithm**, a sampling-based approach. It\n",
        "uses Monte Carlo sampling to estimate the difference between predictions\n",
        "with and without a feature, approximating the average marginal\n",
        "contribution.\n",
        "\n",
        "### 82. Strumbelj Visualization\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_82.png\"\n",
        "alt=\"Slide 82\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 82</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:04](https://youtu.be/ZRckw_fE56Q&t=3064s))\n",
        "\n",
        "The slide visualizes the sampling process: creating synthetic instances\n",
        "by mixing the feature of interest with random values from the dataset to\n",
        "estimate its effect.\n",
        "\n",
        "### 83. Approximating Shapley Values: Shap Kernel\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_83.png\"\n",
        "alt=\"Slide 83\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 83</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:04](https://youtu.be/ZRckw_fE56Q&t=3064s))\n",
        "\n",
        "**Kernel SHAP** is introduced as a model-agnostic method. It connects\n",
        "LIME and Shapley values. It uses a weighted linear regression (like\n",
        "LIME) but uses specific “Shapley weights” to ensure the result is a\n",
        "valid Shapley value approximation.\n",
        "\n",
        "### 84. Shap Kernel: Generating Data (1)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_84.png\"\n",
        "alt=\"Slide 84\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 84</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "*Note: The speaker skips detailed explanations of these calculation\n",
        "slides due to time constraints, but the slides detail the technical\n",
        "steps.*\n",
        "\n",
        "This slide shows the setup for Kernel SHAP, defining a “background\n",
        "dataset” to serve as the reference value for “missing” features.\n",
        "\n",
        "### 85. Shap Kernel: Generating Data (2)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_85.png\"\n",
        "alt=\"Slide 85\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 85</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "The method involves treating features as “missing” by replacing them\n",
        "with background values to simulate their absence from a coalition.\n",
        "\n",
        "### 86. Shap Kernel: Generating Data (3)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_86.png\"\n",
        "alt=\"Slide 86\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 86</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "Permutations of feature coalitions are generated to create a synthetic\n",
        "dataset for the local regression.\n",
        "\n",
        "### 87. Shap Kernel: Generating Data (4)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_87.png\"\n",
        "alt=\"Slide 87\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 87</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "A linear model is fit to this synthetic data. The coefficients of this\n",
        "linear model, when weighted correctly, correspond to the Shapley values.\n",
        "\n",
        "### 88. Shap Kernel: Generating Data (5)\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_88.png\"\n",
        "alt=\"Slide 88\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 88</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "The result is the attribution value for the specific prediction.\n",
        "\n",
        "### 89. Mimic Shap\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_89.png\"\n",
        "alt=\"Slide 89\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 89</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "**Mimic SHAP** is another approximation where a global surrogate model\n",
        "(like a Gradient Boosted Tree) is trained to mimic the black box, and\n",
        "then Tree SHAP is used on the surrogate.\n",
        "\n",
        "### 90. Gradient Shap\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_90.png\"\n",
        "alt=\"Slide 90\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 90</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "**Gradient SHAP** is designed for Deep Learning models (differentiable\n",
        "models). It combines Integrated Gradients with Shapley values for\n",
        "efficient computation in neural networks.\n",
        "\n",
        "### 91. GkmExplain\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_91.png\"\n",
        "alt=\"Slide 91\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 91</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "A specialized method for non-linear Support Vector Machines (SVMs).\n",
        "\n",
        "### 92. DASP\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_92.png\"\n",
        "alt=\"Slide 92\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 92</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:15](https://youtu.be/ZRckw_fE56Q&t=3075s))\n",
        "\n",
        "**DASP** is a polynomial-time algorithm for approximating Shapley values\n",
        "specifically in Deep Neural Networks.\n",
        "\n",
        "### 93. Aggregating Shapley Values: Feature Importance\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_93.png\"\n",
        "alt=\"Slide 93\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 93</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:32](https://youtu.be/ZRckw_fE56Q&t=3092s))\n",
        "\n",
        "The speaker returns to practical applications. Once you have local SHAP\n",
        "values for every prediction, you can aggregate them.\n",
        "\n",
        "By summing the **absolute** SHAP values across all data points, you get\n",
        "a global **Feature Importance** plot. This tells you which features are\n",
        "most important overall, derived directly from the local explanations.\n",
        "\n",
        "### 94. Aggregating Shapley Values: Feature Interactions\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_94.png\"\n",
        "alt=\"Slide 94\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 94</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 51:46](https://youtu.be/ZRckw_fE56Q&t=3106s))\n",
        "\n",
        "SHAP can also quantify **Interactions**. The slide shows the interaction\n",
        "between Age and Sex. It reveals that for males, a certain age range\n",
        "increases risk (prediction), whereas for females, it might be different.\n",
        "\n",
        "This allows data scientists to see exactly how features modify each\n",
        "other’s effects, solving the problem of hidden interactions in complex\n",
        "models.\n",
        "\n",
        "### 95. Aggregating Shapley Values: Feature Selection\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_95.png\"\n",
        "alt=\"Slide 95\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 95</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 52:24](https://youtu.be/ZRckw_fE56Q&t=3144s))\n",
        "\n",
        "SHAP values can be used for **Feature Selection**. By ranking features\n",
        "by their mean absolute SHAP value, you can identify the top contributors\n",
        "and remove noise variables, potentially simplifying the model without\n",
        "losing accuracy.\n",
        "\n",
        "### 96. Aggregating Shapley Values: Supervised Clustering\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_96.png\"\n",
        "alt=\"Slide 96\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 96</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 52:45](https://youtu.be/ZRckw_fE56Q&t=3165s))\n",
        "\n",
        "A “cool advanced technique” is **Explanation Clustering** (Supervised\n",
        "Clustering). Instead of clustering the raw data, you cluster the\n",
        "*explanations* (the SHAP values).\n",
        "\n",
        "This groups data points not by their raw values, but by *why* the model\n",
        "made a prediction for them. This can reveal distinct subpopulations or\n",
        "“reasons” for high risk (e.g., a group of high-risk dragons due to age\n",
        "vs. a group due to weight).\n",
        "\n",
        "### 97. Model Agnostic Explanation Tools Summary\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_97.png\"\n",
        "alt=\"Slide 97\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 97</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 53:26](https://youtu.be/ZRckw_fE56Q&t=3206s))\n",
        "\n",
        "The presentation wraps up by reviewing the three key tools covered: 1.\n",
        "**Feature Importance** (Permutation based) 2. **Partial Dependence**\n",
        "(for directionality) 3. **Prediction Explanations** (Shapley Values)\n",
        "\n",
        "The speaker encourages the audience to use these tools to build trust\n",
        "and understanding in their machine learning workflows.\n",
        "\n",
        "### 98. Question Time\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://rajivshah.com/blog/images/model-interpretability-explainability/slide_98.png\"\n",
        "alt=\"Slide 98\" />\n",
        "<figcaption aria-hidden=\"true\">Slide 98</figcaption>\n",
        "</figure>\n",
        "\n",
        "([Timestamp: 53:34](https://youtu.be/ZRckw_fE56Q&t=3214s))\n",
        "\n",
        "The final slide opens the floor for questions and provides contact\n",
        "information. The speaker mentions that the slides and notebooks\n",
        "(including the age/milk and LIME examples) are available on his GitHub\n",
        "for those who want to explore the code.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "*This annotated presentation was generated from the talk using\n",
        "AI-assisted tools. Each slide includes timestamps and detailed\n",
        "explanations.*"
      ],
      "id": "e77bd94d-2d01-46e3-9a49-6d8fe9dc11ea"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}