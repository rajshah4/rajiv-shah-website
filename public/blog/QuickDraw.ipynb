{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Google’s Quickdraw to create an MNIST style dataset!\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://www.tensorflow.org/images/MNIST.png\"\n",
        "alt=\"https://www.tensorflow.org/images/MNIST.png\" />\n",
        "<figcaption\n",
        "aria-hidden=\"true\">https://www.tensorflow.org/images/MNIST.png</figcaption>\n",
        "</figure>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "For those running deep learning models, MNIST is ubiquotuous. This\n",
        "dataset of handwritten digits serves many purposes from benchmarking\n",
        "numerous algorithms (its referenced in thousands of papers) and as a\n",
        "visualization, its even more prevelant than Napoleon’s 1812 March. The\n",
        "digits look like this:\n",
        "\n",
        "There are many reasons for its enduring use, but much of it is the lack\n",
        "of an alternative. In this post, I want to introduce an alternative, the\n",
        "Google QuickDraw dataset. The quickdraw dataset was captured in 2017 by\n",
        "Google’s drawing game, [Quick,\n",
        "Draw!](https://quickdraw.withgoogle.com/). The dataset consists of 50\n",
        "million drawings across 345 categories. The drawings look like this:\n",
        "\n",
        "<figure>\n",
        "<img\n",
        "src=\"https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/preview.jpg\"\n",
        "alt=\"https://github.com/googlecreativelab/quickdraw-dataset/blob/master/preview.jpg\" />\n",
        "<figcaption\n",
        "aria-hidden=\"true\">https://github.com/googlecreativelab/quickdraw-dataset/blob/master/preview.jpg</figcaption>\n",
        "</figure>\n",
        "\n",
        "### Build your own Quickdraw dataset\n",
        "\n",
        "I want to walk through how you can use this drawings and create your own\n",
        "MNIST like dataset. Google has made available 28x28 grayscale bitmap\n",
        "files of each drawing. These can serve as drop in replacements for the\n",
        "MNIST 28x28 grayscale bitmap images.\n",
        "\n",
        "As a starting point, Google has graciously made the dataset publicly\n",
        "available with [documentation on the\n",
        "dataset](https://github.com/googlecreativelab/quickdraw-dataset). All\n",
        "the data is sitting in Google’s [Cloud\n",
        "Console](https://console.cloud.google.com/storage/browser/quickdraw_dataset/?pli=1),\n",
        "but for the images, you want this link of the\n",
        "[numpy_bitmaps](https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap/?pli=1).\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/GCP2.png\" alt=\"GCP2\" />\n",
        "<figcaption aria-hidden=\"true\">GCP2</figcaption>\n",
        "</figure>\n",
        "\n",
        "You should arrive on a page that allows you to download all the images\n",
        "for any category. So this is when you have fun! Go ahead and pick your\n",
        "own categories. I started with eyeglasses, face, pencil, and television.\n",
        "As I learned from the face, the drawings that have fine points can be\n",
        "more difficult to learn. But you should play around and pick fun\n",
        "categories.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/shortqd.png\"\n",
        "alt=\"shortqd\" />\n",
        "<figcaption aria-hidden=\"true\">shortqd</figcaption>\n",
        "</figure>\n",
        "\n",
        "The next challenge is taking these .npy files and using them. Here is a\n",
        "short [python\n",
        "gist](https://gist.github.com/rajshah4/903d086adb4e5075415381e1f6038a88)\n",
        "that I used to read the .npy files and combine them to create a 80,000\n",
        "images dataset that I could use in place of MNIST. They are saved in a\n",
        "hdf5 format that is cross platform and often used in deep learning.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/gist.png\" alt=\"gist\" />\n",
        "<figcaption aria-hidden=\"true\">gist</figcaption>\n",
        "</figure>\n",
        "\n",
        "### Using Quickdraw instead of MNIST\n",
        "\n",
        "The next thing is to go have fun with it. I used this dataset in place\n",
        "of MNIST for some work playing around with autoencoders in Python from\n",
        "the [Keras\n",
        "tutorials](https://blog.keras.io/building-autoencoders-in-keras.html).\n",
        "The below picture represents the original images at the top and\n",
        "reconstructed ones at the bottom, using an autoencoder.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/vae10.png\" alt=\"vae10\" />\n",
        "<figcaption aria-hidden=\"true\">vae10</figcaption>\n",
        "</figure>\n",
        "\n",
        "I next used this dataset with a [variational autoencoder in\n",
        "R](https://rstudio.github.io/keras/articles/examples/variational_autoencoder.html).\n",
        "Here is the code snippet to import the data:\n",
        "\n",
        "    library(rhdf5)\n",
        "    x_test <- t(h5read(\"x_test.h5\", \"name-of-dataset\"))\n",
        "    x_train <- t(h5read(\"x_train.h5\", \"name-of-dataset\"))\n",
        "    y_test <- (h5read(\"y_test.h5\", \"name-of-dataset\"))\n",
        "    y_train <- (h5read(\"y_train.h5\", \"name-of-dataset\"))\n",
        "\n",
        "Here is a visualization of its latent space using my custom quickdraw\n",
        "dataset. For me, this was a nice fresh alternative to always staring at\n",
        "the MNIST dataset. So next time you see MNIST listed . . . go build your\n",
        "own!\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://rajivshah.com/blog/images/VAEgw.gif\" alt=\"VAE15\" />\n",
        "<figcaption aria-hidden=\"true\">VAE15</figcaption>\n",
        "</figure>"
      ],
      "id": "a87e2498-94dc-4bbe-b2f0-1bc2b5a49715"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}