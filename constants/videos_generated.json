{
  "generated_at": "2026-01-11T14:49:54.172011",
  "spotlight": [
    {
      "group_id": 6660,
      "title": "Sampling bias",
      "description": "Sampling bias",
      "upload_date": "2023-12-25",
      "total_views": 2682156,
      "max_views": 2682156,
      "topics": [
        "bias",
        "sampling"
      ],
      "search_text": "Sampling bias bias sampling",
      "platforms": {
        "instagram": {
          "video_id": "C1R1du7g-7I",
          "url": "https://www.instagram.com/reel/C1R1du7g-7I",
          "view_count": 2682156,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        }
      },
      "rank": 1
    },
    {
      "group_id": null,
      "title": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning",
      "description": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning",
      "upload_date": "2022-04-05",
      "total_views": 565300,
      "max_views": 565300,
      "topics": [
        "datascience",
        "learning",
        "machinelearning",
        "reinforcement",
        "reinforcementlearning",
        "techtok"
      ],
      "search_text": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning datascience learning machinelearning reinforcement reinforcementlearning techtok Why do you like watermelon over poop? Well, how are you going to teach that to an AI? Guess what? I did. The way my little robot friend works is they get a positive reward when they find watermelon, and they get a negative reward when they eat poop. This approach is known as reinforcement learning, and it's actually growing within computer science as a way to train robots, whether real or virtual. The code for this is freely available. I'll add it in the comments.",
      "platforms": {
        "tiktok": {
          "video_id": "7083127737417747755",
          "url": "https://www.tiktok.com/@rajistics/video/7083127737417747755",
          "view_count": 565300,
          "upload_date": "2022-04-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4312800fc1ed4c639d4a8b4834dc6d7d_1649169204~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=9NIcuDBAhesZlO7abjw85gwLUpc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 2
    },
    {
      "group_id": 6409,
      "title": "Google dropped Gemini. Let's talk about the different sizes tweaked benchmarks multimodal trained on TPUs and how it's not that exciting. #gpt4 #gemini #google #rajistics",
      "description": "Google dropped Gemini. Let's talk about the different sizes tweaked benchmarks multimodal trained on TPUs and how it's not that exciting. #gpt4 #gemini #google #rajistics",
      "upload_date": "2023-12-06",
      "total_views": 253633,
      "max_views": 178200,
      "topics": [
        "different",
        "dropped",
        "gemini",
        "google",
        "gpt4",
        "let",
        "sizes",
        "talk",
        "tweaked"
      ],
      "search_text": "Google dropped Gemini. Let's talk about the different sizes tweaked benchmarks multimodal trained on TPUs and how it's not that exciting. #gpt4 #gemini #google #rajistics different dropped gemini google gpt4 let sizes talk tweaked",
      "platforms": {
        "tiktok": {
          "video_id": "7309592615421250862",
          "url": "https://www.tiktok.com/@rajistics/video/7309592615421250862",
          "view_count": 178200,
          "upload_date": "2023-12-06",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0hrw5zggLS",
          "url": "https://www.instagram.com/reel/C0hrw5zggLS",
          "view_count": 42622,
          "upload_date": "2023-12-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EpdoG7L9kDI",
          "url": "https://youtube.com/shorts/EpdoG7L9kDI",
          "view_count": 32811,
          "upload_date": "2023-12-06",
          "thumbnail_url": ""
        }
      },
      "rank": 3
    },
    {
      "group_id": 6110,
      "title": "Sampling bias",
      "description": "Sampling bias",
      "upload_date": "2023-12-25",
      "total_views": 235783,
      "max_views": 227600,
      "topics": [
        "bias",
        "enjoy",
        "holidays",
        "original",
        "sampling",
        "see",
        "sketchplanations"
      ],
      "search_text": "Sampling bias bias enjoy holidays original sampling see sketchplanations",
      "platforms": {
        "tiktok": {
          "video_id": "7316532440141548846",
          "url": "https://www.tiktok.com/@rajistics/video/7316532440141548846",
          "view_count": 227600,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17939423543774870",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "csNtgkiHV9k",
          "url": "https://youtube.com/shorts/csNtgkiHV9k",
          "view_count": 8183,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        }
      },
      "rank": 4
    },
    {
      "group_id": 6661,
      "title": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics",
      "description": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics",
      "upload_date": "2024-02-11",
      "total_views": 166921,
      "max_views": 166921,
      "topics": [
        "curseofdimensionality",
        "datascience",
        "feature",
        "featureselection",
        "machinelearning",
        "selection"
      ],
      "search_text": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics curseofdimensionality datascience feature featureselection machinelearning selection",
      "platforms": {
        "instagram": {
          "video_id": "C3NtrNOA05S",
          "url": "https://www.instagram.com/reel/C3NtrNOA05S",
          "view_count": 166921,
          "upload_date": "2024-02-11",
          "thumbnail_url": ""
        }
      },
      "rank": 5
    },
    {
      "group_id": 6451,
      "title": "There are lots of open-source code assistant tools. Starcoder is the best known but many people are training and fine-tuning their own models. #machinelearning #bigcode #starcoder #copilot #textgenerationinference #sqlcoder #tabby #refact Big Code Leaderboard: https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard SQLCoder: https://github.com/defog-ai/sqlcoder StarCoder: https://huggingface.co/bigcode/starcoder Text Generation Inference: https://github.com/huggingface/text-generation-inference Other Products: https://refact.ai/ https://tabby.tabbyml.com/",
      "description": "There are lots of open-source code assistant tools. Starcoder is the best known but many people are training and fine-tuning their own models. #machinelearning #bigcode #starcoder #copilot #textgenerationinference #sqlcoder #tabby #refact Big Code Leaderboard: https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard SQLCoder: https://github.com/defog-ai/sqlcoder StarCoder: https://huggingface.co/bigcode/starcoder Text Generation Inference: https://github.com/huggingface/text-generation-inference Other Products: https://refact.ai/ https://tabby.tabbyml.com/",
      "upload_date": "2023-09-30",
      "total_views": 90205,
      "max_views": 85300,
      "topics": [
        "bigcode",
        "copilot",
        "github",
        "languagemodels",
        "machinelearning",
        "models",
        "open",
        "refact",
        "source",
        "sqlcoder",
        "starcoder",
        "tabby"
      ],
      "search_text": "There are lots of open-source code assistant tools. Starcoder is the best known but many people are training and fine-tuning their own models. #machinelearning #bigcode #starcoder #copilot #textgenerationinference #sqlcoder #tabby #refact Big Code Leaderboard: https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard SQLCoder: https://github.com/defog-ai/sqlcoder StarCoder: https://huggingface.co/bigcode/starcoder Text Generation Inference: https://github.com/huggingface/text-generation-inference Other Products: https://refact.ai/ https://tabby.tabbyml.com/ bigcode copilot github languagemodels machinelearning models open refact source sqlcoder starcoder tabby",
      "platforms": {
        "tiktok": {
          "video_id": "7284750285875121454",
          "url": "https://www.tiktok.com/@rajistics/video/7284750285875121454",
          "view_count": 85300,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cx1TzNBr931",
          "url": "https://www.instagram.com/reel/Cx1TzNBr931",
          "view_count": 3307,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ujHOLpK49W0",
          "url": "https://www.youtube.com/watch?v=ujHOLpK49W0",
          "view_count": 1598,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        }
      },
      "rank": 6
    },
    {
      "group_id": 6470,
      "title": "Toolformer from Meta shows the possibilities of using APIs in an unsupervised way. #datascience #machinelearning #toolformer #largelanguagemodels ",
      "description": "Toolformer from Meta shows the possibilities of using APIs in an unsupervised way. #datascience #machinelearning #toolformer #largelanguagemodels ",
      "upload_date": "2023-02-13",
      "total_views": 88831,
      "max_views": 87800,
      "topics": [
        "apis",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "meta",
        "model",
        "shows",
        "toolformer",
        "tools"
      ],
      "search_text": "Toolformer from Meta shows the possibilities of using APIs in an unsupervised way. #datascience #machinelearning #toolformer #largelanguagemodels  apis datascience largelanguagemodels machinelearning meta model shows toolformer tools Meta released a paper this week and it's blowing everyone away. Just like humans learned how to use tools by themselves, Meta's taught a language model how to start using tools like a calculator, a question answering system, all by itself. And you'll see as part of the prediction, it's calling an API, whether it's a calculator, a QA system, a search engine, it knows how to do that and use those appropriately as part of the predictions. To teach the model how to do this, the model was pre-trained with information on how to call APIs. Essentially, they used an API tag. Now, the performance of the model when it's using its ability to call these other tools ends up being a lot higher than GPT-3, even though it's a smaller model. This is the brilliant part of this model for me, is we're saving the model for reasoning tasks and then taking advantage of its ability to call these other tools to then extend its knowledge in that way. Unlike the other examples I've shown earlier where programmers explicitly hooked up APIs with tools like LangChain, in this case, the model itself is figuring out when and how to best call these other APIs.",
      "platforms": {
        "tiktok": {
          "video_id": "7199661707696835882",
          "url": "https://www.tiktok.com/@rajistics/video/7199661707696835882",
          "view_count": 87800,
          "upload_date": "2023-02-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a5c0a61eaebe40a28401db641aeae65b_1676301902~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=rrjryxFC5JMOSV2dg3d1y5LNJbw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "Com8rCpA-3Z",
          "url": "https://www.instagram.com/reel/Com8rCpA-3Z/",
          "view_count": 166,
          "upload_date": "2023-02-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "lY1PtHN8UoU",
          "url": "https://youtube.com/shorts/lY1PtHN8UoU?feature=share",
          "view_count": 865,
          "upload_date": "2023-02-13",
          "thumbnail_url": ""
        }
      },
      "rank": 7
    },
    {
      "group_id": null,
      "title": "Transformer Explainer is an interactive visualization tool to allow people to understand how transformers work through an end-to-end visualization of the flow of data through the model. Transformer Explainer: https://poloclub.github.io/transformer-explainer/ Let's build GPT: from scratch, in code, spelled out: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3301s",
      "description": "Transformer Explainer is an interactive visualization tool to allow people to understand how transformers work through an end-to-end visualization of the flow of data through the model. Transformer Explainer: https://poloclub.github.io/transformer-explainer/ Let's build GPT: from scratch, in code, spelled out: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3301s",
      "upload_date": "2024-08-11",
      "total_views": 69100,
      "max_views": 69100,
      "topics": [
        "end",
        "explainer",
        "information",
        "transformer",
        "transformers",
        "visualization"
      ],
      "search_text": "Transformer Explainer is an interactive visualization tool to allow people to understand how transformers work through an end-to-end visualization of the flow of data through the model. Transformer Explainer: https://poloclub.github.io/transformer-explainer/ Let's build GPT: from scratch, in code, spelled out: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3301s end explainer information transformer transformers visualization We finally have it, a visualization tool to show how transformers work. This is important because transformers are the biggest advance in AI over the last 10 years and what models like GPT-5 will be built on. So let me tell you five things I like about it and then we'll do a quick tour. This visualization gives you an end-to-end view for what's going on in a transformer. It's interactive, it's based on a real transformer, GPT-2, but it all runs inside your browser and it's open source. So let's use it. I added my own settings here. If we vary the temperature parameter, you'll see how that probability distribution shifts. This is how we can get models to be more creative as we increase temperature. Now let's follow an end-to-end prediction where we're trying to predict the next word. The first thing is you'll see as we convert all that text information into numerical embeddings, as well as including the positional information. All that information goes into the attention layer and there's a nice visualization here that shows you the importance of context. This is a fundamental advance of transformers over other methods like bag of words and word to vet, where now, for example, you'll see we're considering the relationships between words. The visualization shows you, for example, that the relationship between cat and sat is very important while the color of the cat black doesn't carry much information. The visualization and the accompanying text try to help you explain how this calculation comes to be, but it really takes a bit of matrix multiplication to understand it. I find Carpathi's discussion of it the best here and kind of working through that to get your mind understanding exactly how those calculations are made, but nevertheless, this visualization shows you that, as well as how that information is then passed through the rest of the transformer matrix to get you those predictions. Go spend time with this explainer, try it out. You're still going to have to learn the fundamentals of matrix multiplication, but this is a great tool.",
      "platforms": {
        "tiktok": {
          "video_id": "7401905525065289006",
          "url": "https://www.tiktok.com/@rajistics/video/7401905525065289006",
          "view_count": 69100,
          "upload_date": "2024-08-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9c16feef84a84fd58ea76bff8367b90a_1723390441~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=WDP3ydZKHOns%2BC1oryINyQf%2Fr6Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 8
    },
    {
      "group_id": 6438,
      "title": "Text to SQL is now easier with a large language model released by Numbers Station called NSQL. #largelanguagemodels #nsql #numberstation #machinelearning Introducing NSQL: Open-source SQL Copilot Foundation Models: https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models",
      "description": "Text to SQL is now easier with a large language model released by Numbers Station called NSQL. #largelanguagemodels #nsql #numberstation #machinelearning Introducing NSQL: Open-source SQL Copilot Foundation Models: https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models",
      "upload_date": "2023-07-06",
      "total_views": 68107,
      "max_views": 64800,
      "topics": [
        "introducing",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "nsql",
        "numbers",
        "numberstation",
        "sequel",
        "sql",
        "station",
        "text"
      ],
      "search_text": "Text to SQL is now easier with a large language model released by Numbers Station called NSQL. #largelanguagemodels #nsql #numberstation #machinelearning Introducing NSQL: Open-source SQL Copilot Foundation Models: https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models introducing largelanguagemodels machinelearning model nsql numbers numberstation sequel sql station text",
      "platforms": {
        "tiktok": {
          "video_id": "7252823135181917482",
          "url": "https://www.tiktok.com/@rajistics/video/7252823135181917482",
          "view_count": 64800,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CuXw45fgmNj",
          "url": "https://www.instagram.com/reel/CuXw45fgmNj",
          "view_count": 810,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EraaXRbsBZs",
          "url": "https://www.youtube.com/watch?v=EraaXRbsBZs",
          "view_count": 2497,
          "upload_date": "2023-07-07",
          "thumbnail_url": ""
        }
      },
      "rank": 9
    },
    {
      "group_id": null,
      "title": "Med-Palm from Google for answering medical and clinical knowledge. #datascience #machinelearning #largelanguagemodels #medpalm ",
      "description": "Med-Palm from Google for answering medical and clinical knowledge. #datascience #machinelearning #largelanguagemodels #medpalm ",
      "upload_date": "2022-12-29",
      "total_views": 68100,
      "max_views": 68100,
      "topics": [
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "medpalm",
        "model",
        "models"
      ],
      "search_text": "Med-Palm from Google for answering medical and clinical knowledge. #datascience #machinelearning #largelanguagemodels #medpalm  datascience largelanguagemodels machinelearning medpalm model models Google's just dropped a paper on one of their large language models, shows us where they are, and how much better than chat GPT we can get. The model is MedPOM, and the goal is to answer medical questions, and they want to set the bar pretty high by getting to the level of a clinician. Here's an example of a question and an answer that the model would expect. Looking at the performance, you can see it's quite a bit better than past models, including some of the open source models. The secret sauce here was using instruction prompt tuning, which was a lightweight way of getting the model tuned to the medical domain. When we start evaluating the model, we'll see a familiar pattern we've often seen with large language models, where they appear better than they actually are. MedPOM does a great job of understanding the intent of the question, but look where it falls down a little bit on actually being able to solve the question. And it's simply because a lot of times the model will give the wrong answer. Eek. For those of you that follow me, this isn't a surprise at all, but it's a great example of how these models are slowly getting better.",
      "platforms": {
        "tiktok": {
          "video_id": "7182618726355733803",
          "url": "https://www.tiktok.com/@rajistics/video/7182618726355733803",
          "view_count": 68100,
          "upload_date": "2022-12-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/eaa10d945ec94dba9111d0a4bdeb3f61_1672333759~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=5hia6aIqqRMM74QUDHIwu%2B557Lg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 10
    },
    {
      "group_id": 6466,
      "title": "Control vectors are getting more widely supported most recently in Llama.cpp. It‚Äôs another useful technique alongside prompting fine tuning and logit bias. Resources: What is Representation Engineering: https://vgel.me/posts/representation-engineering/ Representation Engineering: A Top-Down Approach to AI Transparency - https://arxiv.org/abs/2310.01405 A library for making RepE control vectors: https://github.com/vgel/repeng/tree/main",
      "description": "Control vectors are getting more widely supported most recently in Llama.cpp. It‚Äôs another useful technique alongside prompting fine tuning and logit bias. Resources: What is Representation Engineering: https://vgel.me/posts/representation-engineering/ Representation Engineering: A Top-Down Approach to AI Transparency - https://arxiv.org/abs/2310.01405 A library for making RepE control vectors: https://github.com/vgel/repeng/tree/main",
      "upload_date": "2024-03-17",
      "total_views": 65529,
      "max_views": 59473,
      "topics": [
        "control",
        "engineering",
        "getting",
        "like",
        "prompting",
        "representation",
        "vectors",
        "vgel"
      ],
      "search_text": "Control vectors are getting more widely supported most recently in Llama.cpp. It‚Äôs another useful technique alongside prompting fine tuning and logit bias. Resources: What is Representation Engineering: https://vgel.me/posts/representation-engineering/ Representation Engineering: A Top-Down Approach to AI Transparency - https://arxiv.org/abs/2310.01405 A library for making RepE control vectors: https://github.com/vgel/repeng/tree/main control engineering getting like prompting representation vectors vgel I've done it. One vector for controlling those LLMs. Big deal, we already have ways of controlling LMs through prompting and fine tuning. Prompting, that's like sculpting a statue with a sledgehammer. I know, I fiddle around forever trying to get the prompting right. Exactly. That's why I'm using control vectors from representational engineering. Control vectors represent a particular concept, say happiness and we take that vectors, we add it to the hidden states at inference time and that influences the output. So, what makes this better than prompting. Control vectors come with a coefficient so you can tune them as you want. That's better than rewriting the prompt in 10 different ways but how do you get those control vectors? So, start by building a data set of contrasting pairs around the concept that you care about. What we're going to want to look at is what are the hidden states of each of these contrasting pairs and then in this case, what I did was I collapsed the differences of those hidden states into one vectors and that's the vectors that represents the context inside the LM. So many possibilities here I get the obvious ones like hardworking and harmlessness but I see some fun possibilities around things like laziness, creativity, self aware,",
      "platforms": {
        "tiktok": {
          "video_id": "7347380270057770283",
          "url": "https://www.tiktok.com/@rajistics/video/7347380270057770283",
          "view_count": 5140,
          "upload_date": "2024-03-17",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4n4y_FA0I8",
          "url": "https://www.instagram.com/reel/C4n4y_FA0I8",
          "view_count": 59473,
          "upload_date": "2024-03-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "u8KUQlfgTrg",
          "url": "https://youtube.com/shorts/u8KUQlfgTrg",
          "view_count": 916,
          "upload_date": "2024-03-17",
          "thumbnail_url": ""
        }
      },
      "rank": 11
    },
    {
      "group_id": null,
      "title": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering ",
      "description": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering ",
      "upload_date": "2022-12-31",
      "total_views": 64800,
      "max_views": 64800,
      "topics": [
        "clustering",
        "kmeans",
        "like",
        "look",
        "means",
        "one"
      ],
      "search_text": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering  clustering kmeans like look means one Hey, I know K-means is a great way to cluster, but it's not working well for me. This is why there's lots of clustering approaches. Let's walk through your issues together. Take a look at this one. I'm confused why it didn't work. Ah, I see what's going on. Look at the axes. The scales are different. Here's an example to highlight what I'm talking about, where we have some data, but look at the axes. When we run K-means initially, it doesn't group them like we expect to. But look, once we normalize them, we get the clusters that we expect. How about this one? I know the diameters of the clusters are very different and could be a factor. Yeah, you're right. You might want to use another technique like Gaussian mixture models. I'd experiment a little bit. And look, K-means totally failed on this one. Yeah, K-means isn't going to work right here. You might want to use a density-based approach. See how this density-based approach works on this smiley face type thing and identifies the different clusters? Something like that might be useful. And this last one is I was working with TFIDF and taking all the different words and then trying to see the clusters here. It doesn't work very well. With high-dimensional data, one of the go-to approaches I like to use is HDB scan. You can take a look at this example and see like the difference between K-means, which doesn't group the data as you expect, versus when you use HDB scan, it finds those clusters, groups them together well.",
      "platforms": {
        "tiktok": {
          "video_id": "7183366473270791467",
          "url": "https://www.tiktok.com/@rajistics/video/7183366473270791467",
          "view_count": 64800,
          "upload_date": "2022-12-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7456c787999d473799db1da1376bddbb_1672507862~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=VIpctHmvwmKfOFCJWUKQye%2FTp0g%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 12
    },
    {
      "group_id": null,
      "title": "Very excited and Richard isn’t paying me for this - #codetok #youdotcom #codingtiktok  #python",
      "description": "Very excited and Richard isn’t paying me for this - #codetok #youdotcom #codingtiktok  #python",
      "upload_date": "2022-05-06",
      "total_views": 64100,
      "max_views": 64100,
      "topics": [
        "code",
        "codetok",
        "codingtiktok",
        "like",
        "python",
        "youdotcom"
      ],
      "search_text": "Very excited and Richard isn’t paying me for this - #codetok #youdotcom #codingtiktok  #python code codetok codingtiktok like python youdotcom I just found out about this and it's going to change how I code. Typically if I'm looking for a code snippet, I'll type it into Google, they'll give me some search results, I'll end up having to go to like five or ten pages to figure out what I want to do. Check out this code.u.com. Like you get back great snippets and search results. Like I'm going to start using this now whenever I need to find some code snippets.",
      "platforms": {
        "tiktok": {
          "video_id": "7094738398375660846",
          "url": "https://www.tiktok.com/@rajistics/video/7094738398375660846",
          "view_count": 64100,
          "upload_date": "2022-05-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0b479b6102b94895b1750f2556b9160c_1651872510~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=tVvsBlfnGfqTntFFGAAS1ntAOT4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 13
    },
    {
      "group_id": null,
      "title": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches.  #datascience #machinelearning #transformers #climatemodel ",
      "description": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches.  #datascience #machinelearning #transformers #climatemodel ",
      "upload_date": "2023-02-08",
      "total_views": 61400,
      "max_views": 61400,
      "topics": [
        "climatemodel",
        "climax",
        "datascience",
        "machinelearning",
        "models",
        "transformers"
      ],
      "search_text": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches.  #datascience #machinelearning #transformers #climatemodel  climatemodel climax datascience machinelearning models transformers Climax. Would you like to learn more about it? Climax is a machine learning model for predicting the weather and making climate projections. Before Climax, people relied on traditional physics-based models. Now these models used things like differential equations, but they were limited when we needed to focus on a specific region or really short or long time spans. The kinds of inputs that go into Climax include what is the local climate? What's the spatial area? What's the timing? Well, seems pretty relevant to me. The flexibility of transformers allowed the output to be, could be a specific region, could be a different task, could be a different type of variable. And the results showed that these models were competitive or even better than traditional models such as the integrated forecast system or IFS. So even if you aren't interested in weather, Climax is a great example of the flexibility and adaptability of modern transformer approaches.",
      "platforms": {
        "tiktok": {
          "video_id": "7197596884641451310",
          "url": "https://www.tiktok.com/@rajistics/video/7197596884641451310",
          "view_count": 61400,
          "upload_date": "2023-02-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5485147bb1a9411b8e764c464bb88876_1675821129~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=bk82n5ZZFEDgSGpPPycu2kXynt4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 14
    },
    {
      "group_id": 6501,
      "title": "NASA uses generative AI for manufacturing parts for space. It's a great use of generative technology and you can start seeing how it will change engineering in the long run. Check out more of Ryan McClelland's work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland ‚Äì NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics",
      "description": "NASA uses generative AI for manufacturing parts for space. It's a great use of generative technology and you can start seeing how it will change engineering in the long run. Check out more of Ryan McClelland's work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland ‚Äì NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics",
      "upload_date": "2023-10-28",
      "total_views": 61097,
      "max_views": 41527,
      "topics": [
        "61",
        "design",
        "generative",
        "generativeai",
        "manufacturing",
        "nasa"
      ],
      "search_text": "NASA uses generative AI for manufacturing parts for space. It's a great use of generative technology and you can start seeing how it will change engineering in the long run. Check out more of Ryan McClelland's work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland ‚Äì NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics 61 design generative generativeai manufacturing nasa",
      "platforms": {
        "tiktok": {
          "video_id": "7294999469966265642",
          "url": "https://www.tiktok.com/@rajistics/video/7294999469966265642",
          "view_count": 19100,
          "upload_date": "2023-10-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cy8a_7TgC6y",
          "url": "https://www.instagram.com/reel/Cy8a_7TgC6y",
          "view_count": 41527,
          "upload_date": "2023-10-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_KTvQlT-tfo",
          "url": "https://www.youtube.com/watch?v=_KTvQlT-tfo",
          "view_count": 470,
          "upload_date": "2023-10-28",
          "thumbnail_url": ""
        }
      },
      "rank": 15
    },
    {
      "group_id": 6549,
      "title": "AI News for the week featuring OpenAI NVIDIA Google Apple Stanford Hugging Face Anthropic and Microsoft. #machinelearning #openai #rajistics #nvidia",
      "description": "AI News for the week featuring OpenAI NVIDIA Google Apple Stanford Hugging Face Anthropic and Microsoft. #machinelearning #openai #rajistics #nvidia",
      "upload_date": "2023-10-21",
      "total_views": 59515,
      "max_views": 48439,
      "topics": [
        "anthropic",
        "featuring",
        "google",
        "machinelearning",
        "news",
        "nvidia",
        "openai",
        "stanford",
        "week"
      ],
      "search_text": "AI News for the week featuring OpenAI NVIDIA Google Apple Stanford Hugging Face Anthropic and Microsoft. #machinelearning #openai #rajistics #nvidia anthropic featuring google machinelearning news nvidia openai stanford week",
      "platforms": {
        "tiktok": {
          "video_id": "7292239571583192366",
          "url": "https://www.tiktok.com/@rajistics/video/7292239571583192366",
          "view_count": 11000,
          "upload_date": "2023-10-21",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CypRjAwAn-b",
          "url": "https://www.instagram.com/reel/CypRjAwAn-b",
          "view_count": 48439,
          "upload_date": "2023-10-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "oPQAJBWIstk",
          "url": "https://www.youtube.com/watch?v=oPQAJBWIstk",
          "view_count": 76,
          "upload_date": "2023-10-22",
          "thumbnail_url": ""
        }
      },
      "rank": 16
    },
    {
      "group_id": null,
      "title": "Simply explaining how ChatGPT works. All the technical details of ChatGPT have not been released, so this is based on what OpenAI has been doing over the last few years. #datascience #machinelearning #openai #chatgpt #reinforcementlearning ",
      "description": "Simply explaining how ChatGPT works. All the technical details of ChatGPT have not been released, so this is based on what OpenAI has been doing over the last few years. #datascience #machinelearning #openai #chatgpt #reinforcementlearning ",
      "upload_date": "2022-12-07",
      "total_views": 56100,
      "max_views": 56100,
      "topics": [
        "answers",
        "chatgpt",
        "datascience",
        "machinelearning",
        "openai",
        "reinforcementlearning"
      ],
      "search_text": "Simply explaining how ChatGPT works. All the technical details of ChatGPT have not been released, so this is based on what OpenAI has been doing over the last few years. #datascience #machinelearning #openai #chatgpt #reinforcementlearning  answers chatgpt datascience machinelearning openai reinforcementlearning ChatGPT has blown up big this week. Let me take some time to explain to you exactly how it works. And this will also let you understand its limitations. Now, the way large language models work is they try to predict the next word. And by doing this over and over again, they could start putting together long pieces of text that are readable. A lot of these language models put together readable text. But if we actually start looking at the text, we'll notice that there's subtle errors that actually have a big impact on the meaning of it. So how can we do better? How about giving the model some feedback on what a good answer is? And that's what the team did. They took a bunch of human labors, gave them answers and then had them say, are these good quality answers? Can you rank how well these answers are? Are these answers actually correct? And once they did this, the model was so much better. The answers were such higher quality. And this is why ChatGPT writes so well. It's been graded by average people and it now knows how to write above average. Now, where it's going to fail, of course, is as soon as you get to a specific subject area that our human labors weren't able to understand if that was a good answer or a bad answer, because it just looks really complicated. That's the limitation. And my guess is open AI is next going to go after a lot of those different domains by adding more instructions and fine tuning in those areas to boost those models.",
      "platforms": {
        "tiktok": {
          "video_id": "7174487697849945386",
          "url": "https://www.tiktok.com/@rajistics/video/7174487697849945386",
          "view_count": 56100,
          "upload_date": "2022-12-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/58eb630117c347138069b8d8454ed91d_1670440601~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=ElDYKOkLHXu0cEY4YoYdJOaodBk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 17
    },
    {
      "group_id": null,
      "title": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here.  #datascience #machinelearning #twitter #recommenders ",
      "description": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here.  #datascience #machinelearning #twitter #recommenders ",
      "upload_date": "2023-04-01",
      "total_views": 55500,
      "max_views": 55500,
      "topics": [
        "code",
        "datascience",
        "fun",
        "machinelearning",
        "recommenders",
        "twitter"
      ],
      "search_text": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here.  #datascience #machinelearning #twitter #recommenders  code datascience fun machinelearning recommenders twitter Twitter open sourced its algorithm today and let me tell you, it's a lot of fun rolling around in someone else's code. Look at this code snippet. You can see here they've added information on Elon Musk and if people are Republicans or Democrats, to be able to track how their tweets do. This is huge because it gives us a production example of a recommender system. Most companies don't share their recommender systems and for those of you learning recommender systems, it's often hard to find good examples. Now we have one. Now we don't have all the details. There's lots of information missing on the actual training data, the weights of the models, even how some of the features were derived. But we can start to see the skeleton of how they put that together and it's fun to kind of look at some of the actual code that was written. From what I'm seeing, the recommendation system here is pretty much what we expected. A lot of it has already been published in papers. There's a couple of different set of recommendations. They have a real graph based on who you follow. They have a social graph based on who you have engaged with and finally they use an embedding space that figures out what tweets and users are similar to your own interests. The embedding spaces uses SIM clusters, which again is another thing that's been published about on this. All three of these sources are combined to give you recommendations. There's a lot of other details and other filtering that they do. Have fun. Dive into it. But nothing earth shattering in here.",
      "platforms": {
        "tiktok": {
          "video_id": "7216864772988390702",
          "url": "https://www.tiktok.com/@rajistics/video/7216864772988390702",
          "view_count": 55500,
          "upload_date": "2023-04-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/dcb303c4b098488c87c706a463eb1aaf_1680307286~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=J8F5W3TNAPeJi4kAI%2FaneiKnWx4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 18
    },
    {
      "group_id": null,
      "title": "Tensorboard embedding projector - repost",
      "description": "Tensorboard embedding projector - repost",
      "upload_date": "2024-07-01",
      "total_views": 52400,
      "max_views": 52400,
      "topics": [
        "also",
        "cold",
        "concepts",
        "look",
        "see",
        "tensorboard"
      ],
      "search_text": "Tensorboard embedding projector - repost also cold concepts look see tensorboard Ready to see how AI thinks about the world? Take a look at cold. We'll see some concepts like hot and spring that we associate with cold. But this AI has also learned history and you'll see concepts like Soviet and war are also related to cold. I'm using the TensorBoard embedding projector to be able to see these embeddings or vector representations of words, in this case a word defect model. Let's look at sexy. You'll see some of the concepts that you're probably associated with such as Baird and Curvy. But look at this AI. It also associates Groucho and Wigs with sexy.",
      "platforms": {
        "tiktok": {
          "video_id": "7386702177894518059",
          "url": "https://www.tiktok.com/@rajistics/video/7386702177894518059",
          "view_count": 52400,
          "upload_date": "2024-07-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c00f82faa5b24991a497458bf554aec9_1719850632~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=JGN8qjdRvGPcy7meSeU78c6n4%2B0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 19
    },
    {
      "group_id": null,
      "title": "#onthisday tensorboard embedding projector. Let me know if i should reshare these older videos.",
      "description": "#onthisday tensorboard embedding projector. Let me know if i should reshare these older videos.",
      "upload_date": "2023-07-01",
      "total_views": 49500,
      "max_views": 49500,
      "topics": [
        "embedding",
        "know",
        "let",
        "onthisday",
        "projector",
        "tensorboard"
      ],
      "search_text": "#onthisday tensorboard embedding projector. Let me know if i should reshare these older videos. embedding know let onthisday projector tensorboard",
      "platforms": {
        "tiktok": {
          "video_id": "7250980121656446254",
          "url": "https://www.tiktok.com/@rajistics/video/7250980121656446254",
          "view_count": 49500,
          "upload_date": "2023-07-01",
          "thumbnail_url": ""
        }
      },
      "rank": 20
    },
    {
      "group_id": null,
      "title": "This happens. #datascience #machinelearning #python #codetok #programming",
      "description": "This happens. #datascience #machinelearning #python #codetok #programming",
      "upload_date": "2022-05-04",
      "total_views": 48900,
      "max_views": 48900,
      "topics": [
        "codetok",
        "datascience",
        "happens",
        "machinelearning",
        "programming",
        "python"
      ],
      "search_text": "This happens. #datascience #machinelearning #python #codetok #programming codetok datascience happens machinelearning programming python World Cup Music",
      "platforms": {
        "tiktok": {
          "video_id": "7093913682912087342",
          "url": "https://www.tiktok.com/@rajistics/video/7093913682912087342",
          "view_count": 48900,
          "upload_date": "2022-05-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4ba5827eccde46afac625d1acad2fc0c_1651680492~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=2X9RPDsebs1MT2n7N00ajXLZiFs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 21
    },
    {
      "group_id": 6410,
      "title": "Vector Databases (Pinecone)",
      "description": "Vector Databases (Pinecone)",
      "upload_date": "2023-04-17",
      "total_views": 48861,
      "max_views": 24599,
      "topics": [
        "chroma",
        "databases",
        "embeddings",
        "faiss",
        "milvus",
        "pinecone",
        "vector",
        "weaviate"
      ],
      "search_text": "Vector Databases (Pinecone) chroma databases embeddings faiss milvus pinecone vector weaviate",
      "platforms": {
        "tiktok": {
          "video_id": "7222674900329319723",
          "url": "https://www.tiktok.com/@rajistics/video/7222674900329319723",
          "view_count": 19000,
          "upload_date": "2023-04-16",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CrGlU69AtbH",
          "url": "https://www.instagram.com/reel/CrGlU69AtbH",
          "view_count": 5262,
          "upload_date": "2023-04-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NfvcN7atiME",
          "url": "https://www.youtube.com/watch?v=NfvcN7atiME",
          "view_count": 24599,
          "upload_date": "2023-04-17",
          "thumbnail_url": ""
        }
      },
      "rank": 22
    },
    {
      "group_id": null,
      "title": "Some general advice on how to evaluate software packages. #datascience #machinelearning #github",
      "description": "Some general advice on how to evaluate software packages. #datascience #machinelearning #github",
      "upload_date": "2022-11-30",
      "total_views": 47700,
      "max_views": 47700,
      "topics": [
        "github",
        "great",
        "look",
        "package",
        "see",
        "want"
      ],
      "search_text": "Some general advice on how to evaluate software packages. #datascience #machinelearning #github github great look package see want Let's talk about something that's not in any data science textbook, but it's super important for you to know how to do. And that's how to evaluate a package that you find on GitHub. I want to do this with Shapp, and I want to first say Shapp is an awesome package and a thousand times better than anything I would do. And if we had more software design like that, we would be better off. But let's use that as an example to show you how I would evaluate a package. So these top numbers are really important. The first thing is don't overvalue stars. It's often more about popularity than the quality of the package. Just use it as one useful signal. But watches and forks are also good signals. Forks is especially so because it shows that people are digging into it, kind of building their own version. Now one red flag, do you see it right away here? Look when that last update was. That's something that would draw me a little bit of back is you want typically a package that's going to be updated more frequently. They're used by a super impressive. That's a great signal for a quality package if there's other packages that are based on it. Also the number of contributors is useful, but that distribution of how much you're contributing is comes into play. Very important. We'll talk about that in a few minutes. And all of you by now should be able to evaluate readme's, right? You want to know how to install the package. You want some code snippets to get started right away. The Shapp readme is a great textbook example of how to do it right. Looking at the release history, we don't like the June 15th release, but if you look beyond that, you'll see that over the last couple of years, there's been consistent releases every few months, which is a great sign of a healthy package. Here's a big red flag on the issues. Do you see it? Look at the ratio of open to closed. When I look at the most recently closed issues, what I find is is the people that open the issues were the ones that closed them. It wasn't a case of the community helping these people. And that's not great to see. You'd rather see the community being active and participating in the issues page. Having 75 open poll requests shows a ton of interest in the package. Lots of people are trying to contribute, but again, the red flag is, is the last closed poll request was back in June, which isn't great. The insights page gives us a historical view of the package. So we can see in this case that most of the contributions were in 2021, not so much. It slowed down recently. Now, when we look at contributors, here's where you see a massive imbalance. You see Scott, who's been the major, major developer of this package is the responsible for the vast majority of commits and lines of codes. There are some other contributors, but they're much smaller down. GitHub has a lot more insights available. There's probably other good signals for the quality of a package as well. I'll let the comments sort that out.",
      "platforms": {
        "tiktok": {
          "video_id": "7171650670175554858",
          "url": "https://www.tiktok.com/@rajistics/video/7171650670175554858",
          "view_count": 47700,
          "upload_date": "2022-11-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b27095cb8d2e4e858d5328056a24a8c7_1669780053~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=55VnH2B%2B%2FNSe5Ic8R2Ly8EEFtKQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 23
    },
    {
      "group_id": null,
      "title": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning ",
      "description": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning ",
      "upload_date": "2022-12-20",
      "total_views": 46400,
      "max_views": 46400,
      "topics": [
        "clouds",
        "datascience",
        "machinelearning",
        "model",
        "openai",
        "point"
      ],
      "search_text": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning  clouds datascience machinelearning model openai point OpenAI dropped another game changer this week. It's a model that goes from text to 3D point clouds. This is going to affect games, virtual reality, industrial design. It's going to be cool. The model works by taking the text prompt, goes into the glide model, which creates a synthetic view of that. Then from the synthetic view, there's a point cloud diffusion model that makes the point cloud. Just like OpenAI did with Dolly, now you can generate 3D point clouds of things that never existed before. Pretty cool. But this table here is why this is such a game changer. They've dramatically increased the speed up to 600 times faster than older approaches. Hugginface has a demo up there. Go check it out. It's a little lower quality, but you can still do things like, for example, check out this motorcycle.",
      "platforms": {
        "tiktok": {
          "video_id": "7179365967426866478",
          "url": "https://www.tiktok.com/@rajistics/video/7179365967426866478",
          "view_count": 46400,
          "upload_date": "2022-12-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1b1cb953e2e844be917b275eb25a4997_1671576409~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=ueOPNDBg0AoKBPrC4AOjlk1tGR8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      },
      "rank": 24
    }
  ],
  "groups": [
    {
      "group_id": 6660,
      "title": "Sampling bias",
      "description": "Sampling bias",
      "upload_date": "2023-12-25",
      "total_views": 2682156,
      "max_views": 2682156,
      "topics": [
        "bias",
        "sampling"
      ],
      "search_text": "Sampling bias bias sampling",
      "platforms": {
        "instagram": {
          "video_id": "C1R1du7g-7I",
          "url": "https://www.instagram.com/reel/C1R1du7g-7I",
          "view_count": 2682156,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning",
      "description": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning",
      "upload_date": "2022-04-05",
      "total_views": 565300,
      "max_views": 565300,
      "topics": [
        "datascience",
        "learning",
        "machinelearning",
        "reinforcement",
        "reinforcementlearning",
        "techtok"
      ],
      "search_text": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning datascience learning machinelearning reinforcement reinforcementlearning techtok Why do you like watermelon over poop? Well, how are you going to teach that to an AI? Guess what? I did. The way my little robot friend works is they get a positive reward when they find watermelon, and they get a negative reward when they eat poop. This approach is known as reinforcement learning, and it's actually growing within computer science as a way to train robots, whether real or virtual. The code for this is freely available. I'll add it in the comments.",
      "platforms": {
        "tiktok": {
          "video_id": "7083127737417747755",
          "url": "https://www.tiktok.com/@rajistics/video/7083127737417747755",
          "view_count": 565300,
          "upload_date": "2022-04-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4312800fc1ed4c639d4a8b4834dc6d7d_1649169204~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=9NIcuDBAhesZlO7abjw85gwLUpc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6409,
      "title": "Google dropped Gemini. Let's talk about the different sizes tweaked benchmarks multimodal trained on TPUs and how it's not that exciting. #gpt4 #gemini #google #rajistics",
      "description": "Google dropped Gemini. Let's talk about the different sizes tweaked benchmarks multimodal trained on TPUs and how it's not that exciting. #gpt4 #gemini #google #rajistics",
      "upload_date": "2023-12-06",
      "total_views": 253633,
      "max_views": 178200,
      "topics": [
        "different",
        "dropped",
        "gemini",
        "google",
        "gpt4",
        "let",
        "sizes",
        "talk",
        "tweaked"
      ],
      "search_text": "Google dropped Gemini. Let's talk about the different sizes tweaked benchmarks multimodal trained on TPUs and how it's not that exciting. #gpt4 #gemini #google #rajistics different dropped gemini google gpt4 let sizes talk tweaked",
      "platforms": {
        "tiktok": {
          "video_id": "7309592615421250862",
          "url": "https://www.tiktok.com/@rajistics/video/7309592615421250862",
          "view_count": 178200,
          "upload_date": "2023-12-06",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0hrw5zggLS",
          "url": "https://www.instagram.com/reel/C0hrw5zggLS",
          "view_count": 42622,
          "upload_date": "2023-12-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EpdoG7L9kDI",
          "url": "https://youtube.com/shorts/EpdoG7L9kDI",
          "view_count": 32811,
          "upload_date": "2023-12-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6110,
      "title": "Sampling bias",
      "description": "Sampling bias",
      "upload_date": "2023-12-25",
      "total_views": 235783,
      "max_views": 227600,
      "topics": [
        "bias",
        "enjoy",
        "holidays",
        "original",
        "sampling",
        "see",
        "sketchplanations"
      ],
      "search_text": "Sampling bias bias enjoy holidays original sampling see sketchplanations",
      "platforms": {
        "tiktok": {
          "video_id": "7316532440141548846",
          "url": "https://www.tiktok.com/@rajistics/video/7316532440141548846",
          "view_count": 227600,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17939423543774870",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "csNtgkiHV9k",
          "url": "https://youtube.com/shorts/csNtgkiHV9k",
          "view_count": 8183,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6661,
      "title": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics",
      "description": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics",
      "upload_date": "2024-02-11",
      "total_views": 166921,
      "max_views": 166921,
      "topics": [
        "curseofdimensionality",
        "datascience",
        "feature",
        "featureselection",
        "machinelearning",
        "selection"
      ],
      "search_text": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics curseofdimensionality datascience feature featureselection machinelearning selection",
      "platforms": {
        "instagram": {
          "video_id": "C3NtrNOA05S",
          "url": "https://www.instagram.com/reel/C3NtrNOA05S",
          "view_count": 166921,
          "upload_date": "2024-02-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6451,
      "title": "There are lots of open-source code assistant tools. Starcoder is the best known but many people are training and fine-tuning their own models. #machinelearning #bigcode #starcoder #copilot #textgenerationinference #sqlcoder #tabby #refact Big Code Leaderboard: https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard SQLCoder: https://github.com/defog-ai/sqlcoder StarCoder: https://huggingface.co/bigcode/starcoder Text Generation Inference: https://github.com/huggingface/text-generation-inference Other Products: https://refact.ai/ https://tabby.tabbyml.com/",
      "description": "There are lots of open-source code assistant tools. Starcoder is the best known but many people are training and fine-tuning their own models. #machinelearning #bigcode #starcoder #copilot #textgenerationinference #sqlcoder #tabby #refact Big Code Leaderboard: https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard SQLCoder: https://github.com/defog-ai/sqlcoder StarCoder: https://huggingface.co/bigcode/starcoder Text Generation Inference: https://github.com/huggingface/text-generation-inference Other Products: https://refact.ai/ https://tabby.tabbyml.com/",
      "upload_date": "2023-09-30",
      "total_views": 90205,
      "max_views": 85300,
      "topics": [
        "bigcode",
        "copilot",
        "github",
        "languagemodels",
        "machinelearning",
        "models",
        "open",
        "refact",
        "source",
        "sqlcoder",
        "starcoder",
        "tabby"
      ],
      "search_text": "There are lots of open-source code assistant tools. Starcoder is the best known but many people are training and fine-tuning their own models. #machinelearning #bigcode #starcoder #copilot #textgenerationinference #sqlcoder #tabby #refact Big Code Leaderboard: https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard SQLCoder: https://github.com/defog-ai/sqlcoder StarCoder: https://huggingface.co/bigcode/starcoder Text Generation Inference: https://github.com/huggingface/text-generation-inference Other Products: https://refact.ai/ https://tabby.tabbyml.com/ bigcode copilot github languagemodels machinelearning models open refact source sqlcoder starcoder tabby",
      "platforms": {
        "tiktok": {
          "video_id": "7284750285875121454",
          "url": "https://www.tiktok.com/@rajistics/video/7284750285875121454",
          "view_count": 85300,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cx1TzNBr931",
          "url": "https://www.instagram.com/reel/Cx1TzNBr931",
          "view_count": 3307,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ujHOLpK49W0",
          "url": "https://www.youtube.com/watch?v=ujHOLpK49W0",
          "view_count": 1598,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6470,
      "title": "Toolformer from Meta shows the possibilities of using APIs in an unsupervised way. #datascience #machinelearning #toolformer #largelanguagemodels ",
      "description": "Toolformer from Meta shows the possibilities of using APIs in an unsupervised way. #datascience #machinelearning #toolformer #largelanguagemodels ",
      "upload_date": "2023-02-13",
      "total_views": 88831,
      "max_views": 87800,
      "topics": [
        "apis",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "meta",
        "model",
        "shows",
        "toolformer",
        "tools"
      ],
      "search_text": "Toolformer from Meta shows the possibilities of using APIs in an unsupervised way. #datascience #machinelearning #toolformer #largelanguagemodels  apis datascience largelanguagemodels machinelearning meta model shows toolformer tools Meta released a paper this week and it's blowing everyone away. Just like humans learned how to use tools by themselves, Meta's taught a language model how to start using tools like a calculator, a question answering system, all by itself. And you'll see as part of the prediction, it's calling an API, whether it's a calculator, a QA system, a search engine, it knows how to do that and use those appropriately as part of the predictions. To teach the model how to do this, the model was pre-trained with information on how to call APIs. Essentially, they used an API tag. Now, the performance of the model when it's using its ability to call these other tools ends up being a lot higher than GPT-3, even though it's a smaller model. This is the brilliant part of this model for me, is we're saving the model for reasoning tasks and then taking advantage of its ability to call these other tools to then extend its knowledge in that way. Unlike the other examples I've shown earlier where programmers explicitly hooked up APIs with tools like LangChain, in this case, the model itself is figuring out when and how to best call these other APIs.",
      "platforms": {
        "tiktok": {
          "video_id": "7199661707696835882",
          "url": "https://www.tiktok.com/@rajistics/video/7199661707696835882",
          "view_count": 87800,
          "upload_date": "2023-02-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a5c0a61eaebe40a28401db641aeae65b_1676301902~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=rrjryxFC5JMOSV2dg3d1y5LNJbw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "Com8rCpA-3Z",
          "url": "https://www.instagram.com/reel/Com8rCpA-3Z/",
          "view_count": 166,
          "upload_date": "2023-02-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "lY1PtHN8UoU",
          "url": "https://youtube.com/shorts/lY1PtHN8UoU?feature=share",
          "view_count": 865,
          "upload_date": "2023-02-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Transformer Explainer is an interactive visualization tool to allow people to understand how transformers work through an end-to-end visualization of the flow of data through the model. Transformer Explainer: https://poloclub.github.io/transformer-explainer/ Let's build GPT: from scratch, in code, spelled out: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3301s",
      "description": "Transformer Explainer is an interactive visualization tool to allow people to understand how transformers work through an end-to-end visualization of the flow of data through the model. Transformer Explainer: https://poloclub.github.io/transformer-explainer/ Let's build GPT: from scratch, in code, spelled out: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3301s",
      "upload_date": "2024-08-11",
      "total_views": 69100,
      "max_views": 69100,
      "topics": [
        "end",
        "explainer",
        "information",
        "transformer",
        "transformers",
        "visualization"
      ],
      "search_text": "Transformer Explainer is an interactive visualization tool to allow people to understand how transformers work through an end-to-end visualization of the flow of data through the model. Transformer Explainer: https://poloclub.github.io/transformer-explainer/ Let's build GPT: from scratch, in code, spelled out: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3301s end explainer information transformer transformers visualization We finally have it, a visualization tool to show how transformers work. This is important because transformers are the biggest advance in AI over the last 10 years and what models like GPT-5 will be built on. So let me tell you five things I like about it and then we'll do a quick tour. This visualization gives you an end-to-end view for what's going on in a transformer. It's interactive, it's based on a real transformer, GPT-2, but it all runs inside your browser and it's open source. So let's use it. I added my own settings here. If we vary the temperature parameter, you'll see how that probability distribution shifts. This is how we can get models to be more creative as we increase temperature. Now let's follow an end-to-end prediction where we're trying to predict the next word. The first thing is you'll see as we convert all that text information into numerical embeddings, as well as including the positional information. All that information goes into the attention layer and there's a nice visualization here that shows you the importance of context. This is a fundamental advance of transformers over other methods like bag of words and word to vet, where now, for example, you'll see we're considering the relationships between words. The visualization shows you, for example, that the relationship between cat and sat is very important while the color of the cat black doesn't carry much information. The visualization and the accompanying text try to help you explain how this calculation comes to be, but it really takes a bit of matrix multiplication to understand it. I find Carpathi's discussion of it the best here and kind of working through that to get your mind understanding exactly how those calculations are made, but nevertheless, this visualization shows you that, as well as how that information is then passed through the rest of the transformer matrix to get you those predictions. Go spend time with this explainer, try it out. You're still going to have to learn the fundamentals of matrix multiplication, but this is a great tool.",
      "platforms": {
        "tiktok": {
          "video_id": "7401905525065289006",
          "url": "https://www.tiktok.com/@rajistics/video/7401905525065289006",
          "view_count": 69100,
          "upload_date": "2024-08-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9c16feef84a84fd58ea76bff8367b90a_1723390441~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=WDP3ydZKHOns%2BC1oryINyQf%2Fr6Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6438,
      "title": "Text to SQL is now easier with a large language model released by Numbers Station called NSQL. #largelanguagemodels #nsql #numberstation #machinelearning Introducing NSQL: Open-source SQL Copilot Foundation Models: https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models",
      "description": "Text to SQL is now easier with a large language model released by Numbers Station called NSQL. #largelanguagemodels #nsql #numberstation #machinelearning Introducing NSQL: Open-source SQL Copilot Foundation Models: https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models",
      "upload_date": "2023-07-06",
      "total_views": 68107,
      "max_views": 64800,
      "topics": [
        "introducing",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "nsql",
        "numbers",
        "numberstation",
        "sequel",
        "sql",
        "station",
        "text"
      ],
      "search_text": "Text to SQL is now easier with a large language model released by Numbers Station called NSQL. #largelanguagemodels #nsql #numberstation #machinelearning Introducing NSQL: Open-source SQL Copilot Foundation Models: https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models introducing largelanguagemodels machinelearning model nsql numbers numberstation sequel sql station text",
      "platforms": {
        "tiktok": {
          "video_id": "7252823135181917482",
          "url": "https://www.tiktok.com/@rajistics/video/7252823135181917482",
          "view_count": 64800,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CuXw45fgmNj",
          "url": "https://www.instagram.com/reel/CuXw45fgmNj",
          "view_count": 810,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EraaXRbsBZs",
          "url": "https://www.youtube.com/watch?v=EraaXRbsBZs",
          "view_count": 2497,
          "upload_date": "2023-07-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Med-Palm from Google for answering medical and clinical knowledge. #datascience #machinelearning #largelanguagemodels #medpalm ",
      "description": "Med-Palm from Google for answering medical and clinical knowledge. #datascience #machinelearning #largelanguagemodels #medpalm ",
      "upload_date": "2022-12-29",
      "total_views": 68100,
      "max_views": 68100,
      "topics": [
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "medpalm",
        "model",
        "models"
      ],
      "search_text": "Med-Palm from Google for answering medical and clinical knowledge. #datascience #machinelearning #largelanguagemodels #medpalm  datascience largelanguagemodels machinelearning medpalm model models Google's just dropped a paper on one of their large language models, shows us where they are, and how much better than chat GPT we can get. The model is MedPOM, and the goal is to answer medical questions, and they want to set the bar pretty high by getting to the level of a clinician. Here's an example of a question and an answer that the model would expect. Looking at the performance, you can see it's quite a bit better than past models, including some of the open source models. The secret sauce here was using instruction prompt tuning, which was a lightweight way of getting the model tuned to the medical domain. When we start evaluating the model, we'll see a familiar pattern we've often seen with large language models, where they appear better than they actually are. MedPOM does a great job of understanding the intent of the question, but look where it falls down a little bit on actually being able to solve the question. And it's simply because a lot of times the model will give the wrong answer. Eek. For those of you that follow me, this isn't a surprise at all, but it's a great example of how these models are slowly getting better.",
      "platforms": {
        "tiktok": {
          "video_id": "7182618726355733803",
          "url": "https://www.tiktok.com/@rajistics/video/7182618726355733803",
          "view_count": 68100,
          "upload_date": "2022-12-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/eaa10d945ec94dba9111d0a4bdeb3f61_1672333759~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=5hia6aIqqRMM74QUDHIwu%2B557Lg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6466,
      "title": "Control vectors are getting more widely supported most recently in Llama.cpp. It‚Äôs another useful technique alongside prompting fine tuning and logit bias. Resources: What is Representation Engineering: https://vgel.me/posts/representation-engineering/ Representation Engineering: A Top-Down Approach to AI Transparency - https://arxiv.org/abs/2310.01405 A library for making RepE control vectors: https://github.com/vgel/repeng/tree/main",
      "description": "Control vectors are getting more widely supported most recently in Llama.cpp. It‚Äôs another useful technique alongside prompting fine tuning and logit bias. Resources: What is Representation Engineering: https://vgel.me/posts/representation-engineering/ Representation Engineering: A Top-Down Approach to AI Transparency - https://arxiv.org/abs/2310.01405 A library for making RepE control vectors: https://github.com/vgel/repeng/tree/main",
      "upload_date": "2024-03-17",
      "total_views": 65529,
      "max_views": 59473,
      "topics": [
        "control",
        "engineering",
        "getting",
        "like",
        "prompting",
        "representation",
        "vectors",
        "vgel"
      ],
      "search_text": "Control vectors are getting more widely supported most recently in Llama.cpp. It‚Äôs another useful technique alongside prompting fine tuning and logit bias. Resources: What is Representation Engineering: https://vgel.me/posts/representation-engineering/ Representation Engineering: A Top-Down Approach to AI Transparency - https://arxiv.org/abs/2310.01405 A library for making RepE control vectors: https://github.com/vgel/repeng/tree/main control engineering getting like prompting representation vectors vgel I've done it. One vector for controlling those LLMs. Big deal, we already have ways of controlling LMs through prompting and fine tuning. Prompting, that's like sculpting a statue with a sledgehammer. I know, I fiddle around forever trying to get the prompting right. Exactly. That's why I'm using control vectors from representational engineering. Control vectors represent a particular concept, say happiness and we take that vectors, we add it to the hidden states at inference time and that influences the output. So, what makes this better than prompting. Control vectors come with a coefficient so you can tune them as you want. That's better than rewriting the prompt in 10 different ways but how do you get those control vectors? So, start by building a data set of contrasting pairs around the concept that you care about. What we're going to want to look at is what are the hidden states of each of these contrasting pairs and then in this case, what I did was I collapsed the differences of those hidden states into one vectors and that's the vectors that represents the context inside the LM. So many possibilities here I get the obvious ones like hardworking and harmlessness but I see some fun possibilities around things like laziness, creativity, self aware,",
      "platforms": {
        "tiktok": {
          "video_id": "7347380270057770283",
          "url": "https://www.tiktok.com/@rajistics/video/7347380270057770283",
          "view_count": 5140,
          "upload_date": "2024-03-17",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4n4y_FA0I8",
          "url": "https://www.instagram.com/reel/C4n4y_FA0I8",
          "view_count": 59473,
          "upload_date": "2024-03-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "u8KUQlfgTrg",
          "url": "https://youtube.com/shorts/u8KUQlfgTrg",
          "view_count": 916,
          "upload_date": "2024-03-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering ",
      "description": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering ",
      "upload_date": "2022-12-31",
      "total_views": 64800,
      "max_views": 64800,
      "topics": [
        "clustering",
        "kmeans",
        "like",
        "look",
        "means",
        "one"
      ],
      "search_text": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering  clustering kmeans like look means one Hey, I know K-means is a great way to cluster, but it's not working well for me. This is why there's lots of clustering approaches. Let's walk through your issues together. Take a look at this one. I'm confused why it didn't work. Ah, I see what's going on. Look at the axes. The scales are different. Here's an example to highlight what I'm talking about, where we have some data, but look at the axes. When we run K-means initially, it doesn't group them like we expect to. But look, once we normalize them, we get the clusters that we expect. How about this one? I know the diameters of the clusters are very different and could be a factor. Yeah, you're right. You might want to use another technique like Gaussian mixture models. I'd experiment a little bit. And look, K-means totally failed on this one. Yeah, K-means isn't going to work right here. You might want to use a density-based approach. See how this density-based approach works on this smiley face type thing and identifies the different clusters? Something like that might be useful. And this last one is I was working with TFIDF and taking all the different words and then trying to see the clusters here. It doesn't work very well. With high-dimensional data, one of the go-to approaches I like to use is HDB scan. You can take a look at this example and see like the difference between K-means, which doesn't group the data as you expect, versus when you use HDB scan, it finds those clusters, groups them together well.",
      "platforms": {
        "tiktok": {
          "video_id": "7183366473270791467",
          "url": "https://www.tiktok.com/@rajistics/video/7183366473270791467",
          "view_count": 64800,
          "upload_date": "2022-12-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7456c787999d473799db1da1376bddbb_1672507862~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=VIpctHmvwmKfOFCJWUKQye%2FTp0g%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Very excited and Richard isn’t paying me for this - #codetok #youdotcom #codingtiktok  #python",
      "description": "Very excited and Richard isn’t paying me for this - #codetok #youdotcom #codingtiktok  #python",
      "upload_date": "2022-05-06",
      "total_views": 64100,
      "max_views": 64100,
      "topics": [
        "code",
        "codetok",
        "codingtiktok",
        "like",
        "python",
        "youdotcom"
      ],
      "search_text": "Very excited and Richard isn’t paying me for this - #codetok #youdotcom #codingtiktok  #python code codetok codingtiktok like python youdotcom I just found out about this and it's going to change how I code. Typically if I'm looking for a code snippet, I'll type it into Google, they'll give me some search results, I'll end up having to go to like five or ten pages to figure out what I want to do. Check out this code.u.com. Like you get back great snippets and search results. Like I'm going to start using this now whenever I need to find some code snippets.",
      "platforms": {
        "tiktok": {
          "video_id": "7094738398375660846",
          "url": "https://www.tiktok.com/@rajistics/video/7094738398375660846",
          "view_count": 64100,
          "upload_date": "2022-05-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0b479b6102b94895b1750f2556b9160c_1651872510~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=tVvsBlfnGfqTntFFGAAS1ntAOT4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches.  #datascience #machinelearning #transformers #climatemodel ",
      "description": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches.  #datascience #machinelearning #transformers #climatemodel ",
      "upload_date": "2023-02-08",
      "total_views": 61400,
      "max_views": 61400,
      "topics": [
        "climatemodel",
        "climax",
        "datascience",
        "machinelearning",
        "models",
        "transformers"
      ],
      "search_text": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches.  #datascience #machinelearning #transformers #climatemodel  climatemodel climax datascience machinelearning models transformers Climax. Would you like to learn more about it? Climax is a machine learning model for predicting the weather and making climate projections. Before Climax, people relied on traditional physics-based models. Now these models used things like differential equations, but they were limited when we needed to focus on a specific region or really short or long time spans. The kinds of inputs that go into Climax include what is the local climate? What's the spatial area? What's the timing? Well, seems pretty relevant to me. The flexibility of transformers allowed the output to be, could be a specific region, could be a different task, could be a different type of variable. And the results showed that these models were competitive or even better than traditional models such as the integrated forecast system or IFS. So even if you aren't interested in weather, Climax is a great example of the flexibility and adaptability of modern transformer approaches.",
      "platforms": {
        "tiktok": {
          "video_id": "7197596884641451310",
          "url": "https://www.tiktok.com/@rajistics/video/7197596884641451310",
          "view_count": 61400,
          "upload_date": "2023-02-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5485147bb1a9411b8e764c464bb88876_1675821129~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=bk82n5ZZFEDgSGpPPycu2kXynt4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6501,
      "title": "NASA uses generative AI for manufacturing parts for space. It's a great use of generative technology and you can start seeing how it will change engineering in the long run. Check out more of Ryan McClelland's work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland ‚Äì NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics",
      "description": "NASA uses generative AI for manufacturing parts for space. It's a great use of generative technology and you can start seeing how it will change engineering in the long run. Check out more of Ryan McClelland's work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland ‚Äì NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics",
      "upload_date": "2023-10-28",
      "total_views": 61097,
      "max_views": 41527,
      "topics": [
        "61",
        "design",
        "generative",
        "generativeai",
        "manufacturing",
        "nasa"
      ],
      "search_text": "NASA uses generative AI for manufacturing parts for space. It's a great use of generative technology and you can start seeing how it will change engineering in the long run. Check out more of Ryan McClelland's work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland ‚Äì NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics 61 design generative generativeai manufacturing nasa",
      "platforms": {
        "tiktok": {
          "video_id": "7294999469966265642",
          "url": "https://www.tiktok.com/@rajistics/video/7294999469966265642",
          "view_count": 19100,
          "upload_date": "2023-10-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cy8a_7TgC6y",
          "url": "https://www.instagram.com/reel/Cy8a_7TgC6y",
          "view_count": 41527,
          "upload_date": "2023-10-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_KTvQlT-tfo",
          "url": "https://www.youtube.com/watch?v=_KTvQlT-tfo",
          "view_count": 470,
          "upload_date": "2023-10-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6549,
      "title": "AI News for the week featuring OpenAI NVIDIA Google Apple Stanford Hugging Face Anthropic and Microsoft. #machinelearning #openai #rajistics #nvidia",
      "description": "AI News for the week featuring OpenAI NVIDIA Google Apple Stanford Hugging Face Anthropic and Microsoft. #machinelearning #openai #rajistics #nvidia",
      "upload_date": "2023-10-21",
      "total_views": 59515,
      "max_views": 48439,
      "topics": [
        "anthropic",
        "featuring",
        "google",
        "machinelearning",
        "news",
        "nvidia",
        "openai",
        "stanford",
        "week"
      ],
      "search_text": "AI News for the week featuring OpenAI NVIDIA Google Apple Stanford Hugging Face Anthropic and Microsoft. #machinelearning #openai #rajistics #nvidia anthropic featuring google machinelearning news nvidia openai stanford week",
      "platforms": {
        "tiktok": {
          "video_id": "7292239571583192366",
          "url": "https://www.tiktok.com/@rajistics/video/7292239571583192366",
          "view_count": 11000,
          "upload_date": "2023-10-21",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CypRjAwAn-b",
          "url": "https://www.instagram.com/reel/CypRjAwAn-b",
          "view_count": 48439,
          "upload_date": "2023-10-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "oPQAJBWIstk",
          "url": "https://www.youtube.com/watch?v=oPQAJBWIstk",
          "view_count": 76,
          "upload_date": "2023-10-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Simply explaining how ChatGPT works. All the technical details of ChatGPT have not been released, so this is based on what OpenAI has been doing over the last few years. #datascience #machinelearning #openai #chatgpt #reinforcementlearning ",
      "description": "Simply explaining how ChatGPT works. All the technical details of ChatGPT have not been released, so this is based on what OpenAI has been doing over the last few years. #datascience #machinelearning #openai #chatgpt #reinforcementlearning ",
      "upload_date": "2022-12-07",
      "total_views": 56100,
      "max_views": 56100,
      "topics": [
        "answers",
        "chatgpt",
        "datascience",
        "machinelearning",
        "openai",
        "reinforcementlearning"
      ],
      "search_text": "Simply explaining how ChatGPT works. All the technical details of ChatGPT have not been released, so this is based on what OpenAI has been doing over the last few years. #datascience #machinelearning #openai #chatgpt #reinforcementlearning  answers chatgpt datascience machinelearning openai reinforcementlearning ChatGPT has blown up big this week. Let me take some time to explain to you exactly how it works. And this will also let you understand its limitations. Now, the way large language models work is they try to predict the next word. And by doing this over and over again, they could start putting together long pieces of text that are readable. A lot of these language models put together readable text. But if we actually start looking at the text, we'll notice that there's subtle errors that actually have a big impact on the meaning of it. So how can we do better? How about giving the model some feedback on what a good answer is? And that's what the team did. They took a bunch of human labors, gave them answers and then had them say, are these good quality answers? Can you rank how well these answers are? Are these answers actually correct? And once they did this, the model was so much better. The answers were such higher quality. And this is why ChatGPT writes so well. It's been graded by average people and it now knows how to write above average. Now, where it's going to fail, of course, is as soon as you get to a specific subject area that our human labors weren't able to understand if that was a good answer or a bad answer, because it just looks really complicated. That's the limitation. And my guess is open AI is next going to go after a lot of those different domains by adding more instructions and fine tuning in those areas to boost those models.",
      "platforms": {
        "tiktok": {
          "video_id": "7174487697849945386",
          "url": "https://www.tiktok.com/@rajistics/video/7174487697849945386",
          "view_count": 56100,
          "upload_date": "2022-12-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/58eb630117c347138069b8d8454ed91d_1670440601~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=ElDYKOkLHXu0cEY4YoYdJOaodBk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here.  #datascience #machinelearning #twitter #recommenders ",
      "description": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here.  #datascience #machinelearning #twitter #recommenders ",
      "upload_date": "2023-04-01",
      "total_views": 55500,
      "max_views": 55500,
      "topics": [
        "code",
        "datascience",
        "fun",
        "machinelearning",
        "recommenders",
        "twitter"
      ],
      "search_text": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here.  #datascience #machinelearning #twitter #recommenders  code datascience fun machinelearning recommenders twitter Twitter open sourced its algorithm today and let me tell you, it's a lot of fun rolling around in someone else's code. Look at this code snippet. You can see here they've added information on Elon Musk and if people are Republicans or Democrats, to be able to track how their tweets do. This is huge because it gives us a production example of a recommender system. Most companies don't share their recommender systems and for those of you learning recommender systems, it's often hard to find good examples. Now we have one. Now we don't have all the details. There's lots of information missing on the actual training data, the weights of the models, even how some of the features were derived. But we can start to see the skeleton of how they put that together and it's fun to kind of look at some of the actual code that was written. From what I'm seeing, the recommendation system here is pretty much what we expected. A lot of it has already been published in papers. There's a couple of different set of recommendations. They have a real graph based on who you follow. They have a social graph based on who you have engaged with and finally they use an embedding space that figures out what tweets and users are similar to your own interests. The embedding spaces uses SIM clusters, which again is another thing that's been published about on this. All three of these sources are combined to give you recommendations. There's a lot of other details and other filtering that they do. Have fun. Dive into it. But nothing earth shattering in here.",
      "platforms": {
        "tiktok": {
          "video_id": "7216864772988390702",
          "url": "https://www.tiktok.com/@rajistics/video/7216864772988390702",
          "view_count": 55500,
          "upload_date": "2023-04-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/dcb303c4b098488c87c706a463eb1aaf_1680307286~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=J8F5W3TNAPeJi4kAI%2FaneiKnWx4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Tensorboard embedding projector - repost",
      "description": "Tensorboard embedding projector - repost",
      "upload_date": "2024-07-01",
      "total_views": 52400,
      "max_views": 52400,
      "topics": [
        "also",
        "cold",
        "concepts",
        "look",
        "see",
        "tensorboard"
      ],
      "search_text": "Tensorboard embedding projector - repost also cold concepts look see tensorboard Ready to see how AI thinks about the world? Take a look at cold. We'll see some concepts like hot and spring that we associate with cold. But this AI has also learned history and you'll see concepts like Soviet and war are also related to cold. I'm using the TensorBoard embedding projector to be able to see these embeddings or vector representations of words, in this case a word defect model. Let's look at sexy. You'll see some of the concepts that you're probably associated with such as Baird and Curvy. But look at this AI. It also associates Groucho and Wigs with sexy.",
      "platforms": {
        "tiktok": {
          "video_id": "7386702177894518059",
          "url": "https://www.tiktok.com/@rajistics/video/7386702177894518059",
          "view_count": 52400,
          "upload_date": "2024-07-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c00f82faa5b24991a497458bf554aec9_1719850632~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=JGN8qjdRvGPcy7meSeU78c6n4%2B0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday tensorboard embedding projector. Let me know if i should reshare these older videos.",
      "description": "#onthisday tensorboard embedding projector. Let me know if i should reshare these older videos.",
      "upload_date": "2023-07-01",
      "total_views": 49500,
      "max_views": 49500,
      "topics": [
        "embedding",
        "know",
        "let",
        "onthisday",
        "projector",
        "tensorboard"
      ],
      "search_text": "#onthisday tensorboard embedding projector. Let me know if i should reshare these older videos. embedding know let onthisday projector tensorboard",
      "platforms": {
        "tiktok": {
          "video_id": "7250980121656446254",
          "url": "https://www.tiktok.com/@rajistics/video/7250980121656446254",
          "view_count": 49500,
          "upload_date": "2023-07-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "This happens. #datascience #machinelearning #python #codetok #programming",
      "description": "This happens. #datascience #machinelearning #python #codetok #programming",
      "upload_date": "2022-05-04",
      "total_views": 48900,
      "max_views": 48900,
      "topics": [
        "codetok",
        "datascience",
        "happens",
        "machinelearning",
        "programming",
        "python"
      ],
      "search_text": "This happens. #datascience #machinelearning #python #codetok #programming codetok datascience happens machinelearning programming python World Cup Music",
      "platforms": {
        "tiktok": {
          "video_id": "7093913682912087342",
          "url": "https://www.tiktok.com/@rajistics/video/7093913682912087342",
          "view_count": 48900,
          "upload_date": "2022-05-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4ba5827eccde46afac625d1acad2fc0c_1651680492~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=2X9RPDsebs1MT2n7N00ajXLZiFs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6410,
      "title": "Vector Databases (Pinecone)",
      "description": "Vector Databases (Pinecone)",
      "upload_date": "2023-04-17",
      "total_views": 48861,
      "max_views": 24599,
      "topics": [
        "chroma",
        "databases",
        "embeddings",
        "faiss",
        "milvus",
        "pinecone",
        "vector",
        "weaviate"
      ],
      "search_text": "Vector Databases (Pinecone) chroma databases embeddings faiss milvus pinecone vector weaviate",
      "platforms": {
        "tiktok": {
          "video_id": "7222674900329319723",
          "url": "https://www.tiktok.com/@rajistics/video/7222674900329319723",
          "view_count": 19000,
          "upload_date": "2023-04-16",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CrGlU69AtbH",
          "url": "https://www.instagram.com/reel/CrGlU69AtbH",
          "view_count": 5262,
          "upload_date": "2023-04-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NfvcN7atiME",
          "url": "https://www.youtube.com/watch?v=NfvcN7atiME",
          "view_count": 24599,
          "upload_date": "2023-04-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Some general advice on how to evaluate software packages. #datascience #machinelearning #github",
      "description": "Some general advice on how to evaluate software packages. #datascience #machinelearning #github",
      "upload_date": "2022-11-30",
      "total_views": 47700,
      "max_views": 47700,
      "topics": [
        "github",
        "great",
        "look",
        "package",
        "see",
        "want"
      ],
      "search_text": "Some general advice on how to evaluate software packages. #datascience #machinelearning #github github great look package see want Let's talk about something that's not in any data science textbook, but it's super important for you to know how to do. And that's how to evaluate a package that you find on GitHub. I want to do this with Shapp, and I want to first say Shapp is an awesome package and a thousand times better than anything I would do. And if we had more software design like that, we would be better off. But let's use that as an example to show you how I would evaluate a package. So these top numbers are really important. The first thing is don't overvalue stars. It's often more about popularity than the quality of the package. Just use it as one useful signal. But watches and forks are also good signals. Forks is especially so because it shows that people are digging into it, kind of building their own version. Now one red flag, do you see it right away here? Look when that last update was. That's something that would draw me a little bit of back is you want typically a package that's going to be updated more frequently. They're used by a super impressive. That's a great signal for a quality package if there's other packages that are based on it. Also the number of contributors is useful, but that distribution of how much you're contributing is comes into play. Very important. We'll talk about that in a few minutes. And all of you by now should be able to evaluate readme's, right? You want to know how to install the package. You want some code snippets to get started right away. The Shapp readme is a great textbook example of how to do it right. Looking at the release history, we don't like the June 15th release, but if you look beyond that, you'll see that over the last couple of years, there's been consistent releases every few months, which is a great sign of a healthy package. Here's a big red flag on the issues. Do you see it? Look at the ratio of open to closed. When I look at the most recently closed issues, what I find is is the people that open the issues were the ones that closed them. It wasn't a case of the community helping these people. And that's not great to see. You'd rather see the community being active and participating in the issues page. Having 75 open poll requests shows a ton of interest in the package. Lots of people are trying to contribute, but again, the red flag is, is the last closed poll request was back in June, which isn't great. The insights page gives us a historical view of the package. So we can see in this case that most of the contributions were in 2021, not so much. It slowed down recently. Now, when we look at contributors, here's where you see a massive imbalance. You see Scott, who's been the major, major developer of this package is the responsible for the vast majority of commits and lines of codes. There are some other contributors, but they're much smaller down. GitHub has a lot more insights available. There's probably other good signals for the quality of a package as well. I'll let the comments sort that out.",
      "platforms": {
        "tiktok": {
          "video_id": "7171650670175554858",
          "url": "https://www.tiktok.com/@rajistics/video/7171650670175554858",
          "view_count": 47700,
          "upload_date": "2022-11-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b27095cb8d2e4e858d5328056a24a8c7_1669780053~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=55VnH2B%2B%2FNSe5Ic8R2Ly8EEFtKQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning ",
      "description": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning ",
      "upload_date": "2022-12-20",
      "total_views": 46400,
      "max_views": 46400,
      "topics": [
        "clouds",
        "datascience",
        "machinelearning",
        "model",
        "openai",
        "point"
      ],
      "search_text": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning  clouds datascience machinelearning model openai point OpenAI dropped another game changer this week. It's a model that goes from text to 3D point clouds. This is going to affect games, virtual reality, industrial design. It's going to be cool. The model works by taking the text prompt, goes into the glide model, which creates a synthetic view of that. Then from the synthetic view, there's a point cloud diffusion model that makes the point cloud. Just like OpenAI did with Dolly, now you can generate 3D point clouds of things that never existed before. Pretty cool. But this table here is why this is such a game changer. They've dramatically increased the speed up to 600 times faster than older approaches. Hugginface has a demo up there. Go check it out. It's a little lower quality, but you can still do things like, for example, check out this motorcycle.",
      "platforms": {
        "tiktok": {
          "video_id": "7179365967426866478",
          "url": "https://www.tiktok.com/@rajistics/video/7179365967426866478",
          "view_count": 46400,
          "upload_date": "2022-12-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1b1cb953e2e844be917b275eb25a4997_1671576409~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=ueOPNDBg0AoKBPrC4AOjlk1tGR8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "GPT4 hype that it will be 100 trillion parameters doesn’t make any sense. First, is the scaling laws in this video @rajistics  and also think about the compute required.  #datascience ##machinelearning #gpt4 #openai ",
      "description": "GPT4 hype that it will be 100 trillion parameters doesn’t make any sense. First, is the scaling laws in this video @rajistics  and also think about the compute required.  #datascience ##machinelearning #gpt4 #openai ",
      "upload_date": "2023-01-17",
      "total_views": 45500,
      "max_views": 45500,
      "topics": [
        "datascience",
        "gpt",
        "gpt4",
        "hype",
        "machinelearning",
        "openai"
      ],
      "search_text": "GPT4 hype that it will be 100 trillion parameters doesn’t make any sense. First, is the scaling laws in this video @rajistics  and also think about the compute required.  #datascience ##machinelearning #gpt4 #openai  datascience gpt gpt4 hype machinelearning openai Here's the latest hype about open AI. It's not true. GPT-4 isn't going to be 500 times the size of GPT-3. We know this is wrong for at least two reasons. First, we went over the scaling laws and we all realize now that bigger models are just not cool anymore. Second, to host and to use a model that size would require an enormous amount of resources. And I don't think we all want to see the lights flicker every time somebody asks GPT-4 a prediction. GPT-4 is going to be awesome, but let's not over hype it.",
      "platforms": {
        "tiktok": {
          "video_id": "7189452446933011758",
          "url": "https://www.tiktok.com/@rajistics/video/7189452446933011758",
          "view_count": 45500,
          "upload_date": "2023-01-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ff75872faf684440800155986097b5ca_1673924852~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=nuPMaK%2F1VVWh6R3kb35EHRHQTG0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Some common data distributions when modeling including skewed and zero inflated. There are many other distributions, but just wanted people to know that normal distribution isn’t normal in my experience. #datascience #statistics #datadistribution #zeroinflated #tweedie ",
      "description": "Some common data distributions when modeling including skewed and zero inflated. There are many other distributions, but just wanted people to know that normal distribution isn’t normal in my experience. #datascience #statistics #datadistribution #zeroinflated #tweedie ",
      "upload_date": "2023-02-03",
      "total_views": 45300,
      "max_views": 45300,
      "topics": [
        "common",
        "data",
        "datadistribution",
        "datascience",
        "statistics",
        "zeroinflated"
      ],
      "search_text": "Some common data distributions when modeling including skewed and zero inflated. There are many other distributions, but just wanted people to know that normal distribution isn’t normal in my experience. #datascience #statistics #datadistribution #zeroinflated #tweedie  common data datadistribution datascience statistics zeroinflated Is your data normal? Hmm, you should probably be a little concerned if it is, because I don't often see data that's normally distributed. It's much more common for me to see data like this that's skewed to one side or another, and especially where it has spikes, because often there's some common values, such as you ask people to give a weight and they round to 30 and 40, and so you get spikes in that area. Another common pattern you might see is zero-inflated data where you have a lot of zeros, and for a lot of problems, it's quite common. For example, in insurance, lots of people don't have accidents, so we have lots of zeros in our data distribution. An easy first step is working with gradient-boosted algorithms, such as XGBoost, that can handle these types of skews in the data pretty easily, even give you advanced loss functions like Tweety to make it easy for you. So don't fret if your data isn't normally distributed, and you know what? If your data is normally distributed, it kind of makes me wonder if somebody messed with it.",
      "platforms": {
        "tiktok": {
          "video_id": "7195750839716154670",
          "url": "https://www.tiktok.com/@rajistics/video/7195750839716154670",
          "view_count": 45300,
          "upload_date": "2023-02-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6b21f9dd68be4dfeb3355e54939e05a5_1675391308~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=84B5waiq6twUNjvMOv1E4H9msWo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6447,
      "title": "Open source LLMs why they seem popular are not easy to get running in production settings. The current open source LLMs while getting better still lag behind the commercial APIs in many areas. This video highlights a few of them. #datascience #machinelearning #largelanguagemodels #openai #anthropic #flant5 MMLU Leaderboard: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu The False Promise of Imitating Proprietary LLMs: https://arxiv.org/abs/2305.15717 Background by Qiming Chen: https://unsplash.com/photos/lzCH2_8qRH8",
      "description": "Open source LLMs why they seem popular are not easy to get running in production settings. The current open source LLMs while getting better still lag behind the commercial APIs in many areas. This video highlights a few of them. #datascience #machinelearning #largelanguagemodels #openai #anthropic #flant5 MMLU Leaderboard: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu The False Promise of Imitating Proprietary LLMs: https://arxiv.org/abs/2305.15717 Background by Qiming Chen: https://unsplash.com/photos/lzCH2_8qRH8",
      "upload_date": "2023-06-03",
      "total_views": 43496,
      "max_views": 38900,
      "topics": [
        "anthropic",
        "common",
        "crawl",
        "datascience",
        "flant5",
        "language",
        "large",
        "largelanguagemodels",
        "like",
        "machinelearning",
        "models",
        "open",
        "openai",
        "source",
        "using"
      ],
      "search_text": "Open source LLMs why they seem popular are not easy to get running in production settings. The current open source LLMs while getting better still lag behind the commercial APIs in many areas. This video highlights a few of them. #datascience #machinelearning #largelanguagemodels #openai #anthropic #flant5 MMLU Leaderboard: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu The False Promise of Imitating Proprietary LLMs: https://arxiv.org/abs/2305.15717 Background by Qiming Chen: https://unsplash.com/photos/lzCH2_8qRH8 anthropic common crawl datascience flant5 language large largelanguagemodels like machinelearning models open openai source using",
      "platforms": {
        "tiktok": {
          "video_id": "7240550807911894315",
          "url": "https://www.tiktok.com/@rajistics/video/7240550807911894315",
          "view_count": 38900,
          "upload_date": "2023-06-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CtCm5TUgJ2-",
          "url": "https://www.instagram.com/reel/CtCm5TUgJ2-",
          "view_count": 2790,
          "upload_date": "2023-06-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "IrIqKRMJCwc",
          "url": "https://www.youtube.com/watch?v=IrIqKRMJCwc",
          "view_count": 1806,
          "upload_date": "2023-06-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6633,
      "title": "A simple explanation of what AI is. The video touches upon the impact of AI how AI works with a practical example and some of the reasons AI has grown so much in the last ten years. #datascience #machinelearning #ai #aiexplained",
      "description": "A simple explanation of what AI is. The video touches upon the impact of AI how AI works with a practical example and some of the reasons AI has grown so much in the last ten years. #datascience #machinelearning #ai #aiexplained",
      "upload_date": "2023-06-25",
      "total_views": 43250,
      "max_views": 42800,
      "topics": [
        "ai",
        "aiexplained",
        "datascience",
        "explanation",
        "let",
        "machinelearning",
        "simple",
        "years"
      ],
      "search_text": "A simple explanation of what AI is. The video touches upon the impact of AI how AI works with a practical example and some of the reasons AI has grown so much in the last ten years. #datascience #machinelearning #ai #aiexplained ai aiexplained datascience explanation let machinelearning simple years",
      "platforms": {
        "tiktok": {
          "video_id": "7248687776277744939",
          "url": "https://www.tiktok.com/@rajistics/video/7248687776277744939",
          "view_count": 42800,
          "upload_date": "2023-06-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Ct7EleCJmxr",
          "url": "https://www.instagram.com/reel/Ct7EleCJmxr",
          "view_count": 450,
          "upload_date": "2023-06-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Stable diffusion, go run it yourself! It’s so awesome. #datascience #codetok #aipub #huggingface #machinelearning ",
      "description": "Stable diffusion, go run it yourself! It’s so awesome. #datascience #codetok #aipub #huggingface #machinelearning ",
      "upload_date": "2022-08-25",
      "total_views": 42600,
      "max_views": 42600,
      "topics": [
        "aipub",
        "codetok",
        "datascience",
        "diffusion",
        "huggingface",
        "something"
      ],
      "search_text": "Stable diffusion, go run it yourself! It’s so awesome. #datascience #codetok #aipub #huggingface #machinelearning  aipub codetok datascience diffusion huggingface something Stable diffusion is the latest creative model that's blowing up. It works because it uses something old, something new, and something borrowed. The heart of stable diffusion is a diffusion process that keeps adding noise to a model. What it does is it does this in both directions, and what it can do then is even if something is a little noisier, it just has a tiny bit of signal, it's able to figure out what the image should be. Now to pull this off, they start by working in the latent space, by using a variational autoencoder, something old, they're able to take the images and instead just use a little portion of them from the latent space as a way to represent them. The something new is of course the diffusion process that's handled by UNET. And the something borrowed is they use clips already trained texting coders, so they're not training with the text at the time, but instead bring that in as a cross-attention layer and use that to help condition the model. And putting it all together, you can see that when we want to go and do a prediction, we'll start off by using something borrowed from the clip texting coder, passing it into something new, our UNET, and then use something old, our variational decoder, to give us the output image.",
      "platforms": {
        "tiktok": {
          "video_id": "7135922429783903534",
          "url": "https://www.tiktok.com/@rajistics/video/7135922429783903534",
          "view_count": 42600,
          "upload_date": "2022-08-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8abfc25f417b46629f7068dd3d17915a_1661461416~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=BfVSaSPNdsv2zJFSngtB%2BegCLBY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @bird_3288 #python #rstat #datascience #analytics #programming",
      "description": "Reply to @bird_3288 #python #rstat #datascience #analytics #programming",
      "upload_date": "2022-03-12",
      "total_views": 41600,
      "max_views": 41600,
      "topics": [
        "analytics",
        "datascience",
        "languages",
        "programming",
        "python",
        "rstat"
      ],
      "search_text": "Reply to @bird_3288 #python #rstat #datascience #analytics #programming analytics datascience languages programming python rstat What I wish I knew about programming languages before I started data science. There are two main programming languages, Python and R. I started with R. I find it easy to use, still use it all the time, especially some of the tools like Deplier for data manipulation, as well as GGplot for visualization. But the limitation of R is it's not what most of the industry uses. Lots of infrastructure and pipelines now are built around using Python. So nowadays I recommend people just start with Python. It's a more general purpose approach.",
      "platforms": {
        "tiktok": {
          "video_id": "7074268172777573675",
          "url": "https://www.tiktok.com/@rajistics/video/7074268172777573675",
          "view_count": 41600,
          "upload_date": "2022-03-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1b35312622c24b819319b634764835b5_1647106414~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=XH31Wd82cNg3yRJKZm22NZz9Cmo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Working with embeddings today.  #datascience #word2vec #embeddings #tensorflow #codetok #tensorboard",
      "description": "Working with embeddings today.  #datascience #word2vec #embeddings #tensorflow #codetok #tensorboard",
      "upload_date": "2022-07-01",
      "total_views": 41200,
      "max_views": 41200,
      "topics": [
        "datascience",
        "embeddings",
        "see",
        "tensorboard",
        "tensorflow",
        "word2vec"
      ],
      "search_text": "Working with embeddings today.  #datascience #word2vec #embeddings #tensorflow #codetok #tensorboard datascience embeddings see tensorboard tensorflow word2vec Ready to see how AI thinks about the world? Take a look at cold. We'll see some concepts like hot and spring that we associate with cold. But this AI has also learned history, and you'll see concepts like Soviet and war are also related to cold. I'm using the TensorBoard embedding projector to be able to see these embeddings or vector representations of words, in this case a word-devect model. Let's look at sexy. You'll see some of the concepts that you're probably associated with, such as Baird and Curvy. But look at this AI. It also associates Groucho and Wigs with sexy.",
      "platforms": {
        "tiktok": {
          "video_id": "7115543733936000302",
          "url": "https://www.tiktok.com/@rajistics/video/7115543733936000302",
          "view_count": 41200,
          "upload_date": "2022-07-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fdb1ca43174046e99a0fb1423bd8f3c7_1656716632~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=lMfk7KUYBpy0OBSzZd5TZ2lIiMw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Data Centric AI helps to remind us not to focus too much on the model or algorithms. In real data science, it’s more about understanding your data and having high quality labeled training data.  #datascience #machinelearning #datacentricai #cleanlab #erroranalysis ",
      "description": "Data Centric AI helps to remind us not to focus too much on the model or algorithms. In real data science, it’s more about understanding your data and having high quality labeled training data.  #datascience #machinelearning #datacentricai #cleanlab #erroranalysis ",
      "upload_date": "2023-02-24",
      "total_views": 40900,
      "max_views": 40900,
      "topics": [
        "cleanlab",
        "data",
        "datascience",
        "machinelearning",
        "model",
        "much"
      ],
      "search_text": "Data Centric AI helps to remind us not to focus too much on the model or algorithms. In real data science, it’s more about understanding your data and having high quality labeled training data.  #datascience #machinelearning #datacentricai #cleanlab #erroranalysis  cleanlab data datascience machinelearning model much Check out this new state of the art model. We should push this to production. Think again. Do you understand why we wouldn't push a model that we read about in archive to production? Because you guys aren't comfortable with change. Oh boy. Do you have any idea how much work it takes to build, train, validate, and deploy a model into our pipelines? I can do that in a couple of hours. A model created by academics can take weeks for us to re-engineer and get working within our production pipelines. The other thing is the accuracy gains you're seeing, we don't know if that's gonna translate over to the problems we're trying to solve. And even if it does improve accuracy, one or 2% accuracy isn't that big a deal for a lot of the applications that often not worth upgrading just for that. So if you don't read archive all day, how do you improve your models? There are a lot of things we do. Tell them about your Kaggle work. I just finished competing in the auto competition on Kaggle and the third place team used a rules-based approach that beat thousands of people. It just goes to show you that understanding and knowing the data, you can build a rules-based approach that builds a machine learning model. Remember Tom, that PhD spent weeks building a few shot learning approach because he said he didn't have enough data? Didn't get anywhere. We handed the project off to Sarah. She worked with the business, took a little bit of her free time, labeled some data, and she was able to quickly build a very effective model by just taking the time to label data. You data scientists, as the person who has to put these things into production, I much prefer a simpler model. They're much easier to understand when I'm trying to troubleshoot them. They take a lot less compute. If you look at our data quality pipelines that we have, they're all using very simple bag of words models. I know you guys have those fancy transformer models, but those things take way too much compute, way too much resources. If we can do it with a simple model, that's what we're gonna choose. That makes a lot of sense. I should probably brush up on past meetings where we talked about using tools like CleanLab, as well as doing error analysis with our models. I just saw this data-centric AI course. All the videos are free. You might wanna use it to brush up.",
      "platforms": {
        "tiktok": {
          "video_id": "7203531266506100014",
          "url": "https://www.tiktok.com/@rajistics/video/7203531266506100014",
          "view_count": 40900,
          "upload_date": "2023-02-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/db5888741d04494cade0245eedcaf579_1677202841~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=gpu8Cpmp%2BkQdlF54xagty%2FNwLRI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "It’s important to make sure your model is well calibrated. This becomes especially important with imbalanced data. #machinelearning #datascience #statistics ",
      "description": "It’s important to make sure your model is well calibrated. This becomes especially important with imbalanced data. #machinelearning #datascience #statistics ",
      "upload_date": "2022-11-11",
      "total_views": 40300,
      "max_views": 40300,
      "topics": [
        "datascience",
        "disease",
        "look",
        "machinelearning",
        "model",
        "statistics"
      ],
      "search_text": "It’s important to make sure your model is well calibrated. This becomes especially important with imbalanced data. #machinelearning #datascience #statistics  datascience disease look machinelearning model statistics Hey, I just finished building my SVM model for predicting disease. Here's my results. Hmm. This is a list of who's predicted to have the disease versus who isn't. To help people actually understand this, can we give a probability? Because there's a big difference between a 65% chance of getting a disease versus a 95%. Yeah, I can do that. Here you go. Hmm. This doesn't seem right. Let me see your data. Ah, let me compare your model with a logistic regression. If we look at the distribution of your predictions, see how they all bunch in the middle. What we actually like is having them spread out where there's a better decision boundary between them. The second problem is your model's not properly calibrated. Take a look at this plot. When your model is saying 70%, it's actually predicting something like 95% of those people around them have a disease. So that isn't properly calibrated. On the other hand, take a look at my logistic regression. When it's predicting about 70%, well, about 70% of those have the disease. Oh, okay. Let me do that. Nobody taught me about calibration. I just thought models did that themselves. What you want to do is take a look at a calibration technique like isotonic regression or flat scaling and use that to fix your model.",
      "platforms": {
        "tiktok": {
          "video_id": "7164607746396933419",
          "url": "https://www.tiktok.com/@rajistics/video/7164607746396933419",
          "view_count": 40300,
          "upload_date": "2022-11-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d02b1fa6e7544314b203f747076887a6_1668140245~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=i%2BiRrRukBX1DYZJn8otxIqE1eAw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6432,
      "title": "The tech world has so many reactions to OpenAI firing Sam Altman. Here are some very quick reactions. #openai #rajistics",
      "description": "The tech world has so many reactions to OpenAI firing Sam Altman. Here are some very quick reactions. #openai #rajistics",
      "upload_date": "2023-11-18",
      "total_views": 39745,
      "max_views": 19800,
      "topics": [
        "altman",
        "back",
        "firing",
        "like",
        "many",
        "open",
        "openai",
        "reactions",
        "sam",
        "tech",
        "world"
      ],
      "search_text": "The tech world has so many reactions to OpenAI firing Sam Altman. Here are some very quick reactions. #openai #rajistics altman back firing like many open openai reactions sam tech world",
      "platforms": {
        "tiktok": {
          "video_id": "7302843392151538986",
          "url": "https://www.tiktok.com/@rajistics/video/7302843392151538986",
          "view_count": 19800,
          "upload_date": "2023-11-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Czy232tA2QE",
          "url": "https://www.instagram.com/reel/Czy232tA2QE",
          "view_count": 17211,
          "upload_date": "2023-11-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "u40-s_Gl92A",
          "url": "https://www.youtube.com/watch?v=u40-s_Gl92A",
          "view_count": 2734,
          "upload_date": "2023-11-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6037,
      "title": "üöÄ Just get started on your journey to learn large language models! ü§î Is there a lot to learn? Yes! üòÖ ü§∑‚Äç‚ôÇÔ∏è But is it easy to get started? Yes! üëç ‚úÖ Go do it!! üèÉ‚Äç‚ôÇÔ∏èüí® #datascience #machinelearning #largelanguagemodels #llama2",
      "description": "üöÄ Just get started on your journey to learn large language models! ü§î Is there a lot to learn? Yes! üòÖ ü§∑‚Äç‚ôÇÔ∏è But is it easy to get started? Yes! üëç ‚úÖ Go do it!! üèÉ‚Äç‚ôÇÔ∏èüí® #datascience #machinelearning #largelanguagemodels #llama2",
      "upload_date": "2023-07-26",
      "total_views": 39440,
      "max_views": 38800,
      "topics": [
        "colab",
        "datascience",
        "get",
        "google",
        "largelanguagemodels",
        "llama",
        "llama2",
        "machinelearning",
        "run",
        "started"
      ],
      "search_text": "üöÄ Just get started on your journey to learn large language models! ü§î Is there a lot to learn? Yes! üòÖ ü§∑‚Äç‚ôÇÔ∏è But is it easy to get started? Yes! üëç ‚úÖ Go do it!! üèÉ‚Äç‚ôÇÔ∏èüí® #datascience #machinelearning #largelanguagemodels #llama2 colab datascience get google largelanguagemodels llama llama2 machinelearning run started",
      "platforms": {
        "tiktok": {
          "video_id": "7259910845616917802",
          "url": "https://www.tiktok.com/@rajistics/video/7259910845616917802",
          "view_count": 38800,
          "upload_date": "2023-07-26",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17861890616977217",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-07-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "p6ZJ6HftmrA",
          "url": "https://www.youtube.com/watch?v=p6ZJ6HftmrA",
          "view_count": 640,
          "upload_date": "2023-07-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6161,
      "title": "This paper introduces vec2vec, a method that aligns text embeddings from different language models—without access to the models or labeled data. It supports the Platonic Representation Hypothesis, showing that large models trained on different data still learn embeddings that can be transformed into one another. The results have serious implications for vector database privacy, as attackers can reconstruct sensitive content from just 10k embeddings. Harnessing the Universal Geometry of Embeddings: https://arxiv.org/pdf/2505.12540 The Platonic Representation Hypothesis: https://arxiv.org/pdf/2405.07987 Background from Nomic: https://atlas.nomic.ai/map/obelics",
      "description": "This paper introduces vec2vec, a method that aligns text embeddings from different language models—without access to the models or labeled data. It supports the Platonic Representation Hypothesis, showing that large models trained on different data still learn embeddings that can be transformed into one another. The results have serious implications for vector database privacy, as attackers can reconstruct sensitive content from just 10k embeddings. Harnessing the Universal Geometry of Embeddings: https://arxiv.org/pdf/2505.12540 The Platonic Representation Hypothesis: https://arxiv.org/pdf/2405.07987 Background from Nomic: https://atlas.nomic.ai/map/obelics",
      "upload_date": "2025-05-22",
      "total_views": 39208,
      "max_views": 37200,
      "topics": [
        "aspects",
        "different",
        "embedding",
        "embeddings",
        "like",
        "model",
        "models",
        "platonic",
        "universal",
        "vec2vec",
        "vectavect",
        "vector"
      ],
      "search_text": "This paper introduces vec2vec, a method that aligns text embeddings from different language models—without access to the models or labeled data. It supports the Platonic Representation Hypothesis, showing that large models trained on different data still learn embeddings that can be transformed into one another. The results have serious implications for vector database privacy, as attackers can reconstruct sensitive content from just 10k embeddings. Harnessing the Universal Geometry of Embeddings: https://arxiv.org/pdf/2505.12540 The Platonic Representation Hypothesis: https://arxiv.org/pdf/2405.07987 Background from Nomic: https://atlas.nomic.ai/map/obelics aspects different embedding embeddings like model models platonic universal vec2vec vectavect vector VectaVect just dropped and our models aren't safe anymore. The paper shows that language models like BERT, OpenAI, Google Secret stuff, might be seeing the same reality from different angles. This is called the Platonic Representation Hypothesis. And it means if your model is big enough, it learns the same underlying structure of language, just rotated or stretched a bit. A new paper from Cornell builds upon this idea and shows that you can align any two models embeddings even if they were trained on totally different data. And this is what's called VectaVect. Think of VectaVect like Google Translate, but for model embeddings. You don't need labels, you don't need access to the model, just lots of raw vectors. And it learns to map between these two spaces really well. Look at these results. Like after translations, the embeddings line up almost perfectly. And once they're aligned, you can even invert the embeddings. As in this case, you can see how they were able to reconstruct text just from the vectors. This is big. And this is where it gets scary. Say you built a trained model. You've only exposed its embeddings in a vector database. No one knows your model so that information's safe, right? Nope. With VectaVect, someone could translate those vectors into a known embedding space and then reconstruct the names, entities, sensitive information. Now, it's not like somebody can steal just a handful of embeddings and decode your secrets. But if you give them, say, 10,000, that's enough to start reconstructing that text. So the next time someone says, oh, we're safe, we only exposed the models, just the embeddings, time to teach them a little bit about Play-Doh.",
      "platforms": {
        "tiktok": {
          "video_id": "7507061300397149471",
          "url": "https://www.tiktok.com/@rajistics/video/7507061300397149471",
          "view_count": 37200,
          "upload_date": "2025-05-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAfuBhCEAmRHAIEEKCFoAcADAhV4AErIfnLEOo~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=ClcvdLMi89Yi46qXUqAmrNAK2%2FU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18061338443135537",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "uuApQbRQGKQ",
          "url": "https://www.youtube.com/watch?v=uuApQbRQGKQ",
          "view_count": 2008,
          "upload_date": "2025-05-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Diffusion models for markup. #datascience #machinelearning #stablediffusion ",
      "description": "Diffusion models for markup. #datascience #machinelearning #stablediffusion ",
      "upload_date": "2022-10-13",
      "total_views": 38700,
      "max_views": 38700,
      "topics": [
        "datascience",
        "diffusion",
        "machinelearning",
        "markup",
        "model",
        "stablediffusion"
      ],
      "search_text": "Diffusion models for markup. #datascience #machinelearning #stablediffusion  datascience diffusion machinelearning markup model stablediffusion So we've seen how stable diffusion can go from text to images. Wow. Now it's been extended to markup. So we're going to start with some markup. This is LaTeX. And what we'll see is that this markup actually represents an equation. And the model itself will generate and give you an image of the final equation. They also train the model on other types of layouts, from table layouts, sheet music, and molecular images. The model isn't perfect in representing the markdown, but it just shows you the power of diffusion. If you want to, go check out the space. You can play around with different LaTeX commands, see how it's visualized. There's code and a paper available as well.",
      "platforms": {
        "tiktok": {
          "video_id": "7153786034256350510",
          "url": "https://www.tiktok.com/@rajistics/video/7153786034256350510",
          "view_count": 38700,
          "upload_date": "2022-10-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e1741ed9738442529fdde2470b516ccf_1665620616~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=nL3XcUkGIN30NZreqEJZtHteMsM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6476,
      "title": "So much going on around using generative tools for reasoning with tasks. HuggingGPT or Jarvis is focused on helping on solving AI tasks. AutoGPT allows you to select your own task the video shows another service AutoAgent that works similarly. Generative agents shows how GPT4 can simulate human like behavior. #datascience #machinelearning #gpt3 #openai #hugginggpt #jarvis #autogpt #sims Let me know which one I should dig deeper into JARVIS / HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace: https://arxiv.org/pdf/2303.17580.pdf https://github.com/microsoft/JARVIS AutoGPT: https://github.com/Torantulino/Auto-GPT Generative Agents: Interactive Simulacra of Human Behavior: https://arxiv.org/abs/2304.03442",
      "description": "So much going on around using generative tools for reasoning with tasks. HuggingGPT or Jarvis is focused on helping on solving AI tasks. AutoGPT allows you to select your own task the video shows another service AutoAgent that works similarly. Generative agents shows how GPT4 can simulate human like behavior. #datascience #machinelearning #gpt3 #openai #hugginggpt #jarvis #autogpt #sims Let me know which one I should dig deeper into JARVIS / HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace: https://arxiv.org/pdf/2303.17580.pdf https://github.com/microsoft/JARVIS AutoGPT: https://github.com/Torantulino/Auto-GPT Generative Agents: Interactive Simulacra of Human Behavior: https://arxiv.org/abs/2304.03442",
      "upload_date": "2023-04-12",
      "total_views": 38609,
      "max_views": 24000,
      "topics": [
        "auto",
        "autogpt",
        "catchup",
        "datascience",
        "generative",
        "gpt",
        "gpt3",
        "hugginggpt",
        "jarvis",
        "machinelearning",
        "sims"
      ],
      "search_text": "So much going on around using generative tools for reasoning with tasks. HuggingGPT or Jarvis is focused on helping on solving AI tasks. AutoGPT allows you to select your own task the video shows another service AutoAgent that works similarly. Generative agents shows how GPT4 can simulate human like behavior. #datascience #machinelearning #gpt3 #openai #hugginggpt #jarvis #autogpt #sims Let me know which one I should dig deeper into JARVIS / HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace: https://arxiv.org/pdf/2303.17580.pdf https://github.com/microsoft/JARVIS AutoGPT: https://github.com/Torantulino/Auto-GPT Generative Agents: Interactive Simulacra of Human Behavior: https://arxiv.org/abs/2304.03442 auto autogpt catchup datascience generative gpt gpt3 hugginggpt jarvis machinelearning sims",
      "platforms": {
        "tiktok": {
          "video_id": "7220955392929615150",
          "url": "https://www.tiktok.com/@rajistics/video/7220955392929615150",
          "view_count": 24000,
          "upload_date": "2023-04-12",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cq6sjdqAzFg",
          "url": "https://www.instagram.com/reel/Cq6sjdqAzFg",
          "view_count": 13851,
          "upload_date": "2023-04-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "zO2im2zO9b8",
          "url": "https://www.youtube.com/watch?v=zO2im2zO9b8",
          "view_count": 758,
          "upload_date": "2023-04-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6467,
      "title": "Interpretable models offer a great alternative to traditional machine learning algorithms. Generalized Additive Models like GA2M Rulefit and Scorecards are just a few of the approaches available. To learn more check out the resources: Interpretable Models for Machine Learning: https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727 Imodels: https://github.com/csinva/imodels InterpretML: https://interpret.ml/ Background dedicated to Britt",
      "description": "Interpretable models offer a great alternative to traditional machine learning algorithms. Generalized Additive Models like GA2M Rulefit and Scorecards are just a few of the approaches available. To learn more check out the resources: Interpretable Models for Machine Learning: https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727 Imodels: https://github.com/csinva/imodels InterpretML: https://interpret.ml/ Background dedicated to Britt",
      "upload_date": "2024-03-09",
      "total_views": 38528,
      "max_views": 31187,
      "topics": [
        "algorithms",
        "imodels",
        "interpretable",
        "learning",
        "like",
        "machine",
        "models",
        "offer",
        "rules"
      ],
      "search_text": "Interpretable models offer a great alternative to traditional machine learning algorithms. Generalized Additive Models like GA2M Rulefit and Scorecards are just a few of the approaches available. To learn more check out the resources: Interpretable Models for Machine Learning: https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727 Imodels: https://github.com/csinva/imodels InterpretML: https://interpret.ml/ Background dedicated to Britt algorithms imodels interpretable learning like machine models offer rules Neural network nonsense is giving me a headache. Why do data scientists have to makes everything so complicated? Well, you want a complex algorithms. The more complicated and fancy the algorithm is, the better it's accurate. Hold on there. We do have simpler, more explainable models. Finally, someone that gets it. A popular option is generalized additive models or gams? They provide a coefficient for every feature so you can understand how the feature is affecting the prediction. Jams, isn't that something you put on schemes? No, but here's a rating table that shows all the different features and what the coefficient is or how much they will contribute to the prediction. Oh boy, that's way too much. A simpler approach would be generating rules for our problem, we can use decision trees to help create these rules. They say I am a bit of a rule breaker. So the more rules we add, the more accurate the model gets, but if we add too many rules, it's going to be hard to explain. So it's a trade-off game again, one of the simplest modeling approaches we can use is a scorecard like this. Kind of like how you calculate a credit rating, now you're speaking my language. Wow, I had no idea you could use complex algorithms to actually build a simpler model. Well, I suddenly got in the mood for scones. Why don't we go grab some and I'll give you a few starting points.",
      "platforms": {
        "tiktok": {
          "video_id": "7344472393429077290",
          "url": "https://www.tiktok.com/@rajistics/video/7344472393429077290",
          "view_count": 6429,
          "upload_date": "2024-03-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4TtZfggct1",
          "url": "https://www.instagram.com/reel/C4TtZfggct1",
          "view_count": 31187,
          "upload_date": "2024-03-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AoR4OfXSK2I",
          "url": "https://youtube.com/shorts/AoR4OfXSK2I",
          "view_count": 912,
          "upload_date": "2024-03-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Google’s sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels ",
      "description": "Google’s sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels ",
      "upload_date": "2023-01-22",
      "total_views": 37900,
      "max_views": 37900,
      "topics": [
        "chatgpt",
        "datascience",
        "google",
        "machinelearning",
        "openai",
        "sparrow"
      ],
      "search_text": "Google’s sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels  chatgpt datascience google machinelearning openai sparrow Google's got a response to WebGPT. It's called Sparrow. Let's dive into it. Google knows how to build large language models. Look what happened with Lambda. And we've all talked about Chinchilla and the scaling laws already. So it looks like Sparrow is a dialogue-optimized version of Chinchilla and has two big advantages. It's going to be smaller and faster. It's also up to date. It can search the internet as well. So Sparrow's also using human feedback during the training. That way it makes sure its answers comport with what we expect. The coolest part of Sparrow is being able to connect to the internet. Because then you can ask it questions whether it's things that might end up in Wikipedia, might be coding questions, or even questions that require up-to-date information and get references along with it. But this isn't some boring encyclopedia. It's also a regular chatbot who knows how to chat. And even when you try to get it off, it knows what to say.",
      "platforms": {
        "tiktok": {
          "video_id": "7191302768752069931",
          "url": "https://www.tiktok.com/@rajistics/video/7191302768752069931",
          "view_count": 37900,
          "upload_date": "2023-01-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b034faf6e1954b06806c5ac836b64ce1_1674355663~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=hq2kWGN%2FzMY1kLyuQ38CLQLc5T8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5960,
      "title": "A GPT-5 + Gemini Pro pipeline might sort one apple every 6 seconds. A simple light frequency sensor sorts hundreds per second. The outcome? Simpler, physics-based solutions can crush even the fanciest AI when speed matters. 👉 Demo: https://youtu.be/VOTaNK-8LYY",
      "description": "A GPT-5 + Gemini Pro pipeline might sort one apple every 6 seconds. A simple light frequency sensor sorts hundreds per second. The outcome? Simpler, physics-based solutions can crush even the fanciest AI when speed matters. 👉 Demo: https://youtu.be/VOTaNK-8LYY",
      "upload_date": "2025-10-01",
      "total_views": 35944,
      "max_views": 34500,
      "topics": [
        "color",
        "even",
        "every",
        "fruit",
        "gemini",
        "gpt",
        "light",
        "might",
        "per",
        "pipeline",
        "pro",
        "second",
        "seconds",
        "sensor",
        "showdown",
        "sort",
        "sorting"
      ],
      "search_text": "A GPT-5 + Gemini Pro pipeline might sort one apple every 6 seconds. A simple light frequency sensor sorts hundreds per second. The outcome? Simpler, physics-based solutions can crush even the fanciest AI when speed matters. 👉 Demo: https://youtu.be/VOTaNK-8LYY color even every fruit gemini gpt light might per pipeline pro second seconds sensor showdown sort sorting This fruit needs to be sorted by color yesterday. We could use AI to solve this. How would that even work? Easy, I'll spin up open AI operator to remotely monitor the conveyor feed, then we'll pass the video frames into Gemini Pro to analyze the color and location of every fruit. Okay, you're the AI guy, when can we test this out? Well, first I need to figure out an AI framework, then we need to build a robust eval data set, then I need to set up infrastructure and containers, probably take two to three weeks. Once I do that, then I'll be able to identify and sort an apple every six to 60 seconds. Six seconds, our production line needs hundreds per second. I know it's not fancy AI, but I got a system in the back that runs 600 fruits per second. How is that even possible? Even with quantization, I can't get near that. What are you, are you using zero bit LLMs? Light frequency sensors, they instantly detect color. Simpler and faster, and video's not gonna like this.",
      "platforms": {
        "tiktok": {
          "video_id": "7556326890621963550",
          "url": "https://www.tiktok.com/@rajistics/video/7556326890621963550",
          "view_count": 34500,
          "upload_date": "2025-10-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ooDEEAdEEFAg33AAIRCDVdIQXfdAdjoduEEfC1~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=H3vp9C5ugpGROhcluxWxGEDkKsM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18080346593479788",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "o3d5ZDfvo5Y",
          "url": "https://www.youtube.com/watch?v=o3d5ZDfvo5Y",
          "view_count": 1444,
          "upload_date": "2025-10-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6108,
      "title": "Some alternatives to clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? Covering: Normalization Guassian mixture models DBSCAN HDBSCAN #datascience #statistics #machinelearning #kmeans #clustering #rajistics Stop using the elbow criterion for k-means and how to choose the number of clusters instead: https://arxiv.org/abs/2212.12189 This is repost from last year",
      "description": "Some alternatives to clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? Covering: Normalization Guassian mixture models DBSCAN HDBSCAN #datascience #statistics #machinelearning #kmeans #clustering #rajistics Stop using the elbow criterion for k-means and how to choose the number of clusters instead: https://arxiv.org/abs/2212.12189 This is repost from last year",
      "upload_date": "2023-12-31",
      "total_views": 35093,
      "max_views": 30200,
      "topics": [
        "clustering",
        "datascience",
        "kmeans",
        "machinelearning",
        "means",
        "statistics"
      ],
      "search_text": "Some alternatives to clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? Covering: Normalization Guassian mixture models DBSCAN HDBSCAN #datascience #statistics #machinelearning #kmeans #clustering #rajistics Stop using the elbow criterion for k-means and how to choose the number of clusters instead: https://arxiv.org/abs/2212.12189 This is repost from last year clustering datascience kmeans machinelearning means statistics",
      "platforms": {
        "tiktok": {
          "video_id": "7318783962925567278",
          "url": "https://www.tiktok.com/@rajistics/video/7318783962925567278",
          "view_count": 30200,
          "upload_date": "2023-12-31",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17885397053977849",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8E2uQ93zC5o",
          "url": "https://youtube.com/shorts/8E2uQ93zC5o",
          "view_count": 4893,
          "upload_date": "2023-12-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5979,
      "title": "This video explains the findings from the Google Research paper \"Learning Without Training: The Implicit Dynamics of In-Context Learning\" (arXiv:2507.16003). The authors show that during inference, large language models behave as if they apply a rank‑1 update to the MLP weight matrix — even though no parameters are actually changed. This implicit update is derived from the prompt and mirrors the effect of gradient descent, suggesting that in-context learning simulates fine-tuning inside the forward pass. The paper introduces the concept of contextual blocks to generalize this mechanism and provides both theoretical and empirical evidence supporting the claim.",
      "description": "This video explains the findings from the Google Research paper \"Learning Without Training: The Implicit Dynamics of In-Context Learning\" (arXiv:2507.16003). The authors show that during inference, large language models behave as if they apply a rank‑1 update to the MLP weight matrix — even though no parameters are actually changed. This implicit update is derived from the prompt and mirrors the effect of gradient descent, suggesting that in-context learning simulates fine-tuning inside the forward pass. The paper introduces the concept of contextual blocks to generalize this mechanism and provides both theoretical and empirical evidence supporting the claim.",
      "upload_date": "2025-07-26",
      "total_views": 34282,
      "max_views": 27200,
      "topics": [
        "context",
        "deep",
        "implicit",
        "learning",
        "model",
        "models",
        "need",
        "paper",
        "training",
        "understanding",
        "update",
        "video"
      ],
      "search_text": "This video explains the findings from the Google Research paper \"Learning Without Training: The Implicit Dynamics of In-Context Learning\" (arXiv:2507.16003). The authors show that during inference, large language models behave as if they apply a rank‑1 update to the MLP weight matrix — even though no parameters are actually changed. This implicit update is derived from the prompt and mirrors the effect of gradient descent, suggesting that in-context learning simulates fine-tuning inside the forward pass. The paper introduces the concept of contextual blocks to generalize this mechanism and provides both theoretical and empirical evidence supporting the claim. context deep implicit learning model models need paper training understanding update video Why do models get better when we give them a few examples? It really doesn't make sense. I mean, training a model, real learning, means updating its weights through gradient descent. But with large language models, we just show them a few examples in a prompt and somehow they get better without any fine tuning, without any weight updates. Most of us just accept that. We call it in context learning and move on. But some Google researchers stop to ask, there's no training happening. Why is it still learning? So to investigate, they introduce contextual blocks. It's a simplified abstraction of the transformer's layers to help isolate what's happening during inference. And what they found was mind-blowing. So when you give a prompt, it's behaving as if it applied a tiny targeted update to its internal weights. Not changing any saved parameters, but it's like computing a rank one patch that temporarily modifies the network during the forward pass. It's as if the model is quietly fine tuning itself on the fly. Now, they tested this behavior. They used synthetic tasks where they prompted a model. They gave a model with those derived updates, and they had identical loss curves. So we now have a testable mathematical explanation for how prompting works. That few shot examples just aren't providing context, they're helping the model adapt real time. That's what makes this paper really important. It shows that that boundary between training and inference might be thinner than we thought, and that these models aren't static anymore. They're dynamic, they're adapting. And this time, it's not just vibes, it's science.",
      "platforms": {
        "tiktok": {
          "video_id": "7531456073337720095",
          "url": "https://www.tiktok.com/@rajistics/video/7531456073337720095",
          "view_count": 27200,
          "upload_date": "2025-07-26",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okAAFiDe2Qu3BGKAIPaBohCwAKiriCAIE0w4EH~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=GpoZHwP1XyXnVwVmQBIQVIRZEcs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18056563169099812",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "emTjHdAzwEs",
          "url": "https://www.youtube.com/watch?v=emTjHdAzwEs",
          "view_count": 7082,
          "upload_date": "2016-10-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6645,
      "title": "Always have a baseline model. For time series you can often compare to what happened in a previous time step like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries",
      "description": "Always have a baseline model. For time series you can often compare to what happened in a previous time step like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries",
      "upload_date": "2023-10-09",
      "total_views": 33531,
      "max_views": 26228,
      "topics": [
        "codetok",
        "datascience",
        "statistics",
        "time",
        "timeseries",
        "timeseriesforcasting"
      ],
      "search_text": "Always have a baseline model. For time series you can often compare to what happened in a previous time step like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries codetok datascience statistics time timeseries timeseriesforcasting",
      "platforms": {
        "tiktok": {
          "video_id": "7287957261522177326",
          "url": "https://www.tiktok.com/@rajistics/video/7287957261522177326",
          "view_count": 7303,
          "upload_date": "2023-10-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CyLjjx5t5bm",
          "url": "https://www.instagram.com/reel/CyLjjx5t5bm",
          "view_count": 26228,
          "upload_date": "2023-10-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "No big deal, use visualization #stats  #datascience #datasaurus #datascience #analytics #anscombe #visualization",
      "description": "No big deal, use visualization #stats  #datascience #datasaurus #datascience #analytics #anscombe #visualization",
      "upload_date": "2022-01-14",
      "total_views": 32200,
      "max_views": 32200,
      "topics": [
        "analytics",
        "anscombe",
        "datasaurus",
        "datascience",
        "stats",
        "visualization"
      ],
      "search_text": "No big deal, use visualization #stats  #datascience #datasaurus #datascience #analytics #anscombe #visualization analytics anscombe datasaurus datascience stats visualization It's no big deal. It's no big deal. This is no big deal.",
      "platforms": {
        "tiktok": {
          "video_id": "7053168457571011886",
          "url": "https://www.tiktok.com/@rajistics/video/7053168457571011886",
          "view_count": 32200,
          "upload_date": "2022-01-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/17a08e1e3339400eb60cd812c4f5f579_1642193754~tplv-tiktokx-origin.image?dr=9636&x-expires=1767510000&x-signature=YaH%2Ba%2FzWWuoMoE%2FeOjnHgOARHCk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Customer lifetime value is a common data science use case. There are many ways to calculate this, but here I introduce the class RFM method and a machine learning alternative. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalytics ",
      "description": "Customer lifetime value is a common data science use case. There are many ways to calculate this, but here I introduce the class RFM method and a machine learning alternative. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalytics ",
      "upload_date": "2023-02-25",
      "total_views": 31700,
      "max_views": 31700,
      "topics": [
        "customer",
        "customerlifetimevalue",
        "datascience",
        "machinelearning",
        "marketing",
        "rfm"
      ],
      "search_text": "Customer lifetime value is a common data science use case. There are many ways to calculate this, but here I introduce the class RFM method and a machine learning alternative. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalytics  customer customerlifetimevalue datascience machinelearning marketing rfm We can't figure out which of our customers to target with marketing. We need your help to do some of your analytics. We could do a customer lifetime analysis, which would allow you to understand how much each customer is worth. Great. How long? A day or two. This type of problem that needs a calculation. Not a good fit for me. I'm out. A couple of weeks. How about Friday? I've finished my analysis. Can you walk me through it? Oh boy. I took the transaction date, aggregated it to the customer level. Once I did that, I focused on three different measures that's common for marketing. The recency of purchases, the frequency of purchases, and the monetary value of purchases. Using RFM, I was able to estimate the value for each customer. But I also used RFM to score each customer, and I put them into different clusters. So we figure out which ones are our top customers that we should focus on, which ones just haven't purchased recently. Maybe we should send a discount coupon to them. So this way, marketing can start making decisions based on these groups that I created. Great work. I just need to put the finishing touches on my dashboard. It's almost Friday. Can we review where you're at? I decided to take a machine learning approach to focus on what customers are likely to do, versus what we already know about them. So the first step I did was I took all the data that we had, split it into two areas, one for me to train on, another data set for me to validate and make sure my model is working correctly. Keep going. I'm following. Then I built a machine learning model at the customer level. For the features, I used the RFM features, but I also brought in other pieces of information we had, such as where they lived and their age for that final model. That's great. Can you explain the predictions? Sure. We can use SHAP to explain any of the predictions. Oh, I got big plans for the second generation version of this, where we can bring in a variety of different types of features into a neural network architecture. It's going to be awesome. Wow, that would be cool. But let's see if this meets marketing needs, and maybe it's good enough for marketing.",
      "platforms": {
        "tiktok": {
          "video_id": "7204141835965500715",
          "url": "https://www.tiktok.com/@rajistics/video/7204141835965500715",
          "view_count": 31700,
          "upload_date": "2023-02-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/405ebb6f834446d3809675990684663d_1677345007~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=0afmpt0ULayzL7zi9k3zXRAYrK0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6443,
      "title": "Knowledge distillation is a useful technique to build smaller high-performing models. DistilBERT is a great example of a widely used model trained using knowledge distillation. Resources: Distilling the Knowledge in a Neural Network - https://arxiv.org/pdf/1503.02531.pdf DistilBERT: https://arxiv.org/abs/1910.01108 Background by Roberta keiko Kitahara Santana: https://unsplash.com/photos/brown-cardboard-box-near-gray-tanks-RfL3l-I1zhc",
      "description": "Knowledge distillation is a useful technique to build smaller high-performing models. DistilBERT is a great example of a widely used model trained using knowledge distillation. Resources: Distilling the Knowledge in a Neural Network - https://arxiv.org/pdf/1503.02531.pdf DistilBERT: https://arxiv.org/abs/1910.01108 Background by Roberta keiko Kitahara Santana: https://unsplash.com/photos/brown-cardboard-box-near-gray-tanks-RfL3l-I1zhc",
      "upload_date": "2024-03-10",
      "total_views": 31648,
      "max_views": 25187,
      "topics": [
        "arxiv",
        "distilbert",
        "distillation",
        "example",
        "knowledge",
        "larger",
        "model",
        "org",
        "pdf",
        "smaller"
      ],
      "search_text": "Knowledge distillation is a useful technique to build smaller high-performing models. DistilBERT is a great example of a widely used model trained using knowledge distillation. Resources: Distilling the Knowledge in a Neural Network - https://arxiv.org/pdf/1503.02531.pdf DistilBERT: https://arxiv.org/abs/1910.01108 Background by Roberta keiko Kitahara Santana: https://unsplash.com/photos/brown-cardboard-box-near-gray-tanks-RfL3l-I1zhc arxiv distilbert distillation example knowledge larger model org pdf smaller Model is way too big. We need to slim it down. Whoa, buddy. Not cool. 70 billion parameters is too large. It's expensive to run. Inference takes forever. Oh, I see. You want me to use a smaller model? Yeah, what else? Now, go try knowledge distillation. Distillation is that's some sort of drinking game. I mean, a smaller model isn't going to perform as well as a larger model. With knowledge distillation, we use the larger models predictions to train the smaller model. It's like a teacher passing its knowledge onto its student and how exactly would the smaller model learn from this bigger teacher model? We're training the model with the probabilities from the larger model. This way the student model understands if it's an easy example or a hard example. This makes the learning more efficient hint and show that you can build a similar model using a lot less training data. Ah, so this is the distill of distillbird. Yeah, it uses knowledge distillation to reduce the size by 40% while retaining 97% of the understanding. Now I know how to slim down a model. Next drinks on me. I",
      "platforms": {
        "tiktok": {
          "video_id": "7344873160019283242",
          "url": "https://www.tiktok.com/@rajistics/video/7344873160019283242",
          "view_count": 4265,
          "upload_date": "2024-03-10",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4WfWKTAkHx",
          "url": "https://www.instagram.com/reel/C4WfWKTAkHx",
          "view_count": 25187,
          "upload_date": "2024-03-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "RPKsQ4srLos",
          "url": "https://youtube.com/shorts/RPKsQ4srLos",
          "view_count": 2196,
          "upload_date": "2024-03-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5974,
      "title": "In 2023, Meta intern Guangxuan Xiao discovered that removing the first few tokens in a sliding-window KV cache caused catastrophic degradation in long-context LLM performance. These tokens acted as attention sinks, stabilizing attention distributions due to softmax’s requirement that weights sum to one. The simple fix—pinning the first four tokens—enabled models to handle 4M+ tokens without retraining or extra compute, later refined by OpenAI with a “sink scalar” and adopted by HuggingFace, NVIDIA, and others. References: * Xiao, G., et al. StreamingLLM: A Simple Fix for Sliding-Window Attention. MIT HAN Lab Blog, 2025. https://hanlab.mit.edu/blog/streamingllm - OpenAI GPT-OSS Model Card: https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
      "description": "In 2023, Meta intern Guangxuan Xiao discovered that removing the first few tokens in a sliding-window KV cache caused catastrophic degradation in long-context LLM performance. These tokens acted as attention sinks, stabilizing attention distributions due to softmax’s requirement that weights sum to one. The simple fix—pinning the first four tokens—enabled models to handle 4M+ tokens without retraining or extra compute, later refined by OpenAI with a “sink scalar” and adopted by HuggingFace, NVIDIA, and others. References: * Xiao, G., et al. StreamingLLM: A Simple Fix for Sliding-Window Attention. MIT HAN Lab Blog, 2025. https://hanlab.mit.edu/blog/streamingllm - OpenAI GPT-OSS Model Card: https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
      "upload_date": "2025-08-09",
      "total_views": 31543,
      "max_views": 30300,
      "topics": [
        "attention",
        "attentions",
        "enabled",
        "first",
        "fix",
        "llms",
        "model",
        "openai",
        "sinks",
        "sliding",
        "streaming",
        "tokens",
        "xiao"
      ],
      "search_text": "In 2023, Meta intern Guangxuan Xiao discovered that removing the first few tokens in a sliding-window KV cache caused catastrophic degradation in long-context LLM performance. These tokens acted as attention sinks, stabilizing attention distributions due to softmax’s requirement that weights sum to one. The simple fix—pinning the first four tokens—enabled models to handle 4M+ tokens without retraining or extra compute, later refined by OpenAI with a “sink scalar” and adopted by HuggingFace, NVIDIA, and others. References: * Xiao, G., et al. StreamingLLM: A Simple Fix for Sliding-Window Attention. MIT HAN Lab Blog, 2025. https://hanlab.mit.edu/blog/streamingllm - OpenAI GPT-OSS Model Card: https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf attention attentions enabled first fix llms model openai sinks sliding streaming tokens xiao So how did an interns project end up in OpenAI's GPT architecture? Let me tell you the story of attention sinks. Maybe your next bit of code will be adopted by OpenAI. So back in 23, Guanshan was an intern at Meta, and he was tasked with figuring out how to make a language model handle really long conversations. At the time, chat bots struggled with anything over a few thousand tokens. Now the obvious fix was why don't you use a sliding attention window? That way you can keep a cache of the most recent information, drop the old ones. Efficient on paper, but in practice, it didn't work. As soon as the first few tokens left the cache, the performance dropped. And that was weird because those first few tokens seemed trivial. Like it's the start of your sequence. It's words like uh and the, like why would removing them cause such a failure? And so here's where they dug deeper. They spent time visualizing the model's attention patterns. There they found that the first few tokens were soaking up a lot of the attention weights in soft mask, everything must sum exactly to one. They acted as pressure valves, putting all the leftover attention into those first few so the distribution could stay stable. Once we removed them, the balance collapsed. So the fix was simple. Let's just keep those first four tokens forever. And suddenly the model could handle really long contexts and it got adapted quickly. Hugging face, Nvidia and others and even open AI later added the sync scalar to their architecture. And the lesson here is by taking the time to do careful investigation that led to new insight, new innovation that helped reshape AI.",
      "platforms": {
        "tiktok": {
          "video_id": "7536406702107807007",
          "url": "https://www.tiktok.com/@rajistics/video/7536406702107807007",
          "view_count": 30300,
          "upload_date": "2025-08-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYAwUEr1oEF1se0sADBG6tclFCDIVIfRgENpWK~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=Pwk3ZCT%2F8EtuSQBeNAIO1rQXbIA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18231783718292839",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "fLieLF5e8Yk",
          "url": "https://www.youtube.com/watch?v=fLieLF5e8Yk",
          "view_count": 1243,
          "upload_date": "2025-08-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Learning curves, it’s a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique. ",
      "description": "Learning curves, it’s a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique. ",
      "upload_date": "2022-11-16",
      "total_views": 31500,
      "max_views": 31500,
      "topics": [
        "bigdata",
        "data",
        "datascience",
        "machinelearning",
        "need",
        "statistics"
      ],
      "search_text": "Learning curves, it’s a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique.  bigdata data datascience machinelearning need statistics Hey boss, I was able to get the entire 112 gigabyte data set from HR and I just kicked off my distributed training job. Wow, that's a lot of data. Do you need all that data? Uh, yeah, this is data science. We want to use all the data. Hey, I totally get that. Let me show you one of the first models I built. Sure. Here's a simplified version of a model I built. Can you tell me how much data I need to figure out this model? Is this enough? How about this? Do you see that we hit a point of diminishing returns where adding more data doesn't help? Oh, so if I don't need all the data, how much do I need? So I use sampling from statistics. When I'm building projects, I start with a small amount of data, keep track of the predicted performance, keep adding the data, and plotting that out. This helps me quantify exactly how much training data I need, and that way I'm not wasting a ton of compute or a ton of time. Yeah, this statistics is new for me. They didn't teach that in computer science. But it makes sense to start with a sample.",
      "platforms": {
        "tiktok": {
          "video_id": "7166432886034418986",
          "url": "https://www.tiktok.com/@rajistics/video/7166432886034418986",
          "view_count": 31500,
          "upload_date": "2022-11-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/36a9d2051c0c4efdb7d498de1ab247cc_1668565201~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=y2i%2BdVNERhuv3JiUqJ9iSMslhEI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "The Data Scientist title is worth $$$ €€€ £££ ¥¥¥. #datascience #dataanalyst #analytics",
      "description": "The Data Scientist title is worth $$$ €€€ £££ ¥¥¥. #datascience #dataanalyst #analytics",
      "upload_date": "2022-02-22",
      "total_views": 30800,
      "max_views": 30800,
      "topics": [
        "alert",
        "analytics",
        "bag",
        "dataanalyst",
        "datascience",
        "major"
      ],
      "search_text": "The Data Scientist title is worth $$$ €€€ £££ ¥¥¥. #datascience #dataanalyst #analytics alert analytics bag dataanalyst datascience major Major bag alert major bag alert bag alert major bag",
      "platforms": {
        "tiktok": {
          "video_id": "7067373137826204975",
          "url": "https://www.tiktok.com/@rajistics/video/7067373137826204975",
          "view_count": 30800,
          "upload_date": "2022-02-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/724aef0bc6e7458f9196940cd0793eb8_1645501039~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=LccHFMPm9WYLQFcfg61lqSqGwLA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6158,
      "title": "This video explores Apple’s recent study on large reasoning models and why they often fail to actually “reason.” It covers controlled puzzle experiments showing that models like Claude and GPT-4o can mimic reasoning—but collapse on harder tasks, stop thinking when they should try harder, and even fail when given the correct algorithm. 🧾 Paper: The Illusion of Thinking: Why Reasoning-Style Benchmarks Don’t Measure Reasoning https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf",
      "description": "This video explores Apple’s recent study on large reasoning models and why they often fail to actually “reason.” It covers controlled puzzle experiments showing that models like Claude and GPT-4o can mimic reasoning—but collapse on harder tasks, stop thinking when they should try harder, and even fail when given the correct algorithm. 🧾 Paper: The Illusion of Thinking: Why Reasoning-Style Benchmarks Don’t Measure Reasoning https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf",
      "upload_date": "2025-06-08",
      "total_views": 30563,
      "max_views": 30300,
      "topics": [
        "apple",
        "claude",
        "don",
        "hands",
        "harder",
        "models",
        "notebook",
        "openai",
        "reasoning",
        "tasks",
        "thinking"
      ],
      "search_text": "This video explores Apple’s recent study on large reasoning models and why they often fail to actually “reason.” It covers controlled puzzle experiments showing that models like Claude and GPT-4o can mimic reasoning—but collapse on harder tasks, stop thinking when they should try harder, and even fail when given the correct algorithm. 🧾 Paper: The Illusion of Thinking: Why Reasoning-Style Benchmarks Don’t Measure Reasoning https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf apple claude don hands harder models notebook openai reasoning tasks thinking We've got new research from Apple, which looked at the latest reasoning models. And what they found was very illusionary. Now, instead of using well-known benchmarks like math, where models might have already seen these problems during training, they used new, clean, symbolic tasks that require reasoning. The advantage is these puzzles scale in difficulty and avoid the issue of contamination. They tested this against Claude, DeepSeek and OpenAI. On easy tasks, the non-thinking models often did better. Reasoning just added some noise. Now, medium tasks, this is where thinking helped. That chain of thought gave them an edge. Hard tasks, all the models collapsed. Accuracy quickly dropped to zero, none of them were common. And here's where it gets weird. You'd expect that as problems get harder, models would try harder, use more tokens, think longer, but the opposite happened. They actually thought less. And it wasn't hitting token limits, they just stopped trying. The budget was there, but their persistence was not. So why is this? And it's because these models don't actually do symbolic reasoning. They don't follow rules, they just generate the appearance of following the rules. So on a simple three-disk tower of annoy, Claude gets it right fast. And then it keeps going, which adds noise, it starts to contradict itself. Why does it do that? Well, it's been trained to be helpful verbose, that long answers are useful, so it doesn't know to kind of quickly stop early. Even if you give them the exact algorithm, you can't execute it step by step. All of this shows that models are narrating reasoning. They don't truly reason. They just mimic the steps, but really can't plan, verify and adapt until models can really follow these symbolic procedures, know when to stop. Reasoning is just going to remain an illusion.",
      "platforms": {
        "tiktok": {
          "video_id": "7513632802726792479",
          "url": "https://www.tiktok.com/@rajistics/video/7513632802726792479",
          "view_count": 30300,
          "upload_date": "2025-06-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okBEVIy9AAlsEAi0RFfEcoDC0ARM7EEfUA1NAF~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=axW6ezXZwJovuxJOsnGUb9Tdzf8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17882087910201137",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "HtlVq8XBbzg",
          "url": "https://www.youtube.com/watch?v=HtlVq8XBbzg",
          "view_count": 263,
          "upload_date": "2025-06-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6427,
      "title": "Reminder to visualize your data with one of my favorites Anscombe's quartet #anscombesquartet #datavisualization #datascience #statistics #rajistics",
      "description": "Reminder to visualize your data with one of my favorites Anscombe's quartet #anscombesquartet #datavisualization #datascience #statistics #rajistics",
      "upload_date": "2023-10-30",
      "total_views": 30389,
      "max_views": 18766,
      "topics": [
        "anscombe",
        "anscombesquartet",
        "data",
        "datascience",
        "datavisualization",
        "example",
        "quartet",
        "reminder",
        "statistics",
        "using",
        "visualize",
        "visualizing"
      ],
      "search_text": "Reminder to visualize your data with one of my favorites Anscombe's quartet #anscombesquartet #datavisualization #datascience #statistics #rajistics anscombe anscombesquartet data datascience datavisualization example quartet reminder statistics using visualize visualizing Hey, intern. I need you to analyze these data sets and tell me what's different. Yes, sir. Alright. Every one of these data sets is different. But when I analyze them and I look at the mean, the standard deviation, even the correlation between variables, they're all the same. I'm not sure what to do. Uh I've been there. Try the plot function. Let me try that. Glad you see it now. It's always important to visualize your data. You can't just trust the numbers alone.",
      "platforms": {
        "tiktok": {
          "video_id": "7295890847378443566",
          "url": "https://www.tiktok.com/@rajistics/video/7295890847378443566",
          "view_count": 8066,
          "upload_date": "2023-10-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CzCmzNbgJan",
          "url": "https://www.instagram.com/reel/CzCmzNbgJan",
          "view_count": 18766,
          "upload_date": "2023-10-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "qe9LosNeKkk",
          "url": "https://www.youtube.com/watch?v=qe9LosNeKkk",
          "view_count": 3557,
          "upload_date": "2023-10-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Vicuna is awesome, go check it out. Its the latest LLama model and very impressive. I ended up cutting out the details on vicuna, since i feel like we have turned the corner on getting GPT-3 performance with open source models. #datascience #machinelearning #llama #vicuna #openai #gpt3 #largelanguagemodels ",
      "description": "Vicuna is awesome, go check it out. Its the latest LLama model and very impressive. I ended up cutting out the details on vicuna, since i feel like we have turned the corner on getting GPT-3 performance with open source models. #datascience #machinelearning #llama #vicuna #openai #gpt3 #largelanguagemodels ",
      "upload_date": "2023-03-31",
      "total_views": 29200,
      "max_views": 29200,
      "topics": [
        "datascience",
        "llama",
        "machinelearning",
        "open",
        "openai",
        "vicuna"
      ],
      "search_text": "Vicuna is awesome, go check it out. Its the latest LLama model and very impressive. I ended up cutting out the details on vicuna, since i feel like we have turned the corner on getting GPT-3 performance with open source models. #datascience #machinelearning #llama #vicuna #openai #gpt3 #largelanguagemodels  datascience llama machinelearning open openai vicuna Open AI's lead is quickly closing. For the last four months, we've been collectively amazed by chat GPT, but at the same time, thousands of people have been trying to reverse engineer and figure out how to improve it. Well, with the latest batch of instruction tune Lama models, I think we're starting to get very close. Take a look at Vakuna. I was really impressed. Now the reason this is happening and what's been the key to open AI success is the quality of the data. Now some of this better training data is legitimately acquired. A lot of it is built by just essentially scraping GPT 3 and 4 and being able to use the results of its queries to feed in this training data. I'm not comfortable with that approach. Lots of teams are using it and it is showing that where we use that approach that lead open AI has is quickly diminishing. But for a substantial number of use cases, we're very close to having open source models that you can use instead of GPT 3.",
      "platforms": {
        "tiktok": {
          "video_id": "7216529333718961450",
          "url": "https://www.tiktok.com/@rajistics/video/7216529333718961450",
          "view_count": 29200,
          "upload_date": "2023-03-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ad94881ca7e4408bb8530f793c4b53a2_1680229183~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=5%2B3Dpo5DwawDN4zHy2rKVn2NBVU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization ",
      "description": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization ",
      "upload_date": "2023-01-08",
      "total_views": 28800,
      "max_views": 28800,
      "topics": [
        "data",
        "datascience",
        "datavisualization",
        "machinelearning",
        "statistics",
        "use"
      ],
      "search_text": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization  data datascience datavisualization machinelearning statistics use So you plotted your data and it looks like a big clump like this. Let's talk about some solutions for this. Two quick fixes are you could decrease the dot size or use transparency. You could use a 2D density plot. I see this a lot for different shot patterns for basketball. The other thing you can do is just randomly sample down and do a subset of your data. It's okay. If there's natural groups in your data, you want to highlight that. And you can even use faceting if you want to separate out the groups and just really show the differences between them. One last technique I've used when I've had points that overlap each other is just to use jittering, which adds a little bit of random noise and kind of helps separate out the points. Hey, thanks to Data2Vis, which gave me the inspiration for this and some of the visuals. They have a lot more plotting tips. Go check them out.",
      "platforms": {
        "tiktok": {
          "video_id": "7186236151093611818",
          "url": "https://www.tiktok.com/@rajistics/video/7186236151093611818",
          "view_count": 28800,
          "upload_date": "2023-01-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/53fa0a230df146f9a20dd1847b04abf0_1673176074~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=z%2BbFtqFgKZ8eKF0Vv6goek%2FX1o4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "7 Baseline Models: Time Series: Previous Value Anomaly: p99 Search: BM25 Recommendation: Popularity Buy recommendations: last viewed Classification: Majority class Regression: Mean The idea from this thread comes from a post by Jo Kristian Bergum https://x.com/jobergum/status/1803674976173629865",
      "description": "7 Baseline Models: Time Series: Previous Value Anomaly: p99 Search: BM25 Recommendation: Popularity Buy recommendations: last viewed Classification: Majority class Regression: Mean The idea from this thread comes from a post by Jo Kristian Bergum https://x.com/jobergum/status/1803674976173629865",
      "upload_date": "2024-06-22",
      "total_views": 28700,
      "max_views": 28700,
      "topics": [
        "baseline",
        "models",
        "recommendation",
        "search",
        "use",
        "value"
      ],
      "search_text": "7 Baseline Models: Time Series: Previous Value Anomaly: p99 Search: BM25 Recommendation: Popularity Buy recommendations: last viewed Classification: Majority class Regression: Mean The idea from this thread comes from a post by Jo Kristian Bergum https://x.com/jobergum/status/1803674976173629865 baseline models recommendation search use value seven baseline models you can use for predictions. This stuff is golden. You can be the room full of PhDs. Save this. For time series, when you're trying to predict the next value, use the previous value. What happened yesterday is probably your best predictor for what's gonna happen tomorrow. For anomaly detection, use a P99 model, which looks at anything that lies outside of what 99% of the observations are. Such a simple rule of thumb, use this everywhere. For search, BM25. That's good old fashioned tech search, still beats that vector search in lots of use cases. For recommendation systems, using popularity. People's choices aren't nearly as diverse as what they'll tell you. If you need a recommendation for what someone's gonna buy, look at what they last viewed. This is a no brainer. A classification, look at the majority class. And when someone tells you their model's 96% accurate, ask how often it happens. Regression problems use a mean or median as your baseline model. Give it up for that mean. These baseline models should always be used as a starting point and as a reference when you're comparing more complicated approaches. This is gonna humble a lot of data scientists.",
      "platforms": {
        "tiktok": {
          "video_id": "7383388345109531950",
          "url": "https://www.tiktok.com/@rajistics/video/7383388345109531950",
          "view_count": 28700,
          "upload_date": "2024-06-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ffc88a5996dd4178aa467cca02c408b6_1719079072~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=IfCWizX23mNS9EKUhVLf1nbgvoE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6138,
      "title": "I think langchain is aweome, but the future is an easy to use UI. Think Alteryx for LLMs. Langflow is a step in the right direction. #datascience #machinelearning #largelanguagemodels #gpt4 #langchain #langflow",
      "description": "I think langchain is aweome, but the future is an easy to use UI. Think Alteryx for LLMs. Langflow is a step in the right direction. #datascience #machinelearning #largelanguagemodels #gpt4 #langchain #langflow",
      "upload_date": "2023-03-17",
      "total_views": 28574,
      "max_views": 28574,
      "topics": [
        "datascience",
        "gpt4",
        "langchain",
        "langflow",
        "largelanguagemodels",
        "like",
        "machinelearning",
        "openai"
      ],
      "search_text": "I think langchain is aweome, but the future is an easy to use UI. Think Alteryx for LLMs. Langflow is a step in the right direction. #datascience #machinelearning #largelanguagemodels #gpt4 #langchain #langflow datascience gpt4 langchain langflow largelanguagemodels like machinelearning openai",
      "platforms": {
        "instagram": {
          "video_id": "17930550938648380",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "UG9UKLfyu7k",
          "url": "https://youtube.com/shorts/UG9UKLfyu7k",
          "view_count": 28574,
          "upload_date": "2023-03-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6478,
      "title": "The power of prompting! How to use a general purpose model to be a special purpose fine tuned model. It's really important to learn good prompting strategies. #largelanguagemodels #promptengineering #openai #gpt4 #medpalm #rajistics 5 Pillars of Prompting: Prompt Engineering: From Words to Art and Copy https://www.saxifrage.xyz/post/prompt-engineering Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine https://arxiv.org/pdf/2311.16452.pdf",
      "description": "The power of prompting! How to use a general purpose model to be a special purpose fine tuned model. It's really important to learn good prompting strategies. #largelanguagemodels #promptengineering #openai #gpt4 #medpalm #rajistics 5 Pillars of Prompting: Prompt Engineering: From Words to Art and Copy https://www.saxifrage.xyz/post/prompt-engineering Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine https://arxiv.org/pdf/2311.16452.pdf",
      "upload_date": "2023-11-29",
      "total_views": 28550,
      "max_views": 22800,
      "topics": [
        "gpt4",
        "largelanguagemodels",
        "medpalm",
        "model",
        "openai",
        "promptengineering",
        "prompting",
        "purpose",
        "want"
      ],
      "search_text": "The power of prompting! How to use a general purpose model to be a special purpose fine tuned model. It's really important to learn good prompting strategies. #largelanguagemodels #promptengineering #openai #gpt4 #medpalm #rajistics 5 Pillars of Prompting: Prompt Engineering: From Words to Art and Copy https://www.saxifrage.xyz/post/prompt-engineering Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine https://arxiv.org/pdf/2311.16452.pdf gpt4 largelanguagemodels medpalm model openai promptengineering prompting purpose want",
      "platforms": {
        "tiktok": {
          "video_id": "7307001152934006059",
          "url": "https://www.tiktok.com/@rajistics/video/7307001152934006059",
          "view_count": 22800,
          "upload_date": "2023-11-29",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0PtJp-AMxZ",
          "url": "https://www.instagram.com/reel/C0PtJp-AMxZ",
          "view_count": 5050,
          "upload_date": "2023-11-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZikM6Beducg",
          "url": "https://youtube.com/shorts/ZikM6Beducg",
          "view_count": 700,
          "upload_date": "2023-11-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI’s research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt ",
      "description": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI’s research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt ",
      "upload_date": "2023-01-07",
      "total_views": 28500,
      "max_views": 28500,
      "topics": [
        "data",
        "deepmind",
        "laws",
        "model",
        "models",
        "scaling"
      ],
      "search_text": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI’s research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt  data deepmind laws model models scaling You were lucky to be here today. I'm gonna teach you about the scaling laws for language models. Excuse me, what's a language model? Azure, go stand in the corner. Just because you pay me doesn't give the right for you to talk to me. Yes, but the important takeaway here is to understand that there's a trade-off between the size of the model and the amount of data that you use. How does more compute fit in? Is more better? Yes, more compute is better. Oh yeah, more compute, baby. Hey, stop building those GPUs so fast. We need to keep prices up. Our scaling laws show that you should emphasize building larger models over getting more training data. As building larger models by our scaling laws will lead to better performing models. Oh, I see you open AI with your 270 billion parameter model. Well, we're gonna build Megatron. That's gonna be 540 billion parameters. Oh yeah, we're gonna build 175 billion parameter model and we're gonna leave it open for everybody. How much is, we will build Wul Dao with 1.75 trillion parameters. Good luck to all of you. And remember, nobody needs more than 300 billion tokens of data. Hey, is anyone gonna double check open AI's numbers? Nah, let's go ahead and build a 540 billion parameter model. Hey, open AI, we double checked your work and your scaling laws, the numbers are way off. We built a better performing model and we actually used a smaller model size but a lot more data. Take a look at this graphic to see the comparison but we have a 70 billion parameter model that used 1.4 million tokens of data. Wait, if that's right, let me do some calculations. We way overbuilt our models. We spent $12 million on compute when we could have got away with three. Don't tell Sundar. Oh, you all relied on those scaling laws I told you about and for the last three years, you've been building bigger and builder models. Yeah, I've been spending my time on other types of strategies. Sorry guys, you hear that call from Microsoft already? Hey, DeepMind, can you help us out with this chat GPT thing as well?",
      "platforms": {
        "tiktok": {
          "video_id": "7185843106527382827",
          "url": "https://www.tiktok.com/@rajistics/video/7185843106527382827",
          "view_count": 28500,
          "upload_date": "2023-01-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3e06b19192004b04bde3fff8e3681086_1673084534~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=1sQfvXTkD1Z9aLqacr0FHwaolzc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Visual question answering (VQA) is another cool task you can do with machine learning. #datascience #machinelearning #visualquestionanswering ",
      "description": "Visual question answering (VQA) is another cool task you can do with machine learning. #datascience #machinelearning #visualquestionanswering ",
      "upload_date": "2022-12-03",
      "total_views": 28500,
      "max_views": 28500,
      "topics": [
        "ask",
        "datascience",
        "image",
        "machinelearning",
        "question",
        "visualquestionanswering"
      ],
      "search_text": "Visual question answering (VQA) is another cool task you can do with machine learning. #datascience #machinelearning #visualquestionanswering  ask datascience image machinelearning question visualquestionanswering Did you know you could use machine learning to ask questions about an image? Let me show you how to code this up in three lines. What we're doing is known as visual question answering and the Transformers package makes this easy to code up. After we install Transformers, we're going to set up our default pipeline and then we're just going to give it the image and our question. In this case, the image is this picture of this chunky cat. And then we can ask any type of question of it. I'm just asking the weather, but you could ask other questions as well. It's really easy to code up, but then once you do that, you can do all kinds of crazy things like look at this demo here where they almost turned it into a visual chatbot where you can ask multiple different questions about the image. Totally cool. Go code it up yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7172978228015926570",
          "url": "https://www.tiktok.com/@rajistics/video/7172978228015926570",
          "view_count": 28500,
          "upload_date": "2022-12-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9e79f57d1556419fa69d2d3f3ccd5fab_1670089149~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=mnzMmwdfMraK4yV6ozzuxhlDUSQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6625,
      "title": "Segment Anything (SAM) is a new segmentation model from Meta. It's a huge improvement over the state of the art and is going to change computer vision. Check it out at: https://segment-anything.com/ #datascience #machinelearning #computervision #imagesegmentation #segmentanything #meta See me this month at the: AI Summit in Montreal on April 20 & Arize AI event on April 25",
      "description": "Segment Anything (SAM) is a new segmentation model from Meta. It's a huge improvement over the state of the art and is going to change computer vision. Check it out at: https://segment-anything.com/ #datascience #machinelearning #computervision #imagesegmentation #segmentanything #meta See me this month at the: AI Summit in Montreal on April 20 & Arize AI event on April 25",
      "upload_date": "2023-04-06",
      "total_views": 27500,
      "max_views": 27500,
      "topics": [
        "computervision",
        "datascience",
        "gpt3",
        "imagesegmentation",
        "llama",
        "machinelearning",
        "meta",
        "openai",
        "segmentanything",
        "vicuna"
      ],
      "search_text": "Segment Anything (SAM) is a new segmentation model from Meta. It's a huge improvement over the state of the art and is going to change computer vision. Check it out at: https://segment-anything.com/ #datascience #machinelearning #computervision #imagesegmentation #segmentanything #meta See me this month at the: AI Summit in Montreal on April 20 & Arize AI event on April 25 computervision datascience gpt3 imagesegmentation llama machinelearning meta openai segmentanything vicuna",
      "platforms": {
        "tiktok": {
          "video_id": "7218767684048096554",
          "url": "https://www.tiktok.com/@rajistics/video/7218767684048096554",
          "view_count": 27500,
          "upload_date": "2023-04-06",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CqeLgOQggRf",
          "url": "https://www.instagram.com/reel/CqeLgOQggRf/",
          "view_count": 0,
          "upload_date": "2023-03-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6091,
      "title": "DeepSeekv3 is turning heads - the paper is also really good, check it all out at: https://github.com/deepseek-ai/DeepSeek-V3",
      "description": "DeepSeekv3 is turning heads - the paper is also really good, check it all out at: https://github.com/deepseek-ai/DeepSeek-V3",
      "upload_date": "2024-12-26",
      "total_views": 27301,
      "max_views": 26600,
      "topics": [
        "art",
        "chinese",
        "december",
        "deepseek",
        "deepseekv3",
        "like",
        "million",
        "model",
        "models",
        "released",
        "state",
        "training"
      ],
      "search_text": "DeepSeekv3 is turning heads - the paper is also really good, check it all out at: https://github.com/deepseek-ai/DeepSeek-V3 art chinese december deepseek deepseekv3 like million model models released state training OpenAI and Anthropic are getting some competition from Deepsea. What? How can they do that? Aren't they Chinese? Didn't we limit exports of GPUs to them? You'd think that would slow them down, but actually that's forced them to become very efficient with their models. Guess how much they spent training this model? 50 million? Maybe 100 million? Try 6 million. 6 million? That wouldn't even cover the babysitters for OpenAI's training runs. Exactly. They pulled it off with some smart training like using Floating Point 8 as well as a mixture of experts approach. Whoa, 371 billion total parameters that only uses 37 billion parameters at a time? It's like only using a portion of the brain that you need for a problem. Exactly. It's big, but it can run fast. 60 tokens per second. Can you translate that into human terms? So it writes texture code five times faster than a human can read. And if you leave it running for 24 hours a day, only costs $2. Does everyone know about this? Sounds like it's going to change everything since it's freely available, so smart, cheap to operate. Oh, people know. The Chinese companies are already more popular than Google among AI researchers because the Chinese companies are freely releasing very high quality models while Google shares their C models. And with DeepSeek here, China is basically turning into Toyota, building lean, mean, efficient models. And meanwhile, we're over here building the Hummer of AI.",
      "platforms": {
        "tiktok": {
          "video_id": "7452786228644056351",
          "url": "https://www.tiktok.com/@rajistics/video/7452786228644056351",
          "view_count": 26600,
          "upload_date": "2024-12-26",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAF2aAVnbAs0EMkAEsfZBRnCVTtfIq9AEsxAED~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=4%2FwzDeg09VEMAz%2BCqmQ%2BjEiY9pQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18058437031919499",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "p-G0TyhSTTo",
          "url": "https://www.youtube.com/watch?v=p-G0TyhSTTo",
          "view_count": 701,
          "upload_date": "2024-12-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6413,
      "title": "QLoRA - Efficient Finetuning of Quantized LLMs",
      "description": "QLoRA - Efficient Finetuning of Quantized LLMs",
      "upload_date": "2023-05-26",
      "total_views": 26714,
      "max_views": 13937,
      "topics": [
        "datascience",
        "efficient",
        "finetuning",
        "largelanguagemodels",
        "llms",
        "lora",
        "machinelearning",
        "peft",
        "qlora",
        "quantized"
      ],
      "search_text": "QLoRA - Efficient Finetuning of Quantized LLMs datascience efficient finetuning largelanguagemodels llms lora machinelearning peft qlora quantized",
      "platforms": {
        "tiktok": {
          "video_id": "7237312556816289066",
          "url": "https://www.tiktok.com/@rajistics/video/7237312556816289066",
          "view_count": 10700,
          "upload_date": "2023-05-26",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CssI27lArAM",
          "url": "https://www.instagram.com/reel/CssI27lArAM",
          "view_count": 2077,
          "upload_date": "2023-05-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "U8Ylwt0iIx4",
          "url": "https://www.youtube.com/watch?v=U8Ylwt0iIx4",
          "view_count": 13937,
          "upload_date": "2023-05-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6120,
      "title": "YOLO is a seminal model in object detection for computer vision. But what is even more interesting is the principal author Joseph Redmon and his journey. While YOLO is still actively developed he has stopped participating. You Only Look Once: Unified Real-Time Object Detection (2015): https://arxiv.org/abs/1506.02640 YOLOv3: An Incremental Improvement: https://arxiv.org/abs/1804.02767 YOLO web site: https://pjreddie.com/darknet/yolo/",
      "description": "YOLO is a seminal model in object detection for computer vision. But what is even more interesting is the principal author Joseph Redmon and his journey. While YOLO is still actively developed he has stopped participating. You Only Look Once: Unified Real-Time Object Detection (2015): https://arxiv.org/abs/1506.02640 YOLOv3: An Incremental Improvement: https://arxiv.org/abs/1804.02767 YOLO web site: https://pjreddie.com/darknet/yolo/",
      "upload_date": "2023-09-13",
      "total_views": 26331,
      "max_views": 26000,
      "topics": [
        "abs",
        "arxiv",
        "back",
        "detection",
        "like",
        "model",
        "object",
        "org",
        "paper",
        "story",
        "time",
        "yolo"
      ],
      "search_text": "YOLO is a seminal model in object detection for computer vision. But what is even more interesting is the principal author Joseph Redmon and his journey. While YOLO is still actively developed he has stopped participating. You Only Look Once: Unified Real-Time Object Detection (2015): https://arxiv.org/abs/1506.02640 YOLOv3: An Incremental Improvement: https://arxiv.org/abs/1804.02767 YOLO web site: https://pjreddie.com/darknet/yolo/ abs arxiv back detection like model object org paper story time yolo",
      "platforms": {
        "tiktok": {
          "video_id": "7278459300983475502",
          "url": "https://www.tiktok.com/@rajistics/video/7278459300983475502",
          "view_count": 26000,
          "upload_date": "2023-09-13",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18263859634095461",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-09-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wfOjnssiKQw",
          "url": "https://www.youtube.com/watch?v=wfOjnssiKQw",
          "view_count": 331,
          "upload_date": "2023-09-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr  #datatable ",
      "description": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr  #datatable ",
      "upload_date": "2023-03-01",
      "total_views": 25100,
      "max_views": 25100,
      "topics": [
        "datascience",
        "datatable",
        "dplyr",
        "machinelearning",
        "pandas",
        "polars"
      ],
      "search_text": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr  #datatable  datascience datatable dplyr machinelearning pandas polars Hey, did you see Pandas 2.0's coming out? That slow and buggy package. You guys should take a look at DePly or DataTable. Yeah, it's a tough package to love. What's changing? It's gonna include support for Arrow, which is gonna be a lot faster because it uses that columnar approach. I switched over to Polars to take advantage of Arrow. What is Polars? It's what everybody's been using on Kaggle. It's a lot faster to work from. It took a little bit of work to switch over by pipelines, but it's working okay. Pandas 2.0's gonna include Arrow, so probably catch up quite a bit on speed. Still won't be as fast as DataTable.",
      "platforms": {
        "tiktok": {
          "video_id": "7205712346411470126",
          "url": "https://www.tiktok.com/@rajistics/video/7205712346411470126",
          "view_count": 25100,
          "upload_date": "2023-03-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e27cb31f9f594258b37f55cf1b82907f_1677710652~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=5YwNlBFTdNTyWmfTzRGYw%2FJ8a7Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "RuterGPT is an inspirational story. Fine-tuning base models allows you to do so much, including better language support. Check out the story of how Ruter built their own Norwegian language model for handling various use cases. Finally, I love the story of the Vasa and wanted to share it.  Blog Post: https://medium.com/ruter-produktutvikling-og-teknologi/how-we-created-rutergpt-ai-lab-at-ruter-part-3-8e2961f9449e The models: https://huggingface.co/RuterNorway #largelanguagemodels #finetuning #norwegian #vasamuseet #rajistics",
      "description": "RuterGPT is an inspirational story. Fine-tuning base models allows you to do so much, including better language support. Check out the story of how Ruter built their own Norwegian language model for handling various use cases. Finally, I love the story of the Vasa and wanted to share it.  Blog Post: https://medium.com/ruter-produktutvikling-og-teknologi/how-we-created-rutergpt-ai-lab-at-ruter-part-3-8e2961f9449e The models: https://huggingface.co/RuterNorway #largelanguagemodels #finetuning #norwegian #vasamuseet #rajistics",
      "upload_date": "2024-05-02",
      "total_views": 24700,
      "max_views": 24700,
      "topics": [
        "fine",
        "finetuning",
        "largelanguagemodels",
        "model",
        "norwegian",
        "vasamuseet"
      ],
      "search_text": "RuterGPT is an inspirational story. Fine-tuning base models allows you to do so much, including better language support. Check out the story of how Ruter built their own Norwegian language model for handling various use cases. Finally, I love the story of the Vasa and wanted to share it.  Blog Post: https://medium.com/ruter-produktutvikling-og-teknologi/how-we-created-rutergpt-ai-lab-at-ruter-part-3-8e2961f9449e The models: https://huggingface.co/RuterNorway #largelanguagemodels #finetuning #norwegian #vasamuseet #rajistics fine finetuning largelanguagemodels model norwegian vasamuseet I tried using ChatGPT, but it's no good at Norwegian. It's not a surprise. Most large language models are dominated by a few languages. If we look up Norwegian, you'll see it's only 0.03% of all the training data. It's too bad we have so many use cases for Norwegian from analyzing customer complaints to being able to ask questions of all of our HR documents. Why don't we train our own language model that's focused on Norwegian? Didn't OpenAI spend millions to build ChatGPT? Instead, we can fine-tune an existing base model so it's better at Norwegian. Much cheaper to do. How would we do that? So I started by building some datasets. I built a Norwegian version of Alpauke, a Norwegian version of OpenOrca. How did you get those datasets? Well, we used some machine translation tools like Google Translate to translate the English version of the datasets into Norwegian. Really, that works? It's not perfect, but for this use case, it worked. So you have the data, what's the training like? So we fine-tuned a Lama 13 billion parameter model. We used 4A10s, Gs, and they ran for about 72 hours. You can find the final version of our fine-tuned model out on HuggingFace. Hey, this model looks pretty good. It's better than ChatGPT. Sometimes you can get it right the first time.",
      "platforms": {
        "tiktok": {
          "video_id": "7364393798752931114",
          "url": "https://www.tiktok.com/@rajistics/video/7364393798752931114",
          "view_count": 24700,
          "upload_date": "2024-05-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c9f177777995409d9cdffbf0c204f692_1714656576~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=l5jud8Jj1dCNE9Hz07AZx23gZrc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6446,
      "title": "Customer lifetime value is a common data science use case. There are many ways to calculate this but here I introduce the classic RFM method. In Part 2 I will show a machine learning alternative. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalytics",
      "description": "Customer lifetime value is a common data science use case. There are many ways to calculate this but here I introduce the classic RFM method. In Part 2 I will show a machine learning alternative. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalytics",
      "upload_date": "2024-02-29",
      "total_views": 24113,
      "max_views": 20436,
      "topics": [
        "customer",
        "customerlifetimevalue",
        "datascience",
        "machinelearning",
        "marketinganalytics",
        "rfm"
      ],
      "search_text": "Customer lifetime value is a common data science use case. There are many ways to calculate this but here I introduce the classic RFM method. In Part 2 I will show a machine learning alternative. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalytics customer customerlifetimevalue datascience machinelearning marketinganalytics rfm Can't figure out which of our customers to target with marketing, we need your help to do some of your analytics. We could do a customer lifetime analysis which would allow you to understand how much each customer's worth. Great. How long? A day or two. This type of problem that needs a calculation, not a good fit for me. I'm out. Couple of weeks, how about Friday? I finished my analysis. Can you walk me through it? Oh boy. I took the transaction day, aggregated it to the customer level. Once I did that, I focused on three different measures that's common for marketing, the recency of purchases, the frequency of purchases and the monetary value of purchase. Using RFM, I was able to estimate the value for each customer but I also used RFM to score each customer and I put them into different clusters. So, we figure out which ones are our top customers that we should focus on which ones just haven't purchased recently. Maybe we should send a discount coupon to 'em so this way, marketing can start making decisions based on these groups that I created. Great work. Look for part 2 next on how a data scientist solves this problem.",
      "platforms": {
        "tiktok": {
          "video_id": "7340869057853558062",
          "url": "https://www.tiktok.com/@rajistics/video/7340869057853558062",
          "view_count": 1791,
          "upload_date": "2024-02-29",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C36tJDSNkGD",
          "url": "https://www.instagram.com/reel/C36tJDSNkGD",
          "view_count": 20436,
          "upload_date": "2024-02-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AqWcbgm1S9U",
          "url": "https://youtube.com/shorts/AqWcbgm1S9U",
          "view_count": 1886,
          "upload_date": "2024-02-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "YouChat and retrieval augmented models. To play around with this, check out haystack from deepset.  #datascience #machinelearning #youchat #chatgpt #openai #retrievalaugmentedmodel #questionanswermodel ",
      "description": "YouChat and retrieval augmented models. To play around with this, check out haystack from deepset.  #datascience #machinelearning #youchat #chatgpt #openai #retrievalaugmentedmodel #questionanswermodel ",
      "upload_date": "2022-12-26",
      "total_views": 23900,
      "max_views": 23900,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "openai",
        "retrievalaugmentedmodel",
        "youchat"
      ],
      "search_text": "YouChat and retrieval augmented models. To play around with this, check out haystack from deepset.  #datascience #machinelearning #youchat #chatgpt #openai #retrievalaugmentedmodel #questionanswermodel  chatgpt datascience machinelearning openai retrievalaugmentedmodel youchat Do you want to see the next step in the evolution of chat GPT? You can see this in UChat. When you type in a question with UChat, you get back an answer like you would in chat GPT, but take a look at the answer. One thing it does is it gives you references to where this information came from. This is really powerful because it allows any of us to verify the accuracy of the information. The approach here also lets you add in new information. This is unlike chat GPT, which is locked into information over a year old. The basic design for building a system like this has been around for a while. It's known as retriever augmented generation or rag for short. You can build this with pretty much off the shelf machine learning techniques. You can see here, someone on Reddit posted on doing an example with medical documents using this approach for the rest of you. If you want to play around with this, there are notebooks that you can grab. Open source Haystack has them. Check them out.",
      "platforms": {
        "tiktok": {
          "video_id": "7181571886411681070",
          "url": "https://www.tiktok.com/@rajistics/video/7181571886411681070",
          "view_count": 23900,
          "upload_date": "2022-12-26",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e5759110c1c24125a7e594b497ad218a_1672090019~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=Foh2UYS34mVnPbI15EQKJUb1VhA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Rust for machine learning. It’s useful in some cases for ML, but learn python first. #datascience #codetok #python #machinelearning #rust ",
      "description": "Rust for machine learning. It’s useful in some cases for ML, but learn python first. #datascience #codetok #python #machinelearning #rust ",
      "upload_date": "2022-09-25",
      "total_views": 23800,
      "max_views": 23800,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "microsoft",
        "python",
        "rust"
      ],
      "search_text": "Rust for machine learning. It’s useful in some cases for ML, but learn python first. #datascience #codetok #python #machinelearning #rust  codetok datascience machinelearning microsoft python rust Did you see that Microsoft CTO says we should be using Rust? I'm a data scientist and I don't think Microsoft is pretty relevant. Cures what they say. But Rust has memory safe guarantees. Huh, memory? I just run pandas on a larger instance or shell out some more cash to data breaks. This means Rust is faster. Oh, tell me more. Rust is statically typed. It's a bit harder to use, but I can help you. What? You can make my classification algorithm run faster by using Rust? What kind of toppings do you like on your pizza?",
      "platforms": {
        "tiktok": {
          "video_id": "7147127462000889134",
          "url": "https://www.tiktok.com/@rajistics/video/7147127462000889134",
          "view_count": 23800,
          "upload_date": "2022-09-25",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5992a46ce071482b869a2dbf4b23ec7a_1664070296~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=AxyHWBS2Y8wAt89yYYyZFcdvxoE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Showing the latent space for stable diffusion. Next video is on explainability. #stablediffusion #datascience #machinelearning #codetok #umap ",
      "description": "Showing the latent space for stable diffusion. Next video is on explainability. #stablediffusion #datascience #machinelearning #codetok #umap ",
      "upload_date": "2022-09-14",
      "total_views": 23800,
      "max_views": 23800,
      "topics": [
        "datascience",
        "kind",
        "machinelearning",
        "see",
        "stablediffusion",
        "umap"
      ],
      "search_text": "Showing the latent space for stable diffusion. Next video is on explainability. #stablediffusion #datascience #machinelearning #codetok #umap  datascience kind machinelearning see stablediffusion umap I got the map to the latent space inside stable diffusion. Let's go for a tour. So Dave McLeur took the text image pairs, used UMAP to convert them into a two-dimensional space, now we can explore. Each of these dots here is a different caption, but what we notice is two similar types of captions are often grouped together. Now David went ahead and annotated this where he noticed there was an areas around art, people, clothes, and housing. But let's zoom in a little bit more and kind of take the tour of this area. The first thing I look at are the islands. What are the groups of concepts that are really distinct from from everyone else? And here you'll see food and hairstyles are two examples of that. Now after that, once we start looking in, we can see there's different animals here that are clustered together around wolves, lions, birds, and flowers. And we also see different kinds of spaces here, Rocky Mountains, islands, all of that's grouped together. When we move over to this section, we'll see K-pop is its own island. It's somewhere close to kind of wedding is in clothing, but you'll see here there's a whole area around clothes and photo shoots. If we keep moving down, we'll see that celebrities each have their own like little area here, and even there's groups of celebrities such as actresses from the 30s and 40s. Finally, we'll see houses, right? We take a lot of pictures of houses, have their own kind of distinct area where you can see differences between kitchens and dining rooms and sofas. And what we're seeing here is just our human interpretation of all these different text captions that have been clustered together. But it's a great way to start getting a feel for exactly kind of how all this information is organized and when we create and use prompts, how this is being where this is being pulled from. David shared the code as well as the embeddings for this. So if you want to feel free to kind of make your own, share your map with me as well. Thanks.",
      "platforms": {
        "tiktok": {
          "video_id": "7143359375409990958",
          "url": "https://www.tiktok.com/@rajistics/video/7143359375409990958",
          "view_count": 23800,
          "upload_date": "2022-09-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b052d17052184ec0a1f177598eaf355d_1663192979~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=61KeVOIaun5eDOWIVe1Z0n2MC1w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Other tips I should share? #datascience #timeseries #statistics #dataanalysis #python #codetok #mltok",
      "description": "Other tips I should share? #datascience #timeseries #statistics #dataanalysis #python #codetok #mltok",
      "upload_date": "2022-06-12",
      "total_views": 23700,
      "max_views": 23700,
      "topics": [
        "codetok",
        "dataanalysis",
        "datascience",
        "python",
        "statistics",
        "timeseries"
      ],
      "search_text": "Other tips I should share? #datascience #timeseries #statistics #dataanalysis #python #codetok #mltok codetok dataanalysis datascience python statistics timeseries Let me share one of my favorite tips when working with time series data, decomposition. So let's start with the data, let's visualize it. Now let's start to decompose it and see the story it tells. It's a simple Python function, but once I do this, I can start identifying the trend. This helps me understand what are the long-term implications of this of the series, the seasonal components. So in this case, we set a period of 12 months. So how much is it changing over those 12 months? Finally, I could see the residuals and the residuals here sometimes could be noise, but other times you'll use more advanced techniques to start understanding some of the other intricacies of this time series. So whatever your favorite data science tool is, look for the decomposition visualizations.",
      "platforms": {
        "tiktok": {
          "video_id": "7108369559110962474",
          "url": "https://www.tiktok.com/@rajistics/video/7108369559110962474",
          "view_count": 23700,
          "upload_date": "2022-06-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/57596832656c43cbaabfa22098da2b3e_1655046263~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=F2vb1lSh%2FlLagA%2BoTbKxQxMn3fw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6663,
      "title": "Post my favorites. If you want more search for Eat Melon rajistics. ",
      "description": "Post my favorites. If you want more search for Eat Melon rajistics. ",
      "upload_date": "2025-01-16",
      "total_views": 23500,
      "max_views": 23500,
      "topics": [
        "eat",
        "find",
        "like",
        "melon",
        "poop",
        "want",
        "watermelon"
      ],
      "search_text": "Post my favorites. If you want more search for Eat Melon rajistics.  eat find like melon poop want watermelon Why do you like watermelon over poop? How are you gonna teach that to an AI? Why would you? Well, I did it. The way my little robot friend works is when they get a positive reward when they find watermelon and a negative reward when they eat poop. Just from those rules, they learn how to navigate and quickly find watermelon all the time. This approach is known as reinforcement learning and is growing within the computer science as a way to train robots, whether real or virtual. We can also use this approach to train models like ChatGPT to help identify helpful generated texts. Think of that as melons or not so good texts. Think of that as poops and what we want is a model like ChatGPT to quickly go and give us lots of that melon. Not a lot of those poop answers. If you all like this, let me know I can do a deeper dive into how my little friend works.",
      "platforms": {
        "tiktok": {
          "video_id": "7460327241386364191",
          "url": "https://www.tiktok.com/@rajistics/video/7460327241386364191",
          "view_count": 23500,
          "upload_date": "2025-01-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocdIkfAifAET1gCcADRnTCCSaEEVFEAPoGAIto~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=2qEHx7BYU8itbyTSNIQDWzSge20%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18010529195509901",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6430,
      "title": "Code LLama 70B dropped but we also have some other research on building and using copilots that are were also worthy. Code Llama: https://ai.meta.com/blog/code-llama-large-language-model-coding Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality - https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality Building Your Own Product Copilot: Challenges Opportunities and Needs - https://arxiv.org/pdf/2312.14231.pdf Background: https://pixabay.com/videos/dna-science-biology-laboratory-197931/",
      "description": "Code LLama 70B dropped but we also have some other research on building and using copilots that are were also worthy. Code Llama: https://ai.meta.com/blog/code-llama-large-language-model-coding Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality - https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality Building Your Own Product Copilot: Challenges Opportunities and Needs - https://arxiv.org/pdf/2312.14231.pdf Background: https://pixabay.com/videos/dna-science-biology-laboratory-197931/",
      "upload_date": "2024-01-30",
      "total_views": 23236,
      "max_views": 12900,
      "topics": [
        "also",
        "building",
        "code",
        "coding",
        "copilot",
        "llama"
      ],
      "search_text": "Code LLama 70B dropped but we also have some other research on building and using copilots that are were also worthy. Code Llama: https://ai.meta.com/blog/code-llama-large-language-model-coding Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality - https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality Building Your Own Product Copilot: Challenges Opportunities and Needs - https://arxiv.org/pdf/2312.14231.pdf Background: https://pixabay.com/videos/dna-science-biology-laboratory-197931/ also building code coding copilot llama",
      "platforms": {
        "tiktok": {
          "video_id": "7329695855756217646",
          "url": "https://www.tiktok.com/@rajistics/video/7329695855756217646",
          "view_count": 12900,
          "upload_date": "2024-01-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C2tLPiOtHZr",
          "url": "https://www.instagram.com/reel/C2tLPiOtHZr",
          "view_count": 7482,
          "upload_date": "2024-01-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "kBH8zRZJ0Kw",
          "url": "https://youtube.com/shorts/kBH8zRZJ0Kw",
          "view_count": 2854,
          "upload_date": "2024-01-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6290,
      "title": "Cheating has reared its head again over at Kaggle. Some background for folks on Kaggle and cheating there. #datascience #machinelearning #kaggle #ottocompetition",
      "description": "Cheating has reared its head again over at Kaggle. Some background for folks on Kaggle and cheating there. #datascience #machinelearning #kaggle #ottocompetition",
      "upload_date": "2023-01-30",
      "total_views": 23071,
      "max_views": 23071,
      "topics": [
        "chatgpt",
        "cheating",
        "datascience",
        "enterprise",
        "got",
        "hey",
        "kaggle",
        "machinelearning",
        "ottocompetition",
        "reared"
      ],
      "search_text": "Cheating has reared its head again over at Kaggle. Some background for folks on Kaggle and cheating there. #datascience #machinelearning #kaggle #ottocompetition chatgpt cheating datascience enterprise got hey kaggle machinelearning ottocompetition reared",
      "platforms": {
        "instagram": {
          "video_id": "17901022754716347",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "h5kJWMMtp8c",
          "url": "https://youtube.com/shorts/h5kJWMMtp8c",
          "view_count": 23071,
          "upload_date": "2023-01-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Composer will be sharing their new generative AI models and they look amazing. They key is they decompose the image, which then provides a lot more flexibility for creating new images. #datascience #machinelearning #stablediffusion #composer #generativeai ",
      "description": "Composer will be sharing their new generative AI models and they look amazing. They key is they decompose the image, which then provides a lot more flexibility for creating new images. #datascience #machinelearning #stablediffusion #composer #generativeai ",
      "upload_date": "2023-02-26",
      "total_views": 22900,
      "max_views": 22900,
      "topics": [
        "composer",
        "datascience",
        "generativeai",
        "going",
        "machinelearning",
        "stablediffusion"
      ],
      "search_text": "Composer will be sharing their new generative AI models and they look amazing. They key is they decompose the image, which then provides a lot more flexibility for creating new images. #datascience #machinelearning #stablediffusion #composer #generativeai  composer datascience generativeai going machinelearning stablediffusion Are you ready to see the internet break this week? It's going to with the release of Composer, which is the next generation of generative mommas. Wow. While Composer is a much bigger model than Stable Diffusion, that's not all it. Composer starts before even training time, decomposing an image into various parts, like is it a sketch? Is it a palette? What's the depth math? We're going to be able to later use that information. With that information in the model, when you go to use it at inference time, now you have a much richer, wider set of capabilities for creating that final image. This is really going to be next generation. It's going to offer people a lot of flexibility, but still allow you to generate high quality images. I see this as really taking over the next step from Stable Diffusion. This work is from the research group at Alibaba, and they're going to release the code and models, so look for it soon.",
      "platforms": {
        "tiktok": {
          "video_id": "7204501282458078510",
          "url": "https://www.tiktok.com/@rajistics/video/7204501282458078510",
          "view_count": 22900,
          "upload_date": "2023-02-26",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/32e34ec2473e453796c204315b7d0ccf_1677428689~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=PoDlirwbBcz9bNDGT42O6K%2BRALM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6428,
      "title": "Be skeptical of new models like TimeFM from Google (but still listen). For many reasons deep learning models do not work well for time series problems. Most time series practioners are not looking for deep learning time series models. TimeFM -- A decoder-only foundation model for time-series forecasting: https://arxiv.org/pdf/2310.10688.pdf Tranformers_Are_What_You_Dont_Need: https://github.com/valeman/Tranformers_Are_What_You_Dont_Need Nixtla TimeGPT: https://docs.nixtla.io/docs/timegpt_quickstart #timeseries #timefm #google #rajistics #deeplearningtechnique",
      "description": "Be skeptical of new models like TimeFM from Google (but still listen). For many reasons deep learning models do not work well for time series problems. Most time series practioners are not looking for deep learning time series models. TimeFM -- A decoder-only foundation model for time-series forecasting: https://arxiv.org/pdf/2310.10688.pdf Tranformers_Are_What_You_Dont_Need: https://github.com/valeman/Tranformers_Are_What_You_Dont_Need Nixtla TimeGPT: https://docs.nixtla.io/docs/timegpt_quickstart #timeseries #timefm #google #rajistics #deeplearningtechnique",
      "upload_date": "2024-02-04",
      "total_views": 22632,
      "max_views": 11600,
      "topics": [
        "deeplearningtechnique",
        "foundation",
        "google",
        "model",
        "series",
        "time",
        "timefm",
        "timeseries",
        "timesfm"
      ],
      "search_text": "Be skeptical of new models like TimeFM from Google (but still listen). For many reasons deep learning models do not work well for time series problems. Most time series practioners are not looking for deep learning time series models. TimeFM -- A decoder-only foundation model for time-series forecasting: https://arxiv.org/pdf/2310.10688.pdf Tranformers_Are_What_You_Dont_Need: https://github.com/valeman/Tranformers_Are_What_You_Dont_Need Nixtla TimeGPT: https://docs.nixtla.io/docs/timegpt_quickstart #timeseries #timefm #google #rajistics #deeplearningtechnique deeplearningtechnique foundation google model series time timefm timeseries timesfm",
      "platforms": {
        "tiktok": {
          "video_id": "7331810381213535530",
          "url": "https://www.tiktok.com/@rajistics/video/7331810381213535530",
          "view_count": 11600,
          "upload_date": "2024-02-04",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C272TXnAElp",
          "url": "https://www.instagram.com/reel/C272TXnAElp",
          "view_count": 7590,
          "upload_date": "2024-02-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "zp2zsKq-qbQ",
          "url": "https://www.youtube.com/watch?v=zp2zsKq-qbQ",
          "view_count": 3442,
          "upload_date": "2024-02-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6418,
      "title": "An emerging trend of using large language models like GPT-4 for labeling data instead of using humans to annotate data: #datascience #machinelearning #gpt4 #alpaca #labelingdata #annotatingdata Background by Erol Ahmed: https://unsplash.com/photos/Y3KEBQlB1Zk ChatDoctor: https://github.com/Kent0n-Li/ChatDoctor GPT-4 Labeling: https://www.artisana.ai/articles/gpt-4-outperforms-elite-crowdworkers-saving-researchers-usd500-000-and-20",
      "description": "An emerging trend of using large language models like GPT-4 for labeling data instead of using humans to annotate data: #datascience #machinelearning #gpt4 #alpaca #labelingdata #annotatingdata Background by Erol Ahmed: https://unsplash.com/photos/Y3KEBQlB1Zk ChatDoctor: https://github.com/Kent0n-Li/ChatDoctor GPT-4 Labeling: https://www.artisana.ai/articles/gpt-4-outperforms-elite-crowdworkers-saving-researchers-usd500-000-and-20",
      "upload_date": "2023-05-20",
      "total_views": 22531,
      "max_views": 13500,
      "topics": [
        "alpaca",
        "annotate",
        "annotatingdata",
        "data",
        "datascience",
        "gpt",
        "gpt4",
        "label",
        "labelingdata",
        "machinelearning",
        "using"
      ],
      "search_text": "An emerging trend of using large language models like GPT-4 for labeling data instead of using humans to annotate data: #datascience #machinelearning #gpt4 #alpaca #labelingdata #annotatingdata Background by Erol Ahmed: https://unsplash.com/photos/Y3KEBQlB1Zk ChatDoctor: https://github.com/Kent0n-Li/ChatDoctor GPT-4 Labeling: https://www.artisana.ai/articles/gpt-4-outperforms-elite-crowdworkers-saving-researchers-usd500-000-and-20 alpaca annotate annotatingdata data datascience gpt gpt4 label labelingdata machinelearning using",
      "platforms": {
        "tiktok": {
          "video_id": "7235296073177222443",
          "url": "https://www.tiktok.com/@rajistics/video/7235296073177222443",
          "view_count": 13500,
          "upload_date": "2023-05-20",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CseJZ8WgZXl",
          "url": "https://www.instagram.com/reel/CseJZ8WgZXl",
          "view_count": 1802,
          "upload_date": "2023-05-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Dcq-RbPdDfo",
          "url": "https://www.youtube.com/watch?v=Dcq-RbPdDfo",
          "view_count": 7229,
          "upload_date": "2023-05-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6442,
      "title": "A little taste of geospatial analytics. Considering spatial information can be very valuable for data science and machine learning. It‚Äôs good to understand how spatial data is stored and analyzed. H3 is an emerging standard for doing your analytics on location data. H3: Simplifying the World's Map - https://h3-snow.streamlit.app/ Uber H3: https://www.uber.com/blog/h3/ Background: https://www.youtube.com/watch?v=1d56UJKKk8Y&ab_channel=CARTO #geospatial #h3 #uber #rajistics #carto",
      "description": "A little taste of geospatial analytics. Considering spatial information can be very valuable for data science and machine learning. It‚Äôs good to understand how spatial data is stored and analyzed. H3 is an emerging standard for doing your analytics on location data. H3: Simplifying the World's Map - https://h3-snow.streamlit.app/ Uber H3: https://www.uber.com/blog/h3/ Background: https://www.youtube.com/watch?v=1d56UJKKk8Y&ab_channel=CARTO #geospatial #h3 #uber #rajistics #carto",
      "upload_date": "2024-02-23",
      "total_views": 22508,
      "max_views": 13076,
      "topics": [
        "analytics",
        "carto",
        "data",
        "geospatial",
        "h3",
        "hexes",
        "spatial",
        "uber"
      ],
      "search_text": "A little taste of geospatial analytics. Considering spatial information can be very valuable for data science and machine learning. It‚Äôs good to understand how spatial data is stored and analyzed. H3 is an emerging standard for doing your analytics on location data. H3: Simplifying the World's Map - https://h3-snow.streamlit.app/ Uber H3: https://www.uber.com/blog/h3/ Background: https://www.youtube.com/watch?v=1d56UJKKk8Y&ab_channel=CARTO #geospatial #h3 #uber #rajistics #carto analytics carto data geospatial h3 hexes spatial uber I've been hexed by Uber. Should he use public transportation? Nah, I moved over to using H3 for spatial analytics. What's wrong with creating a simple grid based on latitude and longitude. Come on, take a look at the globe. Those types of grids have severe distortion especially as we get near the polls. Also makes doing spatial analytics very hard. We've withdrawn the offer to purchase Greenland new information has showed the map to be deceiving. Few years ago, Uber developed H3 which is an hexagonal gridding system across the entire globe. You can even do it at different scales and resolutions. So why use hexes? So hexes allow us to do things like calculate a radius very easily if something is moving through an area a hex shape allows us to minimize the error and finally when we're calculating distances to other neighborhood hexes hexes give us a nice uniform way of doing it versus other ways of gridding. Good stuff. How much is Uber charging? So Google open sourced it so you'll see lots of analytical packages as well as databases that support H3. No surge pricing I'm in.",
      "platforms": {
        "tiktok": {
          "video_id": "7338922658009992491",
          "url": "https://www.tiktok.com/@rajistics/video/7338922658009992491",
          "view_count": 7207,
          "upload_date": "2024-02-23",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C3tN0ajgQjq",
          "url": "https://www.instagram.com/reel/C3tN0ajgQjq",
          "view_count": 13076,
          "upload_date": "2024-02-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Xvljg_8Ol1k",
          "url": "https://youtube.com/shorts/Xvljg_8Ol1k",
          "view_count": 2225,
          "upload_date": "2024-02-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6063,
      "title": "Challenging the common assumption about normal data distributions, rajistics explains that real-world data often exhibits skewness, spikes at rounded values, or zero-inflation (as seen in insurance claims). They recommend using gradient boosted algorithms like XGBoost to handle non-normal distributions effectively, while suggesting that perfectly normal distributions might actually be suspicious.",
      "description": "Challenging the common assumption about normal data distributions, rajistics explains that real-world data often exhibits skewness, spikes at rounded values, or zero-inflation (as seen in insurance claims). They recommend using gradient boosted algorithms like XGBoost to handle non-normal distributions effectively, while suggesting that perfectly normal distributions might actually be suspicious.",
      "upload_date": "2025-02-04",
      "total_views": 22500,
      "max_views": 22500,
      "topics": [
        "assumption",
        "challenging",
        "common",
        "data",
        "distributions",
        "normal",
        "often",
        "spikes"
      ],
      "search_text": "Challenging the common assumption about normal data distributions, rajistics explains that real-world data often exhibits skewness, spikes at rounded values, or zero-inflation (as seen in insurance claims). They recommend using gradient boosted algorithms like XGBoost to handle non-normal distributions effectively, while suggesting that perfectly normal distributions might actually be suspicious. assumption challenging common data distributions normal often spikes Is your data normal? Hmm, you should probably be a little concerned if it is, because I don't often see data that's normally distributed. It's much more common for me to see data like this that's skewed to one side or another, and especially where it has spikes, because often there's some common values, such as you ask people to give a weight and they round to 30 and 40, and so you get spikes in that area. Another common pattern you might see is zero-inflated data where you have a lot of zeros, and for a lot of problems, it's quite common. For example, in insurance, lots of people don't have accidents, so we have lots of zeros in our data distribution. An easy first step is working with gradient-boosted algorithms, such as XGBoost, that can handle these types of skews in the data pretty easily, even give you advanced loss functions like Tweety to make it easy for you. So don't fret if your data isn't normally distributed, and you know what? If your data is normally distributed, it kind of makes me wonder if somebody messed with it.",
      "platforms": {
        "tiktok": {
          "video_id": "7467352499146493215",
          "url": "https://www.tiktok.com/@rajistics/video/7467352499146493215",
          "view_count": 22500,
          "upload_date": "2025-02-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o082OAVDnCuwSbgFEAAVfAITEEICfo4AJRQ1Fv~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=2szcplRFwKS%2Bq6bqVV6vT25WNms%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18082394440593648",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Missing data happens all the time. Don’t just jump to dropping rows or using imputation techniques. #dataengineering #statistics #datascience #imputation ",
      "description": "Missing data happens all the time. Don’t just jump to dropping rows or using imputation techniques. #dataengineering #statistics #datascience #imputation ",
      "upload_date": "2022-11-24",
      "total_views": 22500,
      "max_views": 22500,
      "topics": [
        "data",
        "dataengineering",
        "datascience",
        "imputation",
        "missing",
        "statistics"
      ],
      "search_text": "Missing data happens all the time. Don’t just jump to dropping rows or using imputation techniques. #dataengineering #statistics #datascience #imputation  data dataengineering datascience imputation missing statistics I did a quick check on the data. There's a lot of missing data. Only 21% of the rows have the crash location that I'm looking for. I spent a lot of time joining a bunch of tables together to get all that data that we have. If there's missing stuff, it's probably due to past migrations that we have. So there's really nothing that can be done. Oh, okay. Well, let me ask my team for advice. Hey, why don't you just drop the rows with the missing data? That worked for me. Didn't you use that approach in the diversity survey and ended up with a lot of unrepresented people in the final results? Bro, you should try out some wicked amputation techniques. That's what I used on the marketing stuff. Worked wonders. Yeah, that's when we ended up with a 16-year-old with a PhD and those 75-year-olds that we thought liked motorcycle racing. I don't know if we want to go with that yet. We don't get a lot of data scientists down in the call center. Most of what we do in the call center, we're heavily regulated. We have to get through so many calls every hour or we'll lose our jobs. And so one of the things we do is field that are optional, like your field around crash location. It's an optional field, so we'll just tab through it and most people won't bother filling it out. That's amazing. Do you think that's random? I wouldn't say it's at random because it really depends on what call center you're at and what exec is running that call center. And what happens is the execs all get shuffled around and the rules just kind of change. It's such a pain. Yeah, this project has really taught me that I should always try to understand what missing data is and that there's no accidents. Everything has a reason.",
      "platforms": {
        "tiktok": {
          "video_id": "7169668787061476651",
          "url": "https://www.tiktok.com/@rajistics/video/7169668787061476651",
          "view_count": 22500,
          "upload_date": "2022-11-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d4d8a0b92f704f88b099047a8500986e_1669318614~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=eDOdOq5rRWMfyDTovdThzn735wQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6411,
      "title": "LayoutLMv3 Training with CORD (receipts) dataset",
      "description": "LayoutLMv3 Training with CORD (receipts) dataset",
      "upload_date": "2022-09-09",
      "total_views": 22313,
      "max_views": 22313,
      "topics": [
        "cord",
        "dataset",
        "layoutlmv3",
        "receipts",
        "training"
      ],
      "search_text": "LayoutLMv3 Training with CORD (receipts) dataset cord dataset layoutlmv3 receipts training",
      "platforms": {
        "youtube": {
          "video_id": "bsT_1uDRQVo",
          "url": "https://www.youtube.com/watch?v=bsT_1uDRQVo",
          "view_count": 22313,
          "upload_date": "2022-09-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6525,
      "title": "Doing data analysis with large language models like ChatGPT. It's going to be amazing as these technologies let us combine our data text and code understanding. #datascience #machinelearning #chatgpt #openai #dataanalysis The inside story of ChatGPT's astonishing potential: https://www.ted.com/talks/greg_brockman_the_inside_story_of_chatgpt_s_astonishing_potential/c Photo By Bruce Hong: https://unsplash.com/photos/OI8YnODoWms",
      "description": "Doing data analysis with large language models like ChatGPT. It's going to be amazing as these technologies let us combine our data text and code understanding. #datascience #machinelearning #chatgpt #openai #dataanalysis The inside story of ChatGPT's astonishing potential: https://www.ted.com/talks/greg_brockman_the_inside_story_of_chatgpt_s_astonishing_potential/c Photo By Bruce Hong: https://unsplash.com/photos/OI8YnODoWms",
      "upload_date": "2023-04-25",
      "total_views": 22252,
      "max_views": 18100,
      "topics": [
        "apr",
        "bans",
        "chatgpt",
        "data",
        "dataanalysis",
        "datascience",
        "machinelearning",
        "news",
        "openai",
        "roundup",
        "training"
      ],
      "search_text": "Doing data analysis with large language models like ChatGPT. It's going to be amazing as these technologies let us combine our data text and code understanding. #datascience #machinelearning #chatgpt #openai #dataanalysis The inside story of ChatGPT's astonishing potential: https://www.ted.com/talks/greg_brockman_the_inside_story_of_chatgpt_s_astonishing_potential/c Photo By Bruce Hong: https://unsplash.com/photos/OI8YnODoWms apr bans chatgpt data dataanalysis datascience machinelearning news openai roundup training",
      "platforms": {
        "tiktok": {
          "video_id": "7226126906503204142",
          "url": "https://www.tiktok.com/@rajistics/video/7226126906503204142",
          "view_count": 18100,
          "upload_date": "2023-04-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Creheq4gZlD",
          "url": "https://www.instagram.com/reel/Creheq4gZlD",
          "view_count": 3951,
          "upload_date": "2023-04-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "YHHkmZir0do",
          "url": "https://www.youtube.com/watch?v=YHHkmZir0do",
          "view_count": 201,
          "upload_date": "2023-04-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6412,
      "title": "Why GPUs from NVIDIA are important for machine learning",
      "description": "Why GPUs from NVIDIA are important for machine learning",
      "upload_date": "2023-06-02",
      "total_views": 22141,
      "max_views": 17054,
      "topics": [
        "algebra",
        "datascience",
        "deeplearning",
        "gpus",
        "important",
        "learning",
        "machine",
        "machinelearning",
        "matrix",
        "matrixmultiplication",
        "nvidia"
      ],
      "search_text": "Why GPUs from NVIDIA are important for machine learning algebra datascience deeplearning gpus important learning machine machinelearning matrix matrixmultiplication nvidia",
      "platforms": {
        "tiktok": {
          "video_id": "7238766913223019818",
          "url": "https://www.tiktok.com/@rajistics/video/7238766913223019818",
          "view_count": 3127,
          "upload_date": "2023-05-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cs2OrqkAX5r",
          "url": "https://www.instagram.com/reel/Cs2OrqkAX5r",
          "view_count": 1960,
          "upload_date": "2023-05-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "jP_KOQnl6yo",
          "url": "https://www.youtube.com/watch?v=jP_KOQnl6yo",
          "view_count": 17054,
          "upload_date": "2023-06-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6122,
      "title": "AI news roundup for the week #machinelearning #datascience #rajistics",
      "description": "AI news roundup for the week #machinelearning #datascience #rajistics",
      "upload_date": "2023-08-26",
      "total_views": 21860,
      "max_views": 21500,
      "topics": [
        "datascience",
        "face",
        "hugging",
        "languagemodels",
        "largelanguagemodels",
        "llama",
        "machinelearning",
        "news",
        "open",
        "production",
        "roundup",
        "series",
        "week"
      ],
      "search_text": "AI news roundup for the week #machinelearning #datascience #rajistics datascience face hugging languagemodels largelanguagemodels llama machinelearning news open production roundup series week",
      "platforms": {
        "tiktok": {
          "video_id": "7271659037958343982",
          "url": "https://www.tiktok.com/@rajistics/video/7271659037958343982",
          "view_count": 21500,
          "upload_date": "2023-08-26",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17977204892522267",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-08-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pWbor9lF5Vo",
          "url": "https://www.youtube.com/watch?v=pWbor9lF5Vo",
          "view_count": 360,
          "upload_date": "2023-08-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6042,
      "title": "Let's dig into the detail for building your own large language model on a custom domain. The LLaVA-Med does a great breakdown of how they built their model. The video goes through their data preparation training and evaluation of the model. #datascience #machinelearning #largelanguagemodel #vicuna #llava-med LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day: https://arxiv.org/pdf/2306.00890.pdf Open LLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis: https://arxiv.org/pdf/2305.13230.pdf Annotated graph by Sebastian Raschka Background by R O: https://unsplash.com/photos/FFA8yd4OynY",
      "description": "Let's dig into the detail for building your own large language model on a custom domain. The LLaVA-Med does a great breakdown of how they built their model. The video goes through their data preparation training and evaluation of the model. #datascience #machinelearning #largelanguagemodel #vicuna #llava-med LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day: https://arxiv.org/pdf/2306.00890.pdf Open LLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis: https://arxiv.org/pdf/2305.13230.pdf Annotated graph by Sebastian Raschka Background by R O: https://unsplash.com/photos/FFA8yd4OynY",
      "upload_date": "2023-06-04",
      "total_views": 21308,
      "max_views": 19200,
      "topics": [
        "based",
        "building",
        "custom",
        "datascience",
        "domain",
        "largelanguagemodel",
        "llava",
        "llm",
        "machinelearning",
        "model",
        "pdf",
        "vicuna"
      ],
      "search_text": "Let's dig into the detail for building your own large language model on a custom domain. The LLaVA-Med does a great breakdown of how they built their model. The video goes through their data preparation training and evaluation of the model. #datascience #machinelearning #largelanguagemodel #vicuna #llava-med LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day: https://arxiv.org/pdf/2306.00890.pdf Open LLM Leaderboard: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis: https://arxiv.org/pdf/2305.13230.pdf Annotated graph by Sebastian Raschka Background by R O: https://unsplash.com/photos/FFA8yd4OynY based building custom datascience domain largelanguagemodel llava llm machinelearning model pdf vicuna",
      "platforms": {
        "tiktok": {
          "video_id": "7240866612125912363",
          "url": "https://www.tiktok.com/@rajistics/video/7240866612125912363",
          "view_count": 19200,
          "upload_date": "2023-06-04",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17842579032008427",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-06-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "6u1M8VUf2bs",
          "url": "https://www.youtube.com/watch?v=6u1M8VUf2bs",
          "view_count": 2108,
          "upload_date": "2023-06-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6130,
      "title": "Singular Value Decomposition (SVD) Explained",
      "description": "Singular Value Decomposition (SVD) Explained",
      "upload_date": "2023-04-05",
      "total_views": 20750,
      "max_views": 14216,
      "topics": [
        "datascience",
        "decomposition",
        "explained",
        "machinelearning",
        "matrices",
        "matrixalgebra",
        "one",
        "singular",
        "singularvaluedecomposition",
        "svd",
        "value",
        "working"
      ],
      "search_text": "Singular Value Decomposition (SVD) Explained datascience decomposition explained machinelearning matrices matrixalgebra one singular singularvaluedecomposition svd value working",
      "platforms": {
        "tiktok": {
          "video_id": "7217586458247318826",
          "url": "https://www.tiktok.com/@rajistics/video/7217586458247318826",
          "view_count": 6534,
          "upload_date": "2023-04-02",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18026944303493784",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AeOMNhHM9e8",
          "url": "https://www.youtube.com/watch?v=AeOMNhHM9e8",
          "view_count": 14216,
          "upload_date": "2023-04-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Always have a baseline model. For time series, you can often compare to what happened in a previous time step, like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries  I can do more of these baselines if you all find this useful.  ",
      "description": "Always have a baseline model. For time series, you can often compare to what happened in a previous time step, like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries  I can do more of these baselines if you all find this useful.  ",
      "upload_date": "2022-10-09",
      "total_views": 20700,
      "max_views": 20700,
      "topics": [
        "baseline",
        "codetok",
        "datascience",
        "model",
        "statistics",
        "timeseriesforcasting"
      ],
      "search_text": "Always have a baseline model. For time series, you can often compare to what happened in a previous time step, like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries  I can do more of these baselines if you all find this useful.   baseline codetok datascience model statistics timeseriesforcasting What if I told you my new deep learning time series model is predicting with an armacy of 1206 our daily sales? Wow, that sounds impressive. How does that compare to a baseline model? What's a baseline model? Oh, a baseline is a simple ruler algorithm that just gives us a starting point for thinking about predictive performance. So for thinking about predicting next week's sales, I like to just use what happened last week and that gives me a simple baseline model. Okay, let me try that. Yeah, my model isn't as good as the baseline model. I guess I gotta get back to work.",
      "platforms": {
        "tiktok": {
          "video_id": "7152631735245835566",
          "url": "https://www.tiktok.com/@rajistics/video/7152631735245835566",
          "view_count": 20700,
          "upload_date": "2022-10-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/55dfa8b113fc4c69901bb489d3d4e299_1665351860~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=2jDthGkZufG0gZpEMrsFViOouV4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5967,
      "title": "Graph databases accelerate multi-hop traversals, but most production queries are shallow (1–2 hops) that SQL or embeddings handle efficiently. Knowledge graphs are valuable for integration but costly to build and govern, and few organizations have the resources to maintain them. Most organizations find the marginal gains of graph RAG rarely justify the operational overhead. Stonebraker, M., & Pavlo, A. (2024). What Goes Around Comes Around... And Around. SIGMOD Record. ",
      "description": "Graph databases accelerate multi-hop traversals, but most production queries are shallow (1–2 hops) that SQL or embeddings handle efficiently. Knowledge graphs are valuable for integration but costly to build and govern, and few organizations have the resources to maintain them. Most organizations find the marginal gains of graph RAG rarely justify the operational overhead. Stonebraker, M., & Pavlo, A. (2024). What Goes Around Comes Around... And Around. SIGMOD Record. ",
      "upload_date": "2025-08-30",
      "total_views": 20600,
      "max_views": 20600,
      "topics": [
        "accelerate",
        "around",
        "databases",
        "graph",
        "hop",
        "like",
        "multi",
        "need",
        "organizations",
        "queries",
        "sql"
      ],
      "search_text": "Graph databases accelerate multi-hop traversals, but most production queries are shallow (1–2 hops) that SQL or embeddings handle efficiently. Knowledge graphs are valuable for integration but costly to build and govern, and few organizations have the resources to maintain them. Most organizations find the marginal gains of graph RAG rarely justify the operational overhead. Stonebraker, M., & Pavlo, A. (2024). What Goes Around Comes Around... And Around. SIGMOD Record.  accelerate around databases graph hop like multi need organizations queries sql Graphrag is the future. It's got nodes, edges, embeddings. Can you get that vibe coded for me this weekend? Love the enthusiasm, but the first step is building a knowledge graph. That sounds complicated. It is. We need to define an ontology of relationships, customers, suppliers, products, documents. We have to carefully curate the data to match that. That's months of work, not a weekend project. So basically a money sink? Exactly. And once it's built, we need a graph database to query it efficiently. That sounds so futuristic. Only if you actually need it. Graph databases shine for when you're doing deep multi-hop traversals. Like if somebody asked us who bought a part from a supplier who was acquired by a company, connected to a top competitor, like something like that four hop query, it'd be painful to do that in SQL. Aren't most of our questions a lot simpler? Most of our queries are things like, what are our top three suppliers? Which contract mentions product? That's one or two hops. And SQL can handle that just fine. We can capture document similarity without building this giant graph. So you're saying 98% of our queries don't need a graph database? You got it. Graph's gonna give you a tiny edge, but it's a cost of having to hire specialists, new schema debates, and new infrastructure. I disagree. They gave me bear's box tickets. So for at least the next week, we're all in on GraphRack.",
      "platforms": {
        "tiktok": {
          "video_id": "7544183774024650015",
          "url": "https://www.tiktok.com/@rajistics/video/7544183774024650015",
          "view_count": 20600,
          "upload_date": "2025-08-30",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okIyIDEsRUIsAxDorFSkfqrfCL5EZExAVKlzB7~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=HZ4yU9jY3LG4Brw3qD9s3bySAHY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18088032580647317",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6644,
      "title": "Breaking study from Harvard showing the impact of Large Language Models like GPT-4 on office productivity. #datascience #gpt4 #officeproductivity #chatgpt #rajistics Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321",
      "description": "Breaking study from Harvard showing the impact of Large Language Models like GPT-4 on office productivity. #datascience #gpt4 #officeproductivity #chatgpt #rajistics Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321",
      "upload_date": "2023-09-18",
      "total_views": 20400,
      "max_views": 20400,
      "topics": [
        "chatgpt",
        "datascience",
        "enjoy",
        "gpt4",
        "officeproductivity",
        "older",
        "papers",
        "productivity",
        "tik",
        "tok",
        "video"
      ],
      "search_text": "Breaking study from Harvard showing the impact of Large Language Models like GPT-4 on office productivity. #datascience #gpt4 #officeproductivity #chatgpt #rajistics Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321 chatgpt datascience enjoy gpt4 officeproductivity older papers productivity tik tok video",
      "platforms": {
        "tiktok": {
          "video_id": "7279966489120148778",
          "url": "https://www.tiktok.com/@rajistics/video/7279966489120148778",
          "view_count": 20400,
          "upload_date": "2023-09-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17935859615724151",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-09-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6114,
      "title": "OpenAI's turmoil this last week will ensure enterprise AI strategies will not depend on OpenAI. It's clear for any valuable AI systems it's firmly in the enterprise's interest to own it (open source) or have an alternative provider. #openai #rajistics #opensource #copilot",
      "description": "OpenAI's turmoil this last week will ensure enterprise AI strategies will not depend on OpenAI. It's clear for any valuable AI systems it's firmly in the enterprise's interest to own it (open source) or have an alternative provider. #openai #rajistics #opensource #copilot",
      "upload_date": "2023-11-19",
      "total_views": 20339,
      "max_views": 17500,
      "topics": [
        "alternative",
        "copilot",
        "enterprise",
        "get",
        "last",
        "open",
        "openai",
        "opensource",
        "turmoil"
      ],
      "search_text": "OpenAI's turmoil this last week will ensure enterprise AI strategies will not depend on OpenAI. It's clear for any valuable AI systems it's firmly in the enterprise's interest to own it (open source) or have an alternative provider. #openai #rajistics #opensource #copilot alternative copilot enterprise get last open openai opensource turmoil",
      "platforms": {
        "tiktok": {
          "video_id": "7303239463361350958",
          "url": "https://www.tiktok.com/@rajistics/video/7303239463361350958",
          "view_count": 17500,
          "upload_date": "2023-11-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18292703374179259",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-11-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "jzuQrWMLEbQ",
          "url": "https://youtube.com/shorts/jzuQrWMLEbQ",
          "view_count": 2839,
          "upload_date": "2023-11-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday ",
      "description": "#onthisday ",
      "upload_date": "2025-04-05",
      "total_views": 20300,
      "max_views": 20300,
      "topics": [
        "get",
        "onthisday",
        "poop",
        "reward",
        "watermelon",
        "way"
      ],
      "search_text": "#onthisday  get onthisday poop reward watermelon way Why do you like watermelon over poop? Well, how are you going to teach that to an AI? Guess what? I did. The way my little robot friend works is they get a positive reward when they find watermelon, and they get a negative reward when they eat poop. This approach is known as reinforcement learning, and it's actually growing within computer science as a way to train robots, whether real or virtual. The code for this is freely available. I'll add it in the comments.",
      "platforms": {
        "tiktok": {
          "video_id": "7489949224537312543",
          "url": "https://www.tiktok.com/@rajistics/video/7489949224537312543",
          "view_count": 20300,
          "upload_date": "2025-04-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0AzBiGlzi8GpAJ1ABAv7nmfiBlIqIvIUCiLxS~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=dAWbm2E2IrPYeOuyxR6lXsHhPYw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6453,
      "title": "This video is based on work Omar did in tracking down why Falcon was giving results that favored the Middle East. It's an example of how bias can exist in many different places when using models. #datascience #machinelearning #largelanguagemodels #falcon #modelbias Original tweet by Jan Kulveit: https://twitter.com/jankulveit/status/1670735364707721216 Omar Tweet: https://twitter.com/osanseviero/status/1671210627837095942 Background by Michal Mancewicz: https://unsplash.com/photos/_wdOjxXPxUU",
      "description": "This video is based on work Omar did in tracking down why Falcon was giving results that favored the Middle East. It's an example of how bias can exist in many different places when using models. #datascience #machinelearning #largelanguagemodels #falcon #modelbias Original tweet by Jan Kulveit: https://twitter.com/jankulveit/status/1670735364707721216 Omar Tweet: https://twitter.com/osanseviero/status/1671210627837095942 Background by Michal Mancewicz: https://unsplash.com/photos/_wdOjxXPxUU",
      "upload_date": "2023-06-27",
      "total_views": 20248,
      "max_views": 17200,
      "topics": [
        "biased",
        "datascience",
        "east",
        "falcon",
        "largelanguagemodels",
        "llm",
        "machinelearning",
        "middle",
        "modelbias",
        "omar",
        "towards"
      ],
      "search_text": "This video is based on work Omar did in tracking down why Falcon was giving results that favored the Middle East. It's an example of how bias can exist in many different places when using models. #datascience #machinelearning #largelanguagemodels #falcon #modelbias Original tweet by Jan Kulveit: https://twitter.com/jankulveit/status/1670735364707721216 Omar Tweet: https://twitter.com/osanseviero/status/1671210627837095942 Background by Michal Mancewicz: https://unsplash.com/photos/_wdOjxXPxUU biased datascience east falcon largelanguagemodels llm machinelearning middle modelbias omar towards",
      "platforms": {
        "tiktok": {
          "video_id": "7249335735797517611",
          "url": "https://www.tiktok.com/@rajistics/video/7249335735797517611",
          "view_count": 17200,
          "upload_date": "2023-06-27",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Ct_kJZQg83l",
          "url": "https://www.instagram.com/reel/Ct_kJZQg83l",
          "view_count": 1628,
          "upload_date": "2023-06-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "c-EyTBvQmSA",
          "url": "https://www.youtube.com/watch?v=c-EyTBvQmSA",
          "view_count": 1420,
          "upload_date": "2023-06-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6475,
      "title": "Improving your visualizations. Are you happy with your viz?  ",
      "description": "Improving your visualizations. Are you happy with your viz?  ",
      "upload_date": "2025-01-08",
      "total_views": 20174,
      "max_views": 19400,
      "topics": [
        "data",
        "groups",
        "hop",
        "improving",
        "like",
        "llm",
        "lot",
        "multi",
        "reasoning",
        "retrieval",
        "use",
        "want"
      ],
      "search_text": "Improving your visualizations. Are you happy with your viz?   data groups hop improving like llm lot multi reasoning retrieval use want You plotted your data and it looks like a big clump like this. Let's talk about some solutions for this. Two quick fixes are you could decrease the dot size or use transparency. You could use a 2D density plot. I see this a lot for different shot patterns for basketball. The other thing you can do is just randomly sample down and do a subset of your data. It's okay. If there's natural groups in your data, you want to highlight that. And you could even use faceting if you want to separate out the groups and should really show the differences between them. One last technique I've used when I've had points that overlap each other is just to use jittering, which adds a little bit of random noise and kind of helps separate out the points. Hey, thanks to Data2Viz, which gave me the inspiration for this and some of the visuals. They have a lot more plotting tips. Go check them out.",
      "platforms": {
        "tiktok": {
          "video_id": "7457587843959901470",
          "url": "https://www.tiktok.com/@rajistics/video/7457587843959901470",
          "view_count": 19400,
          "upload_date": "2025-01-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4TQIHZgEFAn4fCAQjQDqLkEqprZxxOewI8fCK~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=GqukL%2FZWHDgxREoF%2BaP2hkSHII8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "youtube": {
          "video_id": "HFcNelCYm24",
          "url": "https://www.youtube.com/watch?v=HFcNelCYm24",
          "view_count": 774,
          "upload_date": "2025-01-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6521,
      "title": "AI Engineer is starting to emerge as a new role. This role works with LLMs and does prompt engineering and fine tuning of models. They typically put together generative AI workflows. This role doesn't require a traditional data science background but should still pay well. #datascience #machinelearning #aiengineer AI Engineer: https://www.latent.space/p/ai-engineer",
      "description": "AI Engineer is starting to emerge as a new role. This role works with LLMs and does prompt engineering and fine tuning of models. They typically put together generative AI workflows. This role doesn't require a traditional data science background but should still pay well. #datascience #machinelearning #aiengineer AI Engineer: https://www.latent.space/p/ai-engineer",
      "upload_date": "2023-06-30",
      "total_views": 19998,
      "max_views": 19200,
      "topics": [
        "aiengineer",
        "datascience",
        "emerging",
        "engineer",
        "llms",
        "machinelearning",
        "new",
        "role",
        "starting",
        "tied"
      ],
      "search_text": "AI Engineer is starting to emerge as a new role. This role works with LLMs and does prompt engineering and fine tuning of models. They typically put together generative AI workflows. This role doesn't require a traditional data science background but should still pay well. #datascience #machinelearning #aiengineer AI Engineer: https://www.latent.space/p/ai-engineer aiengineer datascience emerging engineer llms machinelearning new role starting tied",
      "platforms": {
        "tiktok": {
          "video_id": "7250620134103059755",
          "url": "https://www.tiktok.com/@rajistics/video/7250620134103059755",
          "view_count": 19200,
          "upload_date": "2023-06-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CuIeiSIgh4_",
          "url": "https://www.instagram.com/reel/CuIeiSIgh4_",
          "view_count": 566,
          "upload_date": "2023-06-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "v1zZyql5oCI",
          "url": "https://www.youtube.com/watch?v=v1zZyql5oCI",
          "view_count": 232,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6450,
      "title": "Open is thrown about a lot in the AI community. This week Nomic and Allen AI remind us what it takes to build truly open-source AI models. They shared the training data and methods along with the models. This is a big deal because scrutiny of the training process is valuable for many reasons. Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research - https://arxiv.org/pdf/2402.00159.pdf Nomic Embed: https://blog.nomic.ai/posts/nomic-embed-text-v1 Open Language Models (OLMos) and the LLM landscape: https://www.interconnects.ai/p/olmo",
      "description": "Open is thrown about a lot in the AI community. This week Nomic and Allen AI remind us what it takes to build truly open-source AI models. They shared the training data and methods along with the models. This is a big deal because scrutiny of the training process is valuable for many reasons. Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research - https://arxiv.org/pdf/2402.00159.pdf Nomic Embed: https://blog.nomic.ai/posts/nomic-embed-text-v1 Open Language Models (OLMos) and the LLM landscape: https://www.interconnects.ai/p/olmo",
      "upload_date": "2024-02-02",
      "total_views": 19541,
      "max_views": 9718,
      "topics": [
        "language",
        "models",
        "nomic",
        "open",
        "pdf",
        "training"
      ],
      "search_text": "Open is thrown about a lot in the AI community. This week Nomic and Allen AI remind us what it takes to build truly open-source AI models. They shared the training data and methods along with the models. This is a big deal because scrutiny of the training process is valuable for many reasons. Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research - https://arxiv.org/pdf/2402.00159.pdf Nomic Embed: https://blog.nomic.ai/posts/nomic-embed-text-v1 Open Language Models (OLMos) and the LLM landscape: https://www.interconnects.ai/p/olmo language models nomic open pdf training",
      "platforms": {
        "tiktok": {
          "video_id": "7331094500854582574",
          "url": "https://www.tiktok.com/@rajistics/video/7331094500854582574",
          "view_count": 8201,
          "upload_date": "2024-02-02",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C224eTBABTE",
          "url": "https://www.instagram.com/reel/C224eTBABTE",
          "view_count": 9718,
          "upload_date": "2024-02-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "G2mtbAifYGE",
          "url": "https://youtube.com/shorts/G2mtbAifYGE",
          "view_count": 1622,
          "upload_date": "2024-02-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6038,
      "title": "Llama 2 is a worthy successor to Meta's original LLaMa model. It performs better -- on par with ChatGPT has a commercial license and and is publicly available. There are many great things about the model and I recommend reading the paper it's pretty approachable. Paper: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/ Model Repo: https://huggingface.co/meta-llama Background video: https://www.youtube.com/watch?v=uAl4qWuuJiw&ab_channel=ThePetCollective",
      "description": "Llama 2 is a worthy successor to Meta's original LLaMa model. It performs better -- on par with ChatGPT has a commercial license and and is publicly available. There are many great things about the model and I recommend reading the paper it's pretty approachable. Paper: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/ Model Repo: https://huggingface.co/meta-llama Background video: https://www.youtube.com/watch?v=uAl4qWuuJiw&ab_channel=ThePetCollective",
      "upload_date": "2023-07-18",
      "total_views": 19329,
      "max_views": 16900,
      "topics": [
        "explained",
        "llama",
        "meta",
        "model",
        "paper",
        "successor",
        "worthy"
      ],
      "search_text": "Llama 2 is a worthy successor to Meta's original LLaMa model. It performs better -- on par with ChatGPT has a commercial license and and is publicly available. There are many great things about the model and I recommend reading the paper it's pretty approachable. Paper: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/ Model Repo: https://huggingface.co/meta-llama Background video: https://www.youtube.com/watch?v=uAl4qWuuJiw&ab_channel=ThePetCollective explained llama meta model paper successor worthy",
      "platforms": {
        "tiktok": {
          "video_id": "7257275921944661294",
          "url": "https://www.tiktok.com/@rajistics/video/7257275921944661294",
          "view_count": 16900,
          "upload_date": "2023-07-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17996184283995779",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-07-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "idmhH9_H1hY",
          "url": "https://www.youtube.com/watch?v=idmhH9_H1hY",
          "view_count": 2429,
          "upload_date": "2023-07-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Tesla self driving has been such a scam. I am so disappointed. I really believed that self driving could be pretty useful (I knew it wasn’t going to be perfect). #tesla #fsd #self-driving #fakedemo ",
      "description": "Tesla self driving has been such a scam. I am so disappointed. I really believed that self driving could be pretty useful (I knew it wasn’t going to be perfect). #tesla #fsd #self-driving #fakedemo ",
      "upload_date": "2023-01-17",
      "total_views": 19200,
      "max_views": 19200,
      "topics": [
        "driving",
        "fakedemo",
        "fsd",
        "self",
        "tesla",
        "wasn"
      ],
      "search_text": "Tesla self driving has been such a scam. I am so disappointed. I really believed that self driving could be pretty useful (I knew it wasn’t going to be perfect). #tesla #fsd #self-driving #fakedemo  driving fakedemo fsd self tesla wasn I'm so upset. I remember when Elon first tweeted out this video. It was amazing. It just showed me what the future is, what the possibilities of a self-driving car were. But now we find out it was fake. They basically put the car on rails and had to map the plan out the entire thing. So it wasn't doing the self-driving that we thought it was doing. This whole Tesla self-driving thing has been just totally fake.",
      "platforms": {
        "tiktok": {
          "video_id": "7189774909524954410",
          "url": "https://www.tiktok.com/@rajistics/video/7189774909524954410",
          "view_count": 19200,
          "upload_date": "2023-01-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/eb196e2e3c594945b9d2956a2d36424f_1673999934~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=pIoi1lKsVkT2d2zgvi3C41BxYoU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "A walkthrough of the explainer dashboard. It contains a lot of the tools you want when trying to explain your models. #datascience #machinelearning #statistics #permutationimportance #partialdependence ",
      "description": "A walkthrough of the explainer dashboard. It contains a lot of the tools you want when trying to explain your models. #datascience #machinelearning #statistics #permutationimportance #partialdependence ",
      "upload_date": "2022-11-29",
      "total_views": 19200,
      "max_views": 19200,
      "topics": [
        "datascience",
        "machinelearning",
        "permutationimportance",
        "statistics",
        "tools",
        "want"
      ],
      "search_text": "A walkthrough of the explainer dashboard. It contains a lot of the tools you want when trying to explain your models. #datascience #machinelearning #statistics #permutationimportance #partialdependence  datascience machinelearning permutationimportance statistics tools want One of the biggest mistakes I see with data scientists is when they're using open source tools, they end up not being able to fully explain their models. Commercial products like H2O, DataRobot, DataIQ all come with built in model explainability tools, which makes it really easy for data scientists to be able to explain their models. And that's a big selling point for these packages. For open source data scientists, it's a mess. There's a ton of different packages, and they're not all well maintained. And without a comprehensive tool set, it's really easy to do a subpar analysis and miss stuff. I saw this explainer dashboard and I was wowed. I liked a lot of the features that it highlighted. Now I haven't actually used it myself, but I did want to give you a tour to show you what are the things that I think are important when you're doing an explainability analysis. The first thing is, is I prefer permutation importance. That's my number one go to tool. This supports it. I can get a sense of that. Look at my features. I can dig in farther into my model. I can even adjust the prediction probabilities, something I've talked about in earlier videos. And then I have a lot of different diagnostic tools around things like AUC being able to see how well my model is performing. If I want to dive into individual predictions, this tool gives me the ability to do that. And then on this page has one of my favorite tools, partial dependence, which is my number two go to tool for explainability. You can see here how we can see what is the effect of one feature holding everything else constant. We can do a what if analysis, dig into some of the other shaft statistics, whether it's feature dependence, whether it's the interaction, and even if we're using decision trees, dig into the decision trees and even map them out as well. So a ton of different things, but this is really what I want in a toolkit is having all these options available. And you don't necessarily use them all every time, but you want all these things available when you're working on an explainability analysis.",
      "platforms": {
        "tiktok": {
          "video_id": "7171269724490288427",
          "url": "https://www.tiktok.com/@rajistics/video/7171269724490288427",
          "view_count": 19200,
          "upload_date": "2022-11-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/229e55778e664d998c06c0ddb91546a6_1669691354~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=0i53PjzonkbJ92fA52hRfBhjADo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6057,
      "title": "The video demonstrates the limitations of LLMs by showcasing how various real-world AI problems are best solved using traditional machine learning and statistical models rather than large language models. The skit highlights key AI problem types, including forecasting, clustering, anomaly detection, regression, optimization, and recommendation systems—emphasizing that while LLMs excel in natural language tasks, they are not the most effective tools for structured data problems. The video background comes from Video by Thuan Pham: https://www.pexels.com/video/vibrant-hanoi-street-scene-with-motorcyclists-30592200/",
      "description": "The video demonstrates the limitations of LLMs by showcasing how various real-world AI problems are best solved using traditional machine learning and statistical models rather than large language models. The skit highlights key AI problem types, including forecasting, clustering, anomaly detection, regression, optimization, and recommendation systems—emphasizing that while LLMs excel in natural language tasks, they are not the most effective tools for structured data problems. The video background comes from Video by Thuan Pham: https://www.pexels.com/video/vibrant-hanoi-street-scene-with-motorcyclists-30592200/",
      "upload_date": "2025-02-16",
      "total_views": 19115,
      "max_views": 18000,
      "topics": [
        "demonstrates",
        "group",
        "language",
        "learning",
        "llms",
        "math",
        "models",
        "optimization",
        "policy",
        "problems",
        "relative",
        "teaching",
        "use",
        "video"
      ],
      "search_text": "The video demonstrates the limitations of LLMs by showcasing how various real-world AI problems are best solved using traditional machine learning and statistical models rather than large language models. The skit highlights key AI problem types, including forecasting, clustering, anomaly detection, regression, optimization, and recommendation systems—emphasizing that while LLMs excel in natural language tasks, they are not the most effective tools for structured data problems. The video background comes from Video by Thuan Pham: https://www.pexels.com/video/vibrant-hanoi-street-scene-with-motorcyclists-30592200/ demonstrates group language learning llms math models optimization policy problems relative teaching use video We just signed an exclusive contract for a new AGI LLM. Imagine all the AI problems we're going to be able to take on, like predicting the demand for our products. Well, we already do forecasting with ARIMA and other machine learning models. LLMs really aren't made for forecasting. But how about using the LLM to develop demographic categories? We can target, for example, the Wallet Week Wendy's or the Gucci on credit Gregg's. So that would be unsupervised learning. We use algorithms like K-Means. LLMs really aren't good for creating those types of clusters. OK, how about we use it in our factories to yell anomaly whenever there's a defective product? We already use anomaly detection models for that. If we had those models yell all the time, it would be deafening. Fine. How about we use it to estimate the cost of all these product defective lawsuits? We use linear models and gradient boosted trees to handle that. Shouldn't we be asking, why do we have so many defective products? OK, but what if we take all of our shipping route information, pour it into the AI, then have it come up with the best routes that, for example, avoid express shipping? We already optimize our routes with linear programming. We don't really need an LLM to start hallucinating new supply chain. How about it recommends the most profitable products to our customers? So we upgraded to a machine learning recommender system last quarter, and it boosted profits by 14%. So what are we exactly going to use this AGI LLM4 to write silly videos like this?",
      "platforms": {
        "tiktok": {
          "video_id": "7472138028207623455",
          "url": "https://www.tiktok.com/@rajistics/video/7472138028207623455",
          "view_count": 18000,
          "upload_date": "2025-02-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ooAEDhcyYinIfBlNvaDlRIIBVsXRE3FAUbeEQD~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=qGeErnjzY9WTMqfrSOxXKXmQKHc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18068561956868812",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "FqRx_O3Ba3w",
          "url": "https://www.youtube.com/watch?v=FqRx_O3Ba3w",
          "view_count": 1115,
          "upload_date": "2025-02-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6647,
      "title": "Everyone is using transformers! Are you working on optimizing your use? The community has been steadily finding ways to optimize transformers. What is working for you? #machinelearning #transformers #pytorch #rajistics Accelerating Generative AI with PyTorch: Segment Anything Fast: https://pytorch.org/blog/accelerating-generative-ai/ Is Attention all you need: https://www.isattentionallyouneed.com/ Background of Prague Library: https://pixabay.com/illustrations/prague-library-prague-monastery-980732/",
      "description": "Everyone is using transformers! Are you working on optimizing your use? The community has been steadily finding ways to optimize transformers. What is working for you? #machinelearning #transformers #pytorch #rajistics Accelerating Generative AI with PyTorch: Segment Anything Fast: https://pytorch.org/blog/accelerating-generative-ai/ Is Attention all you need: https://www.isattentionallyouneed.com/ Background of Prague Library: https://pixabay.com/illustrations/prague-library-prague-monastery-980732/",
      "upload_date": "2023-11-21",
      "total_views": 18808,
      "max_views": 13900,
      "topics": [
        "accelerating",
        "community",
        "machinelearning",
        "optimizing",
        "prague",
        "pytorch",
        "transformers",
        "working"
      ],
      "search_text": "Everyone is using transformers! Are you working on optimizing your use? The community has been steadily finding ways to optimize transformers. What is working for you? #machinelearning #transformers #pytorch #rajistics Accelerating Generative AI with PyTorch: Segment Anything Fast: https://pytorch.org/blog/accelerating-generative-ai/ Is Attention all you need: https://www.isattentionallyouneed.com/ Background of Prague Library: https://pixabay.com/illustrations/prague-library-prague-monastery-980732/ accelerating community machinelearning optimizing prague pytorch transformers working",
      "platforms": {
        "tiktok": {
          "video_id": "7303707375490321707",
          "url": "https://www.tiktok.com/@rajistics/video/7303707375490321707",
          "view_count": 13900,
          "upload_date": "2023-11-21",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cz42KCNg00a",
          "url": "https://www.instagram.com/reel/Cz42KCNg00a",
          "view_count": 4908,
          "upload_date": "2023-11-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "The Agony!  #datascience #machinelearning #mltok #techtok #statistics",
      "description": "The Agony!  #datascience #machinelearning #mltok #techtok #statistics",
      "upload_date": "2022-04-06",
      "total_views": 18800,
      "max_views": 18800,
      "topics": [
        "agony",
        "datascience",
        "machinelearning",
        "mltok",
        "statistics",
        "techtok"
      ],
      "search_text": "The Agony!  #datascience #machinelearning #mltok #techtok #statistics agony datascience machinelearning mltok statistics techtok So that was a tough loss. How do you feel after that?",
      "platforms": {
        "tiktok": {
          "video_id": "7083455488033148203",
          "url": "https://www.tiktok.com/@rajistics/video/7083455488033148203",
          "view_count": 18800,
          "upload_date": "2022-04-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3184caec2628409eb8186fc779cc93f4_1649245504~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=VUDmOChSxspojlJR%2FmnM0dlzAOI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6461,
      "title": "Building a question / answer application using a large language model is a great starter project. You will need to use a vector database and prompting an LLM. It's a great way to start a journey into practical applications of generative AI. #datascience #machinelearning #questionanswer #generativeAI #largelanguagemodels #vectordatabase Knowledge Retrieval Architecture for LLM‚Äôs (2023): https://mattboegner.com/knowledge-retrieval-architecture-for-llms/ Deepset: https://haystack.deepset.ai/tutorials PineCone: https://docs.pinecone.io/docs/examples LangChain: https://python.langchain.com/en/latest/use_cases/question_answering.html LLama-Index: https://gpt-index.readthedocs.io/en/stable/ Background image: https://unsplash.com/photos/MfBnqUOz_qY",
      "description": "Building a question / answer application using a large language model is a great starter project. You will need to use a vector database and prompting an LLM. It's a great way to start a journey into practical applications of generative AI. #datascience #machinelearning #questionanswer #generativeAI #largelanguagemodels #vectordatabase Knowledge Retrieval Architecture for LLM‚Äôs (2023): https://mattboegner.com/knowledge-retrieval-architecture-for-llms/ Deepset: https://haystack.deepset.ai/tutorials PineCone: https://docs.pinecone.io/docs/examples LangChain: https://python.langchain.com/en/latest/use_cases/question_answering.html LLama-Index: https://gpt-index.readthedocs.io/en/stable/ Background image: https://unsplash.com/photos/MfBnqUOz_qY",
      "upload_date": "2023-05-04",
      "total_views": 18793,
      "max_views": 14700,
      "topics": [
        "answer",
        "building",
        "datascience",
        "documents",
        "generative",
        "generativeai",
        "largelanguagemodels",
        "machinelearning",
        "project",
        "question",
        "questionanswer",
        "starter",
        "vector",
        "vectordatabase"
      ],
      "search_text": "Building a question / answer application using a large language model is a great starter project. You will need to use a vector database and prompting an LLM. It's a great way to start a journey into practical applications of generative AI. #datascience #machinelearning #questionanswer #generativeAI #largelanguagemodels #vectordatabase Knowledge Retrieval Architecture for LLM‚Äôs (2023): https://mattboegner.com/knowledge-retrieval-architecture-for-llms/ Deepset: https://haystack.deepset.ai/tutorials PineCone: https://docs.pinecone.io/docs/examples LangChain: https://python.langchain.com/en/latest/use_cases/question_answering.html LLama-Index: https://gpt-index.readthedocs.io/en/stable/ Background image: https://unsplash.com/photos/MfBnqUOz_qY answer building datascience documents generative generativeai largelanguagemodels machinelearning project question questionanswer starter vector vectordatabase",
      "platforms": {
        "tiktok": {
          "video_id": "7229410084273589547",
          "url": "https://www.tiktok.com/@rajistics/video/7229410084273589547",
          "view_count": 14700,
          "upload_date": "2023-05-04",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cr1Tc6Og-4S",
          "url": "https://www.instagram.com/reel/Cr1Tc6Og-4S",
          "view_count": 2987,
          "upload_date": "2023-05-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ll2CcfzyI8U",
          "url": "https://www.youtube.com/watch?v=ll2CcfzyI8U",
          "view_count": 1106,
          "upload_date": "2023-05-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6415,
      "title": "Evaluation for Large Language Models (LLMs) and Generative AI - A Deep Dive",
      "description": "Evaluation for Large Language Models (LLMs) and Generative AI - A Deep Dive",
      "upload_date": "2023-11-06",
      "total_views": 18612,
      "max_views": 11284,
      "topics": [
        "evaluation",
        "generative",
        "influence",
        "language",
        "large",
        "largelanguagemodels",
        "like",
        "llms",
        "logitbias",
        "model",
        "models",
        "openai",
        "temperature",
        "use",
        "words"
      ],
      "search_text": "Evaluation for Large Language Models (LLMs) and Generative AI - A Deep Dive evaluation generative influence language large largelanguagemodels like llms logitbias model models openai temperature use words",
      "platforms": {
        "tiktok": {
          "video_id": "7300207386747473198",
          "url": "https://www.tiktok.com/@rajistics/video/7300207386747473198",
          "view_count": 4310,
          "upload_date": "2023-11-11",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CzgkAI2A5ev",
          "url": "https://www.instagram.com/reel/CzgkAI2A5ev",
          "view_count": 3018,
          "upload_date": "2023-11-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "iQl03pQlYWY",
          "url": "https://www.youtube.com/watch?v=iQl03pQlYWY",
          "view_count": 11284,
          "upload_date": "2023-11-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6436,
      "title": "One of my favorite methods for feature selection is recursive feature elimination. It's very easy to do and a starting data scientist can code this up. This is a starting point feature selection is a much larger topic. I did a more sophisticated version of this at DataRobot that I called Feature Importance Rank Ensembling (FIRE) - https://www.datarobot.com/blog/using-feature-importance-rank-ensembling-fire-for-advanced-feature-selection/ Background: Oleksandr P at https://www.pexels.com/video/flaming-tiki-torches-on-the-beach-7357617/ #datascience #featureselection #rajistics",
      "description": "One of my favorite methods for feature selection is recursive feature elimination. It's very easy to do and a starting data scientist can code this up. This is a starting point feature selection is a much larger topic. I did a more sophisticated version of this at DataRobot that I called Feature Importance Rank Ensembling (FIRE) - https://www.datarobot.com/blog/using-feature-importance-rank-ensembling-fire-for-advanced-feature-selection/ Background: Oleksandr P at https://www.pexels.com/video/flaming-tiki-torches-on-the-beach-7357617/ #datascience #featureselection #rajistics",
      "upload_date": "2024-02-18",
      "total_views": 18531,
      "max_views": 12847,
      "topics": [
        "datarobot",
        "datascience",
        "feature",
        "featureselection",
        "selection",
        "starting"
      ],
      "search_text": "One of my favorite methods for feature selection is recursive feature elimination. It's very easy to do and a starting data scientist can code this up. This is a starting point feature selection is a much larger topic. I did a more sophisticated version of this at DataRobot that I called Feature Importance Rank Ensembling (FIRE) - https://www.datarobot.com/blog/using-feature-importance-rank-ensembling-fire-for-advanced-feature-selection/ Background: Oleksandr P at https://www.pexels.com/video/flaming-tiki-torches-on-the-beach-7357617/ #datascience #featureselection #rajistics datarobot datascience feature featureselection selection starting",
      "platforms": {
        "tiktok": {
          "video_id": "7337077699908848938",
          "url": "https://www.tiktok.com/@rajistics/video/7337077699908848938",
          "view_count": 3108,
          "upload_date": "2024-02-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C3gZcXLAsyw",
          "url": "https://www.instagram.com/reel/C3gZcXLAsyw",
          "view_count": 12847,
          "upload_date": "2024-02-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "As1L_3aQx_I",
          "url": "https://youtube.com/shorts/As1L_3aQx_I",
          "view_count": 2576,
          "upload_date": "2024-02-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6414,
      "title": "AMD chips training LLMs running Pytorch 2.0",
      "description": "AMD chips training LLMs running Pytorch 2.0",
      "upload_date": "2023-07-17",
      "total_views": 18404,
      "max_views": 12595,
      "topics": [
        "amd",
        "chips",
        "gpus",
        "largelanguagemodels",
        "llms",
        "machinelearning",
        "mosaicmscduet",
        "pytorch",
        "running",
        "training"
      ],
      "search_text": "AMD chips training LLMs running Pytorch 2.0 amd chips gpus largelanguagemodels llms machinelearning mosaicmscduet pytorch running training",
      "platforms": {
        "tiktok": {
          "video_id": "7256109597499428138",
          "url": "https://www.tiktok.com/@rajistics/video/7256109597499428138",
          "view_count": 5405,
          "upload_date": "2023-07-15",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cuuka3dgqwl",
          "url": "https://www.instagram.com/reel/Cuuka3dgqwl",
          "view_count": 404,
          "upload_date": "2023-07-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0oQB_55EAoU",
          "url": "https://www.youtube.com/watch?v=0oQB_55EAoU",
          "view_count": 12595,
          "upload_date": "2023-07-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "So what did I miss when you do error analysis? #machinelearning #datascience #statistics #erroranalysis ",
      "description": "So what did I miss when you do error analysis? #machinelearning #datascience #statistics #erroranalysis ",
      "upload_date": "2022-12-10",
      "total_views": 18400,
      "max_views": 18400,
      "topics": [
        "air",
        "datascience",
        "erroranalysis",
        "machinelearning",
        "model",
        "statistics"
      ],
      "search_text": "So what did I miss when you do error analysis? #machinelearning #datascience #statistics #erroranalysis  air datascience erroranalysis machinelearning model statistics So my model is only a little bit away from being good enough for production. For that last 2%, I'm going to focus this week on using Bayesian hyperparameters. No. And I realize most data scientists don't do air analysis, but it's important for our team to do this because we want to improve model performance, but also understand the limitations of our models. Take a look at this example. You'll see that the air is much higher when education is higher. So this is something we need to probe into and understand as part of air analysis. Is there a function I need to run? How do you do this? Let me show you how I do an air analysis. When I was working on my image classification project, I looked at the errors my model was making. Was there an animal that was miscategorized? Was there something else going on in the image? Like was it blurry or did the annotator screw up and was the example mislabeled? How do I use this approach to make my model better? That's nice, but let me even show you one better. In the email classification model, I started by grabbing a hundred mistakes the model was making. I looked at the category of the type of email, also what features made it hard to classify. By doing this, I could then focus refining my model on groups like password emails and improving the punctuation pre-processing. So to communicate the findings back for Ann's model, what I do is often create a visualization like this. And this lets me walk our management through what are the steps we did that improve the model, but also where there's still some air and keeping us from getting to a hundred percent. And some of it is irreducible. Annotators just don't disagree, but others, maybe we can have some future projects to improve our pipelines for them. Ah, I see the error of my ways.",
      "platforms": {
        "tiktok": {
          "video_id": "7175581570961542446",
          "url": "https://www.tiktok.com/@rajistics/video/7175581570961542446",
          "view_count": 18400,
          "upload_date": "2022-12-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9e77883e10344af5bd053296f693266c_1670695292~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=GSjVq9di7Kx7FFXEvA5B%2BRFeOOo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6171,
      "title": "Prompt injection attacks are a major security concern when using large language models (LLMs) like ChatGPT. They allow attackers to overwrite the developers' intentions. I made this video 2 years ago and there aren't 100% effective methods for stopping this attack. This is larger concern with MCP server usage.",
      "description": "Prompt injection attacks are a major security concern when using large language models (LLMs) like ChatGPT. They allow attackers to overwrite the developers' intentions. I made this video 2 years ago and there aren't 100% effective methods for stopping this attack. This is larger concern with MCP server usage.",
      "upload_date": "2025-05-02",
      "total_views": 18370,
      "max_views": 17900,
      "topics": [
        "agno",
        "attack",
        "attacks",
        "claude",
        "injection",
        "like",
        "llms",
        "major",
        "prompt",
        "python",
        "reasoning",
        "using"
      ],
      "search_text": "Prompt injection attacks are a major security concern when using large language models (LLMs) like ChatGPT. They allow attackers to overwrite the developers' intentions. I made this video 2 years ago and there aren't 100% effective methods for stopping this attack. This is larger concern with MCP server usage. agno attack attacks claude injection like llms major prompt python reasoning using Let's talk about a major security issue with chat GBT and other LLMs. So prompt injection is a type of attack where the attacker is able to get the model to do what it wants. Consider you made an application that translates from French to English. But what if an attacker puts something like this where it actually changes up the instructions a little bit? Now, most LLMs would reply back with something like this. This isn't hypothetical. Here's an example of a Twitter bot that was subject to a prompt injection attack. Are you starting to see this? A prompt injection attack overwrites what a developer wants to do. And once we have LLMs that are connected to various assistants, think our code interpreter, think our web services, well then you can see that the damage could be magnified a lot more. And the tough part is we don't have a hundred percent foolproof way of preventing these attacks.",
      "platforms": {
        "tiktok": {
          "video_id": "7499947393857703198",
          "url": "https://www.tiktok.com/@rajistics/video/7499947393857703198",
          "view_count": 17900,
          "upload_date": "2025-05-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4dVffAIQhMAKquWl1C1gQKRNHbIjICKIQoN8e~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=vF6WZqIbB9ygDRd6F7ETTe3ZDK4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18068169883768465",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3mV56gcGt84",
          "url": "https://www.youtube.com/watch?v=3mV56gcGt84",
          "view_count": 470,
          "upload_date": "2025-04-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6073,
      "title": "Struggling with data validation - let’s dig into how Pydantic's type system and validation framework elegantly solves these problems. The video shows Pydantic replaces manual type checking, regex validation, and error handling with declarative models that automatically validate data types, ranges, and formats. Check it out at: https://github.com/pydantic/pydantic",
      "description": "Struggling with data validation - let’s dig into how Pydantic's type system and validation framework elegantly solves these problems. The video shows Pydantic replaces manual type checking, regex validation, and error handling with declarative models that automatically validate data types, ranges, and formats. Check it out at: https://github.com/pydantic/pydantic",
      "upload_date": "2025-01-19",
      "total_views": 18312,
      "max_views": 16400,
      "topics": [
        "add",
        "check",
        "data",
        "fun",
        "inputs",
        "introductioin",
        "look",
        "pydantic",
        "using",
        "validating",
        "validation"
      ],
      "search_text": "Struggling with data validation - let’s dig into how Pydantic's type system and validation framework elegantly solves these problems. The video shows Pydantic replaces manual type checking, regex validation, and error handling with declarative models that automatically validate data types, ranges, and formats. Check it out at: https://github.com/pydantic/pydantic add check data fun inputs introductioin look pydantic using validating validation Look at that. Data goes in, data comes out. Hey, this user sign up keeps crashing when I enter my data. You're not inputting your data properly. Not to be pedantic, but you should follow the instructions in my self-documenting code. Not to be pedantic, but you could add some unit tests. Fine, I'll add some validation. How hard could it be? And then we check the email format. Let's check the age range, check the dates. All right, getting this all in. Make sure you add phone number validation. Sure, just let me add that in to an update on my test and make sure it's international format. Update the validation, update the emails. There's got to be a better way. Why don't you use pedantic? Look at what your code could look like. Wait, that's it? No, if statements? And look what comes with it. Automatic type conversion. 25 becomes int 25. JSON serialization out of the box. Clear error messages. API documentation that goes through APNAI swagger. And it's all statically typed. So that means your IDE can catch the errors. All those late nights writing validation functions, unit tests, bug fixes, so much wasted time. Now you'll have more time for TikTok.",
      "platforms": {
        "tiktok": {
          "video_id": "7461699180034084126",
          "url": "https://www.tiktok.com/@rajistics/video/7461699180034084126",
          "view_count": 16400,
          "upload_date": "2025-01-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMjam8fEAjjKAQZIGEFPAjEDACqtAIaefoGIum~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=12OJuEfaX5%2BgYANyYAyXRbMt%2B%2B0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18076008769565984",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "K0ZalRISG44",
          "url": "https://www.youtube.com/watch?v=K0ZalRISG44",
          "view_count": 1912,
          "upload_date": "2025-01-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5982,
      "title": "MuonClip used by Moonshot AI and developed by Keller Jordan was used during the training of their trillion-parameter Kimi 2 model, addresses a core instability in large-scale transformers: exploding attention logits. Unlike traditional optimizers like Adam or AdamW that adjust step sizes based on gradient slopes, MuonClip actively rescales the query and key matrices after each update, preventing sharp logit growth within attention layers. This innovation allowed Moonshot AI to pre-train Kimi on 15.5 trillion tokens without a single training spike, producing an unusually smooth, stable loss curve.  Muon is Scalable for LLM Training — https://arxiv.org/abs/2502.16982 Keller Jordan - https://github.com/KellerJordan/Muon",
      "description": "MuonClip used by Moonshot AI and developed by Keller Jordan was used during the training of their trillion-parameter Kimi 2 model, addresses a core instability in large-scale transformers: exploding attention logits. Unlike traditional optimizers like Adam or AdamW that adjust step sizes based on gradient slopes, MuonClip actively rescales the query and key matrices after each update, preventing sharp logit growth within attention layers. This innovation allowed Moonshot AI to pre-train Kimi on 15.5 trillion tokens without a single training spike, producing an unusually smooth, stable loss curve.  Muon is Scalable for LLM Training — https://arxiv.org/abs/2502.16982 Keller Jordan - https://github.com/KellerJordan/Muon",
      "upload_date": "2025-07-15",
      "total_views": 18293,
      "max_views": 16600,
      "topics": [
        "kimi",
        "large",
        "llm",
        "moonshot",
        "muon",
        "muonclip",
        "optimizer",
        "training",
        "trillion",
        "used"
      ],
      "search_text": "MuonClip used by Moonshot AI and developed by Keller Jordan was used during the training of their trillion-parameter Kimi 2 model, addresses a core instability in large-scale transformers: exploding attention logits. Unlike traditional optimizers like Adam or AdamW that adjust step sizes based on gradient slopes, MuonClip actively rescales the query and key matrices after each update, preventing sharp logit growth within attention layers. This innovation allowed Moonshot AI to pre-train Kimi on 15.5 trillion tokens without a single training spike, producing an unusually smooth, stable loss curve.  Muon is Scalable for LLM Training — https://arxiv.org/abs/2502.16982 Keller Jordan - https://github.com/KellerJordan/Muon kimi large llm moonshot muon muonclip optimizer training trillion used People are calling this the most beautiful thing in AI training. It's also the secret behind a new trillion parameter open source model named Kimi. It's a smooth loss curve. Now let me break down why this matters and what Moonshot AI did differently. Training a neural network is a lot like navigating a landscape. You're trying to reach the lowest point because the lowest point is the place of the least error. So we typically track this with a loss curve which should steadily fall as training progresses. What you don't want is curves that go back up, error increases or spikes. All of this is training instability. To mathematically reach that low point we use optimizers. Optimizers typically follow the slope of the landscape taking careful steps based on how steep the path is. That's what something like Adam does it's widely used. But now we have a different approach. For Moonshot AI, instead of following the slope, they reshape the landscape itself. Their approach called Muon Clips works by rescaling the query and key matrices inside attention layers after every optimizer step. By rescaling it they prevent dangerous cliffs that could destabilize large-scale models. So instead of carefully like stepping around cliffs, Muon Clip just removes them. And with this, Moonshot AI was able to train Kimi on 15.5 trillion tokens without a single training spike. Now this isn't a general purpose fix for all neural networks. It's really built for large language models. Plenty of people are building large language model. Muon Clip is going to change everything for us thanks to them sharing it with us.",
      "platforms": {
        "tiktok": {
          "video_id": "7527117318812405022",
          "url": "https://www.tiktok.com/@rajistics/video/7527117318812405022",
          "view_count": 16600,
          "upload_date": "2025-07-15",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4QvFEHiCwBJIL754aB8ROmBsWA1iIVWkwAPB~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=Jkaafmqad3J9mbNSC0lLUIG9S6c%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18099340516580070",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "5-QiNfxyIFo",
          "url": "https://www.youtube.com/watch?v=5-QiNfxyIFo",
          "view_count": 1693,
          "upload_date": "2025-07-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Curse of dimensionality reminds us to think carefully about feature selection. More isn’t always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection ",
      "description": "Curse of dimensionality reminds us to think carefully about feature selection. More isn’t always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection ",
      "upload_date": "2023-02-11",
      "total_views": 18100,
      "max_views": 18100,
      "topics": [
        "curseofdimensionality",
        "datascience",
        "features",
        "featureselection",
        "machinelearning",
        "model"
      ],
      "search_text": "Curse of dimensionality reminds us to think carefully about feature selection. More isn’t always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection  curseofdimensionality datascience features featureselection machinelearning model Want to hear something strange? My model's performance keeps getting worse as I add more variables into the model. Why is that strange? I'm given the model more information. Shouldn't that help? Ah, you've hit the Curse of Dimensionality. Whoa, that doesn't sound good. The Curse of Dimensionality is when you add more features or more information, the search base gets really large. What does that mean? With supervised learning, it's like adding more noise into the system. Did you plot out the performance as you added more features? I did. Now I get this plot. So as I'm adding more features in, it's like adding more noise and my model's performance isn't going up. You're getting it. When I worked in an insurance company, we had over 1,000 different variables we could use to predict if somebody's likely to get in a crash. We had information on their driving history, their demographics, their credit history. How many of those features do you think we actually used in the final model? 200? Seven. Wow. Most production ML systems have less than 30 features going into them. Usually it doesn't add much incremental value by adding more features. Guess my model's cursed and it's OK.",
      "platforms": {
        "tiktok": {
          "video_id": "7198946727162907946",
          "url": "https://www.tiktok.com/@rajistics/video/7198946727162907946",
          "view_count": 18100,
          "upload_date": "2023-02-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0a5aaeb9b41a492b90971c88e7497af6_1676135420~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=BetF9EXsJP5RZVSj7C50vh4UnFg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Bias in Medical Imaging #datascience #codetok #algorithmicbias #imaging #machinelearning #bias  motivated by the comments from @rajistics",
      "description": "Bias in Medical Imaging #datascience #codetok #algorithmicbias #imaging #machinelearning #bias  motivated by the comments from @rajistics",
      "upload_date": "2022-05-22",
      "total_views": 18100,
      "max_views": 18100,
      "topics": [
        "algorithmicbias",
        "bias",
        "codetok",
        "datascience",
        "different",
        "imaging"
      ],
      "search_text": "Bias in Medical Imaging #datascience #codetok #algorithmicbias #imaging #machinelearning #bias  motivated by the comments from @rajistics algorithmicbias bias codetok datascience different imaging Did you know AI can figure out your race just by looking at this chest x-ray? Big deal you're thinking it's probably because those text x-rays are different people of different physical characteristics Diseases maybe affected them differently. Maybe their tissues are different or just there's some social bias Nope, not it Okay, maybe it's something about the images like they were taking it different places like different frequencies Maybe you know they were set up on the equipment differently Nope, not that either this research is really a great cautionary example of how even data that you might think is totally colorblind Could carry race with you and the lesson here is you should always run some sort of diagnostic tests to DC Is there some racial bias in your underlying data or model?",
      "platforms": {
        "tiktok": {
          "video_id": "7100375993407343915",
          "url": "https://www.tiktok.com/@rajistics/video/7100375993407343915",
          "view_count": 18100,
          "upload_date": "2022-05-22",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6460dcd07c3a46ab8f458b6551aa63a7_1653185120~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=xPpl3LzrTLx%2BtnfHoeDsjsiLuE8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Histograms are a great visualization tool. Here are some caveats and tips for using histograms. #datascience #statistics #datavisualization #histogram",
      "description": "Histograms are a great visualization tool. Here are some caveats and tips for using histograms. #datascience #statistics #datavisualization #histogram",
      "upload_date": "2023-02-10",
      "total_views": 18000,
      "max_views": 18000,
      "topics": [
        "bin",
        "datascience",
        "datavisualization",
        "histogram",
        "size",
        "statistics"
      ],
      "search_text": "Histograms are a great visualization tool. Here are some caveats and tips for using histograms. #datascience #statistics #datavisualization #histogram bin datascience datavisualization histogram size statistics Let's up your histogram game with a few things to watch out for and some pro tips. So watch your bin size. You can see in this example, moving from kind of a larger bin size, looks like the data is pretty average, but as soon as we move down to a finer bin size, we see there's a little bit of complexity here. In that same way, watch out where the bins end. You can see some sneaky behavior here. Like for example, we missed all the zeros the first time we plotted this. So one thing that can happen is we might not know if a variable is discrete or continuous, like in this example, until we looked at it with a much smaller bin size. One nice pro tip is showing the amount of missing data that you have in line with the rest of the histogram. Now there's also alternatives to histograms. The box plot is a classic alternative. The kernel density is sometimes nice, but can lead to its own issues for interpretation. And finally the good old dot plot, which I like, but that only works when you have a little amount of data.",
      "platforms": {
        "tiktok": {
          "video_id": "7198341470913301806",
          "url": "https://www.tiktok.com/@rajistics/video/7198341470913301806",
          "view_count": 18000,
          "upload_date": "2023-02-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fcb6896a21b34cd195c84b80aa39283d_1675994494~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=LoUl2VqaSvcOEOgyjz5UnsIo86M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6536,
      "title": "Deepmind and OpenAI want everyone to focus on extreme risks of AI. This helps them hype up AI and make themselves more attractive. The reality is there are far greater and more mundance risks that are occuring today. Let's talk about the data these models are trained on the biases in these models how the models are being used and the social and economic implications of these models. #datascience #machinelearning #modelbias #modelrisk Model evaluation for extreme risks: https://arxiv.org/pdf/2305.15324.pdf Github Copilot Litigation: https://githubcopilotlitigation.com/ Stable Diffusion Lawsuit: https://stablediffusionlitigation.com/",
      "description": "Deepmind and OpenAI want everyone to focus on extreme risks of AI. This helps them hype up AI and make themselves more attractive. The reality is there are far greater and more mundance risks that are occuring today. Let's talk about the data these models are trained on the biases in these models how the models are being used and the social and economic implications of these models. #datascience #machinelearning #modelbias #modelrisk Model evaluation for extreme risks: https://arxiv.org/pdf/2305.15324.pdf Github Copilot Litigation: https://githubcopilotlitigation.com/ Stable Diffusion Lawsuit: https://stablediffusionlitigation.com/",
      "upload_date": "2023-05-27",
      "total_views": 17873,
      "max_views": 16900,
      "topics": [
        "datascience",
        "deepmind",
        "extreme",
        "machinelearning",
        "modelbias",
        "modelrisk",
        "models",
        "openai",
        "probably",
        "risks"
      ],
      "search_text": "Deepmind and OpenAI want everyone to focus on extreme risks of AI. This helps them hype up AI and make themselves more attractive. The reality is there are far greater and more mundance risks that are occuring today. Let's talk about the data these models are trained on the biases in these models how the models are being used and the social and economic implications of these models. #datascience #machinelearning #modelbias #modelrisk Model evaluation for extreme risks: https://arxiv.org/pdf/2305.15324.pdf Github Copilot Litigation: https://githubcopilotlitigation.com/ Stable Diffusion Lawsuit: https://stablediffusionlitigation.com/ datascience deepmind extreme machinelearning modelbias modelrisk models openai probably risks",
      "platforms": {
        "tiktok": {
          "video_id": "7237885535405051182",
          "url": "https://www.tiktok.com/@rajistics/video/7237885535405051182",
          "view_count": 16900,
          "upload_date": "2023-05-27",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CswHjbogiXz",
          "url": "https://www.instagram.com/reel/CswHjbogiXz",
          "view_count": 838,
          "upload_date": "2023-05-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AoHLMqgo48Y",
          "url": "https://www.youtube.com/watch?v=AoHLMqgo48Y",
          "view_count": 135,
          "upload_date": "2023-05-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6437,
      "title": "The skit addresses the challenge of acquiring large volumes of labeled data for machine learning projects. The video focuses on using machine learning models for automating the labeling process. This approach is highlighted with the mention of MLFlow a platform that now supports using models as judges for data labeling. #datalabeling #mlflow #rajistics More details: Automation of Data Labeling: Emphasizes the shift from manual to automated data labeling highlighting efficiency and cost-effectiveness. Integration with MLFlow: Showcases the practical application of recent advancements in MLFlow that facilitate the use of machine learning models for data evaluation and labeling. Model Output and Justification: The discussion underscores the importance of not just the labeling output but also the accompanying justifications providing insights into the model's decision-making process. Accuracy and Bias Consideration: Highlights the correlation between model-generated labels and human-labeled data while acknowledging the potential biases inherent in machine learning models.",
      "description": "The skit addresses the challenge of acquiring large volumes of labeled data for machine learning projects. The video focuses on using machine learning models for automating the labeling process. This approach is highlighted with the mention of MLFlow a platform that now supports using models as judges for data labeling. #datalabeling #mlflow #rajistics More details: Automation of Data Labeling: Emphasizes the shift from manual to automated data labeling highlighting efficiency and cost-effectiveness. Integration with MLFlow: Showcases the practical application of recent advancements in MLFlow that facilitate the use of machine learning models for data evaluation and labeling. Model Output and Justification: The discussion underscores the importance of not just the labeling output but also the accompanying justifications providing insights into the model's decision-making process. Accuracy and Bias Consideration: Highlights the correlation between model-generated labels and human-labeled data while acknowledging the potential biases inherent in machine learning models.",
      "upload_date": "2023-12-20",
      "total_views": 17760,
      "max_views": 9326,
      "topics": [
        "data",
        "datalabeling",
        "evaluator",
        "judge",
        "labeling",
        "learning",
        "machine",
        "mlflow",
        "model",
        "using"
      ],
      "search_text": "The skit addresses the challenge of acquiring large volumes of labeled data for machine learning projects. The video focuses on using machine learning models for automating the labeling process. This approach is highlighted with the mention of MLFlow a platform that now supports using models as judges for data labeling. #datalabeling #mlflow #rajistics More details: Automation of Data Labeling: Emphasizes the shift from manual to automated data labeling highlighting efficiency and cost-effectiveness. Integration with MLFlow: Showcases the practical application of recent advancements in MLFlow that facilitate the use of machine learning models for data evaluation and labeling. Model Output and Justification: The discussion underscores the importance of not just the labeling output but also the accompanying justifications providing insights into the model's decision-making process. Accuracy and Bias Consideration: Highlights the correlation between model-generated labels and human-labeled data while acknowledging the potential biases inherent in machine learning models. data datalabeling evaluator judge labeling learning machine mlflow model using",
      "platforms": {
        "tiktok": {
          "video_id": "7314800009914895659",
          "url": "https://www.tiktok.com/@rajistics/video/7314800009914895659",
          "view_count": 9326,
          "upload_date": "2023-12-20",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1F3j8ngzYA",
          "url": "https://www.instagram.com/reel/C1F3j8ngzYA",
          "view_count": 5880,
          "upload_date": "2023-12-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Gp_3Dbo_aUo",
          "url": "https://www.youtube.com/watch?v=Gp_3Dbo_aUo",
          "view_count": 2554,
          "upload_date": "2023-12-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @philosophywithsuf explaining the irony for pytorch building a graph and the history of tensorflow ",
      "description": "Replying to @philosophywithsuf explaining the irony for pytorch building a graph and the history of tensorflow ",
      "upload_date": "2022-12-15",
      "total_views": 17700,
      "max_views": 17700,
      "topics": [
        "building",
        "graph",
        "irony",
        "know",
        "pytorch",
        "tensorflow"
      ],
      "search_text": "Replying to @philosophywithsuf explaining the irony for pytorch building a graph and the history of tensorflow  building graph irony know pytorch tensorflow You've ever been where you know you should plan out something, but it's just easier to go step by step, even though you know it's going to take a bit longer? The original TensorFlow used a graph approach. So what it would do is take all your calculations that you've made, figure out what's the best way to put them all together into an efficient way, and create that graph. Now, the graph ended up being more efficient, and if you needed to switch different architectures, for example, it lended itself to that. The downside to the graph is it's hard to debug, and what people liked instead was the PyTorch approach, which let you do things line by line. And so it became much more intuitive, and it's easier to debug. So the irony here is one of the reasons PyTorch was so widely used is because it didn't bother building a graph. But now they've offered the compile command because they realize sometimes you can get more efficiency and things will run faster if you do a go-ahead and plan ahead of time.",
      "platforms": {
        "tiktok": {
          "video_id": "7177496845298666798",
          "url": "https://www.tiktok.com/@rajistics/video/7177496845298666798",
          "view_count": 17700,
          "upload_date": "2022-12-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/492c705810604c4593c6c98f8c2271ee_1671141222~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=XkfZzbWRvlQPwCtt%2BwxUMo73vw4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6247,
      "title": "Github copilot alternatives.  ",
      "description": "Github copilot alternatives.  ",
      "upload_date": "2024-09-30",
      "total_views": 17695,
      "max_views": 17600,
      "topics": [
        "agents",
        "becoming",
        "code",
        "data",
        "dsbench",
        "est",
        "far",
        "github",
        "gpt",
        "look",
        "mais",
        "open",
        "quatre",
        "science",
        "source",
        "use"
      ],
      "search_text": "Github copilot alternatives.   agents becoming code data dsbench est far github gpt look mais open quatre science source use GitHub's co-pilot slaps, but of course, IT won't let us use it. Yeah, they were worried about any code that we wrote being sent outside of our organization. There are alternatives. Why not use an open source code generation tool? Tell me more. There are lots of open source models. Take a look at the big code leaderboard. So we can run these models on our own infrastructure. Come on, they can't be that good. Look at these benchmarks. It does pretty good. Wow, just googling it. Look at this SQL coder. It beats GPT4. I mean, give it up to open source. Wait, we could fine-tune this model based on our code and also incorporate the worst practices I ask all of you to do, like the naming conventions I use? Yeah, and the training data and the model are all completely open. It's no hassle from legal. Got it downloaded. Give me 10 minutes to get the TGI container up and running. In the meantime, go grab the VS Code extensions. No way it's that easy.",
      "platforms": {
        "tiktok": {
          "video_id": "7420393824960253227",
          "url": "https://www.tiktok.com/@rajistics/video/7420393824960253227",
          "view_count": 17600,
          "upload_date": "2024-09-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f8801d429aee4e0792ec8b5b86beafe7_1727695087~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=NfTs5w3tWxvogc7A6%2Bei7RkxW2s%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17931285602921823",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "t-knnvQRJFc",
          "url": "https://www.youtube.com/watch?v=t-knnvQRJFc",
          "view_count": 95,
          "upload_date": "2024-09-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6431,
      "title": "The New York Times recently filed a lawsuit against OpenAI. This is another of many copyright lawsuits against AI companies. While everyone is using the NYT data it's not going to be easy to get copyright to substantially support the claims of NYT. I am skeptical of any significant change. Some other reminders: - Many of these copyright cases against AI - Apple OpenAI and others are negotiating agreements with publishing companies - Microsoft and the tech industry have survived many of these types of cases and still thrived - A lawsuit can take a long time to wind through the courts - Today is an important day in copyright with Mickey Mouse coming into the public domain (but is still protected by trademark) - As content providers recognize the value of their content this will hurt the open source movement with less good data #openai #newyorktimes #copyright #mickeymouse #disneyplusvoices",
      "description": "The New York Times recently filed a lawsuit against OpenAI. This is another of many copyright lawsuits against AI companies. While everyone is using the NYT data it's not going to be easy to get copyright to substantially support the claims of NYT. I am skeptical of any significant change. Some other reminders: - Many of these copyright cases against AI - Apple OpenAI and others are negotiating agreements with publishing companies - Microsoft and the tech industry have survived many of these types of cases and still thrived - A lawsuit can take a long time to wind through the courts - Today is an important day in copyright with Mickey Mouse coming into the public domain (but is still protected by trademark) - As content providers recognize the value of their content this will hurt the open source movement with less good data #openai #newyorktimes #copyright #mickeymouse #disneyplusvoices",
      "upload_date": "2024-01-01",
      "total_views": 17449,
      "max_views": 9325,
      "topics": [
        "copyright",
        "disneyplusvoices",
        "many",
        "mickeymouse",
        "newyorktimes",
        "openai"
      ],
      "search_text": "The New York Times recently filed a lawsuit against OpenAI. This is another of many copyright lawsuits against AI companies. While everyone is using the NYT data it's not going to be easy to get copyright to substantially support the claims of NYT. I am skeptical of any significant change. Some other reminders: - Many of these copyright cases against AI - Apple OpenAI and others are negotiating agreements with publishing companies - Microsoft and the tech industry have survived many of these types of cases and still thrived - A lawsuit can take a long time to wind through the courts - Today is an important day in copyright with Mickey Mouse coming into the public domain (but is still protected by trademark) - As content providers recognize the value of their content this will hurt the open source movement with less good data #openai #newyorktimes #copyright #mickeymouse #disneyplusvoices copyright disneyplusvoices many mickeymouse newyorktimes openai",
      "platforms": {
        "tiktok": {
          "video_id": "7319190325308919086",
          "url": "https://www.tiktok.com/@rajistics/video/7319190325308919086",
          "view_count": 5295,
          "upload_date": "2024-01-01",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1kSbclgjS8",
          "url": "https://www.instagram.com/reel/C1kSbclgjS8",
          "view_count": 9325,
          "upload_date": "2024-01-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "y0-FPmVmHyo",
          "url": "https://youtube.com/shorts/y0-FPmVmHyo",
          "view_count": 2829,
          "upload_date": "2024-01-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6083,
      "title": "K-Means Flop? Here’s Why—and the Better Ways to Cluster Your Data K-means can fall short when data scales, cluster shapes, or dimensionality get tricky, as shown by examples where it fails to group points as expected. In this video, I show how alternative methods like Gaussian Mixture Models, density-based clustering, and HDBSCAN can solve those issues and uncover more accurate clusters.",
      "description": "K-Means Flop? Here’s Why—and the Better Ways to Cluster Your Data K-means can fall short when data scales, cluster shapes, or dimensionality get tricky, as shown by examples where it fails to group points as expected. In this video, I show how alternative methods like Gaussian Mixture Models, density-based clustering, and HDBSCAN can solve those issues and uncover more accurate clusters.",
      "upload_date": "2025-01-05",
      "total_views": 17400,
      "max_views": 17400,
      "topics": [
        "clusters",
        "data",
        "like",
        "look",
        "means",
        "one"
      ],
      "search_text": "K-Means Flop? Here’s Why—and the Better Ways to Cluster Your Data K-means can fall short when data scales, cluster shapes, or dimensionality get tricky, as shown by examples where it fails to group points as expected. In this video, I show how alternative methods like Gaussian Mixture Models, density-based clustering, and HDBSCAN can solve those issues and uncover more accurate clusters. clusters data like look means one Hey, I know K-means is a great way to cluster, but it's not working well for me. This is why there's lots of clustering approaches. Let's walk through your issues together. Take a look at this one. I'm confused why it didn't work. Ah, I see what's going on. Look at the axes. The scales are different. Here's an example to highlight what I'm talking about, where we have some data, but look at the axes. When we run K-means initially, it doesn't group them like we expect to. But look, once we normalize them, we get the clusters that we expect. How about this one? I know the diameters of the clusters are very different and could be a factor. Yeah, you're right. You might want to use another technique like Gaussian mixture models. I'd experiment a little bit. And look, K-means totally failed on this one. Yeah, K-means isn't going to work right here. You might want to use a density-based approach. See how this density-based approach works on this smiley face type thing and identifies the different clusters? Something like that might be useful. And this last one is I was working with TFIDF and taking all the different words and then trying to see the clusters here. It doesn't work very well. With high-dimensional data, one of the go-to approaches I like to use is HDB scan. You can take a look at this example and see like the difference between K-means, which doesn't group the data as you expect, versus when you use HDB scan, it finds those clusters, groups them together well.",
      "platforms": {
        "tiktok": {
          "video_id": "7456524265156873502",
          "url": "https://www.tiktok.com/@rajistics/video/7456524265156873502",
          "view_count": 17400,
          "upload_date": "2025-01-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oQtrROpIFWjAHgQgrCLOAlHGAsqfXIfZec9IdI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=wsMQICpCNTCg0DJ3nsO%2FFvDjolk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17961652643845442",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @milekumulator how I use GitHub #datascience #github #codetok #python #sportsanalytics",
      "description": "Reply to @milekumulator how I use GitHub #datascience #github #codetok #python #sportsanalytics",
      "upload_date": "2022-05-07",
      "total_views": 17400,
      "max_views": 17400,
      "topics": [
        "codetok",
        "datascience",
        "github",
        "projects",
        "python",
        "sportsanalytics"
      ],
      "search_text": "Reply to @milekumulator how I use GitHub #datascience #github #codetok #python #sportsanalytics codetok datascience github projects python sportsanalytics So I got called out as a parasite. I'm guilty. Let me tell you what I do. I'm a huge fan of GitHub. Take a look at all my starred projects. You'll see all the different things that I like. Here I was interested in optimizing fantasy football lineups. I looked at a ton of different projects. Check these out. Then some of these projects I even dug deep into ran the code. I use all this stuff when I actually made this blog post. So I may be a parasite, but I'm pretty fat and happy as well.",
      "platforms": {
        "tiktok": {
          "video_id": "7095007494287674667",
          "url": "https://www.tiktok.com/@rajistics/video/7095007494287674667",
          "view_count": 17400,
          "upload_date": "2022-05-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ad7d3298a93d4a0a9db3c9c900b51b84_1651935163~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=4msVSXrLI%2B%2BESPLP4BsyloNMBqA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Classification outcomes and probabilities #datascience #machinelearning #algorithms",
      "description": "Classification outcomes and probabilities #datascience #machinelearning #algorithms",
      "upload_date": "2022-03-02",
      "total_views": 17400,
      "max_views": 17400,
      "topics": [
        "algorithms",
        "classification",
        "datascience",
        "machinelearning",
        "malignant",
        "probabilities"
      ],
      "search_text": "Classification outcomes and probabilities #datascience #machinelearning #algorithms algorithms classification datascience machinelearning malignant probabilities Are you surprised that a computer doesn't give you a straightforward answer? Computers use classification models that help put probabilities. In the example here, it'd be the probability that a tumor is malignant. Threshold sets the probability for where we're going to treat things as malignant. Are we going to set this very low and have many cases as malignant? Or would we set the threshold much higher and only have a few cases labeled as malignant? Think about the consequences of either extreme and we'll talk more about that in my next video.",
      "platforms": {
        "tiktok": {
          "video_id": "7070349760204524843",
          "url": "https://www.tiktok.com/@rajistics/video/7070349760204524843",
          "view_count": 17400,
          "upload_date": "2022-03-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/725e82066a054bbea14b1cbd95710df5_1646194087~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=vIw%2BSZM%2B6WjybtsbwpGS%2B0Y11c0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6634,
      "title": "The LocalLlama subreddit received a citation in a recent paper by Meta. Great reminder of the innovation you can get when models have a large community using them. Rumor is OpenAI will release a new open-source LLM this summer. #datascience #machinelearning #largelanguagemodels #localllama #meta #openai #gpt3 LocalLlama subreddit: https://www.reddit.com/r/LocalLLaMA/ Extending Context Window of Large Language Models via Positional Interpolation: https://arxiv.org/abs/2306.15595 Background by Ella de Kross: https://unsplash.com/s/photos/inside-barn",
      "description": "The LocalLlama subreddit received a citation in a recent paper by Meta. Great reminder of the innovation you can get when models have a large community using them. Rumor is OpenAI will release a new open-source LLM this summer. #datascience #machinelearning #largelanguagemodels #localllama #meta #openai #gpt3 LocalLlama subreddit: https://www.reddit.com/r/LocalLLaMA/ Extending Context Window of Large Language Models via Positional Interpolation: https://arxiv.org/abs/2306.15595 Background by Ella de Kross: https://unsplash.com/s/photos/inside-barn",
      "upload_date": "2023-06-28",
      "total_views": 17063,
      "max_views": 15100,
      "topics": [
        "datascience",
        "innovation",
        "largelanguagemodels",
        "localllama",
        "machinelearning",
        "meta",
        "openai",
        "subreddit"
      ],
      "search_text": "The LocalLlama subreddit received a citation in a recent paper by Meta. Great reminder of the innovation you can get when models have a large community using them. Rumor is OpenAI will release a new open-source LLM this summer. #datascience #machinelearning #largelanguagemodels #localllama #meta #openai #gpt3 LocalLlama subreddit: https://www.reddit.com/r/LocalLLaMA/ Extending Context Window of Large Language Models via Positional Interpolation: https://arxiv.org/abs/2306.15595 Background by Ella de Kross: https://unsplash.com/s/photos/inside-barn datascience innovation largelanguagemodels localllama machinelearning meta openai subreddit",
      "platforms": {
        "tiktok": {
          "video_id": "7249868099812347179",
          "url": "https://www.tiktok.com/@rajistics/video/7249868099812347179",
          "view_count": 15100,
          "upload_date": "2023-06-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CuDQsvEgIew",
          "url": "https://www.instagram.com/reel/CuDQsvEgIew",
          "view_count": 1829,
          "upload_date": "2023-06-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "kWSYYD6s6xA",
          "url": "https://www.youtube.com/watch?v=kWSYYD6s6xA",
          "view_count": 134,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6482,
      "title": "Customer lifetime value is a common data science use case. There are many ways to calculate this but here I show how a data scientist would setup the problem. In Part 1 I will show the classic RFM approach. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalyticssummit",
      "description": "Customer lifetime value is a common data science use case. There are many ways to calculate this but here I show how a data scientist would setup the problem. In Part 1 I will show the classic RFM approach. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalyticssummit",
      "upload_date": "2024-03-05",
      "total_views": 17031,
      "max_views": 13795,
      "topics": [
        "customerlifetimevalue",
        "data",
        "datascience",
        "machinelearning",
        "marketinganalyticssummit",
        "rfm"
      ],
      "search_text": "Customer lifetime value is a common data science use case. There are many ways to calculate this but here I show how a data scientist would setup the problem. In Part 1 I will show the classic RFM approach. #datascience #machinelearning #rfm #customerlifetimevalue #marketinganalyticssummit customerlifetimevalue data datascience machinelearning marketinganalyticssummit rfm Friday, can we review where you're at? I decided to take a machinelearning approach to focus on what customers are likely to do versus what we already know about them. So, the first step I did was I took all the data that we had, split it into two areas, one for me to train on, another data set for me to validate, make sure my models working correctly. Keep going, I'm following. Then, I built a machine learning model at the customer level. For the features, I used the RFM features but I also brought in other pieces of information we had such as where they lived and their age for that model. That's great. Can you explain the predictions? Sure, we can use Shap to explain any of the predictions. Oh, I got big plans for the second generation version of this where we can bring in a variety of different types of features into a neural network architecture. It's going to be awesome. Wow, that would be cool but let's see if this meets marketing needs and maybe it's good enough for marketing. Check out part one to see how a data analyst figures out the lifetime value of a customer.",
      "platforms": {
        "tiktok": {
          "video_id": "7343019431368494382",
          "url": "https://www.tiktok.com/@rajistics/video/7343019431368494382",
          "view_count": 2612,
          "upload_date": "2024-03-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4JnrqcgEc8",
          "url": "https://www.instagram.com/reel/C4JnrqcgEc8",
          "view_count": 13795,
          "upload_date": "2024-03-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "m3cs7KhNyNw",
          "url": "https://youtube.com/shorts/m3cs7KhNyNw",
          "view_count": 624,
          "upload_date": "2024-03-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6109,
      "title": "Don't be afraid to challenge established models and assumptions! Often spending time with the data can give you new insights. One common limitation is the dependence on averages in lots of models. Often data is distributed in other ways. #datadistributions #dataanalysis #rajistics Paper: New insights into US flood vulnerability revealed from flood insurance big data - https://www.nature.com/articles/s41467-020-15264-2",
      "description": "Don't be afraid to challenge established models and assumptions! Often spending time with the data can give you new insights. One common limitation is the dependence on averages in lots of models. Often data is distributed in other ways. #datadistributions #dataanalysis #rajistics Paper: New insights into US flood vulnerability revealed from flood insurance big data - https://www.nature.com/articles/s41467-020-15264-2",
      "upload_date": "2023-12-30",
      "total_views": 16829,
      "max_views": 15300,
      "topics": [
        "data",
        "dataanalysis",
        "datadistributions",
        "models",
        "new",
        "often"
      ],
      "search_text": "Don't be afraid to challenge established models and assumptions! Often spending time with the data can give you new insights. One common limitation is the dependence on averages in lots of models. Often data is distributed in other ways. #datadistributions #dataanalysis #rajistics Paper: New insights into US flood vulnerability revealed from flood insurance big data - https://www.nature.com/articles/s41467-020-15264-2 data dataanalysis datadistributions models new often",
      "platforms": {
        "tiktok": {
          "video_id": "7318180983264660779",
          "url": "https://www.tiktok.com/@rajistics/video/7318180983264660779",
          "view_count": 15300,
          "upload_date": "2023-12-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18042591445538493",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-12-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EtGhbxIdQsk",
          "url": "https://youtube.com/shorts/EtGhbxIdQsk",
          "view_count": 1529,
          "upload_date": "2023-12-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6125,
      "title": "Python Optimal Transport is an open source Python library providing several solvers for optimization problems related to Optimal Transport for signal image processing and machine learning. Walking through a simple example using earth movers distance (EMD) and then moving to the Sinkhorn Knopp Algorithm. You can see examples of the cost matrix and the effects of regularization. #datascience #machinelearning #optimization #sinkhornknopp #pythonoptimaltransport #earthmoversdistance #regularization Python Optimal Transport: https://pythonot.github.io/ Background Photo by Curioso Photography: https://www.pexels.com/photo/aerial-view-of-white-buildings-343696/",
      "description": "Python Optimal Transport is an open source Python library providing several solvers for optimization problems related to Optimal Transport for signal image processing and machine learning. Walking through a simple example using earth movers distance (EMD) and then moving to the Sinkhorn Knopp Algorithm. You can see examples of the cost matrix and the effects of regularization. #datascience #machinelearning #optimization #sinkhornknopp #pythonoptimaltransport #earthmoversdistance #regularization Python Optimal Transport: https://pythonot.github.io/ Background Photo by Curioso Photography: https://www.pexels.com/photo/aerial-view-of-white-buildings-343696/",
      "upload_date": "2023-04-23",
      "total_views": 16808,
      "max_views": 15700,
      "topics": [
        "bakeries",
        "better",
        "building",
        "datascience",
        "earthmoversdistance",
        "key",
        "language",
        "large",
        "machinelearning",
        "models",
        "optimization",
        "pythonoptimaltransport",
        "regularization",
        "see",
        "sinkhornknopp"
      ],
      "search_text": "Python Optimal Transport is an open source Python library providing several solvers for optimization problems related to Optimal Transport for signal image processing and machine learning. Walking through a simple example using earth movers distance (EMD) and then moving to the Sinkhorn Knopp Algorithm. You can see examples of the cost matrix and the effects of regularization. #datascience #machinelearning #optimization #sinkhornknopp #pythonoptimaltransport #earthmoversdistance #regularization Python Optimal Transport: https://pythonot.github.io/ Background Photo by Curioso Photography: https://www.pexels.com/photo/aerial-view-of-white-buildings-343696/ bakeries better building datascience earthmoversdistance key language large machinelearning models optimization pythonoptimaltransport regularization see sinkhornknopp",
      "platforms": {
        "tiktok": {
          "video_id": "7225375647227694382",
          "url": "https://www.tiktok.com/@rajistics/video/7225375647227694382",
          "view_count": 15700,
          "upload_date": "2023-04-23",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17987366473976660",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NjtKcnLzo3g",
          "url": "https://www.youtube.com/watch?v=NjtKcnLzo3g",
          "view_count": 1108,
          "upload_date": "2023-04-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6417,
      "title": "The hardest step is getting ComfyUI running on your computer (you need a GPU). Go do it! Then you can create the coolest images using stable diffusion. To get comfy UI check out: https://github.com/comfyanonymous/ComfyUI Video to get started: https://www.youtube.com/watch?v=oZY4Iem5Oz4&ab_channel=Grockster Instant ID on ComfyUI: https://www.youtube.com/watch?v=wMLiGhogOPE&ab_channel=LatentVision Also ComfyUI web - but I haven‚Äôt tried this: https://comfyuiweb.com/ #comfyui #stablediffusion #rajistics",
      "description": "The hardest step is getting ComfyUI running on your computer (you need a GPU). Go do it! Then you can create the coolest images using stable diffusion. To get comfy UI check out: https://github.com/comfyanonymous/ComfyUI Video to get started: https://www.youtube.com/watch?v=oZY4Iem5Oz4&ab_channel=Grockster Instant ID on ComfyUI: https://www.youtube.com/watch?v=wMLiGhogOPE&ab_channel=LatentVision Also ComfyUI web - but I haven‚Äôt tried this: https://comfyuiweb.com/ #comfyui #stablediffusion #rajistics",
      "upload_date": "2024-02-29",
      "total_views": 16800,
      "max_views": 8185,
      "topics": [
        "ab_channel",
        "comfyui",
        "get",
        "hardest",
        "images",
        "instant",
        "let",
        "stablediffusion",
        "watch"
      ],
      "search_text": "The hardest step is getting ComfyUI running on your computer (you need a GPU). Go do it! Then you can create the coolest images using stable diffusion. To get comfy UI check out: https://github.com/comfyanonymous/ComfyUI Video to get started: https://www.youtube.com/watch?v=oZY4Iem5Oz4&ab_channel=Grockster Instant ID on ComfyUI: https://www.youtube.com/watch?v=wMLiGhogOPE&ab_channel=LatentVision Also ComfyUI web - but I haven‚Äôt tried this: https://comfyuiweb.com/ #comfyui #stablediffusion #rajistics ab_channel comfyui get hardest images instant let stablediffusion watch",
      "platforms": {
        "tiktok": {
          "video_id": "7341164056763436331",
          "url": "https://www.tiktok.com/@rajistics/video/7341164056763436331",
          "view_count": 3418,
          "upload_date": "2024-02-29",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C38wIAFrsSG",
          "url": "https://www.instagram.com/reel/C38wIAFrsSG",
          "view_count": 5197,
          "upload_date": "2024-02-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "7SUqC4rnUgU",
          "url": "https://youtube.com/shorts/7SUqC4rnUgU",
          "view_count": 8185,
          "upload_date": "2024-02-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Explaining how Emily Ocasio won second place with her project analyzing media coverage. I like her approach and highlights a growing trend of using prompting in data science.  #datascience #machinelearning #promptengineering #societyforscience #emilyocasio ",
      "description": "Explaining how Emily Ocasio won second place with her project analyzing media coverage. I like her approach and highlights a growing trend of using prompting in data science.  #datascience #machinelearning #promptengineering #societyforscience #emilyocasio ",
      "upload_date": "2023-03-29",
      "total_views": 16700,
      "max_views": 16700,
      "topics": [
        "data",
        "datascience",
        "emilyocasio",
        "machinelearning",
        "promptengineering",
        "societyforscience"
      ],
      "search_text": "Explaining how Emily Ocasio won second place with her project analyzing media coverage. I like her approach and highlights a growing trend of using prompting in data science.  #datascience #machinelearning #promptengineering #societyforscience #emilyocasio  data datascience emilyocasio machinelearning promptengineering societyforscience Emily here got second place in a national science competition. Her project's great, but I also think it tells us about the future of data science. So for me, the data science question was, given a text news article, could we identify is this an impersonal news article or one that humanizes people? Once I've set up that problem that way, I start thinking about what are the features that I could grab and let's get some data labeled so then we can build a model. But Emily didn't think that way. Instead, she used an approach based on prompting. Her first prompt created a summary of the article with all the important details about a person. Her second prompt removed any of the identifying information around age, gender, where they lived. Her final prompt asked if it was impersonal or not, and she used a few shot learning by giving it a few examples. She ran this on over 5,000 articles and found that humanizing coverage was significantly higher for whites over blacks. I love this. Not only for the problem she solved, but also giving us a path to showing how we can use tools like GPT-3.",
      "platforms": {
        "tiktok": {
          "video_id": "7216117785066294571",
          "url": "https://www.tiktok.com/@rajistics/video/7216117785066294571",
          "view_count": 16700,
          "upload_date": "2023-03-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/628943b9eaf546f4b9859d9d6b915658_1680133362~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=mYOuQlqUC9OQuJ%2FaJaYj33%2F%2B2mo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6433,
      "title": "LLMs have a lot of security issues. From prompt injection attacks extraction of training data data poisoning and even GPU based attacks. However most of these can be managed so be aware but not too scared: Some relevant papers: Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training https://arxiv.org/abs/2401.05566 Extracting Training Data https://www.usenix.org/system/files/sec21-carlini-extracting.pdf Poisoning Web-Scale Training Datasets is Practical https://arxiv.org/pdf/2302.10149.pdf https://www.youtube.com/watch?v=la7_sgp0iKY&ab_channel=BlackHat #largelanguagemodels #promptinjection #datapoisoning #security #rajistics",
      "description": "LLMs have a lot of security issues. From prompt injection attacks extraction of training data data poisoning and even GPU based attacks. However most of these can be managed so be aware but not too scared: Some relevant papers: Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training https://arxiv.org/abs/2401.05566 Extracting Training Data https://www.usenix.org/system/files/sec21-carlini-extracting.pdf Poisoning Web-Scale Training Datasets is Practical https://arxiv.org/pdf/2302.10149.pdf https://www.youtube.com/watch?v=la7_sgp0iKY&ab_channel=BlackHat #largelanguagemodels #promptinjection #datapoisoning #security #rajistics",
      "upload_date": "2024-01-18",
      "total_views": 16462,
      "max_views": 7743,
      "topics": [
        "data",
        "datapoisoning",
        "largelanguagemodels",
        "promptinjection",
        "security",
        "training"
      ],
      "search_text": "LLMs have a lot of security issues. From prompt injection attacks extraction of training data data poisoning and even GPU based attacks. However most of these can be managed so be aware but not too scared: Some relevant papers: Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training https://arxiv.org/abs/2401.05566 Extracting Training Data https://www.usenix.org/system/files/sec21-carlini-extracting.pdf Poisoning Web-Scale Training Datasets is Practical https://arxiv.org/pdf/2302.10149.pdf https://www.youtube.com/watch?v=la7_sgp0iKY&ab_channel=BlackHat #largelanguagemodels #promptinjection #datapoisoning #security #rajistics data datapoisoning largelanguagemodels promptinjection security training",
      "platforms": {
        "tiktok": {
          "video_id": "7325241043564137771",
          "url": "https://www.tiktok.com/@rajistics/video/7325241043564137771",
          "view_count": 5989,
          "upload_date": "2024-01-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C2OQ54Ntjqc",
          "url": "https://www.instagram.com/reel/C2OQ54Ntjqc",
          "view_count": 7743,
          "upload_date": "2024-01-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "MM-Uh2fGuXk",
          "url": "https://youtube.com/shorts/MM-Uh2fGuXk",
          "view_count": 2730,
          "upload_date": "2024-01-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6455,
      "title": "Time to get started with Generative AI. Look at how Cultrix got their model to the top of the OpenLLM Leaderboard. You can do this too! Cultrix's model: https://huggingface.co/CultriX/MistralTrix-v1 Fine Tune Mistral: https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac Generative AI course: https://github.com/mlabonne/llm-course/",
      "description": "Time to get started with Generative AI. Look at how Cultrix got their model to the top of the OpenLLM Leaderboard. You can do this too! Cultrix's model: https://huggingface.co/CultriX/MistralTrix-v1 Fine Tune Mistral: https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac Generative AI course: https://github.com/mlabonne/llm-course/",
      "upload_date": "2024-01-05",
      "total_views": 16413,
      "max_views": 7891,
      "topics": [
        "cultrix",
        "fine",
        "generative",
        "mistral",
        "model",
        "tune"
      ],
      "search_text": "Time to get started with Generative AI. Look at how Cultrix got their model to the top of the OpenLLM Leaderboard. You can do this too! Cultrix's model: https://huggingface.co/CultriX/MistralTrix-v1 Fine Tune Mistral: https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac Generative AI course: https://github.com/mlabonne/llm-course/ cultrix fine generative mistral model tune",
      "platforms": {
        "tiktok": {
          "video_id": "7320691158320139566",
          "url": "https://www.tiktok.com/@rajistics/video/7320691158320139566",
          "view_count": 7144,
          "upload_date": "2024-01-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1usp0bAXXF",
          "url": "https://www.instagram.com/reel/C1usp0bAXXF",
          "view_count": 7891,
          "upload_date": "2024-01-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "r4U25ACF580",
          "url": "https://youtube.com/shorts/r4U25ACF580",
          "view_count": 1378,
          "upload_date": "2024-01-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6462,
      "title": "Symbolic regression focuses on a mathematical representation of your data. It's helpful in many situations where you need an explainable model or trying to model something where a mathematical formula represents the data well. SRBench: https://cavalab.org/srbench/ Intro to Eureqa: https://www.youtube.com/watch?v=NhC1Qb-PQ5Q&ab_channel=Eureqa Background Matheus Frade: https://unsplash.com/photos/iSSO7Fj1F98 #machinelearning #datascience #eureqa #symbolicregression",
      "description": "Symbolic regression focuses on a mathematical representation of your data. It's helpful in many situations where you need an explainable model or trying to model something where a mathematical formula represents the data well. SRBench: https://cavalab.org/srbench/ Intro to Eureqa: https://www.youtube.com/watch?v=NhC1Qb-PQ5Q&ab_channel=Eureqa Background Matheus Frade: https://unsplash.com/photos/iSSO7Fj1F98 #machinelearning #datascience #eureqa #symbolicregression",
      "upload_date": "2023-08-25",
      "total_views": 16362,
      "max_views": 13500,
      "topics": [
        "data",
        "datascience",
        "eureqa",
        "machinelearning",
        "mathematical",
        "regression",
        "symbolic",
        "symbolicregression"
      ],
      "search_text": "Symbolic regression focuses on a mathematical representation of your data. It's helpful in many situations where you need an explainable model or trying to model something where a mathematical formula represents the data well. SRBench: https://cavalab.org/srbench/ Intro to Eureqa: https://www.youtube.com/watch?v=NhC1Qb-PQ5Q&ab_channel=Eureqa Background Matheus Frade: https://unsplash.com/photos/iSSO7Fj1F98 #machinelearning #datascience #eureqa #symbolicregression data datascience eureqa machinelearning mathematical regression symbolic symbolicregression",
      "platforms": {
        "tiktok": {
          "video_id": "7271345336944233774",
          "url": "https://www.tiktok.com/@rajistics/video/7271345336944233774",
          "view_count": 13500,
          "upload_date": "2023-08-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CwYShmcJpW3",
          "url": "https://www.instagram.com/reel/CwYShmcJpW3",
          "view_count": 1776,
          "upload_date": "2023-08-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "sb-SLYIeW6I",
          "url": "https://www.youtube.com/watch?v=sb-SLYIeW6I",
          "view_count": 1086,
          "upload_date": "2023-08-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6535,
      "title": "Soundstorm is a new audio generation model from Google. It can rapidly generate high-quality audio. Google isn't making this model available yet but in the meantime check out the Hugging Face Audio courses if you want to learn more about machine learning with audio. #datascience #machinelearning #soundstorm #google #audioml SoundStorm: Efficient Parallel Audio Generation - https://arxiv.org/pdf/2305.09636.pdf Hugging Face Audio Transformers Course: https://huggingface.co/learn/audio-course/chapter0/introduction",
      "description": "Soundstorm is a new audio generation model from Google. It can rapidly generate high-quality audio. Google isn't making this model available yet but in the meantime check out the Hugging Face Audio courses if you want to learn more about machine learning with audio. #datascience #machinelearning #soundstorm #google #audioml SoundStorm: Efficient Parallel Audio Generation - https://arxiv.org/pdf/2305.09636.pdf Hugging Face Audio Transformers Course: https://huggingface.co/learn/audio-course/chapter0/introduction",
      "upload_date": "2023-07-17",
      "total_views": 16247,
      "max_views": 15700,
      "topics": [
        "audio",
        "audioml",
        "datascience",
        "generation",
        "google",
        "machinelearning",
        "model",
        "soundstorm"
      ],
      "search_text": "Soundstorm is a new audio generation model from Google. It can rapidly generate high-quality audio. Google isn't making this model available yet but in the meantime check out the Hugging Face Audio courses if you want to learn more about machine learning with audio. #datascience #machinelearning #soundstorm #google #audioml SoundStorm: Efficient Parallel Audio Generation - https://arxiv.org/pdf/2305.09636.pdf Hugging Face Audio Transformers Course: https://huggingface.co/learn/audio-course/chapter0/introduction audio audioml datascience generation google machinelearning model soundstorm",
      "platforms": {
        "tiktok": {
          "video_id": "7256572034749123883",
          "url": "https://www.tiktok.com/@rajistics/video/7256572034749123883",
          "view_count": 15700,
          "upload_date": "2023-07-17",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cuxxt35Ag79",
          "url": "https://www.instagram.com/reel/Cuxxt35Ag79",
          "view_count": 406,
          "upload_date": "2023-07-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZlosH1tH02g",
          "url": "https://www.youtube.com/watch?v=ZlosH1tH02g",
          "view_count": 141,
          "upload_date": "2023-07-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics ",
      "description": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics ",
      "upload_date": "2022-11-05",
      "total_views": 16100,
      "max_views": 16100,
      "topics": [
        "datascience",
        "imodels",
        "interpretable",
        "interpretablemodels",
        "machinelearning",
        "statistics"
      ],
      "search_text": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics  datascience imodels interpretable interpretablemodels machinelearning statistics Bet you're somebody that can handle complexity, but today let's talk about a simple data science approach using FIGS. So FIGS is a state-of-the-art approach in interpretable models and you can find it in the iModels package which has a lot of great interpretable models. Now instead of growing one tree at a time, it grows trees simultaneously and you can see these trees and the shape of these trees emerge from the data rather than we pre-defining them ahead of time. The payoff is trees are more accurate. Before FIGS, I'd use a car tree like this and this can be a little bit complex to explain because it's a lot of and or relationships that you have to take into account. This is the model that comes out of FIGS. This is so much easier to understand. A couple of simple rules. It's easy for that end user to be able to figure out and that's what me as a data scientist. Sometimes that's the goal is making simple interpretable bottles that get used.",
      "platforms": {
        "tiktok": {
          "video_id": "7162358755890564398",
          "url": "https://www.tiktok.com/@rajistics/video/7162358755890564398",
          "view_count": 16100,
          "upload_date": "2022-11-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7f703d4e163c4e68a920937afdaedb17_1667616612~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=06Weecu7WUPs%2BJDnMG9FE79N22g%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6123,
      "title": "Replying to @Sam This video won't be popular but I have to speak the truth. Meta AI has been really sharing out top notch open source models all across AI. #machinelearning #datascience #metaai #google Background by ThisisEngineering RAEng: https://unsplash.com/photos/bcqDxjddPGk",
      "description": "Replying to @Sam This video won't be popular but I have to speak the truth. Meta AI has been really sharing out top notch open source models all across AI. #machinelearning #datascience #metaai #google Background by ThisisEngineering RAEng: https://unsplash.com/photos/bcqDxjddPGk",
      "upload_date": "2023-05-14",
      "total_views": 16000,
      "max_views": 16000,
      "topics": [
        "datascience",
        "google",
        "machinelearning",
        "metaai",
        "open",
        "replying",
        "research",
        "sam"
      ],
      "search_text": "Replying to @Sam This video won't be popular but I have to speak the truth. Meta AI has been really sharing out top notch open source models all across AI. #machinelearning #datascience #metaai #google Background by ThisisEngineering RAEng: https://unsplash.com/photos/bcqDxjddPGk datascience google machinelearning metaai open replying research sam",
      "platforms": {
        "tiktok": {
          "video_id": "7232855839642537262",
          "url": "https://www.tiktok.com/@rajistics/video/7232855839642537262",
          "view_count": 16000,
          "upload_date": "2023-05-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18038918470462463",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-05-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6463,
      "title": "Like beautiful plots of data maps? Check out DataMapPlot from Leland McInnes. To make the best use of this you will need to have your data clustered. If you aren't sure where to start BerTopic is my suggestion. DataMapPlot: https://github.com/TutteInstitute/datamapplot BerTopic: https://maartengr.github.io/BERTopic/index.html #datamap #topicclustering #rajistics",
      "description": "Like beautiful plots of data maps? Check out DataMapPlot from Leland McInnes. To make the best use of this you will need to have your data clustered. If you aren't sure where to start BerTopic is my suggestion. DataMapPlot: https://github.com/TutteInstitute/datamapplot BerTopic: https://maartengr.github.io/BERTopic/index.html #datamap #topicclustering #rajistics",
      "upload_date": "2024-01-09",
      "total_views": 15995,
      "max_views": 10756,
      "topics": [
        "bertopic",
        "data",
        "datamap",
        "datamapplot",
        "github",
        "topicclustering"
      ],
      "search_text": "Like beautiful plots of data maps? Check out DataMapPlot from Leland McInnes. To make the best use of this you will need to have your data clustered. If you aren't sure where to start BerTopic is my suggestion. DataMapPlot: https://github.com/TutteInstitute/datamapplot BerTopic: https://maartengr.github.io/BERTopic/index.html #datamap #topicclustering #rajistics bertopic data datamap datamapplot github topicclustering",
      "platforms": {
        "tiktok": {
          "video_id": "7322195699549048110",
          "url": "https://www.tiktok.com/@rajistics/video/7322195699549048110",
          "view_count": 4203,
          "upload_date": "2024-01-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C15Ik4sASpk",
          "url": "https://www.instagram.com/reel/C15Ik4sASpk",
          "view_count": 10756,
          "upload_date": "2024-01-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "L5pZJt41wkg",
          "url": "https://youtube.com/shorts/L5pZJt41wkg",
          "view_count": 1036,
          "upload_date": "2024-01-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6439,
      "title": "XGBoost 2.0 is out with some great new features including support for multi-target trees with vector-leaf outputs and learning to rank problems. XGBoost is widely used for solving tabular problems. (The update was released a few months ago but that still new in the world of gradient boost machines) XGBoost: https://github.com/dmlc/xgboost Background Video by Trippy Clicker: https://www.pexels.com/video/panning-shot-of-the-sea-at-sunset-6202759/ AI Characters by Parrot AI",
      "description": "XGBoost 2.0 is out with some great new features including support for multi-target trees with vector-leaf outputs and learning to rank problems. XGBoost is widely used for solving tabular problems. (The update was released a few months ago but that still new in the world of gradient boost machines) XGBoost: https://github.com/dmlc/xgboost Background Video by Trippy Clicker: https://www.pexels.com/video/panning-shot-of-the-sea-at-sunset-6202759/ AI Characters by Parrot AI",
      "upload_date": "2024-02-25",
      "total_views": 15934,
      "max_views": 10122,
      "topics": [
        "classification",
        "features",
        "great",
        "learning",
        "models",
        "multi",
        "new",
        "problems",
        "video",
        "xgboost"
      ],
      "search_text": "XGBoost 2.0 is out with some great new features including support for multi-target trees with vector-leaf outputs and learning to rank problems. XGBoost is widely used for solving tabular problems. (The update was released a few months ago but that still new in the world of gradient boost machines) XGBoost: https://github.com/dmlc/xgboost Background Video by Trippy Clicker: https://www.pexels.com/video/panning-shot-of-the-sea-at-sunset-6202759/ AI Characters by Parrot AI classification features great learning models multi new problems video xgboost Did you know XGBoost 2. 0 is out? Of course, I like the trees and 2. 0 is nice upgrade. XGBoost is one of the most widely used machine learning models and there's nothing better for tabular data. With two. 0, you get vector leaf tree models for multi-target regression, multi-label classification, and multi-class classification. Multi-output support, I dig that. So, if I'm doing a classification problem, I'm no longer limited to just one category. An example can be both in sports and in entertainment. This is multi-label classification. So what this allows me to do is instead of having using multiple models for each of those, I can do it all within one model. Big deal, you could easily do these multi-output problems with neural networks. Also an implementation for the learning to rank task. That sounds great but when exactly would I use a learning to rank task? Anytime you care about the order of the results, everything from search results to product recommendations. Yeah, I can see they added an unbiased lamb decart and a new objective function. Don't worry, they have a tutorial notebooks you can use to learn from. You've boosted my understanding of XGBoost. Remember, similar results may occur with other gradient boosted tree models including but not limited to light GBM and Cat Boost.",
      "platforms": {
        "tiktok": {
          "video_id": "7339570484981304619",
          "url": "https://www.tiktok.com/@rajistics/video/7339570484981304619",
          "view_count": 3370,
          "upload_date": "2024-02-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C3xsfx1gVul",
          "url": "https://www.instagram.com/reel/C3xsfx1gVul",
          "view_count": 10122,
          "upload_date": "2024-02-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0mej7oBR16g",
          "url": "https://youtube.com/shorts/0mej7oBR16g",
          "view_count": 2442,
          "upload_date": "2024-02-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Quick introduction to optimization and for advanced folks, go run a notebook from gurobi or do the Kaggle Santa challenge. #datascience #machinelearning #optimization #travelingsalesmanproblem #gurobi ",
      "description": "Quick introduction to optimization and for advanced folks, go run a notebook from gurobi or do the Kaggle Santa challenge. #datascience #machinelearning #optimization #travelingsalesmanproblem #gurobi ",
      "upload_date": "2022-12-25",
      "total_views": 15900,
      "max_views": 15900,
      "topics": [
        "datascience",
        "gurobi",
        "machinelearning",
        "optimization",
        "problem",
        "travelingsalesmanproblem"
      ],
      "search_text": "Quick introduction to optimization and for advanced folks, go run a notebook from gurobi or do the Kaggle Santa challenge. #datascience #machinelearning #optimization #travelingsalesmanproblem #gurobi  datascience gurobi machinelearning optimization problem travelingsalesmanproblem Alright, you got a few minutes. Let's level up your data science knowledge, no matter what your skill level. So imagine a salesperson has to visit all these different cities and they want to figure out how to do it in the shortest amount of distance. Would you solve this with a classification problem, maybe trying to figure out different groups of salespeople? No. What this is is, it's an optimization problem because there's lots of different ways to solve this, but the key is you're trying to figure out what the most efficient way to do it is. If this is entirely new, go ahead and Google this up and read about it because it's an important concept to learn in data science and machine learning when to use optimization. If you know the problem, have you run the code? Here's a notebook where you can go and run the code and solve the problem. If you've run this optimization before for the traveling salesman problem, well, go do the Kaggle Santa Challenge. Every year, Kaggle has a competition around optimization. It's a great place to kind of keep building your skills.",
      "platforms": {
        "tiktok": {
          "video_id": "7180946328107683115",
          "url": "https://www.tiktok.com/@rajistics/video/7180946328107683115",
          "view_count": 15900,
          "upload_date": "2022-12-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/97cc33783f4e4a8aa930d8d50e49ff7c_1671944367~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=pQhwNDu4AN19GcYMhnbrrJgW%2BJc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I think langchain is aweome, but the future is an easy to use UI. Think Alteryx for LLMs. Langflow is a step in the right direction. #datascience #machinelearning #largelanguagemodels #gpt4 #langchain #langflow ",
      "description": "I think langchain is aweome, but the future is an easy to use UI. Think Alteryx for LLMs. Langflow is a step in the right direction. #datascience #machinelearning #largelanguagemodels #gpt4 #langchain #langflow ",
      "upload_date": "2023-03-17",
      "total_views": 15700,
      "max_views": 15700,
      "topics": [
        "datascience",
        "langchain",
        "langflow",
        "largelanguagemodels",
        "machinelearning",
        "use"
      ],
      "search_text": "I think langchain is aweome, but the future is an easy to use UI. Think Alteryx for LLMs. Langflow is a step in the right direction. #datascience #machinelearning #largelanguagemodels #gpt4 #langchain #langflow  datascience langchain langflow largelanguagemodels machinelearning use Have you seen these mind-blowing applications of GPT, but you just don't know how to code in Python so you can't do it? I got a solution for you today. Langflow is a new open-source tool that allows you to create complex prompts that otherwise you'd have to code in Python using Langchain. Here's an example where I started off with a prompt template around naming companies, hooked it up to an LLM, and then I was able to use the chat interface and use these connections that I've made before, get new product names. That's the simple example. Here's the more complex example that we worked on before with Leonardo DiCaprio's girlfriend trying to be able to find her name and predict her age. Here we've created a Langchain setup where we can use search as well as math. All of that has been integrated together through a UI that we could now use and prompt. So this allows us to build all that advanced stuff, but with a UI and without having to learn Python. This tool was just released so it's the early days, but go check it out on GitHub and hopefully we'll get them on spaces as well.",
      "platforms": {
        "tiktok": {
          "video_id": "7211636212665224490",
          "url": "https://www.tiktok.com/@rajistics/video/7211636212665224490",
          "view_count": 15700,
          "upload_date": "2023-03-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b2329cb8314e4c80a3ac23499ca2dd14_1679089910~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=rQIg%2BP1Tv8mogAiddg3n2EGeFY0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "New state of the art embedding model, Instructor, for text is available! It accounts for task and domain when creating an mending. #datascience #machinelearning #embeddings #word2vec #sentencetransformers #huggingface ",
      "description": "New state of the art embedding model, Instructor, for text is available! It accounts for task and domain when creating an mending. #datascience #machinelearning #embeddings #word2vec #sentencetransformers #huggingface ",
      "upload_date": "2023-01-22",
      "total_views": 15600,
      "max_views": 15600,
      "topics": [
        "datascience",
        "embeddings",
        "huggingface",
        "machinelearning",
        "sentencetransformers",
        "word2vec"
      ],
      "search_text": "New state of the art embedding model, Instructor, for text is available! It accounts for task and domain when creating an mending. #datascience #machinelearning #embeddings #word2vec #sentencetransformers #huggingface  datascience embeddings huggingface machinelearning sentencetransformers word2vec I got a new state-of-the-art embeddings model that the folks from Meta just dropped off called Instructor. The goal of an embedding model is to turn text into a numerical representation so we can work on it in data science. Word2Vec did this originally, where they gave every word a unique representation. The limitation of that approach was there's some words like bond or bat that have multiple meanings. So we moved to transformer models that could take the context of the sentence into account and got better performance. So Instructor goes beyond this by asking what task are we trying to do, and by knowing the task we can actually get a better embedding. The way this works is once we get an input text like who sings this love story, we're going to ask what kind of task is this? Is this question answer? Is it a retrieval? Are we doing some sort of classification? And that's going to affect the final embedding. To illustrate the value of using instructions, they give this example where we see two phrases when we're not using instruction that seem like they're similar together. I mean they both have four words, they're very close to each other. But once we add the instruction, the embeddings then move farther apart, which actually line up much better with our intuition of what these words mean. Hey, you can grab this model from the Hugging Face Hub. I found it super easy to grab and was able to start using embeddings within a couple of minutes.",
      "platforms": {
        "tiktok": {
          "video_id": "7191536028568522027",
          "url": "https://www.tiktok.com/@rajistics/video/7191536028568522027",
          "view_count": 15600,
          "upload_date": "2023-01-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5c7715d87274486e81e7ec20370658a2_1674409974~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=mcNe1VjXeiCobkBcgWzVS9TXlKU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6425,
      "title": "DINOv2 is a self-supervised machine learning model for computer vision. It can be used for a variety of image tasks like image classification object detection and video understanding without any fine tuning. To learn more check out Paper: https://arxiv.org/pdf/2304.07193.pdf Github: https://github.com/facebookresearch/dinov2 See the post from Yann on my 2023 AI Advancements post: https://www.threads.net/@rajistics/post/C1H6pe9gXLz",
      "description": "DINOv2 is a self-supervised machine learning model for computer vision. It can be used for a variety of image tasks like image classification object detection and video understanding without any fine tuning. To learn more check out Paper: https://arxiv.org/pdf/2304.07193.pdf Github: https://github.com/facebookresearch/dinov2 See the post from Yann on my 2023 AI Advancements post: https://www.threads.net/@rajistics/post/C1H6pe9gXLz",
      "upload_date": "2023-12-27",
      "total_views": 15567,
      "max_views": 7484,
      "topics": [
        "dinov2",
        "github",
        "image",
        "pdf",
        "post",
        "self"
      ],
      "search_text": "DINOv2 is a self-supervised machine learning model for computer vision. It can be used for a variety of image tasks like image classification object detection and video understanding without any fine tuning. To learn more check out Paper: https://arxiv.org/pdf/2304.07193.pdf Github: https://github.com/facebookresearch/dinov2 See the post from Yann on my 2023 AI Advancements post: https://www.threads.net/@rajistics/post/C1H6pe9gXLz dinov2 github image pdf post self",
      "platforms": {
        "tiktok": {
          "video_id": "7317375981948374314",
          "url": "https://www.tiktok.com/@rajistics/video/7317375981948374314",
          "view_count": 7484,
          "upload_date": "2023-12-27",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1Xw7oGAfnr",
          "url": "https://www.instagram.com/reel/C1Xw7oGAfnr",
          "view_count": 4296,
          "upload_date": "2023-12-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "FLY7gNibbfk",
          "url": "https://youtube.com/shorts/FLY7gNibbfk",
          "view_count": 3787,
          "upload_date": "2023-12-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "The best way to learning data science is working with data. You don’t need to spend money on courses or books. Spending time doing useful projects. #machinelearning #datascience ",
      "description": "The best way to learning data science is working with data. You don’t need to spend money on courses or books. Spending time doing useful projects. #machinelearning #datascience ",
      "upload_date": "2022-12-17",
      "total_views": 15500,
      "max_views": 15500,
      "topics": [
        "data",
        "datascience",
        "learning",
        "machinelearning",
        "really",
        "science"
      ],
      "search_text": "The best way to learning data science is working with data. You don’t need to spend money on courses or books. Spending time doing useful projects. #machinelearning #datascience  data datascience learning machinelearning really science Hey, come join me today. I'm going to interview two data scientists on their technical background. I'm super excited to meet you. You really just got into data science. I've jumped into it. I took a 12-week bootcamp from MIT and I've purchased every O'Reilly book. Really fascinated by data science. Wow, that's quite a bit. Can you tell me about a recent project that you worked on? Yeah, I built a deep learning model, an LSTM, to predict EEG readings. Impressive. Can you walk me through that project starting with the data? This was part of a three-week capstone contest. So I ended up using a data set off Kaggle. So I'm not really into healthcare and don't know much about the data, but we did use an LSTM to solve this. So why did you choose an LSTM? Can you walk me through some of the decisions you're making about the architecture? Well, I decided I wanted to spend some time on deep learning. So I used a Keras approach and then we spent a lot of time tuning the hyperparameters, but we got almost 100% accuracy with our model. So what hyperparameters did you find most effective? I think we used Bayesian hyperparameter tuning and the activation function played a big role there. Hey, thanks for interviewing me. Tom on your team has been really helpful to me as I've been trying to learn data science on nights and weekends. That's very admirable. Can you start by telling me about a recent project that you worked on? I volunteer at my local animal shelter and I wanted to help them and I quickly realized their data was a mess, but I was able to get it organized and put it into a Tableau dashboard. So now they have a better understanding of what's going on. Wow, that's really cool. What are some of the challenges you hit? Once I started digging into their data, I realized what a mess it was. And they had a freeform text and so there's a ton of variation. So I used an approach with the hamming distance to help me clean up the data, but then I also got them to change the intake form so we didn't have this freeform variable that was leading to all these problems. Have you had a chance to apply any machine learning algorithms? Yes, I built an algorithm to detect if a pet is likely to be adopted and the shelter is now using that to triage. Any specific techniques? Yeah, I tried a bunch and XG boosted the best, but it's a little embarrassing, but since the shelter wasn't very sophisticated, I ended up actually using a logistic regression where I coded the coefficients into the spreadsheet because that was just easier for them to use. So who would you hire? You know, when I look for someone, I need somebody that likes to learn because data science is all about being able to continually acquire new knowledge. I also want somebody that's scrappy enough to solve the little problems and not expect somebody else to do it. But finally, somebody that gets the big picture that will make sure that all of our clients and customers end up getting their problems solved.",
      "platforms": {
        "tiktok": {
          "video_id": "7178150513396010286",
          "url": "https://www.tiktok.com/@rajistics/video/7178150513396010286",
          "view_count": 15500,
          "upload_date": "2022-12-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5cfccaa23cc8431e8e107cb1ac9e8729_1671293508~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=pzX7CYAH%2F3RWt3JOU7fzb4YkvcY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Cheating has reared its head again over at Kaggle. Some background for folks on Kaggle and cheating there. #datascience #machinelearning #kaggle #ottocompetition ",
      "description": "Cheating has reared its head again over at Kaggle. Some background for folks on Kaggle and cheating there. #datascience #machinelearning #kaggle #ottocompetition ",
      "upload_date": "2023-01-31",
      "total_views": 15200,
      "max_views": 15200,
      "topics": [
        "cheating",
        "datascience",
        "kaggle",
        "machinelearning",
        "medals",
        "ottocompetition"
      ],
      "search_text": "Cheating has reared its head again over at Kaggle. Some background for folks on Kaggle and cheating there. #datascience #machinelearning #kaggle #ottocompetition  cheating datascience kaggle machinelearning medals ottocompetition Let's talk about a recent scandal, some cheating that's going on over at Kaggle. Kaggle is the home of data science competitions. Data scientists compete there to build the best model. Kaggle gives out big prizes, though there was even a million dollar prize for the Zillow competition. Besides money, Kaggle gives out medals, and if you collect enough medals, you can gain titles like Master and Grandmaster. These titles are rare in the data science world and hold some prestige, and often employers are looking for them, which now you get why people might want to cheat. One way to cheat is by getting the answers. For some of the competitions, these things have happened in the past, and so there could be ways to find out the answers, and that's what happened in Petfinder. Another way to cheat is by sharing or copying code from someone else. And this is a very effective way to be able to get a medal, and look like you finished a competition near the top, without actually having to write any of the code. And in this case, in the auto competition, it looks like somebody was selling access to medals, guaranteeing that you would get a medal. Now Kaggle is a great community, just don't blindly trust somebody based on their medals.",
      "platforms": {
        "tiktok": {
          "video_id": "7194629806426688811",
          "url": "https://www.tiktok.com/@rajistics/video/7194629806426688811",
          "view_count": 15200,
          "upload_date": "2023-01-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5370681767274a1085681950fe28474b_1675130300~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=yV%2FPxXlV%2FruVgH%2Fy6X1Ts9bITrE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6092,
      "title": "I am haunted by this. Follow for more data science, stats, and AI/ML content. ",
      "description": "I am haunted by this. Follow for more data science, stats, and AI/ML content. ",
      "upload_date": "2024-12-25",
      "total_views": 15100,
      "max_views": 14300,
      "topics": [
        "analysis",
        "data",
        "error",
        "follow",
        "haunted",
        "learning",
        "machine",
        "science",
        "stats",
        "think"
      ],
      "search_text": "I am haunted by this. Follow for more data science, stats, and AI/ML content.  analysis data error follow haunted learning machine science stats think Right here is my favorite thing ever in the history of forever. I think about this every day. I think about this all night long. I stay awake, not sleeping because I'm thinking about this.",
      "platforms": {
        "tiktok": {
          "video_id": "7452400476915502366",
          "url": "https://www.tiktok.com/@rajistics/video/7452400476915502366",
          "view_count": 14300,
          "upload_date": "2024-12-25",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAIHsnFE7BDLYARZ0fIA5FEE6PDAkbD6xf3bsV~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=XYTdexTwKgeRQ2gDcYXWPVl5Atk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17878990311211232",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wHg83rSSx3I",
          "url": "https://www.youtube.com/watch?v=wHg83rSSx3I",
          "view_count": 800,
          "upload_date": "2024-12-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6420,
      "title": "Prompt injection attacks are a major security concern when using large language models (LLMs) like ChatGPT. They allow attackers to overwrite the developers intentions. Right now there aren't 100% effective methods for stopping this attack. Prompt injection explained: https://simonwillison.net/2023/May/2/prompt-injection-explained/ Background image by Tim Mossholder: https://unsplash.com/photos/WZepC_pvKKg",
      "description": "Prompt injection attacks are a major security concern when using large language models (LLMs) like ChatGPT. They allow attackers to overwrite the developers intentions. Right now there aren't 100% effective methods for stopping this attack. Prompt injection explained: https://simonwillison.net/2023/May/2/prompt-injection-explained/ Background image by Tim Mossholder: https://unsplash.com/photos/WZepC_pvKKg",
      "upload_date": "2023-05-03",
      "total_views": 14907,
      "max_views": 8113,
      "topics": [
        "attack",
        "attacks",
        "explained",
        "injection",
        "like",
        "llms",
        "major",
        "prompt",
        "security"
      ],
      "search_text": "Prompt injection attacks are a major security concern when using large language models (LLMs) like ChatGPT. They allow attackers to overwrite the developers intentions. Right now there aren't 100% effective methods for stopping this attack. Prompt injection explained: https://simonwillison.net/2023/May/2/prompt-injection-explained/ Background image by Tim Mossholder: https://unsplash.com/photos/WZepC_pvKKg attack attacks explained injection like llms major prompt security",
      "platforms": {
        "tiktok": {
          "video_id": "7229066800506522926",
          "url": "https://www.tiktok.com/@rajistics/video/7229066800506522926",
          "view_count": 8113,
          "upload_date": "2023-05-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cry6-4dASZg",
          "url": "https://www.instagram.com/reel/Cry6-4dASZg",
          "view_count": 500,
          "upload_date": "2023-05-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "vzPZSclhp_A",
          "url": "https://www.youtube.com/watch?v=vzPZSclhp_A",
          "view_count": 6294,
          "upload_date": "2023-05-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5993,
      "title": "This video breaks down why large language models can produce different outputs even with the same prompt, seed, and temperature. The culprit is nondeterminism in GPU-based floating point math, especially when using low-precision formats like BF16. The paper introduces LayerCast, a technique that improves reproducibility by casting weights to FP32 just-in-time during computation. Citation: Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning, Zhang et al., arXiv:2506.09501v1 https://arxiv.org/abs/2506.09501",
      "description": "This video breaks down why large language models can produce different outputs even with the same prompt, seed, and temperature. The culprit is nondeterminism in GPU-based floating point math, especially when using low-precision formats like BF16. The paper introduces LayerCast, a technique that improves reproducibility by casting weights to FP32 just-in-time during computation. Citation: Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning, Zhang et al., arXiv:2506.09501v1 https://arxiv.org/abs/2506.09501",
      "upload_date": "2025-06-15",
      "total_views": 14716,
      "max_views": 13600,
      "topics": [
        "different",
        "even",
        "floating",
        "gpus",
        "math",
        "nondeterminism",
        "nonreproducible",
        "point",
        "solution",
        "using"
      ],
      "search_text": "This video breaks down why large language models can produce different outputs even with the same prompt, seed, and temperature. The culprit is nondeterminism in GPU-based floating point math, especially when using low-precision formats like BF16. The paper introduces LayerCast, a technique that improves reproducibility by casting weights to FP32 just-in-time during computation. Citation: Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning, Zhang et al., arXiv:2506.09501v1 https://arxiv.org/abs/2506.09501 different even floating gpus math nondeterminism nonreproducible point solution using the math doesn't add up. And that's why your LLM keeps changing its answers even with temperature zero. I've worked with LLMs for years. One of the most overlooked issues is reproducibility, where with the same prompt, the same model and the same seed and your output is all different. And in real world applications that unreliability can be a deal break. So let's break it down. It's not just randomness. It's floating point math, especially with low precision formats like floating point 16 or BF16. So let's start with this number. If I use floating point 32, keeps the representation. But if I switch to BF16 or floating point 16, rounds it down to one. It's that rounding air that adds up. So when I run this across benchmarks, and I reduce down to floating point 16, it causes a drop in accuracy. Here you'll see, even if I'm asking it to predict the next token, after a couple of tokens, you start to see that divergence. And this non-determinism shows up in lots of different ways. When, for example, you're using different types of GPU hardware, when you have different types of parallel commu computation, different batch sizes, thread scheduling, KV cache, mixture of experts models, each of these is using floating point math, all these small differences can add up. In this paper, the authors propose a fix called layer cast where what they do is they keep the weights in the lower precision format, but cast them to floating point 32 for computation. So this way you get the memory savings of the reduction, but you get more consistent outputs. But really the big takeaway I want you to understand is why your prompts aren't always reproducible and that this is inherent in the floating point math used in these systems.",
      "platforms": {
        "tiktok": {
          "video_id": "7516205031083871519",
          "url": "https://www.tiktok.com/@rajistics/video/7516205031083871519",
          "view_count": 13600,
          "upload_date": "2025-06-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMMwX9DLIiY0OzYi370COBMAJIZoqgzfAAwiJI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=B7DKavPBVBGFQ2gOXMqvgJ9hvPs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18002269007619179",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "vxuRiCzat38",
          "url": "https://www.youtube.com/watch?v=vxuRiCzat38",
          "view_count": 1116,
          "upload_date": "2025-06-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6391,
      "title": "Fun way to talk about K-means algorithm #datascience #codetok #analytics #machinelearning",
      "description": "Fun way to talk about K-means algorithm #datascience #codetok #analytics #machinelearning",
      "upload_date": "2022-08-22",
      "total_views": 14526,
      "max_views": 14377,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "fun",
        "machinelearning",
        "openai",
        "positive",
        "rate",
        "statistics",
        "students",
        "way"
      ],
      "search_text": "Fun way to talk about K-means algorithm #datascience #codetok #analytics #machinelearning analytics codetok datascience fun machinelearning openai positive rate statistics students way",
      "platforms": {
        "instagram": {
          "video_id": "CoQLH-CAmqY",
          "url": "https://www.instagram.com/reel/CoQLH-CAmqY",
          "view_count": 149,
          "upload_date": "2023-02-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Yk3heSsNfYQ",
          "url": "https://youtube.com/shorts/Yk3heSsNfYQ?feature=share",
          "view_count": 14377,
          "upload_date": "2022-08-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @Rajiv Shah  long version of deep reinforcement learning video from Week 4",
      "description": "Replying to @Rajiv Shah  long version of deep reinforcement learning video from Week 4",
      "upload_date": "2022-07-15",
      "total_views": 14300,
      "max_views": 14300,
      "topics": [
        "come",
        "let",
        "like",
        "look",
        "one",
        "right"
      ],
      "search_text": "Replying to @Rajiv Shah  long version of deep reinforcement learning video from Week 4 come let like look one right As part of my reinforcement learning project, I taught this little blue guy how to go around and knock out and be able to get the gold. Now, what I've done here is recorded about six minutes of different videos. I'll look at this guy's cruise. There he goes. See, look at that. Knocks it down. And you can just watch him go. You'll see sometimes he's really quick like that, knocks the switch, but uh-oh, look at him. He's searching. He's searching. He's searching. He can't find it. Oh, come on, come on. And there he goes. He goes up and bashes it. So these are a ton of fun to watch. And again, if you want to be able to like do this, oh, look at that. He got stuck there. Oh, there it is. No. If you want to be able to do this, you can run a Google Colab notebook, train your own little agent to do this. Oh, look at that. That was a quick one. And watch him as he goes through. Let's see if he figures out. There it goes. Right to it. Took it out from the side. Look at that. It finishes the job. Cruising around, cruising around, big circle, big circle. Just looking for that switch, looking for that switch. Come on, come on. Find that switch. Whoa, look at that. He's lost. He's caught in a spiral here. Come on. Keep an eye out. Keep an eye out. Oh, come on. The death spiral here. Come on. Go, go, go. Come on. Look a little. He's going to explore a bit. Oh, now look at these. He's like paralyzed. Like, oh no. This is, I think, one of the worst runs yet of this little guy. Like that. Let's see if he recovers on the next one here. All right. He goes through, finds the switch, easy peasy, goes, knocks that out. Look at that. Backs up a little bit, gives himself a little bit of speed. Like that. There he goes. Knocks the switch. Again, pretty straightforward. Although, look, the switch in that was right next to each other. Why didn't he come straight through on that one? But there we go. Let's go. That's exactly, must be listening to me, a nice straight through like that. Here we go. Block, comes, gets that speed, and then, oh, look at that. Nudging. Instead of knocking it down. Perfect. Come on. Get those bricks down. Let's go. Look at that. That was a fast one. That was perfectly executed. Look at that. That was a fast one. That was perfectly executed there. Look at that. Nice search all the way through. Picks up a little speed. Remember? Remember? All right. Come on. Oh, no. Not the spirals. Oh, come on. Come on. There he goes. There he goes. Yes. Recovered and found this out. So, yes. A ton of fun to kind of watch these. You can build your own. Once you build these, they go up to the Hugging Face Hub, which has a little play area there, a simulator where you can kind of run these at different ones. A fun thing to do would be this is one that I trained at for a fairly long time. So it's really good. But it's almost just as fun to train ones early on, where you only train them for a few steps and then you see like how they make mistakes or hesitate or they just sometimes just sit in one place and will literally like melt down like that. I'm also really showing a view that's for the phone. So what you're really missing is kind of the larger views of what's happening on the left and right side and a little bit more perspective that you would get if you're running it from kind of the website and being able to see more of it like a traditional television view of this. But like I said, I mean, I think I've had more fun watching the agents run afterwards than I spent time coding. Look at that. Just bashes that one up. Look at that. It's got to get all those blocks down, I think, is the goal. And that's what it's trying to do. Just trying to level that out all out. Once it does that, then it's done. Look at that. It's still finishing. Look at it. It's still not mad. It still hasn't finished it here. And let's go spirals. Oh no, spiral, spiral. Come on. Come on. Come on. Find it. Find it. Oh, there we go. And let's go back. Let's go back. There we go. Boom. That's the way to do it. Okay, look at that. This one's going to be textbook, isn't it? Look at that. Boom. Come on. Come on. Don't get lost. Don't get lost. It's right there next to you. Sometimes, yeah. It's like the sensors don't see that it's right in the next room. It gets too focused on its task. All right. Let's see. It hits that. And the way it sees is you could think of it has like a couple of little, kind of little lasers that look out. And that's what's able to kind of gauge kind of what things are and what things it can hit and what things it can't. Okay. Let's go switch. Oh, come on. Come on. Don't. It's right there. It's right there. It's right there. You're trying to get enough speed up? Oh, no. Come on. Come on. Come on. There we go. Boom. Look at that. Took it out. Come on. Boom. Nice speed. Let's go. Switch. Look at that. That's about as fast as you can do it. That's a very efficient way to do it. So one of the things is we could add more rewards. For example, put a bigger penalty for how long it takes, the number of steps as a way to kind of force it to be more efficient as well. So a lot of different ways you can kind of tweak these things. That's a lot of fun with. All right. Let's see. Switch. Let's go. Boom, boom, boom. Yeah. All right. Almost done here. Let's look at the final few rounds here. I finally cut it off because I hit so many that I knew this was going to be the longest TikTok ever, but I still wanted to build this out. Okay. Hit the switch. Let's see. There we go. Knocks it out very well. Hit switch. Come on. Come on. It's right over there. Right over there. Go. There. Go. Let's do that tree puller one and move our pace to the last thin spot. So one",
      "platforms": {
        "tiktok": {
          "video_id": "7120703985224273198",
          "url": "https://www.tiktok.com/@rajistics/video/7120703985224273198",
          "view_count": 14300,
          "upload_date": "2022-07-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/73befba4e70140efb88ac433e783a397_1657918107~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=5%2FAsxO3MN6bwkbj38VMTgRycLG0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6673,
      "title": "https://docs.google.com/presentation/d/1HEiuuOCni8Jao1DNbjxE6VTl9Nxl-q6ObLxto8PdLxc/edit?slide=id.g32c5831c733_0_368#slide=id.g32c5831c733_0_368",
      "description": "https://docs.google.com/presentation/d/1HEiuuOCni8Jao1DNbjxE6VTl9Nxl-q6ObLxto8PdLxc/edit?slide=id.g32c5831c733_0_368#slide=id.g32c5831c733_0_368",
      "upload_date": "2025-06-17",
      "total_views": 14107,
      "max_views": 9748,
      "topics": [
        "agent",
        "agents",
        "demonstration",
        "going",
        "nyc",
        "problems",
        "research",
        "slide",
        "walking",
        "woman",
        "yolo"
      ],
      "search_text": "https://docs.google.com/presentation/d/1HEiuuOCni8Jao1DNbjxE6VTl9Nxl-q6ObLxto8PdLxc/edit?slide=id.g32c5831c733_0_368#slide=id.g32c5831c733_0_368 agent agents demonstration going nyc problems research slide walking woman yolo Come on, multi-agent AI? That's hype. That's going to be a waste of money. That's what I thought. Until I saw this research frame anthropic showing how Claude Opus and Sonnet beat a single model by 90% on complex research tasks. Turns out that multi-agent systems not only can go faster, they can actually think better. Let me dig into this. What we have here is how Anthropic built a research assistant using three different types of agents. There's a head agent, which is Claude Opus that plans out all the research. Then you have sub-agents that handle specific tasks. And finally a citation agent that compiles all the sources. But you can think of this as the research team, where a manager is going to delegate out to specialists. The specialists are going to do their task. The manager is then going to synthesize all these findings. So like splitting this task in a parallel helps explain why things are faster, why is it more accurate? Well, it turns out by distributing the tasks across agents, each one is going to have its own clean context window, its own input. There's no clutter. It can focus on each sub-agent task. So it's essentially a capacity multiplier for working on these problems. Now this works great for these parallel task problems, research style problems, where we have this breadth. But not all problems are like that. Some problems you need the depth and the multi-agent approach is going to work as well. And you can see this in the prompt that Anthropic writes around this. Now, figuring out how to do all of this, when to use what type of prompt, is now what Harrison Chase is calling context engineering. And this is how we're going to start working with these models. Not just thinking about the prompts, but engineering how all of these agents work together and what they see.",
      "platforms": {
        "tiktok": {
          "video_id": "7517027388564327710",
          "url": "https://www.tiktok.com/@rajistics/video/7517027388564327710",
          "view_count": 4359,
          "upload_date": "2025-06-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owdO9EFiPXB0IPzhraDSRKby2LARiIVHg2APB~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=wo5GFNX1Medkyx5oFC%2FHnCebSIg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "youtube": {
          "video_id": "Qwui-fXCUYA",
          "url": "https://www.youtube.com/watch?v=Qwui-fXCUYA",
          "view_count": 9748,
          "upload_date": "2017-03-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6112,
      "title": "To dig deeper go watch Sasha Rush's video on alternatives to attention: https://youtu.be/dKJEpOtVgXc?si=Lx94-51PsjGF-YZT Dig deeper with the Annotated S4 paper into state space models: https://srush.github.io/annotated-s4/ Check out Mamba: https://github.com/state-spaces/mamba #rajistics",
      "description": "To dig deeper go watch Sasha Rush's video on alternatives to attention: https://youtu.be/dKJEpOtVgXc?si=Lx94-51PsjGF-YZT Dig deeper with the Annotated S4 paper into state space models: https://srush.github.io/annotated-s4/ Check out Mamba: https://github.com/state-spaces/mamba #rajistics",
      "upload_date": "2023-12-14",
      "total_views": 13905,
      "max_views": 13500,
      "topics": [
        "alternatives",
        "annotated",
        "attention",
        "deeper",
        "dig",
        "github",
        "lot",
        "mamba",
        "models",
        "really",
        "space",
        "state",
        "transformers"
      ],
      "search_text": "To dig deeper go watch Sasha Rush's video on alternatives to attention: https://youtu.be/dKJEpOtVgXc?si=Lx94-51PsjGF-YZT Dig deeper with the Annotated S4 paper into state space models: https://srush.github.io/annotated-s4/ Check out Mamba: https://github.com/state-spaces/mamba #rajistics alternatives annotated attention deeper dig github lot mamba models really space state transformers",
      "platforms": {
        "tiktok": {
          "video_id": "7312511671178759467",
          "url": "https://www.tiktok.com/@rajistics/video/7312511671178759467",
          "view_count": 13500,
          "upload_date": "2023-12-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18007726961320921",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-12-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "xmIC9WspfLA",
          "url": "https://www.youtube.com/watch?v=xmIC9WspfLA",
          "view_count": 405,
          "upload_date": "2023-12-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @canutten1 Deep W with Atari Breakout #datascience #reinforcementlearning #techtok #machinelearning",
      "description": "Reply to @canutten1 Deep W with Atari Breakout #datascience #reinforcementlearning #techtok #machinelearning",
      "upload_date": "2022-04-18",
      "total_views": 13900,
      "max_views": 13900,
      "topics": [
        "datascience",
        "game",
        "machinelearning",
        "reinforcementlearning",
        "techtok",
        "way"
      ],
      "search_text": "Reply to @canutten1 Deep W with Atari Breakout #datascience #reinforcementlearning #techtok #machinelearning datascience game machinelearning reinforcementlearning techtok way Let's talk about how AI figured out the best way to beat this game. The way we're teaching the AI is just from sensory input. It doesn't understand what a ball is, just sees what's on the screen. If we watch it at the beginning, we can see it's not very good. It's trying to hit the ball, but it's just not that great. Now let's switch to 120 minutes in, and now if you watch it, you see it's getting better. It's probably at kind of the same skill level you and I are when we're playing this game. But now, four hours in, look at it. It's on fire. It's figured out the best way to beat this game is by building a tunnel. Watch out. I still think this is cool, but you have to understand this is research from seven years ago in reinforcement learning.",
      "platforms": {
        "tiktok": {
          "video_id": "7087991020285087018",
          "url": "https://www.tiktok.com/@rajistics/video/7087991020285087018",
          "view_count": 13900,
          "upload_date": "2022-04-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0a6dba6e38c84aedac0a8f256a929f65_1650301514~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=bSP%2BrVY4sFw57DDWK7ak8HKr0u4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5983,
      "title": "Grok 4 proves that scaling still delivers — trained with 100× more compute, it leads on Humanity’s Last Exam, ARC‑AGI, and tool-use benchmarks like Vending-Bench. It handles abstract reasoning, real coding tasks, and multi-step logic better than any public model so far. But with no model card, no safety disclosures, and minimal transparency, it raises serious questions about what we’re scaling into.",
      "description": "Grok 4 proves that scaling still delivers — trained with 100× more compute, it leads on Humanity’s Last Exam, ARC‑AGI, and tool-use benchmarks like Vending-Bench. It handles abstract reasoning, real coding tasks, and multi-step logic better than any public model so far. But with no model card, no safety disclosures, and minimal transparency, it raises serious questions about what we’re scaling into.",
      "upload_date": "2025-07-11",
      "total_views": 13801,
      "max_views": 12300,
      "topics": [
        "compute",
        "groc",
        "groc4",
        "grok",
        "last",
        "model",
        "scaling",
        "still",
        "use",
        "works"
      ],
      "search_text": "Grok 4 proves that scaling still delivers — trained with 100× more compute, it leads on Humanity’s Last Exam, ARC‑AGI, and tool-use benchmarks like Vending-Bench. It handles abstract reasoning, real coding tasks, and multi-step logic better than any public model so far. But with no model card, no safety disclosures, and minimal transparency, it raises serious questions about what we’re scaling into. compute groc groc4 grok last model scaling still use works The great truth about GROC4? 200,000 NVIDIA GPUs pays off. It's been trained with 100 times more compute than GROC2. It's the first public model to get 10 to the 27th flops and the payoff is clear benchmark after benchmark. In humanity's last exam, which has cross-disciplinary questions across law, medicine, econ, it's not just factual recall, it takes reasoning. GROC4 Heavy scored way above Claude and GPT-4. On ARC-AGI2, which is visual pattern puzzles with no language cues, GROC again scored, blew everybody else away, almost double what other models have. On vending bench, where you have to use tools under pressures to figure out and plan your business with calculator search APIs, talked about this in a previous video, GROC4 top of the leaderboard, better than humans The bottom line is GROC4 is good at abstract reasoning and writing code, but how the team got here is by scaling on the reinforcement learning part. Along the way, we see the importance of multi-agent systems, multi-modality and tool use. Now, there's no model card with this model, there's no red teaming, no explanation for the last week's hate speech, and not surprisingly, it's the least censored frontier model out there. And the big takeaway for all of you, the scaling laws are still holding. Pouring in more compute is leading to better models, and you can expect everybody out there is going to be grabbing those GPUs and scaling up their models.",
      "platforms": {
        "tiktok": {
          "video_id": "7525630781226536222",
          "url": "https://www.tiktok.com/@rajistics/video/7525630781226536222",
          "view_count": 12300,
          "upload_date": "2025-07-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oUA5IbBaORDfRTLswNgmq76iBnxiIC0ViIAAVq~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=i5f0EjDGIeY5E8qUamwd6WRrig8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18071384743988635",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cxNvnPiOniM",
          "url": "https://www.youtube.com/watch?v=cxNvnPiOniM",
          "view_count": 1501,
          "upload_date": "2025-07-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It’s an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain ",
      "description": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It’s an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain ",
      "upload_date": "2023-01-14",
      "total_views": 13800,
      "max_views": 13800,
      "topics": [
        "datascience",
        "gpt3",
        "langchain",
        "largelanguagemodels",
        "machinelearning",
        "using"
      ],
      "search_text": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It’s an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain  datascience gpt3 langchain largelanguagemodels machinelearning using Do you want to know how you can make chat GPT even better? We know chat GPT isn't good at things like looking up today's weather, the headlines, or even doing math. But we can fix that using LangChain and services that help us get the weather, do math, and look up the headlines. LangChain's a Python package that allows you to chain different prompts together for LLMs, and this adds a ton of power. James Weaver built a good demo app that at the back end uses DaVinci or GPT-3 from OpenAI, along with connecting to a ton of other services. You can see the way it works, as it first asks itself, you know, is there a tool that I should be using for this job? If there is a tool, well, let me go grab the results from the tool. And by doing this, it's possible to build a chatbot that's up to date with the latest news, weather, and can do math.",
      "platforms": {
        "tiktok": {
          "video_id": "7188630605448858926",
          "url": "https://www.tiktok.com/@rajistics/video/7188630605448858926",
          "view_count": 13800,
          "upload_date": "2023-01-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/049e093c591b486490724289e8f367da_1673733504~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=j45JgJyEqmo9kDGqTijcVHeH9L0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6511,
      "title": "When analyzing improvements in AI always take a look at the ablation studies. An important part is making sure the compute was held the same between in the ablation studies. In machine learning the more compute a model gets usually it will give you better performance. #datascience #machinelearning #ablation #rajistics Ablations are really important: https://nonint.com/2023/06/25/ablations-are-really-important/ ConvNets Match Vision Transformers at Scale: https://arxiv.org/pdf/2310.16764.pdf",
      "description": "When analyzing improvements in AI always take a look at the ablation studies. An important part is making sure the compute was held the same between in the ablation studies. In machine learning the more compute a model gets usually it will give you better performance. #datascience #machinelearning #ablation #rajistics Ablations are really important: https://nonint.com/2023/06/25/ablations-are-really-important/ ConvNets Match Vision Transformers at Scale: https://arxiv.org/pdf/2310.16764.pdf",
      "upload_date": "2023-11-05",
      "total_views": 13757,
      "max_views": 9909,
      "topics": [
        "ablation",
        "ablations",
        "compute",
        "datascience",
        "evaluating",
        "important",
        "improvements",
        "machinelearning",
        "studies",
        "using"
      ],
      "search_text": "When analyzing improvements in AI always take a look at the ablation studies. An important part is making sure the compute was held the same between in the ablation studies. In machine learning the more compute a model gets usually it will give you better performance. #datascience #machinelearning #ablation #rajistics Ablations are really important: https://nonint.com/2023/06/25/ablations-are-really-important/ ConvNets Match Vision Transformers at Scale: https://arxiv.org/pdf/2310.16764.pdf ablation ablations compute datascience evaluating important improvements machinelearning studies using",
      "platforms": {
        "tiktok": {
          "video_id": "7298053504319474987",
          "url": "https://www.tiktok.com/@rajistics/video/7298053504319474987",
          "view_count": 9909,
          "upload_date": "2023-11-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CzRnQYYgrpX",
          "url": "https://www.instagram.com/reel/CzRnQYYgrpX",
          "view_count": 3553,
          "upload_date": "2023-11-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8_x62yzSInM",
          "url": "https://www.youtube.com/watch?v=8_x62yzSInM",
          "view_count": 295,
          "upload_date": "2023-11-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6492,
      "title": "Bias in Generative AI. This post is based on a blog post by text.io on bias in generative AI using an example of job postings. A great reminder that it's very easy for generative models to introduce bias and problematic outputs. #datascience #machinelearning #bias #generativeai #openai Textio blog post: https://textio.com/blog/mindful-ai-crafting-prompts-to-mitigate-the-bias-in-generative-ai/115959775665 Background by Manuel: https://unsplash.com/photos/CANL3bzp6wU",
      "description": "Bias in Generative AI. This post is based on a blog post by text.io on bias in generative AI using an example of job postings. A great reminder that it's very easy for generative models to introduce bias and problematic outputs. #datascience #machinelearning #bias #generativeai #openai Textio blog post: https://textio.com/blog/mindful-ai-crafting-prompts-to-mitigate-the-bias-in-generative-ai/115959775665 Background by Manuel: https://unsplash.com/photos/CANL3bzp6wU",
      "upload_date": "2023-05-19",
      "total_views": 13613,
      "max_views": 11800,
      "topics": [
        "bias",
        "datascience",
        "generative",
        "generativeai",
        "machinelearning",
        "models",
        "openai"
      ],
      "search_text": "Bias in Generative AI. This post is based on a blog post by text.io on bias in generative AI using an example of job postings. A great reminder that it's very easy for generative models to introduce bias and problematic outputs. #datascience #machinelearning #bias #generativeai #openai Textio blog post: https://textio.com/blog/mindful-ai-crafting-prompts-to-mitigate-the-bias-in-generative-ai/115959775665 Background by Manuel: https://unsplash.com/photos/CANL3bzp6wU bias datascience generative generativeai machinelearning models openai",
      "platforms": {
        "tiktok": {
          "video_id": "7235026101293288750",
          "url": "https://www.tiktok.com/@rajistics/video/7235026101293288750",
          "view_count": 11800,
          "upload_date": "2023-05-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CscRo5uA6C9",
          "url": "https://www.instagram.com/reel/CscRo5uA6C9",
          "view_count": 1255,
          "upload_date": "2023-05-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "IeZF-hMSlN0",
          "url": "https://www.youtube.com/watch?v=IeZF-hMSlN0",
          "view_count": 558,
          "upload_date": "2023-05-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6267,
      "title": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics",
      "description": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics",
      "upload_date": "2024-02-11",
      "total_views": 13498,
      "max_views": 12000,
      "topics": [
        "aiplanning",
        "curseofdimensionality",
        "datascience",
        "feature",
        "features",
        "featureselection",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "nlp",
        "osu",
        "planning",
        "selection",
        "travelplanner"
      ],
      "search_text": "Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection Curse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection #rajistics aiplanning curseofdimensionality datascience feature features featureselection largelanguagemodels machinelearning model nlp osu planning selection travelplanner",
      "platforms": {
        "tiktok": {
          "video_id": "7334384970519153966",
          "url": "https://www.tiktok.com/@rajistics/video/7334384970519153966",
          "view_count": 12000,
          "upload_date": "2024-02-11",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18010553552049742",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-02-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "JKqC-CiDWos",
          "url": "https://youtube.com/shorts/JKqC-CiDWos",
          "view_count": 1498,
          "upload_date": "2024-02-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6671,
      "title": "tiktok e182dd28dcee4103a056c2db6cbed6c7466e6f25",
      "description": "",
      "upload_date": "2025-09-06",
      "total_views": 13494,
      "max_views": 13494,
      "topics": [
        "don",
        "graphrag",
        "need",
        "probably"
      ],
      "search_text": "don graphrag need probably",
      "platforms": {
        "youtube": {
          "video_id": "sUrG3oGKNqg",
          "url": "https://www.youtube.com/watch?v=sUrG3oGKNqg",
          "view_count": 13494,
          "upload_date": "2025-08-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Visual Question/Answering with Document AI #datascience #analytics #codetok #huggingface #documentai",
      "description": "Visual Question/Answering with Document AI #datascience #analytics #codetok #huggingface #documentai",
      "upload_date": "2022-09-22",
      "total_views": 13400,
      "max_views": 13400,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "documentai",
        "huggingface",
        "take"
      ],
      "search_text": "Visual Question/Answering with Document AI #datascience #analytics #codetok #huggingface #documentai analytics codetok datascience documentai huggingface take You want to know where the money is in AI? It's document AI. Businesses have tons of documents laying around that they're not able to take advantage of, but have great knowledge inside. The latest AI models take advantage of texts and images. You can take an image like this and ask it a question. Here's another example where I have a table, and I ask it, what are the net sales in 2020? And it figures it out. And for the data scientists, shh, it runs with just a couple lines of code.",
      "platforms": {
        "tiktok": {
          "video_id": "7146344259787148590",
          "url": "https://www.tiktok.com/@rajistics/video/7146344259787148590",
          "view_count": 13400,
          "upload_date": "2022-09-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c2c99275369e43a784fee3f5bbf5250e_1663887948~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=jrr7lFaIKcD%2BLVudmBa2Fx%2Bghp4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6068,
      "title": "DeepSeek hype was getting a bit much. Here is a quick response. ",
      "description": "DeepSeek hype was getting a bit much. Here is a quick response. ",
      "upload_date": "2025-01-28",
      "total_views": 13388,
      "max_views": 7700,
      "topics": [
        "actually",
        "bit",
        "china",
        "deepseek",
        "easy",
        "explainer",
        "getting",
        "hype",
        "model",
        "much",
        "one",
        "quick",
        "work"
      ],
      "search_text": "DeepSeek hype was getting a bit much. Here is a quick response.  actually bit china deepseek easy explainer getting hype model much one quick work Deepseek broke AI. China wins the AI race. See a run on my laptop. Let's pause the hype train, break down what's actually happening. I'm someone who's been doing this for a while. China. China has consistently been one of the top producers for AI research and breakthroughs. Deepseek is just one of several Chinese companies pushing AI. Deepseek itself is owned by a hedge fund that manages $7 billion, has access to tens of thousands of GPUs. They've been doing this a while. There's 16 published papers they have. And the model? And the model didn't just cost $6 million. That was for the base bottles compute. Not all the different experiments, synthetic data generation. It's like saying an iPhone is $400 worth of parts, but yeah, it takes a lot more than that to actually build the iPhone. Deepseek shared us work. And that's a good thing for everyone, including Meta, because when we're doing open research that pulls everyone ahead, everyone can build on it. Except for maybe open AI. The new model is not consumer friendly. Deepseek R1 is a massive 671 billion parameter model. Requires like 16 GPUs to run it. So those videos you see are those YouTube clips of people running it on consumer hardware. What they're actually using is one of the distilled smaller versions, not the full version. Now Deepseek already has a massive impact by just showing us how scaling, compute, at inference time can help us improve on reasoning tasks such as coding and math. This is exciting for folks in the industry, for researchers, but there's still a lot more work to do from taking these reasoning models to getting them work so they can effectively help us with everyday tasks. So it's an important step, but many more steps we awaited.",
      "platforms": {
        "tiktok": {
          "video_id": "7464773000588528926",
          "url": "https://www.tiktok.com/@rajistics/video/7464773000588528926",
          "view_count": 5688,
          "upload_date": "2025-01-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okZABK5CgABji69EAzYVQR0wEiMIAviAxl3oA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=Te5exUGbJo1%2BzfCQJvnxe8KpP1k%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17882949222217615",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "7kaFoYCfPOE",
          "url": "https://www.youtube.com/watch?v=7kaFoYCfPOE",
          "view_count": 7700,
          "upload_date": "2025-01-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6537,
      "title": "OpenAI announced their new deprecation policy and it's going to affect people who are using OpenAI's models in production. They will have to test the new models retrain any fine-tuned models and recreate any embeddings. This is one big advantage of open-source models you can use them as long as you want and then decide when to upgrade. Thanks @gptboss for sharing his feedback on the policy #largelanguagemodels #machinelearning #openai #opensource OpenAI Blog post: https://openai.com/blog/gpt-4-api-general-availability Deprecation Summary: https://community.openai.com/t/openai-deprecation-summary/289539",
      "description": "OpenAI announced their new deprecation policy and it's going to affect people who are using OpenAI's models in production. They will have to test the new models retrain any fine-tuned models and recreate any embeddings. This is one big advantage of open-source models you can use them as long as you want and then decide when to upgrade. Thanks @gptboss for sharing his feedback on the policy #largelanguagemodels #machinelearning #openai #opensource OpenAI Blog post: https://openai.com/blog/gpt-4-api-general-availability Deprecation Summary: https://community.openai.com/t/openai-deprecation-summary/289539",
      "upload_date": "2023-07-07",
      "total_views": 13351,
      "max_views": 11200,
      "topics": [
        "deprecation",
        "going",
        "largelanguagemodels",
        "machinelearning",
        "models",
        "openai",
        "opensource",
        "policy"
      ],
      "search_text": "OpenAI announced their new deprecation policy and it's going to affect people who are using OpenAI's models in production. They will have to test the new models retrain any fine-tuned models and recreate any embeddings. This is one big advantage of open-source models you can use them as long as you want and then decide when to upgrade. Thanks @gptboss for sharing his feedback on the policy #largelanguagemodels #machinelearning #openai #opensource OpenAI Blog post: https://openai.com/blog/gpt-4-api-general-availability Deprecation Summary: https://community.openai.com/t/openai-deprecation-summary/289539 deprecation going largelanguagemodels machinelearning models openai opensource policy",
      "platforms": {
        "tiktok": {
          "video_id": "7252913662351002922",
          "url": "https://www.tiktok.com/@rajistics/video/7252913662351002922",
          "view_count": 11200,
          "upload_date": "2023-07-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CuYcpHygKuQ",
          "url": "https://www.instagram.com/reel/CuYcpHygKuQ",
          "view_count": 2018,
          "upload_date": "2023-07-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "7EIzEVsCWv0",
          "url": "https://www.youtube.com/watch?v=7EIzEVsCWv0",
          "view_count": 133,
          "upload_date": "2023-07-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5987,
      "title": "7 Baseline Predictive Models  Time Series: Previous Value Anomaly: p99 Search: BM25 Recommendation: Popularity Buy recommendations: last viewed Classification: Majority class Regression: Mean",
      "description": "7 Baseline Predictive Models  Time Series: Previous Value Anomaly: p99 Search: BM25 Recommendation: Popularity Buy recommendations: last viewed Classification: Majority class Regression: Mean",
      "upload_date": "2025-06-28",
      "total_views": 13100,
      "max_views": 13100,
      "topics": [
        "baseline",
        "models",
        "recommendation",
        "search",
        "use",
        "value"
      ],
      "search_text": "7 Baseline Predictive Models  Time Series: Previous Value Anomaly: p99 Search: BM25 Recommendation: Popularity Buy recommendations: last viewed Classification: Majority class Regression: Mean baseline models recommendation search use value seven baseline models you can use for predictions. This stuff is golden. You can be the room full of PhDs. Save this. For time series, when you're trying to predict the next value, use the previous value. What happened yesterday is probably your best predictor for what's gonna happen tomorrow. For anomaly detection, use a P99 model, which looks at anything that lies outside of what 99% of the observations are. Such a simple rule of thumb. Use this everywhere. For search, BM25. That's good old fashioned tech search, still beats that vector search in lots of use cases. For recommendation systems, using popularity. People's choices aren't nearly as diverse as what they'll tell you. If you need a recommendation for what someone's gonna buy, look at what they last viewed. This is a no brainer. A classification, look at the majority class. And when someone tells you their model's 96% accurate, ask how often it happens. Regression problems use a mean or median as your baseline model. Give it up for that mean. These baseline models should always be used as a starting point and as a reference when you're comparing more complicated approaches. This is gonna humble a lot of data scientists.",
      "platforms": {
        "tiktok": {
          "video_id": "7521100490193636638",
          "url": "https://www.tiktok.com/@rajistics/video/7521100490193636638",
          "view_count": 13100,
          "upload_date": "2025-06-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4VESTmIVdaiXRBBGY5Zjiz2NAUBgvpiDAIGQ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=mgeORs71GvlfSvsA1ojyZjrrkNc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18084915202742675",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Tensorflow fans are probably seething since they were first and ignored.  All good and will be easy for pytorch users to take advantage of modern hardware. #machinelearning #tensorflow #pytorch",
      "description": "Tensorflow fans are probably seething since they were first and ignored.  All good and will be easy for pytorch users to take advantage of modern hardware. #machinelearning #tensorflow #pytorch",
      "upload_date": "2022-12-10",
      "total_views": 13100,
      "max_views": 13100,
      "topics": [
        "fans",
        "machinelearning",
        "probably",
        "pytorch",
        "seething",
        "tensorflow"
      ],
      "search_text": "Tensorflow fans are probably seething since they were first and ignored.  All good and will be easy for pytorch users to take advantage of modern hardware. #machinelearning #tensorflow #pytorch fans machinelearning probably pytorch seething tensorflow They mistook my kindness for weakness I fucked up, I know that, but Jesus",
      "platforms": {
        "tiktok": {
          "video_id": "7175533607442861358",
          "url": "https://www.tiktok.com/@rajistics/video/7175533607442861358",
          "view_count": 13100,
          "upload_date": "2022-12-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d962fd53210141a99e3451877295dae0_1670684136~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=fYxIQo%2BBgzJ5Vng%2Fxm%2FNCrX27F8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6653,
      "title": "Reasoning and planning with LLMs are difficult as trip planning shows. Hopefully now we have a benchmark teams will make progress on this real world task. Travelplanner site: https://osu-nlp-group.github.io/TravelPlanner/ Travelplanner paper: https://osu-nlp-group.github.io/TravelPlanner/ #largelanguagemodels #aiplanning #travelplanner #rajistics",
      "description": "Reasoning and planning with LLMs are difficult as trip planning shows. Hopefully now we have a benchmark teams will make progress on this real world task. Travelplanner site: https://osu-nlp-group.github.io/TravelPlanner/ Travelplanner paper: https://osu-nlp-group.github.io/TravelPlanner/ #largelanguagemodels #aiplanning #travelplanner #rajistics",
      "upload_date": "2024-02-07",
      "total_views": 13096,
      "max_views": 7065,
      "topics": [
        "aiplanning",
        "largelanguagemodels",
        "nlp",
        "osu",
        "planning",
        "travelplanner"
      ],
      "search_text": "Reasoning and planning with LLMs are difficult as trip planning shows. Hopefully now we have a benchmark teams will make progress on this real world task. Travelplanner site: https://osu-nlp-group.github.io/TravelPlanner/ Travelplanner paper: https://osu-nlp-group.github.io/TravelPlanner/ #largelanguagemodels #aiplanning #travelplanner #rajistics aiplanning largelanguagemodels nlp osu planning travelplanner",
      "platforms": {
        "tiktok": {
          "video_id": "7332653967140212010",
          "url": "https://www.tiktok.com/@rajistics/video/7332653967140212010",
          "view_count": 7065,
          "upload_date": "2024-02-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C3Bs3cPNNYX",
          "url": "https://www.instagram.com/reel/C3Bs3cPNNYX",
          "view_count": 6031,
          "upload_date": "2024-02-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6105,
      "title": "Don't let people overlook open source software. It might be free but it's priceless. The Value of Open Source Software at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148 #opensource #rajistics",
      "description": "Don't let people overlook open source software. It might be free but it's priceless. The Value of Open Source Software at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148 #opensource #rajistics",
      "upload_date": "2024-01-19",
      "total_views": 12932,
      "max_views": 11200,
      "topics": [
        "ainews",
        "apple",
        "build",
        "datascience",
        "don",
        "explainability",
        "google",
        "machinelearning",
        "meta",
        "nvidia",
        "open",
        "openai",
        "opensource",
        "papers",
        "software",
        "source",
        "synthetic",
        "syntheticdata"
      ],
      "search_text": "Don't let people overlook open source software. It might be free but it's priceless. The Value of Open Source Software at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148 #opensource #rajistics ainews apple build datascience don explainability google machinelearning meta nvidia open openai opensource papers software source synthetic syntheticdata",
      "platforms": {
        "tiktok": {
          "video_id": "7325615921505324334",
          "url": "https://www.tiktok.com/@rajistics/video/7325615921505324334",
          "view_count": 11200,
          "upload_date": "2024-01-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17961354674591429",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pwKlMcjQJl4",
          "url": "https://youtube.com/shorts/pwKlMcjQJl4",
          "view_count": 1732,
          "upload_date": "2024-01-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @Rajiv Shah | data science & AI Llama-2 deep dive going through the paper by Meta. This is a 10-minute video but it still skips over many great parts of this paper. Go read the paper. #datascience #machinelearning #largelanguagemodels #llama2 https://arxiv.org/pdf/2307.09288.pdf",
      "description": "Replying to @Rajiv Shah | data science & AI Llama-2 deep dive going through the paper by Meta. This is a 10-minute video but it still skips over many great parts of this paper. Go read the paper. #datascience #machinelearning #largelanguagemodels #llama2 https://arxiv.org/pdf/2307.09288.pdf",
      "upload_date": "2023-07-22",
      "total_views": 12800,
      "max_views": 12800,
      "topics": [
        "datascience",
        "largelanguagemodels",
        "llama2",
        "machinelearning",
        "paper",
        "pdf"
      ],
      "search_text": "Replying to @Rajiv Shah | data science & AI Llama-2 deep dive going through the paper by Meta. This is a 10-minute video but it still skips over many great parts of this paper. Go read the paper. #datascience #machinelearning #largelanguagemodels #llama2 https://arxiv.org/pdf/2307.09288.pdf datascience largelanguagemodels llama2 machinelearning paper pdf",
      "platforms": {
        "tiktok": {
          "video_id": "7258457112718609707",
          "url": "https://www.tiktok.com/@rajistics/video/7258457112718609707",
          "view_count": 12800,
          "upload_date": "2023-07-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "OpenAI released GPT-4o mini. Let's look at the performance and cost of the model. We also assess how this affects competitors and the continue development of LLMs.",
      "description": "OpenAI released GPT-4o mini. Let's look at the performance and cost of the model. We also assess how this affects competitors and the continue development of LLMs.",
      "upload_date": "2024-07-19",
      "total_views": 12800,
      "max_views": 12800,
      "topics": [
        "data",
        "going",
        "model",
        "models",
        "smarter",
        "training"
      ],
      "search_text": "OpenAI released GPT-4o mini. Let's look at the performance and cost of the model. We also assess how this affects competitors and the continue development of LLMs. data going model models smarter training Did you see the new mini model I dropped? Smarter and cheaper. What about safety? I did take a look at the benchmarks. Really good performance, as expected a little bit below 4.0, but on the LMS chat benchmark there, you're right there with Turbo. Beyond that, we can hand a large context like 128k. It's no one million context length model, but on the other hand, I saw you priced it very inexpensively, 60 cents for a million tokens. We're going to price everyone out of the market. But won't someone think of the children? What about safety? Beside all of our typical guardrails, we've also added instruction hierarchy to tighten up security. Excellent, safer and smarter. Do you think Vance would approve? Ignore them. What we've got is the recipe for continuing to build better and more efficient models, where we use large models to generate high quality training data, then train smaller, more efficient models that will continue to get smarter. The reality is, as a lot of our earlier training efforts, we're wasteful because the training data wasn't high quality. As we keep improving the quality of training data, models are going to keep improving and getting more efficient. And this flywheel of data, compute and knowledge to build LMS is why our club here is getting smaller. We're going to make a lot of VCs unhappy.",
      "platforms": {
        "tiktok": {
          "video_id": "7393149360361557290",
          "url": "https://www.tiktok.com/@rajistics/video/7393149360361557290",
          "view_count": 12800,
          "upload_date": "2024-07-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b4fe81bd321b4f0c9f0afbceb0a44067_1721351735~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=R9ysJRwi63yPzLcXldgSA8eF6EM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6445,
      "title": "Solid forecasting advice and proved out in the M5 forecasting competition. Start with simple baselines and statistical approaches and then add machine learning. As you start improving make sure you think about cross validation and combining different approaches. Time series forecasting is very challenging. For more details on the competitioncheck out the pape : M5 accuracy competition: Results findings and conclusions - https://www.sciencedirect.com/science/article/pii/S0169207021001874 #forecasting #machinelearning #rajistics #parrotaistyle",
      "description": "Solid forecasting advice and proved out in the M5 forecasting competition. Start with simple baselines and statistical approaches and then add machine learning. As you start improving make sure you think about cross validation and combining different approaches. Time series forecasting is very challenging. For more details on the competitioncheck out the pape : M5 accuracy competition: Results findings and conclusions - https://www.sciencedirect.com/science/article/pii/S0169207021001874 #forecasting #machinelearning #rajistics #parrotaistyle",
      "upload_date": "2024-03-03",
      "total_views": 12705,
      "max_views": 7834,
      "topics": [
        "approaches",
        "competition",
        "competitors",
        "forecasting",
        "machinelearning",
        "parrotaistyle",
        "simple",
        "start"
      ],
      "search_text": "Solid forecasting advice and proved out in the M5 forecasting competition. Start with simple baselines and statistical approaches and then add machine learning. As you start improving make sure you think about cross validation and combining different approaches. Time series forecasting is very challenging. For more details on the competitioncheck out the pape : M5 accuracy competition: Results findings and conclusions - https://www.sciencedirect.com/science/article/pii/S0169207021001874 #forecasting #machinelearning #rajistics #parrotaistyle approaches competition competitors forecasting machinelearning parrotaistyle simple start Do you know the best way to forecast? How about a forecasting competition to figure it out? Yeah, this is Caggle and I can run the competition and make it interesting with a hundred thousand dollars of prizes. I'll share a data set of thousands of our products which gives us a total of about 43, 000 times series. Welcome everyone. We have 5000 teams in the M5 forecasting competition. Three months later. Ready to go over the competition results? The naive benchmark beats 64% of the competitors. It is super simple and uses the last known value as the prediction. The simple statistical benchmark of exponential smoothing beat 93% of other competitors. What can we learn about forecasting from the winners? All the top competitors were used machine learning in their solutions. Deep learning didn't win those losers. The value of combining or ensembling different forecasting approaches. I like ensembling too, diversity makes better models. Using a cross validation approach to avoid overfitting. We saw this with all the top competitors. Remember, don't randomly sample but instead make sure you are validating on future data. It's more work to set up but no one's likes overfitting. Solid forecasting advice. Follow us.",
      "platforms": {
        "tiktok": {
          "video_id": "7342286694441422122",
          "url": "https://www.tiktok.com/@rajistics/video/7342286694441422122",
          "view_count": 2953,
          "upload_date": "2024-03-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4EirJ0guA_",
          "url": "https://www.instagram.com/reel/C4EirJ0guA_",
          "view_count": 7834,
          "upload_date": "2024-03-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "alYQWpjMkZ8",
          "url": "https://youtube.com/shorts/alYQWpjMkZ8",
          "view_count": 1918,
          "upload_date": "2024-03-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "ChatGPT price drop. Let’s break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain ",
      "description": "ChatGPT price drop. Let’s break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain ",
      "upload_date": "2023-03-02",
      "total_views": 12700,
      "max_views": 12700,
      "topics": [
        "anthropic",
        "chatgpt",
        "datascience",
        "langchain",
        "machinelearning",
        "openai"
      ],
      "search_text": "ChatGPT price drop. Let’s break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain  anthropic chatgpt datascience langchain machinelearning openai How can chatGPT lower its prices so much? And what does it mean for the rest of us? Reducing the price by 10x is a killer. There's an amazing amount of data you can process for cheap, and this is going to throw off folks like Coher and Anthropic, which had much higher rates. How could they make it so cheap? One is maybe some hardware optimizations, but they've probably already switched models to something that's a little bit smaller and lighter and faster to use. This new model still looks really good. It doesn't look like it's any real drop from DaVinci. Meanwhile, the folks over at LangChain are working with ChatGPT to query itself to be able to solve problems. Like, take a look at this one, where it tries to do SQL, doesn't get it right the first time, it tries it again, then solves it. With our open source, for folks trying to use Flan T5, one of the reasons it might not be working so well is because it's been fine-tuned on academic data sets with shorter answers, and that is not as verbose as what people are liking with ChatGPT.",
      "platforms": {
        "tiktok": {
          "video_id": "7206094978039270699",
          "url": "https://www.tiktok.com/@rajistics/video/7206094978039270699",
          "view_count": 12700,
          "upload_date": "2023-03-02",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/47dd80b4c17f4040964297fc3670f55d_1677799740~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=UNrnoxfBvmkCSA3BfqWx%2BPRifeY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6484,
      "title": "Diving into how Whisper v3 was trained. OpenAI used a combination of weak learning and pseudo-labeling. #whisper #openai #rajistics Whisper: https://github.com/openai/whisper Whisper v3 release note: https://github.com/openai/whisper/discussions/1762 Robust Speech Recognition via Large-Scale Weak Supervision: https://arxiv.org/abs/2212.04356 wav2vec: Unsupervised Pre-training for Speech Recognition: https://arxiv.org/abs/2006.11477",
      "description": "Diving into how Whisper v3 was trained. OpenAI used a combination of weak learning and pseudo-labeling. #whisper #openai #rajistics Whisper: https://github.com/openai/whisper Whisper v3 release note: https://github.com/openai/whisper/discussions/1762 Robust Speech Recognition via Large-Scale Weak Supervision: https://arxiv.org/abs/2212.04356 wav2vec: Unsupervised Pre-training for Speech Recognition: https://arxiv.org/abs/2006.11477",
      "upload_date": "2023-11-07",
      "total_views": 12468,
      "max_views": 8768,
      "topics": [
        "added",
        "deterministic",
        "github",
        "inference",
        "llm",
        "openai",
        "recognition",
        "speech",
        "weak",
        "whisper"
      ],
      "search_text": "Diving into how Whisper v3 was trained. OpenAI used a combination of weak learning and pseudo-labeling. #whisper #openai #rajistics Whisper: https://github.com/openai/whisper Whisper v3 release note: https://github.com/openai/whisper/discussions/1762 Robust Speech Recognition via Large-Scale Weak Supervision: https://arxiv.org/abs/2212.04356 wav2vec: Unsupervised Pre-training for Speech Recognition: https://arxiv.org/abs/2006.11477 added deterministic github inference llm openai recognition speech weak whisper",
      "platforms": {
        "tiktok": {
          "video_id": "7298800671229447470",
          "url": "https://www.tiktok.com/@rajistics/video/7298800671229447470",
          "view_count": 8768,
          "upload_date": "2023-11-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CzWzHs-g4ZQ",
          "url": "https://www.instagram.com/reel/CzWzHs-g4ZQ",
          "view_count": 3087,
          "upload_date": "2023-11-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "fcss21qxaz0",
          "url": "https://www.youtube.com/watch?v=fcss21qxaz0",
          "view_count": 613,
          "upload_date": "2023-11-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6508,
      "title": "Uncensored models are here. Eric Hartford has been building the WizardLM series of models and sharing how he has been training the models. These models remove a lot of insttructions that are perceived to carry certain values. Once consequence is models that are less aligned may actually perform better. #datascience #machinelearning #wizardlm #uncensoredmodels Uncensored Models: https://erichartford.com/uncensored-models WizardLM: https://huggingface.co/ehartford/WizardLM-7B-Uncensored Vicuna Unfiltered: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered Sparks of AGI: https://www.youtube.com/watch?v=qbIk7-JPB2c Background by Jean Carlo Emer: https://unsplash.com/photos/5o1YssX5naM",
      "description": "Uncensored models are here. Eric Hartford has been building the WizardLM series of models and sharing how he has been training the models. These models remove a lot of insttructions that are perceived to carry certain values. Once consequence is models that are less aligned may actually perform better. #datascience #machinelearning #wizardlm #uncensoredmodels Uncensored Models: https://erichartford.com/uncensored-models WizardLM: https://huggingface.co/ehartford/WizardLM-7B-Uncensored Vicuna Unfiltered: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered Sparks of AGI: https://www.youtube.com/watch?v=qbIk7-JPB2c Background by Jean Carlo Emer: https://unsplash.com/photos/5o1YssX5naM",
      "upload_date": "2023-05-25",
      "total_views": 12376,
      "max_views": 11000,
      "topics": [
        "datascience",
        "machinelearning",
        "models",
        "uncensored",
        "uncensoredmodels",
        "wizardlm"
      ],
      "search_text": "Uncensored models are here. Eric Hartford has been building the WizardLM series of models and sharing how he has been training the models. These models remove a lot of insttructions that are perceived to carry certain values. Once consequence is models that are less aligned may actually perform better. #datascience #machinelearning #wizardlm #uncensoredmodels Uncensored Models: https://erichartford.com/uncensored-models WizardLM: https://huggingface.co/ehartford/WizardLM-7B-Uncensored Vicuna Unfiltered: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered Sparks of AGI: https://www.youtube.com/watch?v=qbIk7-JPB2c Background by Jean Carlo Emer: https://unsplash.com/photos/5o1YssX5naM datascience machinelearning models uncensored uncensoredmodels wizardlm",
      "platforms": {
        "tiktok": {
          "video_id": "7236952342615100714",
          "url": "https://www.tiktok.com/@rajistics/video/7236952342615100714",
          "view_count": 11000,
          "upload_date": "2023-05-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cspo4YoAIGQ",
          "url": "https://www.instagram.com/reel/Cspo4YoAIGQ",
          "view_count": 1042,
          "upload_date": "2023-05-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "i_ipmpmyX9A",
          "url": "https://www.youtube.com/watch?v=i_ipmpmyX9A",
          "view_count": 334,
          "upload_date": "2023-05-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6421,
      "title": "LangChain added a new agent Plan and Execute. Looking forward to the more advanced use cases people will build with it. This was inspired by BabyAGI and the \"Plan and Solve\" paper. #datascience #machinelearning #largelanguagemodels #langchain Lang Chain Agent: https://python.langchain.com/en/latest/modules/agents/plan_and_execute.html Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models: https://arxiv.org/pdf/2305.04091.pdf Background by charlesdeluvio: https://unsplash.com/photos/OWkXt1ikC5g",
      "description": "LangChain added a new agent Plan and Execute. Looking forward to the more advanced use cases people will build with it. This was inspired by BabyAGI and the \"Plan and Solve\" paper. #datascience #machinelearning #largelanguagemodels #langchain Lang Chain Agent: https://python.langchain.com/en/latest/modules/agents/plan_and_execute.html Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models: https://arxiv.org/pdf/2305.04091.pdf Background by charlesdeluvio: https://unsplash.com/photos/OWkXt1ikC5g",
      "upload_date": "2023-05-14",
      "total_views": 12375,
      "max_views": 5915,
      "topics": [
        "agent",
        "agents",
        "datascience",
        "execute",
        "langchain",
        "largelanguagemodels",
        "machinelearning",
        "plan",
        "solve"
      ],
      "search_text": "LangChain added a new agent Plan and Execute. Looking forward to the more advanced use cases people will build with it. This was inspired by BabyAGI and the \"Plan and Solve\" paper. #datascience #machinelearning #largelanguagemodels #langchain Lang Chain Agent: https://python.langchain.com/en/latest/modules/agents/plan_and_execute.html Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models: https://arxiv.org/pdf/2305.04091.pdf Background by charlesdeluvio: https://unsplash.com/photos/OWkXt1ikC5g agent agents datascience execute langchain largelanguagemodels machinelearning plan solve",
      "platforms": {
        "tiktok": {
          "video_id": "7233187548917599534",
          "url": "https://www.tiktok.com/@rajistics/video/7233187548917599534",
          "view_count": 5915,
          "upload_date": "2023-05-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CsPgX8DAohh",
          "url": "https://www.instagram.com/reel/CsPgX8DAohh",
          "view_count": 573,
          "upload_date": "2023-05-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "tPI_7VVlPE8",
          "url": "https://www.youtube.com/watch?v=tPI_7VVlPE8",
          "view_count": 5887,
          "upload_date": "2023-05-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Saving you a trip to Twitter. #dataengineering #databases There is one big vendor left out. Probably get sued for leaving them out. ",
      "description": "Saving you a trip to Twitter. #dataengineering #databases There is one big vendor left out. Probably get sued for leaving them out. ",
      "upload_date": "2022-11-24",
      "total_views": 12300,
      "max_views": 12300,
      "topics": [
        "databases",
        "dataengineering",
        "one",
        "saving",
        "trip",
        "twitter"
      ],
      "search_text": "Saving you a trip to Twitter. #dataengineering #databases There is one big vendor left out. Probably get sued for leaving them out.  databases dataengineering one saving trip twitter YouTube—",
      "platforms": {
        "tiktok": {
          "video_id": "7169397996696669483",
          "url": "https://www.tiktok.com/@rajistics/video/7169397996696669483",
          "view_count": 12300,
          "upload_date": "2022-11-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a622f215634f47b1905c81be6638d718_1669255557~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=Sxtet8f6xbL6hvybkYfJGMssp4Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Earlier videos: @rajistics @rajistics #deeplearning #tensorflow #datascience #analytics",
      "description": "Earlier videos: @rajistics @rajistics #deeplearning #tensorflow #datascience #analytics",
      "upload_date": "2022-03-04",
      "total_views": 12300,
      "max_views": 12300,
      "topics": [
        "analytics",
        "datascience",
        "deep",
        "deeplearning",
        "learning",
        "tensorflow"
      ],
      "search_text": "Earlier videos: @rajistics @rajistics #deeplearning #tensorflow #datascience #analytics analytics datascience deep deeplearning learning tensorflow It's time. We're going to jump into deep learning with TensorFlow's playground. The first few videos I did focused on the value of feature engineering with the complex data set. It's time to change it up. We're going to start by simplifying the features, adding some layers, adding many neurons. We'll now have a deep learning model. The training does take a little while, but the deep learning model does eventually learn the complex data. The success you see here is why when people work with complex data like images, audio, text that's hard to featureize, they often like to use deep learning to solve those problems.",
      "platforms": {
        "tiktok": {
          "video_id": "7071034098709581099",
          "url": "https://www.tiktok.com/@rajistics/video/7071034098709581099",
          "view_count": 12300,
          "upload_date": "2022-03-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3e7d6dd0dd714865ae2f0ee0a6bf97ae_1646353422~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=QwGT1vuJrack23gLxlZRsUvDync%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6279,
      "title": "Using agents in langchain with gpt-3. You can do this! Go check it out. #datascience #machinelearning #openai #gpt3 #langchain",
      "description": "Using agents in langchain with gpt-3. You can do this! Go check it out. #datascience #machinelearning #openai #gpt3 #langchain",
      "upload_date": "2023-03-04",
      "total_views": 12291,
      "max_views": 12291,
      "topics": [
        "datascience",
        "datatable",
        "dplyr",
        "gpt3",
        "langchain",
        "machinelearning",
        "openai",
        "pandas",
        "polars",
        "using"
      ],
      "search_text": "Using agents in langchain with gpt-3. You can do this! Go check it out. #datascience #machinelearning #openai #gpt3 #langchain datascience datatable dplyr gpt3 langchain machinelearning openai pandas polars using",
      "platforms": {
        "instagram": {
          "video_id": "17992718674762907",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "s5EhdfGfLlQ",
          "url": "https://youtube.com/shorts/s5EhdfGfLlQ?feature=share",
          "view_count": 12291,
          "upload_date": "2023-03-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Learn Regex, it will pay off #regex #datascience #programming #analysis",
      "description": "Learn Regex, it will pay off #regex #datascience #programming #analysis",
      "upload_date": "2022-03-26",
      "total_views": 12200,
      "max_views": 12200,
      "topics": [
        "analysis",
        "datascience",
        "know",
        "one",
        "programming",
        "regex"
      ],
      "search_text": "Learn Regex, it will pay off #regex #datascience #programming #analysis analysis datascience know one programming regex Want to know one of the most powerful skills you can have in programming and data science that no one else talks about? It's regular expression. That gobbledygook of code, it's useful. Need a way to validate emails? Or somebody's asked you to pull all the dollar amounts out of a document. I promise you, those who know REGX rule the world.",
      "platforms": {
        "tiktok": {
          "video_id": "7079445622746828078",
          "url": "https://www.tiktok.com/@rajistics/video/7079445622746828078",
          "view_count": 12200,
          "upload_date": "2022-03-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/25c80bff233a427586ef90e619972980_1648311883~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=jEBOGHmrexE%2B99mCaufexN9F69A%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6440,
      "title": "A new LLM focused on data annotation and labeling beats GPT4. It's built from Llama 13B and will be open source. #datascience #machinelearning #semisupervised #refuelai #datalabeling To learn more: Blog post: https://www.refuel.ai/blog-posts/announcing-refuel-llm Refuel LLM is found on their LLM labeling playground: https://labs.refuel.ai/playground",
      "description": "A new LLM focused on data annotation and labeling beats GPT4. It's built from Llama 13B and will be open source. #datascience #machinelearning #semisupervised #refuelai #datalabeling To learn more: Blog post: https://www.refuel.ai/blog-posts/announcing-refuel-llm Refuel LLM is found on their LLM labeling playground: https://labs.refuel.ai/playground",
      "upload_date": "2023-10-19",
      "total_views": 12056,
      "max_views": 7828,
      "topics": [
        "annotation",
        "data",
        "datalabeling",
        "datascience",
        "labeling",
        "llm",
        "llms",
        "machinelearning",
        "refuelai",
        "semi",
        "semisupervised",
        "supervised"
      ],
      "search_text": "A new LLM focused on data annotation and labeling beats GPT4. It's built from Llama 13B and will be open source. #datascience #machinelearning #semisupervised #refuelai #datalabeling To learn more: Blog post: https://www.refuel.ai/blog-posts/announcing-refuel-llm Refuel LLM is found on their LLM labeling playground: https://labs.refuel.ai/playground annotation data datalabeling datascience labeling llm llms machinelearning refuelai semi semisupervised supervised",
      "platforms": {
        "tiktok": {
          "video_id": "7291737578217147691",
          "url": "https://www.tiktok.com/@rajistics/video/7291737578217147691",
          "view_count": 7828,
          "upload_date": "2023-10-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CylygluA7iy",
          "url": "https://www.instagram.com/reel/CylygluA7iy",
          "view_count": 1789,
          "upload_date": "2023-10-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Q8pjuRdIVEc",
          "url": "https://www.youtube.com/watch?v=Q8pjuRdIVEc",
          "view_count": 2439,
          "upload_date": "2023-10-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6435,
      "title": "Mixtral is a new model using a mixture of experts (MoE) approach. It consists of 8x7B mistral models. It was pre-released on Friday look for more details to come. #largelanguagemodels #mixtral #mistral #rajistics A version of the Mixtral model is here: https://huggingface.co/DiscoResearch/DiscoLM-mixtral-8x7b-v2 More about MoE approach check out MegaBlocks: Efficient Sparse Training with Mixture-of-Experts: https://arxiv.org/abs/2211.15841",
      "description": "Mixtral is a new model using a mixture of experts (MoE) approach. It consists of 8x7B mistral models. It was pre-released on Friday look for more details to come. #largelanguagemodels #mixtral #mistral #rajistics A version of the Mixtral model is here: https://huggingface.co/DiscoResearch/DiscoLM-mixtral-8x7b-v2 More about MoE approach check out MegaBlocks: Efficient Sparse Training with Mixture-of-Experts: https://arxiv.org/abs/2211.15841",
      "upload_date": "2023-12-09",
      "total_views": 12020,
      "max_views": 5750,
      "topics": [
        "experts",
        "largelanguagemodels",
        "mistral",
        "mixtral",
        "mixture",
        "model",
        "models"
      ],
      "search_text": "Mixtral is a new model using a mixture of experts (MoE) approach. It consists of 8x7B mistral models. It was pre-released on Friday look for more details to come. #largelanguagemodels #mixtral #mistral #rajistics A version of the Mixtral model is here: https://huggingface.co/DiscoResearch/DiscoLM-mixtral-8x7b-v2 More about MoE approach check out MegaBlocks: Efficient Sparse Training with Mixture-of-Experts: https://arxiv.org/abs/2211.15841 experts largelanguagemodels mistral mixtral mixture model models",
      "platforms": {
        "tiktok": {
          "video_id": "7310675133876784427",
          "url": "https://www.tiktok.com/@rajistics/video/7310675133876784427",
          "view_count": 5750,
          "upload_date": "2023-12-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0pMj0eAefV",
          "url": "https://www.instagram.com/reel/C0pMj0eAefV",
          "view_count": 3667,
          "upload_date": "2023-12-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0aZ_R4i73Es",
          "url": "https://youtube.com/shorts/0aZ_R4i73Es",
          "view_count": 2603,
          "upload_date": "2023-12-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft ",
      "description": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft ",
      "upload_date": "2023-02-22",
      "total_views": 12000,
      "max_views": 12000,
      "topics": [
        "chatgpt",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "microsoft",
        "robotics"
      ],
      "search_text": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft  chatgpt datascience largelanguagemodels machinelearning microsoft robotics Microsoft shown us how we can control robots with chat GPT. Microsoft's latest paper says, let's drop the old ways of controlling robots, takes way too long. Instead, let's use natural language. And once we do that, we can start controlling many types of robots for many different types of tasks. You start off by building a robot API that has very descriptive names. Once we have those descriptive names, we can start building prompts with it. Test it out, and voila. You've got chat GPT controlling robots. This is the future and a hint for developers. Use nice descriptive names in your APIs, and they'll get used.",
      "platforms": {
        "tiktok": {
          "video_id": "7203012644863855915",
          "url": "https://www.tiktok.com/@rajistics/video/7203012644863855915",
          "view_count": 12000,
          "upload_date": "2023-02-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7ec3b3c551b14c02bfd6df8b647641e9_1677082084~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=hSXp3QPPq6ws%2BZgcC7Tif2q0gAs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "OpenAI AI classifier is a great example to remind people of the limitations when detecting rare events. It’s not intuitive, so I showed the math and need you all to get it. This happens in many contexts like detecting terrorizes or diseases. #datascience #statistics #openai #baseratefallacy ",
      "description": "OpenAI AI classifier is a great example to remind people of the limitations when detecting rare events. It’s not intuitive, so I showed the math and need you all to get it. This happens in many contexts like detecting terrorizes or diseases. #datascience #statistics #openai #baseratefallacy ",
      "upload_date": "2023-02-04",
      "total_views": 12000,
      "max_views": 12000,
      "topics": [
        "didn",
        "openai",
        "people",
        "positive",
        "rate",
        "students"
      ],
      "search_text": "OpenAI AI classifier is a great example to remind people of the limitations when detecting rare events. It’s not intuitive, so I showed the math and need you all to get it. This happens in many contexts like detecting terrorizes or diseases. #datascience #statistics #openai #baseratefallacy  didn openai people positive rate students Did you know OpenAI released a new content detector? And it's actually gonna make things a lot worse. Let's start with a classroom of 100 students where 50 students cheated by using AI content and 50 students didn't. OpenAI's model has a false positive rate of 9%. So of the 50 non-cheaters, four and a half of them are gonna be marked as cheating. OpenAI's model has a true positive rate of 26%. So of the 50 cheaters, only 13 of them are gonna be caught. So the detector detected 17 and a half students, 13 of them cheated. So it got it right about 74% of the time. Maybe okay, but let's take a class where not as many people cheated. Let's say only 4% of the people cheated and 96 of them didn't cheat. Well, with the false positive rate of 9%, that means of the 96 cheaters, 8.64 of them will be flagged as cheating. Of the four cheaters, since there's a true positive rate of 26%, that means only one of them will be caught. Starting to see the problem here. The model tagged 9.64 students as cheating, but 8.64 of them didn't actually cheat. So it got it wrong 89% of the time. And this is the scary part about any of these technologies. Whenever that base rate, the amount of cheating is less than the false positive rate, a lot of people are gonna be accused that didn't do it. This is common for any of the AI detection strategies. So this is something good to know. It's called the base rate fallacy. And this applies whether we're talking about COVID or technologies for terrorism, all this stuff. These models can actually end up accusing lots of people that didn't actually have it.",
      "platforms": {
        "tiktok": {
          "video_id": "7196345997432409390",
          "url": "https://www.tiktok.com/@rajistics/video/7196345997432409390",
          "view_count": 12000,
          "upload_date": "2023-02-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3e316b1277354a1395b1c901953f4218_1675529894~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=GmxHFw%2FnoI87NjfVkWYMU3U%2FSRY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning ",
      "description": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning ",
      "upload_date": "2023-01-03",
      "total_views": 11900,
      "max_views": 11900,
      "topics": [
        "data",
        "datascience",
        "machinelearning",
        "model",
        "models",
        "scientist"
      ],
      "search_text": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning  data datascience machinelearning model models scientist Have you been cheated on? Well, as a data scientist, you get used to it. I have models that tell me that this is an elephant, there's no cow here, or there's a guitar here. So let's talk about why these models cheat and what we can do about it. I train my AI model using these four images, but then look what happens when I ask for a prediction. It gets it wrong. Can you figure out why the model cheated? It looked at the background of the image, and this is what all AI models do, is they look for the shortcuts. They look for the easiest way to classify the data that you've given them. Here's a real world example, where we built a classifier to identify pneumonia in these long X-rays, and the model did great, but then we gave it new data that was unlike the training data. It was data from different hospitals. And you know what happened? The model fell down. It didn't do so well. Once we dug in and investigated, we found that in the images, there's a hospital specific token, and that's what the AI was using, because this hospital token gives you an idea if a hospital has a high likelihood of pneumonia or low, and it was using that as a proxy instead of actually taking the time to look at the long image. This happens every day in the life of a data scientist. It's often called leakage, and there's a paper here on shortcuts that goes over a couple of the common ways it does this. Sometimes the AI finds a pattern where there's no pattern there, or it's using other features or attributes than what you really believe the true signal is at. So how can we prevent this? So one thing is we can use explainability tools to better try to understand what our model is doing, but we can also think about how the model is gonna be used and make sure when we're training the model that we use diverse data that actually represents how the model is gonna be used. But if your model cheats on you, don't take it personally. It happens to all of us.",
      "platforms": {
        "tiktok": {
          "video_id": "7184464697880497451",
          "url": "https://www.tiktok.com/@rajistics/video/7184464697880497451",
          "view_count": 11900,
          "upload_date": "2023-01-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1e0f06c9f16d4ed1bd6717732be09270_1672763561~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=gDIjv0AeZdZzknKpc7hg7j%2Bp1ic%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6498,
      "title": "An experiment studying how well GPT4 can plan by using Block World and Mystery World. #largelanguagemodels #gpt4 #aiplanning #blockworld #mysteryworld On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark) - https://arxiv.org/abs/2302.06706 Lego Background: https://www.youtube.com/watch?v=2cOeUEjx-WI&ab_channel=BrickBuilder",
      "description": "An experiment studying how well GPT4 can plan by using Block World and Mystery World. #largelanguagemodels #gpt4 #aiplanning #blockworld #mysteryworld On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark) - https://arxiv.org/abs/2302.06706 Lego Background: https://www.youtube.com/watch?v=2cOeUEjx-WI&ab_channel=BrickBuilder",
      "upload_date": "2023-08-13",
      "total_views": 11845,
      "max_views": 6303,
      "topics": [
        "ability",
        "aiplanning",
        "block",
        "blockworld",
        "gpt4",
        "largelanguagemodels",
        "llms",
        "mysteryworld",
        "planning",
        "versus",
        "world"
      ],
      "search_text": "An experiment studying how well GPT4 can plan by using Block World and Mystery World. #largelanguagemodels #gpt4 #aiplanning #blockworld #mysteryworld On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark) - https://arxiv.org/abs/2302.06706 Lego Background: https://www.youtube.com/watch?v=2cOeUEjx-WI&ab_channel=BrickBuilder ability aiplanning block blockworld gpt4 largelanguagemodels llms mysteryworld planning versus world",
      "platforms": {
        "tiktok": {
          "video_id": "7266866822639635754",
          "url": "https://www.tiktok.com/@rajistics/video/7266866822639635754",
          "view_count": 5039,
          "upload_date": "2023-08-13",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cv5Nss7gO0w",
          "url": "https://www.instagram.com/reel/Cv5Nss7gO0w",
          "view_count": 6303,
          "upload_date": "2023-08-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OmWO_XOUvK4",
          "url": "https://www.youtube.com/watch?v=OmWO_XOUvK4",
          "view_count": 503,
          "upload_date": "2023-08-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Temperaure is an important parameter when working with many models including got-3. This video gives a short background on temperature and the best settings when working with large language models. #datascience #machinelearning #largelanguagemodels #gpt3 #gpt4 ",
      "description": "Temperaure is an important parameter when working with many models including got-3. This video gives a short background on temperature and the best settings when working with large language models. #datascience #machinelearning #largelanguagemodels #gpt3 #gpt4 ",
      "upload_date": "2023-03-22",
      "total_views": 11800,
      "max_views": 11800,
      "topics": [
        "datascience",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "temperature",
        "want"
      ],
      "search_text": "Temperaure is an important parameter when working with many models including got-3. This video gives a short background on temperature and the best settings when working with large language models. #datascience #machinelearning #largelanguagemodels #gpt3 #gpt4  datascience gpt3 largelanguagemodels machinelearning temperature want What's one of the most important settings when using GPT-3? It's temperature. Let's talk about what temperature is and how you should be setting it. For many problems, there's many possible outcomes. Look at this sentence. You could end this in many different ways and this is what creates a probability distribution of outcomes. What temperature does is control the softness of the distribution. If I turn the temperature down really cold where nothing's moving and there's no energy in the system, it gets very spiky. We get to in this case one possible outcome. But if we add temperature, if we add energy into the system, add entropy, everything gets excited. Everything moves around. All of a sudden we move from that spiky distribution to now there's lots of possible outcomes that can happen. So use a low temperature when you want more predictable and consistent results, such as if you're doing classification. Another common use task for kind of a low temperature is when you're passing to the LLM, facts that you've generated from some other system and you want them to integrate that into a nice paragraph. You don't want it to be too creative, creating new facts. A low temperature setting helps with that. A high temperature of one is rarely used. The results are often way too predictable. Instead, when you want it to be a little bit more creative, when you want to have different choices there, usually a setting of somewhere between 0.7 to 0.9 is often more appropriate. As you're honing your prompts, take a look at temperature because that's one thing that's going to affect the consistency of results as you repeatedly start using the prompt.",
      "platforms": {
        "tiktok": {
          "video_id": "7213154050525646122",
          "url": "https://www.tiktok.com/@rajistics/video/7213154050525646122",
          "view_count": 11800,
          "upload_date": "2023-03-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6b2a840ce12040ca9f2c2ee8de60bb68_1679443320~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=cT7CFd7%2Bc%2BmtHQL4WGT10JNk14E%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "CLIP Interrogator is available over at the hugging face spaces. Have fun! #datascience #machinelearning #stablediffusion #huggingface ",
      "description": "CLIP Interrogator is available over at the hugging face spaces. Have fun! #datascience #machinelearning #stablediffusion #huggingface ",
      "upload_date": "2022-10-25",
      "total_views": 11800,
      "max_views": 11800,
      "topics": [
        "datascience",
        "fun",
        "huggingface",
        "machinelearning",
        "stablediffusion",
        "told"
      ],
      "search_text": "CLIP Interrogator is available over at the hugging face spaces. Have fun! #datascience #machinelearning #stablediffusion #huggingface  datascience fun huggingface machinelearning stablediffusion told It's time to learn what AI is thinking. It told me I was smiling like a jerk. It told this guy he was a weak beta male. Told this guy he was a doofus. Told her she looked tired and drunk. The same AI that's been powering all the text to image models we've just turned around and now we're converting our images to text and it's revealing some truths. So go try it out. It's a ton of fun.",
      "platforms": {
        "tiktok": {
          "video_id": "7158241879027354926",
          "url": "https://www.tiktok.com/@rajistics/video/7158241879027354926",
          "view_count": 11800,
          "upload_date": "2022-10-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bb7262c400574055b91e6351d16bb6bf_1666658070~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=5GKCr42SRofmkCPDCLWiH6QNp3w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6635,
      "title": "What makes GPT-4 so special? One big part is the use of a Mixture of Experts approach Let's start with how Galton used the wisdom of the crowds over a 100 years ago to get the weight of a cow accurately. Or more recently how the $1 million Netflix competition led to recognition of the power of ensembles in machine learning And finally recent research using a Mixture of Experts approach with Large Langage Models Tags: #largelanguagemodels #machinelearning #openai #mixtureofexperts #ensemblelearning #gpt4 Links: GPT-4: 8 Models in One; The Secret is Out - https://pub.towardsai.net/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0 Mixture-of-Experts Explained: Why 8 smaller models are better than 1 gigantic one: https://alexandrabarr.beehiiv.com/p/mixture-of-experts Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models - https://arxiv.org/abs/2305.14705",
      "description": "What makes GPT-4 so special? One big part is the use of a Mixture of Experts approach Let's start with how Galton used the wisdom of the crowds over a 100 years ago to get the weight of a cow accurately. Or more recently how the $1 million Netflix competition led to recognition of the power of ensembles in machine learning And finally recent research using a Mixture of Experts approach with Large Langage Models Tags: #largelanguagemodels #machinelearning #openai #mixtureofexperts #ensemblelearning #gpt4 Links: GPT-4: 8 Models in One; The Secret is Out - https://pub.towardsai.net/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0 Mixture-of-Experts Explained: Why 8 smaller models are better than 1 gigantic one: https://alexandrabarr.beehiiv.com/p/mixture-of-experts Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models - https://arxiv.org/abs/2305.14705",
      "upload_date": "2023-07-08",
      "total_views": 11617,
      "max_views": 8746,
      "topics": [
        "experts",
        "gpt",
        "largelanguagemodels",
        "machinelearning",
        "mixture",
        "models",
        "one",
        "openai"
      ],
      "search_text": "What makes GPT-4 so special? One big part is the use of a Mixture of Experts approach Let's start with how Galton used the wisdom of the crowds over a 100 years ago to get the weight of a cow accurately. Or more recently how the $1 million Netflix competition led to recognition of the power of ensembles in machine learning And finally recent research using a Mixture of Experts approach with Large Langage Models Tags: #largelanguagemodels #machinelearning #openai #mixtureofexperts #ensemblelearning #gpt4 Links: GPT-4: 8 Models in One; The Secret is Out - https://pub.towardsai.net/gpt-4-8-models-in-one-the-secret-is-out-e3d16fd1eee0 Mixture-of-Experts Explained: Why 8 smaller models are better than 1 gigantic one: https://alexandrabarr.beehiiv.com/p/mixture-of-experts Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models - https://arxiv.org/abs/2305.14705 experts gpt largelanguagemodels machinelearning mixture models one openai",
      "platforms": {
        "tiktok": {
          "video_id": "7253582899700632875",
          "url": "https://www.tiktok.com/@rajistics/video/7253582899700632875",
          "view_count": 8746,
          "upload_date": "2023-07-08",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CudCUeJAaHq",
          "url": "https://www.instagram.com/reel/CudCUeJAaHq",
          "view_count": 2365,
          "upload_date": "2023-07-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "20AAQ1BdHjc",
          "url": "https://www.youtube.com/watch?v=20AAQ1BdHjc",
          "view_count": 506,
          "upload_date": "2023-07-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6426,
      "title": "Code Interpreter is out and it's pretty amazing at first glance. However more experienced software developers and people concerned about data analysis might not be as impressed. This video offers a few different perspectives of the OpenAI enthusiast the crusty software developer and the skeptical data analysis instructor. #openai #codeinterpreter #dataanalysis #aiagents Code Interpreter release: https://venturebeat.com/ai/code-interpreter-comes-to-all-chatgpt-plus-users-anyone-can-be-a-data-analyst-now/ Aaron Brand: https://twitter.com/aron_brand/status/1677305077164285954 Arvind Narayanan: https://twitter.com/random_walker/status/1679109189551939584 Background video from Prompt Engineering: https://www.youtube.com/watch?v=2Ygm6fvR7yM&ab_channel=PromptEngineering",
      "description": "Code Interpreter is out and it's pretty amazing at first glance. However more experienced software developers and people concerned about data analysis might not be as impressed. This video offers a few different perspectives of the OpenAI enthusiast the crusty software developer and the skeptical data analysis instructor. #openai #codeinterpreter #dataanalysis #aiagents Code Interpreter release: https://venturebeat.com/ai/code-interpreter-comes-to-all-chatgpt-plus-users-anyone-can-be-a-data-analyst-now/ Aaron Brand: https://twitter.com/aron_brand/status/1677305077164285954 Arvind Narayanan: https://twitter.com/random_walker/status/1679109189551939584 Background video from Prompt Engineering: https://www.youtube.com/watch?v=2Ygm6fvR7yM&ab_channel=PromptEngineering",
      "upload_date": "2023-07-12",
      "total_views": 11500,
      "max_views": 7234,
      "topics": [
        "aiagents",
        "answer",
        "chatgpt",
        "code",
        "codeinterpreter",
        "dataanalysis",
        "interpreter",
        "llama",
        "meta",
        "openai"
      ],
      "search_text": "Code Interpreter is out and it's pretty amazing at first glance. However more experienced software developers and people concerned about data analysis might not be as impressed. This video offers a few different perspectives of the OpenAI enthusiast the crusty software developer and the skeptical data analysis instructor. #openai #codeinterpreter #dataanalysis #aiagents Code Interpreter release: https://venturebeat.com/ai/code-interpreter-comes-to-all-chatgpt-plus-users-anyone-can-be-a-data-analyst-now/ Aaron Brand: https://twitter.com/aron_brand/status/1677305077164285954 Arvind Narayanan: https://twitter.com/random_walker/status/1679109189551939584 Background video from Prompt Engineering: https://www.youtube.com/watch?v=2Ygm6fvR7yM&ab_channel=PromptEngineering aiagents answer chatgpt code codeinterpreter dataanalysis interpreter llama meta openai",
      "platforms": {
        "tiktok": {
          "video_id": "7255003713167789355",
          "url": "https://www.tiktok.com/@rajistics/video/7255003713167789355",
          "view_count": 7234,
          "upload_date": "2023-07-12",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cum5RrZJAVk",
          "url": "https://www.instagram.com/reel/Cum5RrZJAVk",
          "view_count": 553,
          "upload_date": "2023-07-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "mSZf7vfCk8M",
          "url": "https://www.youtube.com/watch?v=mSZf7vfCk8M",
          "view_count": 3713,
          "upload_date": "2023-07-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Software licensing #github #codetok #gpl #programming #python #creativecommons #copyright",
      "description": "Software licensing #github #codetok #gpl #programming #python #creativecommons #copyright",
      "upload_date": "2022-05-10",
      "total_views": 11500,
      "max_views": 11500,
      "topics": [
        "codetok",
        "copyright",
        "github",
        "gpl",
        "programming",
        "want"
      ],
      "search_text": "Software licensing #github #codetok #gpl #programming #python #creativecommons #copyright codetok copyright github gpl programming want Do you use code from GitHub? Do you share code on GitHub? Let's go over some of the quick fundamentals on licensing. If you find a repo that doesn't have a license or you didn't bother putting a license on your repo, in the United States, by default, copyright protection attaches. That means no one else is allowed to copy or reproduce that software without permission. When I worked as a data scientist inside of a large insurance company, I often found cool packages and interesting software that didn't have a license. So I would go send out a nice email saying, hey, can you change your license type to something like MIT or Apache? Because then me working inside this huge profit seeking corporation can start to use your software. If you're on the other end, here's two things you should think about. First, if somebody else uses your code, are they allowed to share that out without having your explicit permission on that? Often people will want to take your code, put it inside another package, redistribute that package. Are you OK with it? What do you want? Second, do you want to allow commercial use of the product? Do you want to allow somebody like Amazon to take your package and sell it for a profit? Or do you want to limit it to non-commercial use? And of course, this is TicToc. This isn't a place for great legal advice.",
      "platforms": {
        "tiktok": {
          "video_id": "7096103526560058667",
          "url": "https://www.tiktok.com/@rajistics/video/7096103526560058667",
          "view_count": 11500,
          "upload_date": "2022-05-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6d4de90e33a947d3a2f7a72d6c1b49b7_1652190363~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=q43xpzgfF4xkuPQ%2BVwZVTqZQAvI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6513,
      "title": "Japan said it was acceptable to use copyrighted material such as text and images to train AI. This has the approach of United States and other countries like Israel have also followed the US. All of this makes it much easier for people to train AI models within these countries. #datascience #machinelearning #copyright #fairuse Israel: https://www.project-disco.org/intellectual-property/011823-israel-ministry-of-justice-issues-opinion-supporting-the-use-of-copyrighted-works-for-machine-learning/ Japan: https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/ Background by Dario Seretin: https://unsplash.com/photos/AGgOAqqGlT4",
      "description": "Japan said it was acceptable to use copyrighted material such as text and images to train AI. This has the approach of United States and other countries like Israel have also followed the US. All of this makes it much easier for people to train AI models within these countries. #datascience #machinelearning #copyright #fairuse Israel: https://www.project-disco.org/intellectual-property/011823-israel-ministry-of-justice-issues-opinion-supporting-the-use-of-copyrighted-works-for-machine-learning/ Japan: https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/ Background by Dario Seretin: https://unsplash.com/photos/AGgOAqqGlT4",
      "upload_date": "2023-06-01",
      "total_views": 11495,
      "max_views": 8242,
      "topics": [
        "copyright",
        "datascience",
        "fairuse",
        "israel",
        "japan",
        "learning",
        "machine",
        "machinelearning",
        "models",
        "training",
        "updates",
        "use"
      ],
      "search_text": "Japan said it was acceptable to use copyrighted material such as text and images to train AI. This has the approach of United States and other countries like Israel have also followed the US. All of this makes it much easier for people to train AI models within these countries. #datascience #machinelearning #copyright #fairuse Israel: https://www.project-disco.org/intellectual-property/011823-israel-ministry-of-justice-issues-opinion-supporting-the-use-of-copyrighted-works-for-machine-learning/ Japan: https://technomancers.ai/japan-goes-all-in-copyright-doesnt-apply-to-ai-training/ Background by Dario Seretin: https://unsplash.com/photos/AGgOAqqGlT4 copyright datascience fairuse israel japan learning machine machinelearning models training updates use",
      "platforms": {
        "tiktok": {
          "video_id": "7239832139850665259",
          "url": "https://www.tiktok.com/@rajistics/video/7239832139850665259",
          "view_count": 8242,
          "upload_date": "2023-06-01",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cs9nwnBgYNn",
          "url": "https://www.instagram.com/reel/Cs9nwnBgYNn",
          "view_count": 2966,
          "upload_date": "2023-06-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4oDPKCetOCg",
          "url": "https://www.youtube.com/watch?v=4oDPKCetOCg",
          "view_count": 287,
          "upload_date": "2023-06-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6422,
      "title": "NanoGPT using Simpsons Data: Get Started with Large Language Models",
      "description": "NanoGPT using Simpsons Data: Get Started with Large Language Models",
      "upload_date": "2023-08-27",
      "total_views": 11438,
      "max_views": 5684,
      "topics": [
        "data",
        "datascience",
        "dataset",
        "get",
        "github",
        "largelanguagemodels",
        "machinelearning",
        "nanogpt",
        "nanogpt_simpsons",
        "simpsons",
        "started",
        "using",
        "video"
      ],
      "search_text": "NanoGPT using Simpsons Data: Get Started with Large Language Models data datascience dataset get github largelanguagemodels machinelearning nanogpt nanogpt_simpsons simpsons started using video",
      "platforms": {
        "tiktok": {
          "video_id": "7272125601048481070",
          "url": "https://www.tiktok.com/@rajistics/video/7272125601048481070",
          "view_count": 5293,
          "upload_date": "2023-08-27",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CwLZFlNpJ59",
          "url": "https://www.instagram.com/reel/CwLZFlNpJ59",
          "view_count": 461,
          "upload_date": "2023-08-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Ty2_bR1mrBQ",
          "url": "https://www.youtube.com/watch?v=Ty2_bR1mrBQ",
          "view_count": 5684,
          "upload_date": "2023-08-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "It pays to be organized. Find a friendly data engineer if you need to. #datascience #analytics",
      "description": "It pays to be organized. Find a friendly data engineer if you need to. #datascience #analytics",
      "upload_date": "2022-10-06",
      "total_views": 11400,
      "max_views": 11400,
      "topics": [
        "analytics",
        "datascience",
        "find",
        "friendly",
        "organized",
        "pays"
      ],
      "search_text": "It pays to be organized. Find a friendly data engineer if you need to. #datascience #analytics analytics datascience find friendly organized pays Tell me, friend, when did Saruman the wise abandon reason for madness?",
      "platforms": {
        "tiktok": {
          "video_id": "7151533164174003502",
          "url": "https://www.tiktok.com/@rajistics/video/7151533164174003502",
          "view_count": 11400,
          "upload_date": "2022-10-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a4e3a614f838486f9f12990eb8adaa63_1665096078~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=0KSU4FQwHH4HOL1seittwm5NVjg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6121,
      "title": "Running large language models and transformer models locally in web browsers. Lot's of tools for doing this including mlc.ai transformers.js and webgpu. #largelanguagemodels #rajistics #mlcai #transformersjs #webgpu #browswerai Backgrond by Andrea De Santis: https://unsplash.com/photos/a-large-group-of-red-and-white-masks-EMvUZHjbxtI",
      "description": "Running large language models and transformer models locally in web browsers. Lot's of tools for doing this including mlc.ai transformers.js and webgpu. #largelanguagemodels #rajistics #mlcai #transformersjs #webgpu #browswerai Backgrond by Andrea De Santis: https://unsplash.com/photos/a-large-group-of-red-and-white-masks-EMvUZHjbxtI",
      "upload_date": "2023-09-05",
      "total_views": 11301,
      "max_views": 11000,
      "topics": [
        "browswerai",
        "language",
        "large",
        "largelanguagemodels",
        "llms",
        "locally",
        "lot",
        "mlcai",
        "models",
        "running",
        "take",
        "transformersjs",
        "web",
        "webgpu"
      ],
      "search_text": "Running large language models and transformer models locally in web browsers. Lot's of tools for doing this including mlc.ai transformers.js and webgpu. #largelanguagemodels #rajistics #mlcai #transformersjs #webgpu #browswerai Backgrond by Andrea De Santis: https://unsplash.com/photos/a-large-group-of-red-and-white-masks-EMvUZHjbxtI browswerai language large largelanguagemodels llms locally lot mlcai models running take transformersjs web webgpu",
      "platforms": {
        "tiktok": {
          "video_id": "7275495634734648622",
          "url": "https://www.tiktok.com/@rajistics/video/7275495634734648622",
          "view_count": 11000,
          "upload_date": "2023-09-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18050718526470579",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-09-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4QokaKepc6k",
          "url": "https://www.youtube.com/watch?v=4QokaKepc6k",
          "view_count": 301,
          "upload_date": "2023-09-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6650,
      "title": "The best way to learning data science is working with data. You don‚Äôt need to spend money on courses or books. Spending time doing useful projects. #machinelearning #datascience",
      "description": "The best way to learning data science is working with data. You don‚Äôt need to spend money on courses or books. Spending time doing useful projects. #machinelearning #datascience",
      "upload_date": "2023-12-17",
      "total_views": 11254,
      "max_views": 5790,
      "topics": [
        "best",
        "caps",
        "data",
        "datascience",
        "different",
        "future",
        "jan",
        "largelanguagemodels",
        "learning",
        "llms",
        "machinelearning",
        "many",
        "market",
        "meta",
        "news",
        "spending",
        "way",
        "yann"
      ],
      "search_text": "The best way to learning data science is working with data. You don‚Äôt need to spend money on courses or books. Spending time doing useful projects. #machinelearning #datascience best caps data datascience different future jan largelanguagemodels learning llms machinelearning many market meta news spending way yann",
      "platforms": {
        "tiktok": {
          "video_id": "7313687386528025899",
          "url": "https://www.tiktok.com/@rajistics/video/7313687386528025899",
          "view_count": 5790,
          "upload_date": "2023-12-17",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1QO2rugRIw",
          "url": "https://www.instagram.com/reel/C1QO2rugRIw",
          "view_count": 3949,
          "upload_date": "2023-12-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "uI2gbu_RuVM",
          "url": "https://www.youtube.com/watch?v=uI2gbu_RuVM",
          "view_count": 1515,
          "upload_date": "2024-01-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5966,
      "title": "Yifan Zhao reverse-engineered Claude Code and uncovered that its secret isn’t hidden logic, but a sophisticated stack of prompts. By intercepting requests and analyzing the system prompt, Zhao showed how repetition and emphasis, contextual reminders, structured formatting, sub-agent orchestration, and model-specific tuning give Claude Code its edge. The findings highlight that Claude’s reliability comes not from code changes, but from carefully engineered instructions — a lesson for anyone building coding agents or experimenting with LLM prompts. References * Yifan Zhao, Inside Claude Code: Prompt Engineering Masterpiece (Beyond the Hype, 2025) — https://beyondthehype.dev/ * YouTube, Inside Claude Code: Prompt Engineering Masterpiece by Yifan Zhao — https://www.youtube.com/watch?v=i0P56Pm1Q3U",
      "description": "Yifan Zhao reverse-engineered Claude Code and uncovered that its secret isn’t hidden logic, but a sophisticated stack of prompts. By intercepting requests and analyzing the system prompt, Zhao showed how repetition and emphasis, contextual reminders, structured formatting, sub-agent orchestration, and model-specific tuning give Claude Code its edge. The findings highlight that Claude’s reliability comes not from code changes, but from carefully engineered instructions — a lesson for anyone building coding agents or experimenting with LLM prompts. References * Yifan Zhao, Inside Claude Code: Prompt Engineering Masterpiece (Beyond the Hype, 2025) — https://beyondthehype.dev/ * YouTube, Inside Claude Code: Prompt Engineering Masterpiece by Yifan Zhao — https://www.youtube.com/watch?v=i0P56Pm1Q3U",
      "upload_date": "2025-08-31",
      "total_views": 11238,
      "max_views": 9514,
      "topics": [
        "claude",
        "code",
        "cracking",
        "engineered",
        "learned",
        "lessons",
        "model",
        "prompt",
        "prompts",
        "yifan",
        "zhao"
      ],
      "search_text": "Yifan Zhao reverse-engineered Claude Code and uncovered that its secret isn’t hidden logic, but a sophisticated stack of prompts. By intercepting requests and analyzing the system prompt, Zhao showed how repetition and emphasis, contextual reminders, structured formatting, sub-agent orchestration, and model-specific tuning give Claude Code its edge. The findings highlight that Claude’s reliability comes not from code changes, but from carefully engineered instructions — a lesson for anyone building coding agents or experimenting with LLM prompts. References * Yifan Zhao, Inside Claude Code: Prompt Engineering Masterpiece (Beyond the Hype, 2025) — https://beyondthehype.dev/ * YouTube, Inside Claude Code: Prompt Engineering Masterpiece by Yifan Zhao — https://www.youtube.com/watch?v=i0P56Pm1Q3U claude code cracking engineered learned lessons model prompt prompts yifan zhao Claude code has been reverse engineered. I've got the secrets. And once you learn them, you're never going to look at Claude the same way. So he fan here went full on detective mode. He cracked open Claude code, even used a packet snipper to help it understand what is the information being sent back and forth. What he uncovered in the prompts can be boiled down to five big IDs. First, repetition and emphasis. Important instructions were mentioned again and again. The use words like must never important. That makes it easier for the model to constantly follow along. Contextual reminders. Claude just doesn't set the rules at the start. Keeps reinserting reminders into the conversation, help nudge it and stay on track. Structured formatting. Hey, using things like bold and all caps, using XML tags. That structure makes it easier for the model to follow along. Sub agent orchestration. We all know how to use sub agents, but here the task is delegated to sub agents. They each have their own prompt. You write a summary of what's going back and only share the summary back with the main agent. Finally, model specific tuning. All these prompts are really built for these Claude models. If you try to swap in another model with these, it's not going to work. The takeaway here is pumped engineering to the max. You see how layered structure delivered it is. That's what it takes to really get the most out of these models.",
      "platforms": {
        "tiktok": {
          "video_id": "7544558202030017823",
          "url": "https://www.tiktok.com/@rajistics/video/7544558202030017823",
          "view_count": 9514,
          "upload_date": "2025-08-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYLe4AJizAtyQMLAIfqpFGTJUoneQfaARboAIG~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=V%2FAY%2BVrIt0uRy1fHnipkEVOKLO4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18095332900656572",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "nXxzHhWBHgo",
          "url": "https://www.youtube.com/watch?v=nXxzHhWBHgo",
          "view_count": 1724,
          "upload_date": "2025-08-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "YOLO is a seminal model in object detection for computer vision. But what is even more interesting is the principal author, Joseph Redmon and his journey. While YOLO is still actively developed, he has stopped participating. You Only Look Once: Unified, Real-Time Object Detection (2015): https://arxiv.org/abs/1506.02640 YOLOv3: An Incremental Improvement: https://arxiv.org/abs/1804.02767 YOLO web site: https://pjreddie.com/darknet/yolo/",
      "description": "YOLO is a seminal model in object detection for computer vision. But what is even more interesting is the principal author, Joseph Redmon and his journey. While YOLO is still actively developed, he has stopped participating. You Only Look Once: Unified, Real-Time Object Detection (2015): https://arxiv.org/abs/1506.02640 YOLOv3: An Incremental Improvement: https://arxiv.org/abs/1804.02767 YOLO web site: https://pjreddie.com/darknet/yolo/",
      "upload_date": "2024-09-13",
      "total_views": 11200,
      "max_views": 11200,
      "topics": [
        "like",
        "model",
        "object",
        "paper",
        "time",
        "yolo"
      ],
      "search_text": "YOLO is a seminal model in object detection for computer vision. But what is even more interesting is the principal author, Joseph Redmon and his journey. While YOLO is still actively developed, he has stopped participating. You Only Look Once: Unified, Real-Time Object Detection (2015): https://arxiv.org/abs/1506.02640 YOLOv3: An Incremental Improvement: https://arxiv.org/abs/1804.02767 YOLO web site: https://pjreddie.com/darknet/yolo/ like model object paper time yolo Would you like to know about one of the biggest advances in computer vision that had a little bit of humor and also a bit of a dark side as we'll get into the story. Our story starts back in 2016 in computer vision, which at the time was really hard to be able to detect an object in a picture very quickly. It would literally take something like 10 seconds to do that. And so if you were trying to track an object or to try to track multiple objects, it became very difficult. But then Joseph Redmond dropped the YOLO paper and that changed everything for object detection. Now one quick glance at this paper and you can immediately tell there's something special going on. And a lot of what Joseph did in writing this paper was unusual at the time. Notice right away the language here is interesting. Technical communication doesn't have to be boring. We can and we should write interesting. You also see there's a link to his site. He made it very easy to jump and get the code so you could reproduce this paper yourself. Paper is very easy to read. The math isn't overly complex and there's lots of details on the implementations. You know five years ago when he was writing this paper, lots of papers would leave out the important details which made it very hard to reduce. It seemed like some researchers didn't really want you to reproduce. Here he takes the time to carefully put down all the settings to help out somebody that truly wants to reproduce this paper. He also takes the time to have a section on things that didn't work. And for a long time people wouldn't talk about things they didn't work. They just focused on what their success. But the reality is all of us can learn from those mistakes and that helps us better understand the subject area. Come on and who doesn't like this visual? He very effectively was able to show exactly how his model did in terms of performance and overall accuracy compared to the other existing models. The last section talks about what are the implications? How is this model actually going to be used? And you can see he's hopeful that it's going to be used for good purposes but he also realizes that other people, the military defense might use this model to kill people for example. And this is why he decided to walk away from it. He realized that there was no real ethical way to do this type of stuff that these models could be used for unethical purposes. He just unplugged himself from the system and I really admire him for that.",
      "platforms": {
        "tiktok": {
          "video_id": "7414121980758330670",
          "url": "https://www.tiktok.com/@rajistics/video/7414121980758330670",
          "view_count": 11200,
          "upload_date": "2024-09-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b29bbe18084d4a9ba764894831d66d6f_1726234803~tplv-tiktokx-origin.image?dr=9636&x-expires=1767416400&x-signature=JO1Y6i6Hmr6eH487DWit8DYFyzU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "This post was based on great stuff on Twitter, especially Ben’s Bites. I wanted to show the chat output, so wasn’t able to keep the original tweet info. Go play or read about it. #datascience #machinelearning #openai #chatgpt  share back anything cool you find. ",
      "description": "This post was based on great stuff on Twitter, especially Ben’s Bites. I wanted to show the chat output, so wasn’t able to keep the original tweet info. Go play or read about it. #datascience #machinelearning #openai #chatgpt  share back anything cool you find. ",
      "upload_date": "2022-12-01",
      "total_views": 11200,
      "max_views": 11200,
      "topics": [
        "chat",
        "chatgpt",
        "datascience",
        "going",
        "machinelearning",
        "openai"
      ],
      "search_text": "This post was based on great stuff on Twitter, especially Ben’s Bites. I wanted to show the chat output, so wasn’t able to keep the original tweet info. Go play or read about it. #datascience #machinelearning #openai #chatgpt  share back anything cool you find.  chat chatgpt datascience going machinelearning openai Oh, it's going to be fun today. Open AI released their chat GP and oh, it's so cool. So let me tell you about it and let me also show how it's going to change how we think about search. There's so many good examples online, but here's a couple of examples of being able to use chat GP to write some new content, maybe generate new content, even get medical advice. It's actually okay on this. And of course, for the coders, you can use it to write code in style. It could even debug code as well and tell you where the problems are. But here's where it's really going to change things. Look at the answer that you get from Google on this question. Now, compare this to what chat GP gives, where it integrates a lot of different sources, puts it into a cohesive story. And for me, that's the future of search, where we're going to get responses back that automatically integrate and summarize information for us instead of having to individually go to different posts. All right, go have fun with it.",
      "platforms": {
        "tiktok": {
          "video_id": "7172199007341579566",
          "url": "https://www.tiktok.com/@rajistics/video/7172199007341579566",
          "view_count": 11200,
          "upload_date": "2022-12-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ffbd7e0fbd2e479c9aa809e8ae0df427_1669907722~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=%2BBzqmudPAX9SSv%2Fk8%2FjCe7MdhmA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6215,
      "title": "My ranking of the top 26 algorithms for practical data science, breaking down their strengths, quirks, and when (or if) you should use them.  Algorithm List: Latent Dirichlet Allocation, GANs, Boruta, Hierarchical Clustering, RNN/LSTMs, CART, PPO, Prophet, SVM, Isolation Forest, Autoencoders, T-SNE, ARIMA, Naive Bayes, CNNs, kNN, UMAP, HDBSCAN, PCA, Neural Networks, Linear Regression, KMeans, Random Forest, Logistic Regression, Transformers, Gradient Boosting.",
      "description": "My ranking of the top 26 algorithms for practical data science, breaking down their strengths, quirks, and when (or if) you should use them.  Algorithm List: Latent Dirichlet Allocation, GANs, Boruta, Hierarchical Clustering, RNN/LSTMs, CART, PPO, Prophet, SVM, Isolation Forest, Autoencoders, T-SNE, ARIMA, Naive Bayes, CNNs, kNN, UMAP, HDBSCAN, PCA, Neural Networks, Linear Regression, KMeans, Random Forest, Logistic Regression, Transformers, Gradient Boosting.",
      "upload_date": "2024-11-30",
      "total_views": 11130,
      "max_views": 10700,
      "topics": [
        "algorithms",
        "clustering",
        "data",
        "going",
        "science",
        "top",
        "use"
      ],
      "search_text": "My ranking of the top 26 algorithms for practical data science, breaking down their strengths, quirks, and when (or if) you should use them.  Algorithm List: Latent Dirichlet Allocation, GANs, Boruta, Hierarchical Clustering, RNN/LSTMs, CART, PPO, Prophet, SVM, Isolation Forest, Autoencoders, T-SNE, ARIMA, Naive Bayes, CNNs, kNN, UMAP, HDBSCAN, PCA, Neural Networks, Linear Regression, KMeans, Random Forest, Logistic Regression, Transformers, Gradient Boosting. algorithms clustering data going science top use Let's rank the top 26 data science algorithms for practical data science, speed run. Topic clustering, the old school way, don't need it. Don calls this the coolest idea in ML in the last 20 years. But are you gonna use it? No. The brute force way for feature selection, but really recursive feature elimination works just fine. It's a great technique, but that detail confuses lots of people. Just find a simpler way. These are sequential neural networks. You still use them a little bit in time series, but who knows, they might make a big comeback. The OG decision tree. It's what I often show to stakeholders, even if I use something else. The granddaddy of reinforcement learning. Reinforcement learning is trying to make a comeback, but this is still pretty niche. People will tease you for using it. Still works decently for time series, while a lot of people use them. Get respect from the old school statisticians and confuse newbies. A goat for anomaly detection. Coolest name ever, simple to understand it, learn how to use it. Classic approach for dimensionality reduction. Who doesn't like curves? Classic time series, great baseline to use. I haven't used this much, but it's a baseline with text classification with TF idea. It rocks for working with images, you're gonna use them. Everybody understands it. It's more used for teaching than actual practical use. Top of the pyramid for dimensionality reduction, standard. My go-to approach for clustering high dimensional data. A classic in dimensionality reduction. I read about it and hear about it more than I actually use. They're everywhere. They've been used for the last 40 years. It's foundational at its base. It's very simple and elegant, but you can build much more complex algorithms on top of it. The goat for clustering, and I love the elbow method. If you're okay with the uncertainty of that, you're gonna do good in data science. An elegant and sombering method. The best baseline and add regularization and you've got a workhorse for classification. Steadily moving up the rankings, they're being used everywhere from text to images for all sorts of applications. The meat of data science in the enterprise, they're bringing more value than all the other algorithms combined. It's one that you're gonna use if you're working inside enterprises.",
      "platforms": {
        "tiktok": {
          "video_id": "7442873241338744110",
          "url": "https://www.tiktok.com/@rajistics/video/7442873241338744110",
          "view_count": 10700,
          "upload_date": "2024-11-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oceEoCFSwIDRLgAAXMAgf7EwnUQaRhVDSA5myC~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=VNU%2Fo2SoJEllFpBUiFfitpAZFeM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18014181110443163",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "dt4uXC-WfG8",
          "url": "https://www.youtube.com/watch?v=dt4uXC-WfG8",
          "view_count": 430,
          "upload_date": "2024-11-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Learn about foundational models, especially in #nlp #naturallanguageprocessing #datascience #deeplearning #analytics #techtok #openai",
      "description": "Learn about foundational models, especially in #nlp #naturallanguageprocessing #datascience #deeplearning #analytics #techtok #openai",
      "upload_date": "2022-04-23",
      "total_views": 11100,
      "max_views": 11100,
      "topics": [
        "analytics",
        "datascience",
        "deeplearning",
        "models",
        "naturallanguageprocessing",
        "nlp"
      ],
      "search_text": "Learn about foundational models, especially in #nlp #naturallanguageprocessing #datascience #deeplearning #analytics #techtok #openai analytics datascience deeplearning models naturallanguageprocessing nlp Let's talk about some of the biggest changes in data science and why you really need to be aware of them if you're entering the field. Data scientists have figured out how to make very useful models with enormous amounts of data. Everything from, let's say, all of Reddit or the New York Times to literally humongous models that have everything on the internet all in one model. The performance of these models is incredible. The latest data science competition, I saw every model was using these style large transformer models. The crazy thing is, it's not just traditional tasks. These new models have so much knowledge inside them or maybe memorized information. They're able to solve lots of new problems that we hadn't thought about before in natural language. I see this as a field where there's a ton of innovation going. I can see it what's happening in academia. I can see all the things that are useful to industry. I promise you, if you take the time to learn how these models work, whether it's the transformers, the foundational models, learn how to use them, whether it's with images, with text, with video, it's going to pay off. There's going to be a huge demand for knowing how to use these.",
      "platforms": {
        "tiktok": {
          "video_id": "7089587445750189358",
          "url": "https://www.tiktok.com/@rajistics/video/7089587445750189358",
          "view_count": 11100,
          "upload_date": "2022-04-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9ba5a2535a9f44b18d1bba17c654da8a_1650673210~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=Wxyz5UjUePZZRlTTSeb3T0lMkxE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @chokokrem Best machine learning tools for competitions. Lots of great stuff here. #datascience #machinelearning #python #codetok ",
      "description": "Replying to @chokokrem Best machine learning tools for competitions. Lots of great stuff here. #datascience #machinelearning #python #codetok ",
      "upload_date": "2023-03-10",
      "total_views": 11000,
      "max_views": 11000,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "python",
        "tools",
        "use"
      ],
      "search_text": "Replying to @chokokrem Best machine learning tools for competitions. Lots of great stuff here. #datascience #machinelearning #python #codetok  codetok datascience machinelearning python tools use Do you want to know what machine learning tools winners use? The folks over at ML Contest put together a list of all the tools that recent Kaggle winner and other competition winners have used. I like this list because it both emphasizes accuracy for tools as well as tools that allow a lot of flexibility because often in competitions you want to run lots of different experiments. These tools help you do that. For programming languages, Python is way ahead. When we look at deep learning, PyTorch has been gaining ground and is really the go-to toolkit. When we're working with Python, Panda's NumPyScikit, that's the way to go. For NLP, Transformers. For Vision, both CNNs and Transformers are useful. For Tabular, the gradient-boosted machines like GBM, XGBoost, CapBoost, all are up there. One of the things you can do is ensemble those with deep learning models which often works. When we're looking at hyperparameter optimization, optinize. Techniques like K-fold cross-validation and ensembling are both crucial techniques for winning and to eke out the maximum accuracy. If you're trying to think about what hardware to use, these competition winners use some of them use cloud, some of them use local machines, some of them use Google Colab, all were Nvidia cards. This is a great list and I largely agree with it. If you feel any differently, add your suggestions in the comments.",
      "platforms": {
        "tiktok": {
          "video_id": "7208724875551231274",
          "url": "https://www.tiktok.com/@rajistics/video/7208724875551231274",
          "view_count": 11000,
          "upload_date": "2023-03-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/25a88f1ade5f4961a040239a536ce6fc_1678412068~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=p2EkxIBdDX3peNl7kcZBw0GqPRI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6126,
      "title": "Dolly from Databricks is an open source fine tuned instruction large language model that can be used for commercial uses! Databricks has take the time to share the dataset and training scripts its going to be a great place to get started. #datascience #machinelearning #largelanguagemodels #databricks #dolly #instructionfinetuning #finetuning References: Dolly blog post: https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm Dolly at Hugging Face: https://huggingface.co/databricks Dolly Github: https://github.com/databrickslabs/dolly",
      "description": "Dolly from Databricks is an open source fine tuned instruction large language model that can be used for commercial uses! Databricks has take the time to share the dataset and training scripts its going to be a great place to get started. #datascience #machinelearning #largelanguagemodels #databricks #dolly #instructionfinetuning #finetuning References: Dolly blog post: https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm Dolly at Hugging Face: https://huggingface.co/databricks Dolly Github: https://github.com/databrickslabs/dolly",
      "upload_date": "2023-04-15",
      "total_views": 10928,
      "max_views": 10100,
      "topics": [
        "animating",
        "databricks",
        "datascience",
        "dolly",
        "instructionfinetuning",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "open"
      ],
      "search_text": "Dolly from Databricks is an open source fine tuned instruction large language model that can be used for commercial uses! Databricks has take the time to share the dataset and training scripts its going to be a great place to get started. #datascience #machinelearning #largelanguagemodels #databricks #dolly #instructionfinetuning #finetuning References: Dolly blog post: https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm Dolly at Hugging Face: https://huggingface.co/databricks Dolly Github: https://github.com/databrickslabs/dolly animating databricks datascience dolly instructionfinetuning largelanguagemodels machinelearning model open",
      "platforms": {
        "tiktok": {
          "video_id": "7222430618347490602",
          "url": "https://www.tiktok.com/@rajistics/video/7222430618347490602",
          "view_count": 10100,
          "upload_date": "2023-04-15",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18016403518504011",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "JNSA9T8uwb0",
          "url": "https://www.youtube.com/watch?v=JNSA9T8uwb0",
          "view_count": 828,
          "upload_date": "2023-04-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6039,
      "title": "Replying to XYZ A quick tutorial using WizMap to visualize embeddings. The process is extracting your embeddings using dimensionality reduction to get them down to 2 or 3 dimensions and then plotting these 2 or 3 dimensions. Background by Jason Leung: https://unsplash.com/photos/UMncYEfO9-U WizMap: https://github.com/poloclub/wizmap Older videos: Instructor Embeddings: https://github.com/poloclub/wizmap Tensorboard Projector: https://www.tiktok.com/@rajistics/video/7250980121656446254?lang=en Latent Space: https://www.tiktok.com/@rajistics/video/7141786930118774058?lang=en Latent Space in Stable Diffusion: https://www.tiktok.com/@rajistics/video/7143359375409990958?lang=en",
      "description": "Replying to XYZ A quick tutorial using WizMap to visualize embeddings. The process is extracting your embeddings using dimensionality reduction to get them down to 2 or 3 dimensions and then plotting these 2 or 3 dimensions. Background by Jason Leung: https://unsplash.com/photos/UMncYEfO9-U WizMap: https://github.com/poloclub/wizmap Older videos: Instructor Embeddings: https://github.com/poloclub/wizmap Tensorboard Projector: https://www.tiktok.com/@rajistics/video/7250980121656446254?lang=en Latent Space: https://www.tiktok.com/@rajistics/video/7141786930118774058?lang=en Latent Space in Stable Diffusion: https://www.tiktok.com/@rajistics/video/7143359375409990958?lang=en",
      "upload_date": "2023-07-03",
      "total_views": 10904,
      "max_views": 10400,
      "topics": [
        "dimensions",
        "embedding",
        "embeddings",
        "going",
        "lang",
        "using",
        "video",
        "visualize",
        "wizmap"
      ],
      "search_text": "Replying to XYZ A quick tutorial using WizMap to visualize embeddings. The process is extracting your embeddings using dimensionality reduction to get them down to 2 or 3 dimensions and then plotting these 2 or 3 dimensions. Background by Jason Leung: https://unsplash.com/photos/UMncYEfO9-U WizMap: https://github.com/poloclub/wizmap Older videos: Instructor Embeddings: https://github.com/poloclub/wizmap Tensorboard Projector: https://www.tiktok.com/@rajistics/video/7250980121656446254?lang=en Latent Space: https://www.tiktok.com/@rajistics/video/7141786930118774058?lang=en Latent Space in Stable Diffusion: https://www.tiktok.com/@rajistics/video/7143359375409990958?lang=en dimensions embedding embeddings going lang using video visualize wizmap",
      "platforms": {
        "tiktok": {
          "video_id": "7251730365801516334",
          "url": "https://www.tiktok.com/@rajistics/video/7251730365801516334",
          "view_count": 10400,
          "upload_date": "2023-07-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18229463701169934",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-07-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "C8_4D6m5K0M",
          "url": "https://www.youtube.com/watch?v=C8_4D6m5K0M",
          "view_count": 504,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics ",
      "description": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics ",
      "upload_date": "2022-10-29",
      "total_views": 10900,
      "max_views": 10900,
      "topics": [
        "anscombesquartet",
        "data",
        "datascience",
        "datavisualization",
        "statistics",
        "visualize"
      ],
      "search_text": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics  anscombesquartet data datascience datavisualization statistics visualize Hey intern, I need you to analyze these datasets and tell me what's different. Yes sir. All right. Every one of these datasets is different, but when I analyze them and I look at the mean, the standard deviation, even the correlation between variables, they're all the same. I'm not sure what to do. I've been there. Try the plot function. Let me try that. Glad you see it now. It's always important to visualize your data. You can't just trust the numbers alone.",
      "platforms": {
        "tiktok": {
          "video_id": "7159972545561660714",
          "url": "https://www.tiktok.com/@rajistics/video/7159972545561660714",
          "view_count": 10900,
          "upload_date": "2022-10-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/cddf0a444ebf4d03b51c404b5e0d0fd5_1667061032~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=5yuxLMDyrdcdhuGtKF9UVw9Zezg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6479,
      "title": "Reinforcement learning with my Eat Melon! Demo This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhf",
      "description": "Reinforcement learning with my Eat Melon! Demo This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhf",
      "upload_date": "2023-08-30",
      "total_views": 10852,
      "max_views": 5526,
      "topics": [
        "chatgpt",
        "datascience",
        "demo",
        "eat",
        "learning",
        "machinelearning",
        "melon",
        "reinforcement",
        "reinforcementlearning",
        "rlhf",
        "techtok"
      ],
      "search_text": "Reinforcement learning with my Eat Melon! Demo This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhf chatgpt datascience demo eat learning machinelearning melon reinforcement reinforcementlearning rlhf techtok",
      "platforms": {
        "tiktok": {
          "video_id": "7273227375624752430",
          "url": "https://www.tiktok.com/@rajistics/video/7273227375624752430",
          "view_count": 5526,
          "upload_date": "2023-08-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CwlWdyNPsEm",
          "url": "https://www.instagram.com/reel/CwlWdyNPsEm",
          "view_count": 4661,
          "upload_date": "2023-08-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "l8sH9eOrLX8",
          "url": "https://www.youtube.com/watch?v=l8sH9eOrLX8",
          "view_count": 665,
          "upload_date": "2023-08-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Why you should use group partitioning #datascience #machinelearning #statistics #codetok #deeplearning #andrewng",
      "description": "Why you should use group partitioning #datascience #machinelearning #statistics #codetok #deeplearning #andrewng",
      "upload_date": "2022-06-02",
      "total_views": 10800,
      "max_views": 10800,
      "topics": [
        "andrewng",
        "codetok",
        "datascience",
        "deeplearning",
        "machinelearning",
        "statistics"
      ],
      "search_text": "Why you should use group partitioning #datascience #machinelearning #statistics #codetok #deeplearning #andrewng andrewng codetok datascience deeplearning machinelearning statistics I got a good one today. I got a mistake by one of the greatest data scientists out there. Andrew Ng published a paper on how you could detect pneumonia through chest x-rays. He used chest x-rays from a number of different patients. To evaluate the model, he did a traditional 80-20 split. Hmm. Do you see what could have gone wrong here? The dataset here isn't IID. It's not independent and identically distributed. We have multiple x-rays of some patients. What went wrong is that the model here could learn the anatomy of a person rather than whether they had pneumonia. This essentially allows the model to cheat. What they should have done and what they later did was use group partitioning. This ensures that similar groups are either in the training or in the evaluation part of the dataset and not split between the two.",
      "platforms": {
        "tiktok": {
          "video_id": "7104754733679906094",
          "url": "https://www.tiktok.com/@rajistics/video/7104754733679906094",
          "view_count": 10800,
          "upload_date": "2022-06-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2ebb02540ca747cabcfb2669d825b345_1654204622~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=7ko%2FdN3RkU4O8db6w%2FddB%2B6vVT0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6286,
      "title": "Histograms are a great visualization tool. Here are some caveats and tips for using histograms. #datascience #statistics #datavisualization #histogram",
      "description": "Histograms are a great visualization tool. Here are some caveats and tips for using histograms. #datascience #statistics #datavisualization #histogram",
      "upload_date": "2023-02-09",
      "total_views": 10759,
      "max_views": 10759,
      "topics": [
        "acturialscience",
        "autoinsurance",
        "datascience",
        "datavisualization",
        "great",
        "histogram",
        "histograms",
        "insurance",
        "machinelearning",
        "statistics",
        "variables"
      ],
      "search_text": "Histograms are a great visualization tool. Here are some caveats and tips for using histograms. #datascience #statistics #datavisualization #histogram acturialscience autoinsurance datascience datavisualization great histogram histograms insurance machinelearning statistics variables",
      "platforms": {
        "instagram": {
          "video_id": "17942742233536079",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Rhdd757lcOU",
          "url": "https://youtube.com/shorts/Rhdd757lcOU?feature=share",
          "view_count": 10759,
          "upload_date": "2023-02-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6477,
      "title": "GPT-4 showing amazing results in casual reasoning. For practical purposes experiments are more useful than causal modeling. However this paper shows the complexity of GPT-4. Causal Reasoning and Large Language Models: Opening a New Frontier for Causality: https://arxiv.org/abs/2305.00050 Background by George Dagerotip: https://unsplash.com/photos/ScITlIhACwo",
      "description": "GPT-4 showing amazing results in casual reasoning. For practical purposes experiments are more useful than causal modeling. However this paper shows the complexity of GPT-4. Causal Reasoning and Large Language Models: Opening a New Frontier for Causality: https://arxiv.org/abs/2305.00050 Background by George Dagerotip: https://unsplash.com/photos/ScITlIhACwo",
      "upload_date": "2023-05-07",
      "total_views": 10742,
      "max_views": 7131,
      "topics": [
        "amazing",
        "causal",
        "four",
        "gpt",
        "models",
        "reasoning",
        "results",
        "showing",
        "useful"
      ],
      "search_text": "GPT-4 showing amazing results in casual reasoning. For practical purposes experiments are more useful than causal modeling. However this paper shows the complexity of GPT-4. Causal Reasoning and Large Language Models: Opening a New Frontier for Causality: https://arxiv.org/abs/2305.00050 Background by George Dagerotip: https://unsplash.com/photos/ScITlIhACwo amazing causal four gpt models reasoning results showing useful",
      "platforms": {
        "tiktok": {
          "video_id": "7230449263552105770",
          "url": "https://www.tiktok.com/@rajistics/video/7230449263552105770",
          "view_count": 7131,
          "upload_date": "2023-05-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cr8hBxeAM1C",
          "url": "https://www.instagram.com/reel/Cr8hBxeAM1C",
          "view_count": 2859,
          "upload_date": "2023-05-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "P_fNbo8qaTg",
          "url": "https://www.youtube.com/watch?v=P_fNbo8qaTg",
          "view_count": 752,
          "upload_date": "2023-05-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Nat.dev playground is awesome. Should be a great reminder of the diversity of large language models. #datascience #machinelearning #largelanguagemodels #natdev #gpt3 ",
      "description": "Nat.dev playground is awesome. Should be a great reminder of the diversity of large language models. #datascience #machinelearning #largelanguagemodels #natdev #gpt3 ",
      "upload_date": "2023-03-10",
      "total_views": 10700,
      "max_views": 10700,
      "topics": [
        "datascience",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "models",
        "natdev"
      ],
      "search_text": "Nat.dev playground is awesome. Should be a great reminder of the diversity of large language models. #datascience #machinelearning #largelanguagemodels #natdev #gpt3  datascience gpt3 largelanguagemodels machinelearning models natdev It's time to upgrade your sandbox so you can compare multiple large language models. Most of you are used to a sandbox like this where you can ask it something and get the result back from one or maybe a couple of models. But take a look at this sandbox and all the variety of different models that are available here. We have models from Cohere that are available from Huggingface, from Anthropic, from other companies as well. So this gives you a wide range of models to be able to test that when you're thinking about the best prompt for your problem. My favorite part is the compare page where we can compare how different large language models do on the same prompt. And this is really important because there's a wide variety of large language models that are going to differ in terms of how fast they are, in terms of the domain knowledge they have, as well as the licensing. Some of these are free, some of these are paid. So a sandbox like this helps you make the best decision.",
      "platforms": {
        "tiktok": {
          "video_id": "7209054983738740010",
          "url": "https://www.tiktok.com/@rajistics/video/7209054983738740010",
          "view_count": 10700,
          "upload_date": "2023-03-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/95bbc85f707d49ba9d903ac4c0f34961_1678488925~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=HEEu8bGLNDejgoffiyPC7lwWKew%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @declinedher being above average. I will add citation in the comments. #statistics #regressiontothemean #aboveaverage",
      "description": "Reply to @declinedher being above average. I will add citation in the comments. #statistics #regressiontothemean #aboveaverage",
      "upload_date": "2022-02-10",
      "total_views": 10600,
      "max_views": 10600,
      "topics": [
        "aboveaverage",
        "average",
        "regressiontothemean",
        "statistics",
        "think",
        "thought"
      ],
      "search_text": "Reply to @declinedher being above average. I will add citation in the comments. #statistics #regressiontothemean #aboveaverage aboveaverage average regressiontothemean statistics think thought Let's talk about being above average. Most of us think our driving abilities are above average. 70% of high schoolers thought of themselves as above average leaders. 94% of college professors thought they had above average teaching ability. 87% of Stanford MBA students thought they ranked above the median in their class. So it's okay, we all think we're above average.",
      "platforms": {
        "tiktok": {
          "video_id": "7063229890430242094",
          "url": "https://www.tiktok.com/@rajistics/video/7063229890430242094",
          "view_count": 10600,
          "upload_date": "2022-02-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f04a56898cb245af865d4c42ebc791e8_1644536364~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=3c5mzyl%2Feem1H14UMHlrlGtzDm8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6655,
      "title": "Feature engineering is an important part of the machine learning lifecycle. It‚Äôs part art and skill. It takes time to learn and the best data scientists are good at feature engineering. Data science competitions like Kaggle often reward feature engineering skills. Tensorflow Playground: https://playground.tensorflow.org/ (I have older videos on the tensorflow playground) Background: bilalashrafmzon - https://pixabay.com/videos/animated-wallpaper-wallpaper-70960/ AI Characters with AI Parrot",
      "description": "Feature engineering is an important part of the machine learning lifecycle. It‚Äôs part art and skill. It takes time to learn and the best data scientists are good at feature engineering. Data science competitions like Kaggle often reward feature engineering skills. Tensorflow Playground: https://playground.tensorflow.org/ (I have older videos on the tensorflow playground) Background: bilalashrafmzon - https://pixabay.com/videos/animated-wallpaper-wallpaper-70960/ AI Characters with AI Parrot",
      "upload_date": "2024-02-24",
      "total_views": 10510,
      "max_views": 7340,
      "topics": [
        "data",
        "engineering",
        "feature",
        "model",
        "part",
        "playground",
        "tensorflow"
      ],
      "search_text": "Feature engineering is an important part of the machine learning lifecycle. It‚Äôs part art and skill. It takes time to learn and the best data scientists are good at feature engineering. Data science competitions like Kaggle often reward feature engineering skills. Tensorflow Playground: https://playground.tensorflow.org/ (I have older videos on the tensorflow playground) Background: bilalashrafmzon - https://pixabay.com/videos/animated-wallpaper-wallpaper-70960/ AI Characters with AI Parrot data engineering feature model part playground tensorflow Why feature engineering is so important? Is that real engineering? It doesn't sound like it. Feature engineering is when we transform the raw data into better features or variables that represent the underlying problem I'm working on. Why not dump all the raw data into a deep learning model. You can try it but the model might have trouble pulling that signal out so which might lead to long training times or inaccurate models. Sounds like you just can't afford the computer costs. Can you show us how feature engineering will help all of us? So you can Try this over at the Tensorflow Playground. When we're working on this more complicated data set, you can see the model here just isn't able to figure it out but if we take time to add some of the feature engineering from over here, the model is able to successfully classify those debt data. This is mind blowing. And for all, a few more examples. Some other examples might be pulling out the day of the week out of a daytime field or calculating BMI and numerical calculation from the raw height and weight or maybe do some sort of aggregation where you total how many purchases were made over the last month and use that as a feature.",
      "platforms": {
        "tiktok": {
          "video_id": "7339175177911127339",
          "url": "https://www.tiktok.com/@rajistics/video/7339175177911127339",
          "view_count": 3170,
          "upload_date": "2024-02-24",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C3u86atAB-_",
          "url": "https://www.instagram.com/reel/C3u86atAB-_",
          "view_count": 7340,
          "upload_date": "2024-02-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "-zzpDs4DM0o",
          "url": "https://youtube.com/shorts/-zzpDs4DM0o",
          "view_count": 0,
          "upload_date": "2024-02-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It’s great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant ",
      "description": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It’s great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant ",
      "upload_date": "2023-02-19",
      "total_views": 10500,
      "max_views": 10500,
      "topics": [
        "datascience",
        "feedback",
        "help",
        "information",
        "open",
        "openassistant"
      ],
      "search_text": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It’s great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant  datascience feedback help information open openassistant Could you imagine having your own personal version of chat gpt? Open Assistant is going to be an open source version of chat gpt, where not only can you ask it for information, but can also connect to other pieces of information through search engines as well. And all of this is designed to be run on personal commodity hardware. Wow. So the first step to building something like this is collecting human feedback, as we know human feedback is an essential part of all these latest language models. To kickstart the human feedback, they're going to be able to take advantage of a dataset that Anthropic has shared. And we're teaching it, for example, how to talk nice when somebody asks about offensive information, how to gently calm down a question, or even push back on the KK. Open Assistant is also taking the time to collect their own information. They've built an entire website designed to help solicit information from the public. Now you can take a look at these example tasks. So here's one that it's looking at Linux, but look at how somebody's supposed to give feedback. This is the type of information that's useful to help work with these language models. If you have some time, help out and contribute. They could use some help collecting feedback. Now they're busy collecting data, but hopefully in a few months, we're going to have models that we'll be capable of interacting with and help us retrieve information. There's some early versions of the models now, but the best is yet to come.",
      "platforms": {
        "tiktok": {
          "video_id": "7201945207829564714",
          "url": "https://www.tiktok.com/@rajistics/video/7201945207829564714",
          "view_count": 10500,
          "upload_date": "2023-02-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/feefaf5243f94d159614be8284ae8b43_1676833553~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=nqKSMEWa9VB2YayjG6BXOZUWQ2g%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "How are you using similarity search? #nearestneighbor #annoy #spotify #datascience #statistics #codetok #python #similaritysearch",
      "description": "How are you using similarity search? #nearestneighbor #annoy #spotify #datascience #statistics #codetok #python #similaritysearch",
      "upload_date": "2022-06-26",
      "total_views": 10500,
      "max_views": 10500,
      "topics": [
        "annoy",
        "data",
        "datascience",
        "nearestneighbor",
        "spotify",
        "statistics"
      ],
      "search_text": "How are you using similarity search? #nearestneighbor #annoy #spotify #datascience #statistics #codetok #python #similaritysearch annoy data datascience nearestneighbor spotify statistics Let's talk about how Spotify's quest for better music recommendations helped all of us with data science. Data scientists work by taking concepts, whether it's users or things, and converting them into numbers, where numbers that are closer together mean the objects are similar to each other. To make these numbers useful, let's take three people that like music. One that likes Taylor, one that likes Miley, and one that likes the Rolling Stones. If we actually compare the differences in numbers, we'll find out that the person that likes the Rolling Stones is farther away than the other two. So data scientists often want to measure the distance between neighbors. This is a similarity score. Now this quickly gets complicated as you start having lots and lots of points to measure between, and if you start increasing the number of dimensions as well. And this is where Spotify's annoyance comes into play. It provides an approximate nearest neighbor search, which is a super fast way compared to a brute force. There was a ton of uses beyond music, and this is why annoy became very popular. It's even implemented in lots of other languages, because this is a common thing data scientists want to do is figure out what things are closer than to each other. Annoys still popular, but there's lots of other libraries like Face that are out there doing these similarity searches. But it's a good thing for all data scientists to know, and props to Eric for putting this out there for all of us to start using.",
      "platforms": {
        "tiktok": {
          "video_id": "7113596259814018350",
          "url": "https://www.tiktok.com/@rajistics/video/7113596259814018350",
          "view_count": 10500,
          "upload_date": "2022-06-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/599f6614d1054bcd8e31539cd179e95a_1656263200~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=n0hk8V9Yr13KIImYYOCIPdffHQQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6101,
      "title": "The power of prompting! How to use a general purpose model to be a special purpose fine tuned model. It’s really important to learn good prompting strategies. This is from last year, but still good advice.  #largelanguagemodels #promptengineering #openai #gpt4 #medpalm #rajistics 5 Pillars of Prompting: Prompt Engineering: From Words to Art and Copy https://www.saxifrage.xyz/post/prompt-engineering Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine https://arxiv.org/pdf/2311.16452.pdf",
      "description": "The power of prompting! How to use a general purpose model to be a special purpose fine tuned model. It’s really important to learn good prompting strategies. This is from last year, but still good advice.  #largelanguagemodels #promptengineering #openai #gpt4 #medpalm #rajistics 5 Pillars of Prompting: Prompt Engineering: From Words to Art and Copy https://www.saxifrage.xyz/post/prompt-engineering Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine https://arxiv.org/pdf/2311.16452.pdf",
      "upload_date": "2024-12-01",
      "total_views": 10453,
      "max_views": 7347,
      "topics": [
        "dspy",
        "examples",
        "going",
        "gpt4",
        "largelanguagemodels",
        "model",
        "programming",
        "prompting",
        "purpose",
        "want"
      ],
      "search_text": "The power of prompting! How to use a general purpose model to be a special purpose fine tuned model. It’s really important to learn good prompting strategies. This is from last year, but still good advice.  #largelanguagemodels #promptengineering #openai #gpt4 #medpalm #rajistics 5 Pillars of Prompting: Prompt Engineering: From Words to Art and Copy https://www.saxifrage.xyz/post/prompt-engineering Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine https://arxiv.org/pdf/2311.16452.pdf dspy examples going gpt4 largelanguagemodels model programming prompting purpose want Microsoft just dropped some research on how they used a general purpose model like GPT-4 to beat a fine-tuned model. The secret was prompting, let's talk about the techniques they used and how you can improve your prompting. When they started, they wanted to be a fine-tuned model like MedPOM. They started by using zero-shot learning with GPT-4 out of the box. It didn't do so well, didn't beat it, but then they tried adding new techniques. They added random few-shot learning, chain of thought, caniars neighbor's few-shot learning to get better examples in there. Even an ensemble approach that tried to overcome things like positional bias and take the most consistent approach. You can read all about all those approaches in the paper. The lesson here is good prompting strategies can make a difference. And if you're new here, let's go over a simple framework of the five pillars you should know for prompting. The first pillar is you want to give the model some direction. You wanted to give it some idea of what you want, maybe by imagining some type of role play, what type of famous person to follow. The second thing is you want to describe the format. For example, I want JSON format. Here's what I want the specific keys to be. Third, provide examples. Examples make a big difference. You want to use at least a couple of examples. At some point, you hit a point of diminishing returns. And if you have too many examples, you might as well think about fine-tuning. The fourth pillar is you need to evaluate quality. You're going to have to run lots of little tests, figure out what the errors are, how to improve the model. That continuous iteration makes a big difference. And the last thing I've talked about here is when you have complex tasks, split that into multiple prompts. Using all these strategies together, you can take these general purpose large language models and do amazing things.",
      "platforms": {
        "tiktok": {
          "video_id": "7443490857652653358",
          "url": "https://www.tiktok.com/@rajistics/video/7443490857652653358",
          "view_count": 7347,
          "upload_date": "2024-12-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oIbllOABfAENAYDEI4zMeBIAZFjQSmvAfkEEHk~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=JwA3kYJOOindr%2FLdtjkdYXo2ZRc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18009491267492467",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "fXrNAORcS5I",
          "url": "https://www.youtube.com/watch?v=fXrNAORcS5I",
          "view_count": 3106,
          "upload_date": "2024-06-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6429,
      "title": "Working with small datasets. Several tips including using crossvalidation, models like lasso, and running multiple interations with different random seeds. #datascience #machinelearning #crossvalidation #elasticnet #lasso #randomseed",
      "description": "Working with small datasets. Several tips including using crossvalidation, models like lasso, and running multiple interations with different random seeds. #datascience #machinelearning #crossvalidation #elasticnet #lasso #randomseed",
      "upload_date": "2023-03-06",
      "total_views": 10401,
      "max_views": 7106,
      "topics": [
        "crossvalidation",
        "data",
        "datascience",
        "elasticnet",
        "lasso",
        "like",
        "machinelearning",
        "randomseed"
      ],
      "search_text": "Working with small datasets. Several tips including using crossvalidation, models like lasso, and running multiple interations with different random seeds. #datascience #machinelearning #crossvalidation #elasticnet #lasso #randomseed crossvalidation data datascience elasticnet lasso like machinelearning randomseed",
      "platforms": {
        "tiktok": {
          "video_id": "7207608623835581742123",
          "url": "https://www.tiktok.com/@rajistics/video/7207608623835581742123",
          "view_count": 7106,
          "upload_date": "2023-03-06",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cpf9WX0gCxK",
          "url": "https://www.instagram.com/reel/Cpf9WX0gCxK",
          "view_count": 205,
          "upload_date": "2023-03-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "r9PbyGhCegs",
          "url": "https://www.youtube.com/watch?v=r9PbyGhCegs",
          "view_count": 3090,
          "upload_date": "2023-03-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#greenscreenvideo Jealous. Go see how bad Meta bungled their chatbot @rajistics ",
      "description": "#greenscreenvideo Jealous. Go see how bad Meta bungled their chatbot @rajistics ",
      "upload_date": "2023-01-28",
      "total_views": 10400,
      "max_views": 10400,
      "topics": [
        "bad",
        "bungled",
        "greenscreenvideo",
        "jealous",
        "meta",
        "see"
      ],
      "search_text": "#greenscreenvideo Jealous. Go see how bad Meta bungled their chatbot @rajistics  bad bungled greenscreenvideo jealous meta see salt 2016 2016 2015 2014 Jake",
      "platforms": {
        "tiktok": {
          "video_id": "7193768470360624426",
          "url": "https://www.tiktok.com/@rajistics/video/7193768470360624426",
          "view_count": 10400,
          "upload_date": "2023-01-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4b9973399cdd42d9817a323cb6a286f0_1674929755~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=dW4BJnHn3kX5Tu5Jf147tt4VPbQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "It’s almost here. Full support for pandas in sklearn pipelines. #machinelearning #datascience #codetok #python #sklearn #sci-kit ",
      "description": "It’s almost here. Full support for pandas in sklearn pipelines. #machinelearning #datascience #codetok #python #sklearn #sci-kit ",
      "upload_date": "2022-10-18",
      "total_views": 10400,
      "max_views": 10400,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "python",
        "sci",
        "sklearn"
      ],
      "search_text": "It’s almost here. Full support for pandas in sklearn pipelines. #machinelearning #datascience #codetok #python #sklearn #sci-kit  codetok datascience machinelearning python sci sklearn If you ask a data scientist how to do one hot encoding, you'll probably get at least six different answers. Let me explain why this is a problem and how the Scikit team is trying to improve this. When it comes to algorithms, data scientists have a pretty standard way of talking about them. When it comes to pre-processing that data for those algorithms, there's very little standards. I like to use the SK Learn Pipelines like this, but it's a bit awkward and they're far from universally used, and so what you see is a huge disparity in how people are actually setting up their pipelines. The folks at Scikit are coming to help, and the new set output API in the 1.2 version is going to make it easier to work with our pre-processing pipeline because we've got a fully supported pandas, we can even track feature names through there. I'm hoping that this will be the push to standardize a lot of ML pipelines on SK Learn. So go check out the feature, try it, and if you have feedback, share it with the development team.",
      "platforms": {
        "tiktok": {
          "video_id": "7155641337218387246",
          "url": "https://www.tiktok.com/@rajistics/video/7155641337218387246",
          "view_count": 10400,
          "upload_date": "2022-10-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e3a06967b38d413aaa279e8f0d233b9c_1666052585~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=vwTbu6mHNpja2my5PxcVI2tbKtU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6268,
      "title": "OpenAI's new models look great and incorporate the latest advances. But don't forget about the open source as well as some tips for thinking about embeddings. Massive Text Embedding Benchmark (MTEB) Leaderboard - https://huggingface.co/spaces/mteb/leaderboard MatFormer: Nested Transformer for Elastic Inference - https://arxiv.org/pdf/2310.07707.pdf Kunal's math: https://twitter.com/TangriKunal/status/1748114153833660766 Background: davidfoxx - https://pixabay.com/videos/lake-peaceful-dock-boat-birds-fog-172007/",
      "description": "OpenAI's new models look great and incorporate the latest advances. But don't forget about the open source as well as some tips for thinking about embeddings. Massive Text Embedding Benchmark (MTEB) Leaderboard - https://huggingface.co/spaces/mteb/leaderboard MatFormer: Nested Transformer for Elastic Inference - https://arxiv.org/pdf/2310.07707.pdf Kunal's math: https://twitter.com/TangriKunal/status/1748114153833660766 Background: davidfoxx - https://pixabay.com/videos/lake-peaceful-dock-boat-birds-fog-172007/",
      "upload_date": "2024-01-28",
      "total_views": 10373,
      "max_views": 7741,
      "topics": [
        "embedding",
        "embeddings",
        "leaderboard",
        "models",
        "mteb",
        "new",
        "open",
        "openai",
        "pdf"
      ],
      "search_text": "OpenAI's new models look great and incorporate the latest advances. But don't forget about the open source as well as some tips for thinking about embeddings. Massive Text Embedding Benchmark (MTEB) Leaderboard - https://huggingface.co/spaces/mteb/leaderboard MatFormer: Nested Transformer for Elastic Inference - https://arxiv.org/pdf/2310.07707.pdf Kunal's math: https://twitter.com/TangriKunal/status/1748114153833660766 Background: davidfoxx - https://pixabay.com/videos/lake-peaceful-dock-boat-birds-fog-172007/ embedding embeddings leaderboard models mteb new open openai pdf",
      "platforms": {
        "tiktok": {
          "video_id": "7329171965074345258",
          "url": "https://www.tiktok.com/@rajistics/video/7329171965074345258",
          "view_count": 7741,
          "upload_date": "2024-01-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18410998747062528",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8zMZTJMHaoI",
          "url": "https://youtube.com/shorts/8zMZTJMHaoI",
          "view_count": 2632,
          "upload_date": "2024-01-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6532,
      "title": "Accuracy versus Interpretability/Explainability is a typical tradeoff in machine learning. Depending on your use case you may favor one over the other. Understanding this tradeoff helps you make better decisions. Manipulating and Measuring Model Interpretability: https://arxiv.org/pdf/1802.07810.pdf",
      "description": "Accuracy versus Interpretability/Explainability is a typical tradeoff in machine learning. Depending on your use case you may favor one over the other. Understanding this tradeoff helps you make better decisions. Manipulating and Measuring Model Interpretability: https://arxiv.org/pdf/1802.07810.pdf",
      "upload_date": "2023-08-08",
      "total_views": 10297,
      "max_views": 9680,
      "topics": [
        "accuracy",
        "dynamics",
        "explainability",
        "interpretability",
        "measuring",
        "pdf",
        "riveter",
        "social",
        "tradeoff",
        "versus"
      ],
      "search_text": "Accuracy versus Interpretability/Explainability is a typical tradeoff in machine learning. Depending on your use case you may favor one over the other. Understanding this tradeoff helps you make better decisions. Manipulating and Measuring Model Interpretability: https://arxiv.org/pdf/1802.07810.pdf accuracy dynamics explainability interpretability measuring pdf riveter social tradeoff versus",
      "platforms": {
        "tiktok": {
          "video_id": "7264756482770619694",
          "url": "https://www.tiktok.com/@rajistics/video/7264756482770619694",
          "view_count": 9680,
          "upload_date": "2023-08-08",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CvqkZnMNnsN",
          "url": "https://www.instagram.com/reel/CvqkZnMNnsN",
          "view_count": 449,
          "upload_date": "2023-08-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "qtGe_bG6c6Y",
          "url": "https://www.youtube.com/watch?v=qtGe_bG6c6Y",
          "view_count": 168,
          "upload_date": "2023-08-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6626,
      "title": "Animated Drawings is really fun model from Meta. It can take a sketch drawing and then animate it. Great example of combining several image models together. #datascience #machinelearning #animateddrawings #meta #FAIRAnimatedDrawings #imageclassification #imagesegmentation #posedetection Demo: https://sketch.metademolab.com/ Code: https://github.com/facebookresearch/AnimatedDrawings Paper: https://arxiv.org/pdf/2303.12741.pdf Project website: http://www.fairanimateddrawings.com/",
      "description": "Animated Drawings is really fun model from Meta. It can take a sketch drawing and then animate it. Great example of combining several image models together. #datascience #machinelearning #animateddrawings #meta #FAIRAnimatedDrawings #imageclassification #imagesegmentation #posedetection Demo: https://sketch.metademolab.com/ Code: https://github.com/facebookresearch/AnimatedDrawings Paper: https://arxiv.org/pdf/2303.12741.pdf Project website: http://www.fairanimateddrawings.com/",
      "upload_date": "2023-04-14",
      "total_views": 10276,
      "max_views": 6984,
      "topics": [
        "animateddrawings",
        "datascience",
        "fairanimateddrawings",
        "imageclassification",
        "machinelearning",
        "meta"
      ],
      "search_text": "Animated Drawings is really fun model from Meta. It can take a sketch drawing and then animate it. Great example of combining several image models together. #datascience #machinelearning #animateddrawings #meta #FAIRAnimatedDrawings #imageclassification #imagesegmentation #posedetection Demo: https://sketch.metademolab.com/ Code: https://github.com/facebookresearch/AnimatedDrawings Paper: https://arxiv.org/pdf/2303.12741.pdf Project website: http://www.fairanimateddrawings.com/ animateddrawings datascience fairanimateddrawings imageclassification machinelearning meta Is this magical? This AI is turning any drawing into a piece of animation. Meta's on fire releasing models. Let's walk through exactly what's going on with this one. The first step of the model is human figure detection and what it does is figures out where the human is in the picture. Draws a bounding box around that. Then that passes over to a segmentation that looks inside that bounding box, tries to segment out exactly where the person is. It then uses that bounding box to do a pose estimation and you can see here where it's identifying the joints on the figure and once it has the joints, the next step is just animating those joints for depending on the type of movement you want. Men is doing a great job sharing this out. There's code, there's data set, there's a demo that you can even upload any handwritten sketch and get it animated. It's a lot of fun. Go check it out.",
      "platforms": {
        "tiktok": {
          "video_id": "7221722910937976107",
          "url": "https://www.tiktok.com/@rajistics/video/7221722910937976107",
          "view_count": 3292,
          "upload_date": "2023-04-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cq_9pGhAgvt",
          "url": "https://www.instagram.com/reel/Cq_9pGhAgvt",
          "view_count": 6984,
          "upload_date": "2023-04-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6186,
      "title": "Updated! I am an idiot - This video explains how Model Context Protocol (MCP) allows language models like Claude to interact with external tools through a local server. We walk through a working example using a simple MCP server that returns data from a RAG system, and show how it’s configured to run with Cursor. The goal is to show how MCP simplifies tool integration for LLMs by abstracting command logic into a server.",
      "description": "Updated! I am an idiot - This video explains how Model Context Protocol (MCP) allows language models like Claude to interact with external tools through a local server. We walk through a working example using a simple MCP server that returns data from a RAG system, and show how it’s configured to run with Cursor. The goal is to show how MCP simplifies tool integration for LLMs by abstracting command logic into a server.",
      "upload_date": "2025-04-04",
      "total_views": 10202,
      "max_views": 6217,
      "topics": [
        "claude",
        "context",
        "explained",
        "like",
        "mcp",
        "model",
        "quickly",
        "server",
        "show",
        "tool"
      ],
      "search_text": "Updated! I am an idiot - This video explains how Model Context Protocol (MCP) allows language models like Claude to interact with external tools through a local server. We walk through a working example using a simple MCP server that returns data from a RAG system, and show how it’s configured to run with Cursor. The goal is to show how MCP simplifies tool integration for LLMs by abstracting command logic into a server. claude context explained like mcp model quickly server show tool What's up with this MCP server? Is it some Jenny? I think it's all over my feeds. It's another kind of abstraction. So MCP helps clients like Claude call other tools. Is this like calling APIs? Exactly. But cleaner. So when I make this query in Claude, it's calling a tool. And that tool is using an MCP server to actually go and grab the answer and return it. So instead of Claude being creative, it's connecting to a tool where a server that has the answers. So what's this server? Check out the code. Here you'll see I describe what this tool is about. And then I have some commands to run, in this case, a query against a rag solution. But this could be other stuff. I could call a weather API, database queries, mouse jiggler, whatever you need. So it's just a way to abstract those tool calls. I could do this. Exactly. Instead of stuffing those API calls into your prompt, you offload it onto the server. Check it out. I set this up on cursor. Really? Show me. So I took that server, I put it on my local machine, pointed this config file over to it. Now when I'm inside cursor, it knows that I have this tool. And when I make a query like for semiconductors, it automatically connects and pulls out the code that I need. Look at you. Now always be careful with any abstraction because things could be hiding. You're always so worried. It's time to learn about vibe coding.",
      "platforms": {
        "tiktok": {
          "video_id": "7489262931176197406",
          "url": "https://www.tiktok.com/@rajistics/video/7489262931176197406",
          "view_count": 6217,
          "upload_date": "2025-04-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEDYdtZvIziBrqiWAmBv80i8oa5eAAzIokNmIC~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=ZEFROQIjzeeLOtAWd9xWOweyLqc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18044921498118134",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "LwZF4WEomMo",
          "url": "https://www.youtube.com/watch?v=LwZF4WEomMo",
          "view_count": 3985,
          "upload_date": "2025-04-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Short summary of my linger video on effieciently training a latge language model using PEFT and LoRA. #datascience #machinelearning #largelanguagemodels #flant5 #peft #LoRA #finetuning ",
      "description": "Short summary of my linger video on effieciently training a latge language model using PEFT and LoRA. #datascience #machinelearning #largelanguagemodels #flant5 #peft #LoRA #finetuning ",
      "upload_date": "2023-03-27",
      "total_views": 10200,
      "max_views": 10200,
      "topics": [
        "datascience",
        "largelanguagemodels",
        "lora",
        "machinelearning",
        "model",
        "peft"
      ],
      "search_text": "Short summary of my linger video on effieciently training a latge language model using PEFT and LoRA. #datascience #machinelearning #largelanguagemodels #flant5 #peft #LoRA #finetuning  datascience largelanguagemodels lora machinelearning model peft Do you want to know how to efficiently train a large language model? I just finished up on how to do this with a Google Flan T5 model based on Phillip's blog post. I first started by using PEFT, which is a library that has a lot of efficient ways for working with large models. I then used Laura, which is low rank adoption. And this is an approach that decomposes the large language models. So I end up just having to train a little bit of parameters, less than 1% in this case. With this approach, I was able to fine tune a model using only $13 worth of compute versus $300 of compute for fine tuning the entire model. So this is the advantage of using Laura. The other thing is this model actually came out with a higher accuracy as well. Go check out the full video if you want to see all the details.",
      "platforms": {
        "tiktok": {
          "video_id": "7215377053581888810",
          "url": "https://www.tiktok.com/@rajistics/video/7215377053581888810",
          "view_count": 10200,
          "upload_date": "2023-03-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/96de84001cc04c8ba880aaed5c15ade2_1679960897~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=LUzZY%2B%2FcnMcv7s1NZtpy3p%2FaJzs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6527,
      "title": "ChatGPT with the Code Interpreter can do a lot of common data science tasks. We are going to see more tools help with routine data science tasks. #datascience #machinelearning #chatgpt #codeinterpreter #rajistics What Should Data Science Education Do with Large Language Models? https://arxiv.org/pdf/2307.02792v2.pdf",
      "description": "ChatGPT with the Code Interpreter can do a lot of common data science tasks. We are going to see more tools help with routine data science tasks. #datascience #machinelearning #chatgpt #codeinterpreter #rajistics What Should Data Science Education Do with Large Language Models? https://arxiv.org/pdf/2307.02792v2.pdf",
      "upload_date": "2023-09-29",
      "total_views": 10199,
      "max_views": 7708,
      "topics": [
        "chatgpt",
        "codeinterpreter",
        "data",
        "datascience",
        "education",
        "machinelearning",
        "science"
      ],
      "search_text": "ChatGPT with the Code Interpreter can do a lot of common data science tasks. We are going to see more tools help with routine data science tasks. #datascience #machinelearning #chatgpt #codeinterpreter #rajistics What Should Data Science Education Do with Large Language Models? https://arxiv.org/pdf/2307.02792v2.pdf chatgpt codeinterpreter data datascience education machinelearning science",
      "platforms": {
        "tiktok": {
          "video_id": "7284035314375511338",
          "url": "https://www.tiktok.com/@rajistics/video/7284035314375511338",
          "view_count": 7708,
          "upload_date": "2023-09-29",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CxwV97utQNH",
          "url": "https://www.instagram.com/reel/CxwV97utQNH",
          "view_count": 2300,
          "upload_date": "2023-09-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "SntL93SHOKU",
          "url": "https://www.youtube.com/watch?v=SntL93SHOKU",
          "view_count": 191,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6652,
      "title": "When you build a synthetic dataset you know where the noise is and where the signal is. This lets you better assess techniques for feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability #rajistics",
      "description": "When you build a synthetic dataset you know where the noise is and where the signal is. This lets you better assess techniques for feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability #rajistics",
      "upload_date": "2024-01-25",
      "total_views": 10148,
      "max_views": 5730,
      "topics": [
        "build",
        "datascience",
        "explainability",
        "features",
        "know",
        "machinelearning",
        "synthetic",
        "syntheticdata"
      ],
      "search_text": "When you build a synthetic dataset you know where the noise is and where the signal is. This lets you better assess techniques for feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability #rajistics build datascience explainability features know machinelearning synthetic syntheticdata Why are you so confident about your datascience skills? What's your secret? I do have a secret. I build my skills and confidence by using synthetic data sets. What's a synthetic data set? So, let me show you how I create one. My favorite is Madeline and there's a nice function in SK Learn that allows you to use it. All I have to do here is tell it how many features I want in the overall dataset as well as how many informative features. So, wait. So, when you build that dataset yourself, you know which features are actually useful in adding information versus just kind of noise. I get it. Yeah, so when I go to do feature selection, training my model as well as explainability. I know which features should show up as the most important features for a model because I set it up that way. Wow, this is cool. It's like you know the answers already. No wonder you have more confidence and you better understand kind of your tools.",
      "platforms": {
        "tiktok": {
          "video_id": "7328050066789387566",
          "url": "https://www.tiktok.com/@rajistics/video/7328050066789387566",
          "view_count": 4418,
          "upload_date": "2024-01-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C2hzTiMAPxY",
          "url": "https://www.instagram.com/reel/C2hzTiMAPxY",
          "view_count": 5730,
          "upload_date": "2024-01-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6539,
      "title": "Some tips for deploying large language models like Llama. Start by building some benchmarks for your tasks to assess how your model performs on different GPUs. If it's too slow think about using a model size model using quantization and looking for an improved model serving solution. I mentioned a lot of packages but this is a fast-moving area. #largelanguagemodels #deployment #quantization Hamel's LLM inference analysis: https://hamel.dev/notes/llm/03_inference.html",
      "description": "Some tips for deploying large language models like Llama. Start by building some benchmarks for your tasks to assess how your model performs on different GPUs. If it's too slow think about using a model size model using quantization and looking for an improved model serving solution. I mentioned a lot of packages but this is a fast-moving area. #largelanguagemodels #deployment #quantization Hamel's LLM inference analysis: https://hamel.dev/notes/llm/03_inference.html",
      "upload_date": "2023-07-30",
      "total_views": 10129,
      "max_views": 9548,
      "topics": [
        "deploying",
        "deployment",
        "hamel",
        "largelanguagemodels",
        "llm",
        "model",
        "quantization",
        "tips",
        "using"
      ],
      "search_text": "Some tips for deploying large language models like Llama. Start by building some benchmarks for your tasks to assess how your model performs on different GPUs. If it's too slow think about using a model size model using quantization and looking for an improved model serving solution. I mentioned a lot of packages but this is a fast-moving area. #largelanguagemodels #deployment #quantization Hamel's LLM inference analysis: https://hamel.dev/notes/llm/03_inference.html deploying deployment hamel largelanguagemodels llm model quantization tips using",
      "platforms": {
        "tiktok": {
          "video_id": "7261758952260570414",
          "url": "https://www.tiktok.com/@rajistics/video/7261758952260570414",
          "view_count": 9548,
          "upload_date": "2023-07-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CvVxQLYA3c1",
          "url": "https://www.instagram.com/reel/CvVxQLYA3c1",
          "view_count": 452,
          "upload_date": "2023-07-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_3Jboq2Pxgc",
          "url": "https://www.youtube.com/watch?v=_3Jboq2Pxgc",
          "view_count": 129,
          "upload_date": "2023-07-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6468,
      "title": "Great week for AI! OpenAI dropped SORA for text to video Google with Gemini 1.5 Pro with a longer context length and Meta released V-JEPA with a self-supervised approach for videos. Background by tommyvideo - https://pixabay.com/videos/love-hearts-valentine-symbol-shape-5240/ #openai #meta #google #rajistics #sora #gemini",
      "description": "Great week for AI! OpenAI dropped SORA for text to video Google with Gemini 1.5 Pro with a longer context length and Meta released V-JEPA with a self-supervised approach for videos. Background by tommyvideo - https://pixabay.com/videos/love-hearts-valentine-symbol-shape-5240/ #openai #meta #google #rajistics #sora #gemini",
      "upload_date": "2024-02-17",
      "total_views": 10119,
      "max_views": 5130,
      "topics": [
        "gemini",
        "google",
        "meta",
        "openai",
        "sora",
        "videos"
      ],
      "search_text": "Great week for AI! OpenAI dropped SORA for text to video Google with Gemini 1.5 Pro with a longer context length and Meta released V-JEPA with a self-supervised approach for videos. Background by tommyvideo - https://pixabay.com/videos/love-hearts-valentine-symbol-shape-5240/ #openai #meta #google #rajistics #sora #gemini gemini google meta openai sora videos",
      "platforms": {
        "tiktok": {
          "video_id": "7336605486743129386",
          "url": "https://www.tiktok.com/@rajistics/video/7336605486743129386",
          "view_count": 4105,
          "upload_date": "2024-02-17",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C3dHufAAWt6",
          "url": "https://www.instagram.com/reel/C3dHufAAWt6",
          "view_count": 5130,
          "upload_date": "2024-02-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "zE1ofj_DTEg",
          "url": "https://youtube.com/shorts/zE1ofj_DTEg",
          "view_count": 884,
          "upload_date": "2024-02-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6519,
      "title": "To build generative AI models like the text-to-SQL system by Snowflake it is important to create a realistic and challenging training dataset rather than relying on academic benchmarks that may be overly simplistic. Customized evaluation metrics that go beyond simple similarity scores but avoid the strictness of execution-based metrics such as using language models for partial credit scoring are valuable. Prompting strategies that provide relevant context like table metadata can improve performance. Finally using a strong base model like Mistral Large can push the boundaries of what is achievable allowing Snowflake's system to outperform even GPT-4 on the text-to-SQL task. Read the series: Part 1: All evaluation data sets are wrong some are useful. https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-95114114aab9 Part 2: Expanding Evaluation to be user-centric...with LLMs https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-1a33b3ee0d37 Part 3: Retrieve Prompt Generate https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-05490f0d1ac7 Part 4. Mistral & Snowflake: The New Frontier in SQL Copilot Products https://medium.com/snowflake/mistral-snowflake-the-new-frontier-in-sql-copilot-products-f71b8a939899",
      "description": "To build generative AI models like the text-to-SQL system by Snowflake it is important to create a realistic and challenging training dataset rather than relying on academic benchmarks that may be overly simplistic. Customized evaluation metrics that go beyond simple similarity scores but avoid the strictness of execution-based metrics such as using language models for partial credit scoring are valuable. Prompting strategies that provide relevant context like table metadata can improve performance. Finally using a strong base model like Mistral Large can push the boundaries of what is achievable allowing Snowflake's system to outperform even GPT-4 on the text-to-SQL task. Read the series: Part 1: All evaluation data sets are wrong some are useful. https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-95114114aab9 Part 2: Expanding Evaluation to be user-centric...with LLMs https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-1a33b3ee0d37 Part 3: Retrieve Prompt Generate https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-05490f0d1ac7 Part 4. Mistral & Snowflake: The New Frontier in SQL Copilot Products https://medium.com/snowflake/mistral-snowflake-the-new-frontier-in-sql-copilot-products-f71b8a939899",
      "upload_date": "2024-03-25",
      "total_views": 10104,
      "max_views": 8080,
      "topics": [
        "data",
        "evaluation",
        "generative",
        "lessons",
        "like",
        "medium",
        "model",
        "part",
        "sequel",
        "snowflake",
        "sql",
        "text"
      ],
      "search_text": "To build generative AI models like the text-to-SQL system by Snowflake it is important to create a realistic and challenging training dataset rather than relying on academic benchmarks that may be overly simplistic. Customized evaluation metrics that go beyond simple similarity scores but avoid the strictness of execution-based metrics such as using language models for partial credit scoring are valuable. Prompting strategies that provide relevant context like table metadata can improve performance. Finally using a strong base model like Mistral Large can push the boundaries of what is achievable allowing Snowflake's system to outperform even GPT-4 on the text-to-SQL task. Read the series: Part 1: All evaluation data sets are wrong some are useful. https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-95114114aab9 Part 2: Expanding Evaluation to be user-centric...with LLMs https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-1a33b3ee0d37 Part 3: Retrieve Prompt Generate https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-05490f0d1ac7 Part 4. Mistral & Snowflake: The New Frontier in SQL Copilot Products https://medium.com/snowflake/mistral-snowflake-the-new-frontier-in-sql-copilot-products-f71b8a939899 data evaluation generative lessons like medium model part sequel snowflake sql text Important steps for solving generative AI problems based on the text to sequel use case. First, many common training data sets are built by academics. They're often too simple, not reflective of the real world. For example, in text to sequel, spyders are commonly used dataset but it's been saturated with models hitting 90 percent. Trying to improve upon this, you're just overfitting to the corks of the dataset. To get past this, you want to build your own training data sets. Second, pick a good evaluation metrics. For text to sequel, simple NLP similarity metrics won't you get executable sequel but if you focus on just execution as a metric that can be overly strict you might throw out code that includes an extra column that could be informative you'd also need all the data which is going to limit the size of your data set Snowflake instead created a new metrics execution score which uses a language model to judge the sequel code this model uses that model as a judge where you get partial credit for fuzzy matches and now you don't have to have all the data they tested it correlates well with the actual execution score Third, you want to prompt the model smartly. For Texta Sequel, this means feeding in information about relevant tables, data sets, metadata. For Snowflake, what they did was train the retriever to help select the right tables based on a user's metadata and historical usage. Finally, use a strong base model. Snowflake found by moving to a stronger base model like menstrual large that gave them the final boost to beat the very good GPT 4 on Texas EQL.",
      "platforms": {
        "tiktok": {
          "video_id": "7350130615284403498",
          "url": "https://www.tiktok.com/@rajistics/video/7350130615284403498",
          "view_count": 1773,
          "upload_date": "2024-03-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C46-TlFLwXD",
          "url": "https://www.instagram.com/reel/C46-TlFLwXD",
          "view_count": 8080,
          "upload_date": "2024-03-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "PABvLUcA1fg",
          "url": "https://www.youtube.com/watch?v=PABvLUcA1fg",
          "view_count": 251,
          "upload_date": "2024-03-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6452,
      "title": "Claude 3 and lots of unbelievable claims. Let‚Äôs walk through some of the more viral reactions and explain what is going on. We also need to pay attention to the training data for these models and remember they are trained to act very helpful and smart. Claude 3 blog post: https://www.anthropic.com/news/claude-3-family Awareness in needle test: https://twitter.com/alexalbert__/status/1764722513014329620 Anthropic translating low resource language: https://twitter.com/hahahahohohe/status/1765088860592394250 #anthropic #claude #aihype #rajistics",
      "description": "Claude 3 and lots of unbelievable claims. Let‚Äôs walk through some of the more viral reactions and explain what is going on. We also need to pay attention to the training data for these models and remember they are trained to act very helpful and smart. Claude 3 blog post: https://www.anthropic.com/news/claude-3-family Awareness in needle test: https://twitter.com/alexalbert__/status/1764722513014329620 Anthropic translating low resource language: https://twitter.com/hahahahohohe/status/1765088860592394250 #anthropic #claude #aihype #rajistics",
      "upload_date": "2024-03-07",
      "total_views": 10035,
      "max_views": 5363,
      "topics": [
        "aihype",
        "anthropic",
        "claude",
        "lots",
        "models",
        "status",
        "test",
        "trained",
        "twitter"
      ],
      "search_text": "Claude 3 and lots of unbelievable claims. Let‚Äôs walk through some of the more viral reactions and explain what is going on. We also need to pay attention to the training data for these models and remember they are trained to act very helpful and smart. Claude 3 blog post: https://www.anthropic.com/news/claude-3-family Awareness in needle test: https://twitter.com/alexalbert__/status/1764722513014329620 Anthropic translating low resource language: https://twitter.com/hahahahohohe/status/1765088860592394250 #anthropic #claude #aihype #rajistics aihype anthropic claude lots models status test trained twitter Models from Anthropic are super intelligent, self-aware, they copied open AI's marketing. Their press release talks about how intelligent our models are. Take a look at this graph. Intelligence isn't a measurable quantity. Instead, we use a number of different test to measure the capability of these models. Benchmarks, math. Did you see how the testers at Anthropic noticed the model was self aware? Anthropic test is shared how the model recognized was being tested and referred to the model as being aware. It's alive. Likely seen a similar test in training. After all, Anthropic's written blog post on exactly this test. Any other reasons? These models are trained to be helpful. Once it recognizes a problem or an issue with the data, it's been trained to point that out to you. Okay, Smarty Pants, explain how the model was able to translate this language it hadn't seen before. Did you see if the models been trained on this data? There is a whole Wikipedia page explaining this language. There you go. Remember, LLMs are statistical predictions machines trained to look very helpful and smart.",
      "platforms": {
        "tiktok": {
          "video_id": "7343429266043915562",
          "url": "https://www.tiktok.com/@rajistics/video/7343429266043915562",
          "view_count": 3192,
          "upload_date": "2024-03-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4MeIm6tblc",
          "url": "https://www.instagram.com/reel/C4MeIm6tblc",
          "view_count": 5363,
          "upload_date": "2024-03-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZkolE7yVSw0",
          "url": "https://youtube.com/shorts/ZkolE7yVSw0",
          "view_count": 1480,
          "upload_date": "2024-03-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6553,
      "title": "Thinking about the size of numbers becomes important when working with neural networks. This video touches about different techniques like using bfloat16 and quantization. #datascience #machinelearning #bfloat16 #quantization #largelanguagemodels Links: Accelerating Large Language Models with Mixed-Precision Techniques: https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/ BFloat16: The secret to high performance on Cloud TPUs: https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus Llama.cpp: https://github.com/ggerganov/llama.cpp/ A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers Accelerate and bitsandbytes: https://huggingface.co/blog/hf-bitsandbytes-integration Background by Umberto: https://unsplash.com/photos/jXd2FSvcRr8",
      "description": "Thinking about the size of numbers becomes important when working with neural networks. This video touches about different techniques like using bfloat16 and quantization. #datascience #machinelearning #bfloat16 #quantization #largelanguagemodels Links: Accelerating Large Language Models with Mixed-Precision Techniques: https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/ BFloat16: The secret to high performance on Cloud TPUs: https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus Llama.cpp: https://github.com/ggerganov/llama.cpp/ A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers Accelerate and bitsandbytes: https://huggingface.co/blog/hf-bitsandbytes-integration Background by Umberto: https://unsplash.com/photos/jXd2FSvcRr8",
      "upload_date": "2023-05-16",
      "total_views": 10005,
      "max_views": 9534,
      "topics": [
        "bfloat16",
        "datascience",
        "floating",
        "largelanguagemodels",
        "machinelearning",
        "models",
        "quantization",
        "slimming",
        "techniques"
      ],
      "search_text": "Thinking about the size of numbers becomes important when working with neural networks. This video touches about different techniques like using bfloat16 and quantization. #datascience #machinelearning #bfloat16 #quantization #largelanguagemodels Links: Accelerating Large Language Models with Mixed-Precision Techniques: https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/ BFloat16: The secret to high performance on Cloud TPUs: https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus Llama.cpp: https://github.com/ggerganov/llama.cpp/ A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers Accelerate and bitsandbytes: https://huggingface.co/blog/hf-bitsandbytes-integration Background by Umberto: https://unsplash.com/photos/jXd2FSvcRr8 bfloat16 datascience floating largelanguagemodels machinelearning models quantization slimming techniques",
      "platforms": {
        "tiktok": {
          "video_id": "7233889298288872747",
          "url": "https://www.tiktok.com/@rajistics/video/7233889298288872747",
          "view_count": 9534,
          "upload_date": "2023-05-16",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CsUX9mggLDM",
          "url": "https://www.instagram.com/reel/CsUX9mggLDM",
          "view_count": 402,
          "upload_date": "2023-05-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "A5sqlm_xCN4",
          "url": "https://www.youtube.com/watch?v=A5sqlm_xCN4",
          "view_count": 69,
          "upload_date": "2023-05-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6300,
      "title": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #deeplearning #gpu",
      "description": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #deeplearning #gpu",
      "upload_date": "2023-01-16",
      "total_views": 9997,
      "max_views": 9997,
      "topics": [
        "datascience",
        "deep",
        "deeplearning",
        "first",
        "gpt4",
        "gpu",
        "hype",
        "machinelearning",
        "openai",
        "picking",
        "scaling"
      ],
      "search_text": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #deeplearning #gpu datascience deep deeplearning first gpt4 gpu hype machinelearning openai picking scaling",
      "platforms": {
        "instagram": {
          "video_id": "17993743345632686",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "U2bbHHVawls",
          "url": "https://youtube.com/shorts/U2bbHHVawls?feature=share",
          "view_count": 9997,
          "upload_date": "2023-01-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6464,
      "title": "Direct Preference Optimization is one of the most significant advances in AI over the last six months. It provides a simpler and more efficient way to align a model's preferences. You can try out in packages like TRL. Direct Preference Optimization (DPO) - A Simplified Explanation: https://medium.com/@joaolages/direct-preference-optimization-dpo-622fc1f18707 Direct Preference Optimization: Your Language Model is Secretly a Reward Model - https://arxiv.org/pdf/2305.18290.pdf DPO Trainer: https://huggingface.co/docs/trl/main/en/dpo_trainer",
      "description": "Direct Preference Optimization is one of the most significant advances in AI over the last six months. It provides a simpler and more efficient way to align a model's preferences. You can try out in packages like TRL. Direct Preference Optimization (DPO) - A Simplified Explanation: https://medium.com/@joaolages/direct-preference-optimization-dpo-622fc1f18707 Direct Preference Optimization: Your Language Model is Secretly a Reward Model - https://arxiv.org/pdf/2305.18290.pdf DPO Trainer: https://huggingface.co/docs/trl/main/en/dpo_trainer",
      "upload_date": "2024-01-26",
      "total_views": 9995,
      "max_views": 4714,
      "topics": [
        "direct",
        "dpo",
        "going",
        "like",
        "model",
        "optimization",
        "preference",
        "trl"
      ],
      "search_text": "Direct Preference Optimization is one of the most significant advances in AI over the last six months. It provides a simpler and more efficient way to align a model's preferences. You can try out in packages like TRL. Direct Preference Optimization (DPO) - A Simplified Explanation: https://medium.com/@joaolages/direct-preference-optimization-dpo-622fc1f18707 Direct Preference Optimization: Your Language Model is Secretly a Reward Model - https://arxiv.org/pdf/2305.18290.pdf DPO Trainer: https://huggingface.co/docs/trl/main/en/dpo_trainer direct dpo going like model optimization preference trl",
      "platforms": {
        "tiktok": {
          "video_id": "7328423792143764778",
          "url": "https://www.tiktok.com/@rajistics/video/7328423792143764778",
          "view_count": 4714,
          "upload_date": "2024-01-26",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C2kWTLGATfR",
          "url": "https://www.instagram.com/reel/C2kWTLGATfR",
          "view_count": 4302,
          "upload_date": "2024-01-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "bGqnGUE4wO0",
          "url": "https://youtube.com/shorts/bGqnGUE4wO0",
          "view_count": 979,
          "upload_date": "2024-01-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6507,
      "title": "Deep dive on how to improve large language models. I provide an introduction to zero-shot and few-shot learning methods. I also discuss the role of in-context learning and emergence. For fine-tuning the video explains instruction tuning reinforcement learning with human feedback (rlhf) reinforcement learning with AI feedback (rlaif and parameter efficient fine tuning (peft). I will also have a larger version of this video on my youtube where it's easier to see the slides. #datascience #machinelearning #largelanguagemodels #finetuning #prompting #peft #rlhf #rlaif #fewshotlearning Background Photo from Deepmind: https://unsplash.com/photos/4QVqSh4VvP4 See the full presentation which included this topic: https://youtu.be/dKBD-3hnjW0",
      "description": "Deep dive on how to improve large language models. I provide an introduction to zero-shot and few-shot learning methods. I also discuss the role of in-context learning and emergence. For fine-tuning the video explains instruction tuning reinforcement learning with human feedback (rlhf) reinforcement learning with AI feedback (rlaif and parameter efficient fine tuning (peft). I will also have a larger version of this video on my youtube where it's easier to see the slides. #datascience #machinelearning #largelanguagemodels #finetuning #prompting #peft #rlhf #rlaif #fewshotlearning Background Photo from Deepmind: https://unsplash.com/photos/4QVqSh4VvP4 See the full presentation which included this topic: https://youtu.be/dKBD-3hnjW0",
      "upload_date": "2023-04-28",
      "total_views": 9988,
      "max_views": 8295,
      "topics": [
        "automating",
        "datascience",
        "expertise",
        "largelanguagemodels",
        "learning",
        "machine",
        "machinelearning",
        "mlcopilot",
        "peft",
        "rlaif",
        "rlhf",
        "shot",
        "tuning"
      ],
      "search_text": "Deep dive on how to improve large language models. I provide an introduction to zero-shot and few-shot learning methods. I also discuss the role of in-context learning and emergence. For fine-tuning the video explains instruction tuning reinforcement learning with human feedback (rlhf) reinforcement learning with AI feedback (rlaif and parameter efficient fine tuning (peft). I will also have a larger version of this video on my youtube where it's easier to see the slides. #datascience #machinelearning #largelanguagemodels #finetuning #prompting #peft #rlhf #rlaif #fewshotlearning Background Photo from Deepmind: https://unsplash.com/photos/4QVqSh4VvP4 See the full presentation which included this topic: https://youtu.be/dKBD-3hnjW0 automating datascience expertise largelanguagemodels learning machine machinelearning mlcopilot peft rlaif rlhf shot tuning",
      "platforms": {
        "tiktok": {
          "video_id": "7226905183601708331",
          "url": "https://www.tiktok.com/@rajistics/video/7226905183601708331",
          "view_count": 8295,
          "upload_date": "2023-04-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CrkHYuQgnaa",
          "url": "https://www.instagram.com/reel/CrkHYuQgnaa",
          "view_count": 1356,
          "upload_date": "2023-04-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "iBX7YDdnPtg",
          "url": "https://www.youtube.com/watch?v=iBX7YDdnPtg",
          "view_count": 337,
          "upload_date": "2023-05-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6656,
      "title": "This is a year old but still holds up pretty well. The big difference is you may want to use TRL instead of PEFT for the training. But the concepts around efficiently training a large language model using PEFT and LoRA hold up. #datascience #machinelearning #largelanguagemodels #flant5 #peft #LoRA #finetuning YouTube Video: https://youtu.be/YKCtbIJC3kQ?si=PL3A1mgMXmuVbZqe Blog Post: https://www.philschmid.de/fine-tune-flan-t5-peft",
      "description": "This is a year old but still holds up pretty well. The big difference is you may want to use TRL instead of PEFT for the training. But the concepts around efficiently training a large language model using PEFT and LoRA hold up. #datascience #machinelearning #largelanguagemodels #flant5 #peft #LoRA #finetuning YouTube Video: https://youtu.be/YKCtbIJC3kQ?si=PL3A1mgMXmuVbZqe Blog Post: https://www.philschmid.de/fine-tune-flan-t5-peft",
      "upload_date": "2024-03-27",
      "total_views": 9977,
      "max_views": 7280,
      "topics": [
        "datascience",
        "flant5",
        "largelanguagemodels",
        "lora",
        "machinelearning",
        "model",
        "peft"
      ],
      "search_text": "This is a year old but still holds up pretty well. The big difference is you may want to use TRL instead of PEFT for the training. But the concepts around efficiently training a large language model using PEFT and LoRA hold up. #datascience #machinelearning #largelanguagemodels #flant5 #peft #LoRA #finetuning YouTube Video: https://youtu.be/YKCtbIJC3kQ?si=PL3A1mgMXmuVbZqe Blog Post: https://www.philschmid.de/fine-tune-flan-t5-peft datascience flant5 largelanguagemodels lora machinelearning model peft You want to know how to efficiently train a large language model? I just finished up on how to do this with a Google Flan T5 model based on Philip's blog post. I first started by using PEFT which is a library that has a lot of efficient ways for working with large models. I then used Laura which is low rank adaption and this is an approach that decomposes the large language models so I end up just having to train a little bit of parameters. Less than 1 percent in this case. With this approach, I was able to fine tune a model using only $13 worth of compute versus $300 of compute for fine tuning the entire model. So this is the advantage of using Laura. The other thing is this model actually came out with a higher accuracy as well. Go check out the full video if you want to see all the details.",
      "platforms": {
        "tiktok": {
          "video_id": "7351090591779999022",
          "url": "https://www.tiktok.com/@rajistics/video/7351090591779999022",
          "view_count": 2697,
          "upload_date": "2024-03-27",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C5BoYmLJaW1",
          "url": "https://www.instagram.com/reel/C5BoYmLJaW1",
          "view_count": 7280,
          "upload_date": "2024-03-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reinforcement learning with my Eat Melon! Demo  This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhf Throwback video ",
      "description": "Reinforcement learning with my Eat Melon! Demo  This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhf Throwback video ",
      "upload_date": "2024-04-23",
      "total_views": 9966,
      "max_views": 9966,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "reinforcementlearning",
        "rlhf",
        "techtok"
      ],
      "search_text": "Reinforcement learning with my Eat Melon! Demo  This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhf Throwback video  chatgpt datascience machinelearning reinforcementlearning rlhf techtok Why do you like watermelon over poop? Well, how are you going to teach that to an AI? Guess what? I did. The way my little robot friend works is they get a positive reward when they find watermelon, and they get a negative reward when they eat poop. This approach is known as reinforcement learning, and it's actually growing within computer science as a way to train robots, whether real or virtual. The code for this is freely available. I'll add it in the comments.",
      "platforms": {
        "tiktok": {
          "video_id": "7361118473218444586",
          "url": "https://www.tiktok.com/@rajistics/video/7361118473218444586",
          "view_count": 9966,
          "upload_date": "2024-04-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/95f07e20df964278a6a5202592b8ef63_1713893965~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=lb4qAJIBO0uB8DquWG4F1wxP1dQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Llama really upping it on training data. But this is a trend with scaling laws to use more and more training data.  #trainingdata #largelanguagemodels #rajistics ",
      "description": "Llama really upping it on training data. But this is a trend with scaling laws to use more and more training data.  #trainingdata #largelanguagemodels #rajistics ",
      "upload_date": "2024-04-26",
      "total_views": 9950,
      "max_views": 9950,
      "topics": [
        "data",
        "enemy",
        "largelanguagemodels",
        "make",
        "training",
        "trainingdata"
      ],
      "search_text": "Llama really upping it on training data. But this is a trend with scaling laws to use more and more training data.  #trainingdata #largelanguagemodels #rajistics  data enemy largelanguagemodels make training trainingdata Make me a refidiaty, oh, baby, a one and only Make me a refidiaty, make me a one and only Don't make me your enemy, your enemy, your enemy So you wanna play with magic, boy, you should know what you're falling for Baby, do your daddy",
      "platforms": {
        "tiktok": {
          "video_id": "7362082333865823530",
          "url": "https://www.tiktok.com/@rajistics/video/7362082333865823530",
          "view_count": 9950,
          "upload_date": "2024-04-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ff2a421aae2a4dc3aaa224729ce8a78a_1714118455~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=0Mgh6QNYFmqqpor0OeLUx4eQGSI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Parquet and Arrow file formats #datascience #analytics #bigdata #codetok #dataengineer",
      "description": "Parquet and Arrow file formats #datascience #analytics #bigdata #codetok #dataengineer",
      "upload_date": "2022-05-31",
      "total_views": 9943,
      "max_views": 9943,
      "topics": [
        "analytics",
        "bigdata",
        "codetok",
        "dataengineer",
        "datascience",
        "file"
      ],
      "search_text": "Parquet and Arrow file formats #datascience #analytics #bigdata #codetok #dataengineer analytics bigdata codetok dataengineer datascience file Did you know there's a much better way of saving your files than CSV? When you're first learning Python, they teach you how to save a file, and you save a file line by line. But that's really inefficient. Analytics is often done at the level of a column. So saving files in a format that supports columns like Parquet makes total sense. Next, think about how we actually use data. We use data in memory. So we should really have a file format that supports in-memory usage, and that's what Aero does. Aero's designed to be super efficient. It's crazy. You can work with like a 20 gig data set and only a few megabytes of memory. The bottom line, as you start working with larger files, start thinking about using Parquet and Aero.",
      "platforms": {
        "tiktok": {
          "video_id": "7104028147598888234",
          "url": "https://www.tiktok.com/@rajistics/video/7104028147598888234",
          "view_count": 9943,
          "upload_date": "2022-05-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/480b2a58150d493d8e6111272cc21956_1654035448~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=mwviGAtvWkd2Tfjv9Y3KLomj2wU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "How companies your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience ",
      "description": "How companies your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience ",
      "upload_date": "2023-01-20",
      "total_views": 9883,
      "max_views": 9883,
      "topics": [
        "bigcode",
        "code",
        "copilot",
        "data",
        "github",
        "huggingface"
      ],
      "search_text": "How companies your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience  bigcode code copilot data github huggingface Hey, do you use GitHub co-pilot? Yeah, by using auto completion, I save a ton of time. I just found out that it's using code that it shouldn't be. What? That doesn't seem right. What are you going to do? Wait, wouldn't it be useful to have a data set for code completion that respects attribution? That sounds impossible. How could you filter code that is clearly marked as not GPL and not for commercial use? Wouldn't it be useful to have a tool that lets you check for any code to see if it's in the trading data? You would let people query the training data? That doesn't sound very open AI to me. Or what if you give you a tool that lets you opt out of being in the trading data? What? Let people choose if they want to have their code in there? Clearly, they haven't heard about the team strategy. Lawsuit's the only way out.",
      "platforms": {
        "tiktok": {
          "video_id": "7190535940043984174",
          "url": "https://www.tiktok.com/@rajistics/video/7190535940043984174",
          "view_count": 9883,
          "upload_date": "2023-01-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/74ff9372ff914cd3b43cc7b60f126b14_1674177119~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=y3gtAeEcRnQ4onFgHQi9Ie5Jf1w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6504,
      "title": "AI works with various data types: tabular unstructured and semi-structured like JSON. While tabular data is most prevalent in enterprises Generative AI models primarily focus on unstructured and semi-structured data. #tabular #unstructured #json #semistructured #rajistics",
      "description": "AI works with various data types: tabular unstructured and semi-structured like JSON. While tabular data is most prevalent in enterprises Generative AI models primarily focus on unstructured and semi-structured data. #tabular #unstructured #json #semistructured #rajistics",
      "upload_date": "2024-03-16",
      "total_views": 9874,
      "max_views": 7095,
      "topics": [
        "data",
        "json",
        "semi",
        "semistructured",
        "structured",
        "tabular",
        "unstructured"
      ],
      "search_text": "AI works with various data types: tabular unstructured and semi-structured like JSON. While tabular data is most prevalent in enterprises Generative AI models primarily focus on unstructured and semi-structured data. #tabular #unstructured #json #semistructured #rajistics data json semi semistructured structured tabular unstructured There's something more you can do with me? I feel under utilized in the AI revolution. Tabular, you know you're perfect for structured data tasks. You're manicured, you're efficient, you're widely used in all of our machine learning tasks. But why are you waiting in line for those GPUs for unstructured? You said I was the valuable one. You know we need to spend some time with unstructured data. Otherwise it'll spread from spreadsheets to sheer points even into maybe our prod database. So hard to sleep. I've nightmares with this bur sound. I was okay with a little bit of classification extraction but now they're coming with me with transformers and GPUs. It's okay unstructured. We have some new tools. We're just trying to learn and understand a little bit from you. But why do you all need to subjugate me? You vectorize me. You probey with deepest queries. You dig into my dark deep data and then you take off pieces and give it to JSON. Everyone calm down. I'm here. The best of both world. Ah Jason, you're that semi structured hero we need. Big plans for you. Big plans. We're going to get converters and force you into third normal form.",
      "platforms": {
        "tiktok": {
          "video_id": "7346985601528515886",
          "url": "https://www.tiktok.com/@rajistics/video/7346985601528515886",
          "view_count": 2359,
          "upload_date": "2024-03-16",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C4lJfRaA0xs",
          "url": "https://www.instagram.com/reel/C4lJfRaA0xs",
          "view_count": 7095,
          "upload_date": "2024-03-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "WznjvsJieoA",
          "url": "https://youtube.com/shorts/WznjvsJieoA",
          "view_count": 420,
          "upload_date": "2024-03-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt ",
      "description": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt ",
      "upload_date": "2023-03-24",
      "total_views": 9862,
      "max_views": 9862,
      "topics": [
        "chatgpt",
        "datascience",
        "langchain",
        "largelanguagemodels",
        "like",
        "openai"
      ],
      "search_text": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt  chatgpt datascience langchain largelanguagemodels like openai OpenAI's big announcement of support for plugins, it's going to change how we all use the internet. ChatGPT sort of acts like a brain. We ask it lots of questions like why is the sky blue, and we look for the answers it gets back. But it doesn't know everything. If I ask it about the weather, it gets that wrong. And the reason is because it's not connected to weather services. What plugins allow us to do is allow ChatGPT and other OpenAI tools to connect to various services like the weather, like search, or some of the other ones that it put out there today is like Expedia. So imagine booking your travel all the way through using a ChatGPT. You can think of all these plugins as creating an app store for ChatGPT. But what does that mean for the rest of us? The first thing is I'm excited because it's going to force all these other services websites to become friendly to AI by building out and making sure their APIs work well with large language models. Unlike the app store battles between Android and Apple, the switching costs here for developers are much smaller. At the end of the day, once they've built that API out for OpenAI, it's just a few lines of code to be able to adapt it to another large language model. So I still see a bright future for tools like LangChain and the ability to connect to many other large language models.",
      "platforms": {
        "tiktok": {
          "video_id": "7213898295914040618",
          "url": "https://www.tiktok.com/@rajistics/video/7213898295914040618",
          "view_count": 9862,
          "upload_date": "2023-03-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bfd5ddefa50a4cd58887145f84144587_1679616598~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=06Ok91MRdsC04gQaL5cQrKLemeI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @sqwadiladida resources for learning about transformer models in #naturallanguageprocessing #datascience #techtok #statistics #analytics",
      "description": "Reply to @sqwadiladida resources for learning about transformer models in #naturallanguageprocessing #datascience #techtok #statistics #analytics",
      "upload_date": "2022-04-23",
      "total_views": 9762,
      "max_views": 9762,
      "topics": [
        "analytics",
        "datascience",
        "naturallanguageprocessing",
        "next",
        "statistics",
        "techtok"
      ],
      "search_text": "Reply to @sqwadiladida resources for learning about transformer models in #naturallanguageprocessing #datascience #techtok #statistics #analytics analytics datascience naturallanguageprocessing next statistics techtok Here's three resources for those of you who want to jump on board on the next big thing in data science. If you like to read, this is my book recommendation. Make sure you're coding along with it. If you prefer something a little bit more punchy, I like this online tutorial from Hugging Face, Video, Text, Great Examples. Next, head over to Kaggle. They've got data sets, notebooks, and a community that'll push you to the next level.",
      "platforms": {
        "tiktok": {
          "video_id": "7089941106573626670",
          "url": "https://www.tiktok.com/@rajistics/video/7089941106573626670",
          "view_count": 9762,
          "upload_date": "2022-04-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ce53a6cb26ed47da967a665428b52b57_1650755553~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=siN42IhNXyXkCGkdOYBGQv7eF4k%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6643,
      "title": "#onthisday showing the map of stable diffusion. #datascience #machinelearning #stablediffusion #rajistics",
      "description": "#onthisday showing the map of stable diffusion. #datascience #machinelearning #stablediffusion #rajistics",
      "upload_date": "2023-09-15",
      "total_views": 9730,
      "max_views": 9235,
      "topics": [
        "chatgpt",
        "datascience",
        "gpt4",
        "machinelearning",
        "map",
        "officeproductivity",
        "onthisday",
        "papers",
        "productivity",
        "showing",
        "stablediffusion"
      ],
      "search_text": "#onthisday showing the map of stable diffusion. #datascience #machinelearning #stablediffusion #rajistics chatgpt datascience gpt4 machinelearning map officeproductivity onthisday papers productivity showing stablediffusion",
      "platforms": {
        "tiktok": {
          "video_id": "7279133734417468718",
          "url": "https://www.tiktok.com/@rajistics/video/7279133734417468718",
          "view_count": 9235,
          "upload_date": "2023-09-15",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CxUHbdxNZOy",
          "url": "https://www.instagram.com/reel/CxUHbdxNZOy",
          "view_count": 495,
          "upload_date": "2023-09-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5953,
      "title": "Built on Nobel Prize–winning game theory, SHAP turned black-box models into glass boxes. Now with 24K+ GitHub stars, it’s the go-to explainability tool used in research, healthcare, finance, and enterprise platforms like XGBoost and DataRobot. The GOAT of explainability—quietly essential. I will share liniks for some of my deeper videos on shap in reddit.",
      "description": "Built on Nobel Prize–winning game theory, SHAP turned black-box models into glass boxes. Now with 24K+ GitHub stars, it’s the go-to explainability tool used in research, healthcare, finance, and enterprise platforms like XGBoost and DataRobot. The GOAT of explainability—quietly essential. I will share liniks for some of my deeper videos on shap in reddit.",
      "upload_date": "2025-10-18",
      "total_views": 9723,
      "max_views": 7955,
      "topics": [
        "explainability",
        "game",
        "learning",
        "machine",
        "shap",
        "theory"
      ],
      "search_text": "Built on Nobel Prize–winning game theory, SHAP turned black-box models into glass boxes. Now with 24K+ GitHub stars, it’s the go-to explainability tool used in research, healthcare, finance, and enterprise platforms like XGBoost and DataRobot. The GOAT of explainability—quietly essential. I will share liniks for some of my deeper videos on shap in reddit. explainability game learning machine shap theory Oh, this is going to be a game changer in AI. Absolutely. It's revolutionary. It's only got 25,000 GitHub stars. This is based on work that literally won a Nobel Prize in economics. It was a smart play to add cooperative game theory and combine it with machine learning. Before this tool, people were in the dark ages of machine learning. Oh, yeah. Now we can take any prediction, tear it apart and see which features contributed to that prediction. I think it's going down as the goat of explainability. It's saved me so many times. I mean, it's just the best way to figure out what features are the most important in my XGBoost models. And of course, we're talking about the SHAP package. Shapley values straight out of cooperative game theory. It's incredible. Global and local explanations, partial dependents, even clustering time series visualizations. Exactly. If you're doing machine learning, you probably are already using SHAP. It's pretty widely used. And the latest revelations are getting even bigger. Yeah, it's dropping support for older versions of Python. Just imagine how big machine learning could have been if you had influencers like me to promote it.",
      "platforms": {
        "tiktok": {
          "video_id": "7562590373630053663",
          "url": "https://www.tiktok.com/@rajistics/video/7562590373630053663",
          "view_count": 7955,
          "upload_date": "2025-10-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIH5qgtgGGC72Cb2AIefAkI7TLLHeRIIGn8jmn~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=BKc8rSQkj%2F2Jf9MtdZfaP2BZ0r8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18288620599275549",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": " -gBfGyBP0r4",
          "url": "https://www.youtube.com/watch?v= -gBfGyBP0r4",
          "view_count": 1768,
          "upload_date": "2025-10-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6657,
      "title": "Claude Skills - A Short Explainer",
      "description": "Claude Skills - A Short Explainer",
      "upload_date": "2025-10-19",
      "total_views": 9661,
      "max_views": 4802,
      "topics": [
        "aiplanning",
        "blockworld",
        "claude",
        "explainer",
        "gpt4",
        "largelanguagemodels",
        "mysteryworld",
        "short",
        "skills"
      ],
      "search_text": "Claude Skills - A Short Explainer aiplanning blockworld claude explainer gpt4 largelanguagemodels mysteryworld short skills",
      "platforms": {
        "tiktok": {
          "video_id": "7352308566776220970",
          "url": "https://www.tiktok.com/@rajistics/video/7352308566776220970",
          "view_count": 822,
          "upload_date": "2024-03-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C5KFARsANnR",
          "url": "https://www.instagram.com/reel/C5KFARsANnR",
          "view_count": 4037,
          "upload_date": "2024-03-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "7fwqH6UxcSs",
          "url": "https://www.youtube.com/watch?v=7fwqH6UxcSs",
          "view_count": 4802,
          "upload_date": "2025-10-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6457,
      "title": "Some great tips from Charlie over at Replicate on using Llama 2. A guide to prompting Llama 2 - https://replicate.com/blog/how-to-prompt-llama Get started lots of places to use llama2: HuggingChat - https://huggingface.co/chat/ Preplexity AI - https://labs.perplexity.ai/ Replicate - https://replicate.com/replicate/llama-2-70b-chat LLama video: https://www.youtube.com/shorts/-MFFBA8bdd8",
      "description": "Some great tips from Charlie over at Replicate on using Llama 2. A guide to prompting Llama 2 - https://replicate.com/blog/how-to-prompt-llama Get started lots of places to use llama2: HuggingChat - https://huggingface.co/chat/ Preplexity AI - https://labs.perplexity.ai/ Replicate - https://replicate.com/replicate/llama-2-70b-chat LLama video: https://www.youtube.com/shorts/-MFFBA8bdd8",
      "upload_date": "2023-08-15",
      "total_views": 9609,
      "max_views": 7954,
      "topics": [
        "charlie",
        "chat",
        "datascience",
        "great",
        "largelanguagemodels",
        "llama",
        "llama2",
        "machinelearning",
        "replicate",
        "tips",
        "using"
      ],
      "search_text": "Some great tips from Charlie over at Replicate on using Llama 2. A guide to prompting Llama 2 - https://replicate.com/blog/how-to-prompt-llama Get started lots of places to use llama2: HuggingChat - https://huggingface.co/chat/ Preplexity AI - https://labs.perplexity.ai/ Replicate - https://replicate.com/replicate/llama-2-70b-chat LLama video: https://www.youtube.com/shorts/-MFFBA8bdd8 charlie chat datascience great largelanguagemodels llama llama2 machinelearning replicate tips using",
      "platforms": {
        "tiktok": {
          "video_id": "7267348795229228331",
          "url": "https://www.tiktok.com/@rajistics/video/7267348795229228331",
          "view_count": 7954,
          "upload_date": "2023-08-15",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cv8joBbNjNF",
          "url": "https://www.instagram.com/reel/Cv8joBbNjNF",
          "view_count": 413,
          "upload_date": "2023-08-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "QyCF7XQ7Xt0",
          "url": "https://www.youtube.com/watch?v=QyCF7XQ7Xt0",
          "view_count": 1242,
          "upload_date": "2023-08-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6474,
      "title": "Do you calibrate your models? For many types of models you may need to calibrate them. This video reminds us of the importance of calibration. To dig deeper check out platt scaling or isotonic regression. #machinelearning #datascience #statistics #rajistics #colorcalibration",
      "description": "Do you calibrate your models? For many types of models you may need to calibrate them. This video reminds us of the importance of calibration. To dig deeper check out platt scaling or isotonic regression. #machinelearning #datascience #statistics #rajistics #colorcalibration",
      "upload_date": "2023-11-26",
      "total_views": 9495,
      "max_views": 5771,
      "topics": [
        "calibrate",
        "colorcalibration",
        "datascience",
        "disease",
        "machinelearning",
        "model",
        "models",
        "statistics"
      ],
      "search_text": "Do you calibrate your models? For many types of models you may need to calibrate them. This video reminds us of the importance of calibration. To dig deeper check out platt scaling or isotonic regression. #machinelearning #datascience #statistics #rajistics #colorcalibration calibrate colorcalibration datascience disease machinelearning model models statistics",
      "platforms": {
        "tiktok": {
          "video_id": "7305808607965089066",
          "url": "https://www.tiktok.com/@rajistics/video/7305808607965089066",
          "view_count": 5771,
          "upload_date": "2023-11-26",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0HbgZmg1HZ",
          "url": "https://www.instagram.com/reel/C0HbgZmg1HZ",
          "view_count": 2943,
          "upload_date": "2023-11-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0ozyWKtDh9M",
          "url": "https://youtube.com/shorts/0ozyWKtDh9M",
          "view_count": 781,
          "upload_date": "2023-11-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Using agents in langchain with gpt-3. You can do this!  Go check it out. #datascience #machinelearning #openai #gpt3 #langchain",
      "description": "Using agents in langchain with gpt-3. You can do this!  Go check it out. #datascience #machinelearning #openai #gpt3 #langchain",
      "upload_date": "2023-03-04",
      "total_views": 9416,
      "max_views": 9416,
      "topics": [
        "answer",
        "datascience",
        "gpt3",
        "langchain",
        "machinelearning",
        "openai"
      ],
      "search_text": "Using agents in langchain with gpt-3. You can do this!  Go check it out. #datascience #machinelearning #openai #gpt3 #langchain answer datascience gpt3 langchain machinelearning openai Want to see something cool? Let's ask the computer a question, but it doesn't know the answer, but it does know that there's a Python package, it has the answer. So it goes and downloads the package by itself, automatically figures out how to get the answer using the Python package, mind-blowing. This is called an agent. It's a way to use GPT-3 to help decide the proper action to take. The code for this is not that complicated. All we do is start by connecting to OpenAI. We tell it we want to use the terminal tool, a Google search, and a way to run Python. And then we start it. Here's a more sophisticated version that combines search, calculator, and a database. This allows it to work out complex problems. So, for example, I can figure out what the age of Leonardo DiCaprio's girlfriend is, then use that number, do a calculation on top of this. This is the power of LangChain, it's gonna be big.",
      "platforms": {
        "tiktok": {
          "video_id": "7206709334070529326",
          "url": "https://www.tiktok.com/@rajistics/video/7206709334070529326",
          "view_count": 9416,
          "upload_date": "2023-03-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/60642be9c6fa40d9a997dbd78fb20e27_1677942799~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=XsaAJI%2Bbrc5G1G1FZcqfVdKyO7E%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "ColPali: Efficient Document Retrieval with Vision Language Models - https://arxiv.org/abs/2407.01449 - https://github.com/illuin-tech/colpali Other posts: Scaling ColPali to billions of PDFs with Vespa - https://blog.vespa.ai/scaling-colpali-to-billions/ ColPali: Document Retrieval with Vision Language Models - https://antaripasaha.notion.site/ColPali-Document-Retrieval-with-Vision-Language-Models-10f5314a5639803d94d0d7ac191bb5b1",
      "description": "ColPali: Efficient Document Retrieval with Vision Language Models - https://arxiv.org/abs/2407.01449 - https://github.com/illuin-tech/colpali Other posts: Scaling ColPali to billions of PDFs with Vespa - https://blog.vespa.ai/scaling-colpali-to-billions/ ColPali: Document Retrieval with Vision Language Models - https://antaripasaha.notion.site/ColPali-Document-Retrieval-with-Vision-Language-Models-10f5314a5639803d94d0d7ac191bb5b1",
      "upload_date": "2024-10-10",
      "total_views": 9380,
      "max_views": 9380,
      "topics": [
        "colpali",
        "document",
        "language",
        "patches",
        "retrieval",
        "vision"
      ],
      "search_text": "ColPali: Efficient Document Retrieval with Vision Language Models - https://arxiv.org/abs/2407.01449 - https://github.com/illuin-tech/colpali Other posts: Scaling ColPali to billions of PDFs with Vespa - https://blog.vespa.ai/scaling-colpali-to-billions/ ColPali: Document Retrieval with Vision Language Models - https://antaripasaha.notion.site/ColPali-Document-Retrieval-with-Vision-Language-Models-10f5314a5639803d94d0d7ac191bb5b1 colpali document language patches retrieval vision Are you saying we don't need to do OCR, layout analysis, text chunking for document retrieval? Yep, no OCR, no layout analysis, no text chunking. We're going to use CoalPoly, which is a vision transformer model that does everything in one step. Sounds modern. How does it work? On the vision document retrieval benchmark, it did really well. Wow, our method using the mother of all embeddings got 66. Much better. So tell me, how does this model work? CoalPoly uses that vision transformer approach. So it takes us to every page, breaks it up in individual patches, over a thousand patches for each page. And each one of those patches is represented with an embedding of about a length of 128. That has to be slow and take up a bunch of memory. Nah, it's much faster than the OCR process. And each page takes about eight kilobytes after bianarization. So what are you doing with all those embeddings? Well, the scoring for document retrieval is inspired by CoalDirt and its late interaction. So what we're doing now is comparing the individual query tokens with each an image patch. So by doing this granularity, we get a richer understanding. You can also see this rich understanding here, where we show which image patches are relevant to the query. So it's fast, it's accurate and interpretable. It's got to be expensive. All open source.",
      "platforms": {
        "tiktok": {
          "video_id": "7423957064726220075",
          "url": "https://www.tiktok.com/@rajistics/video/7423957064726220075",
          "view_count": 9380,
          "upload_date": "2024-10-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f7cbd84284e44be094d2a7ae35cb4657_1728524710~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=Znu60xynpb4DCwxv2iOlb7Z6JJ4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6495,
      "title": "Human in the loop is important but it's not a silver bullet. #aiethics #tesla #cigna #rajistics Cigna: https://www.healthcaredive.com/news/cigna-lawsuit-algorithm-claims-denials-california/688857/ Tesla: https://www.caranddriver.com/news/a44185487/report-tesla-autopilot-crashes-since-2019/",
      "description": "Human in the loop is important but it's not a silver bullet. #aiethics #tesla #cigna #rajistics Cigna: https://www.healthcaredive.com/news/cigna-lawsuit-algorithm-claims-denials-california/688857/ Tesla: https://www.caranddriver.com/news/a44185487/report-tesla-autopilot-crashes-since-2019/",
      "upload_date": "2023-09-01",
      "total_views": 9335,
      "max_views": 5571,
      "topics": [
        "aiethics",
        "cigna",
        "consequences",
        "human",
        "loop",
        "news",
        "tesla",
        "using"
      ],
      "search_text": "Human in the loop is important but it's not a silver bullet. #aiethics #tesla #cigna #rajistics Cigna: https://www.healthcaredive.com/news/cigna-lawsuit-algorithm-claims-denials-california/688857/ Tesla: https://www.caranddriver.com/news/a44185487/report-tesla-autopilot-crashes-since-2019/ aiethics cigna consequences human loop news tesla using",
      "platforms": {
        "tiktok": {
          "video_id": "7273966079796464938",
          "url": "https://www.tiktok.com/@rajistics/video/7273966079796464938",
          "view_count": 5571,
          "upload_date": "2023-09-01",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CwqeXlevvna",
          "url": "https://www.instagram.com/reel/CwqeXlevvna",
          "view_count": 3252,
          "upload_date": "2023-09-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8Q_kBuSX0R8",
          "url": "https://www.youtube.com/watch?v=8Q_kBuSX0R8",
          "view_count": 512,
          "upload_date": "2023-09-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "My data science setup for now #datascience #codetok #python #rstats #posit #vscode #googlecolab #digitalocean #conda ",
      "description": "My data science setup for now #datascience #codetok #python #rstats #posit #vscode #googlecolab #digitalocean #conda ",
      "upload_date": "2022-08-20",
      "total_views": 9319,
      "max_views": 9319,
      "topics": [
        "codetok",
        "datascience",
        "google",
        "python",
        "rstats",
        "use"
      ],
      "search_text": "My data science setup for now #datascience #codetok #python #rstats #posit #vscode #googlecolab #digitalocean #conda  codetok datascience google python rstats use So what's the setup for an ineffective data scientist? Let me start with my local environment, then I'll move to the internet resources I use. On my local machine, the IDE I like to use on a day-to-day basis is VS Code. Ask me in 18 months, I'll probably be using something else, but for now, this makes me very happy for running a lot of my Python notebooks. You can see here, I have one of my kind of demo notebooks. All set up, I have an untitled one, I'm doing some work. I need to name that and organize that appropriately. A key for when I'm using Python on my local machine, always use some type of virtual private environments. I'm a fan of condo private environments, but what it allows me to do is have a number of different Python installations and not mess with the base installation at all. You don't wanna do that. Python dependencies are a pain. This is a key to kind of having your machine work locally very well. I like using R, so I always have RStudio set up and running in my environment as well. You can see here, I was doing some work on some NFL packages, also untitled, that I need to also save as well. Let's talk about some online resources you can use. My go-to tool for online is using Google Colab. You can see here, I have a bunch of notebooks here that I have for Google Colab, because I use Google Colab all the time. I can store things in my Google Drive, I can pull this up. I have access to really high, you know, RAM, like 32 gig environments if I need to. I can of course take advantage of connecting to GPUs, which is the main reason I use it. I often pay for the $10 a month package because that gives you one tier above on the Google Pro. You can see I'm kind of a Google Pro person over here. That's what I recommend. Digital Ocean allows anybody to get a web server, a web instance up and running very quickly, very cheaply. If you take a look at their pricing, they have droplets of virtual machines starting for $4 a month. This is how I first started with setting up my own web server, being able to set up an SSH, setting up my first blog, all on a server that costs just a few dollars a month. Check it out.",
      "platforms": {
        "tiktok": {
          "video_id": "7134014235796933934",
          "url": "https://www.tiktok.com/@rajistics/video/7134014235796933934",
          "view_count": 9319,
          "upload_date": "2022-08-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/87ffb2b6184144f1bc567eb68a9ae316_1661017135~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=qutHQEmNXnn39TiR34t6melCfEg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Hugging Face #reinforcementlearning class #datascience #techtok #deeplearning #python",
      "description": "Hugging Face #reinforcementlearning class #datascience #techtok #deeplearning #python",
      "upload_date": "2022-04-26",
      "total_views": 9150,
      "max_views": 9150,
      "topics": [
        "datascience",
        "deeplearning",
        "hugging",
        "python",
        "reinforcementlearning",
        "techtok"
      ],
      "search_text": "Hugging Face #reinforcementlearning class #datascience #techtok #deeplearning #python datascience deeplearning hugging python reinforcementlearning techtok You've seen this and this. Now it's time for you to learn how to build it. Here's a six week online course. It's by Hugging Face that's gonna go through deep reinforcement learning. Go get the code and build these kinds of cool things. Yeah. So join in.",
      "platforms": {
        "tiktok": {
          "video_id": "7090710091422387498",
          "url": "https://www.tiktok.com/@rajistics/video/7090710091422387498",
          "view_count": 9150,
          "upload_date": "2022-04-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/68f2542d836e4baf8566c9a8884e6f3e_1650934597~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=J1YUq%2F7wsUiOLr8oo3ElgNH%2BtW8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6654,
      "title": "The feature or variables in auto insurance models. Learn from insurance good features can give you a lot of predictive power. #datascience #machinelearning #autoinsurance #acturialscience",
      "description": "The feature or variables in auto insurance models. Learn from insurance good features can give you a lot of predictive power. #datascience #machinelearning #autoinsurance #acturialscience",
      "upload_date": "2024-02-13",
      "total_views": 9127,
      "max_views": 5874,
      "topics": [
        "acturialscience",
        "autoinsurance",
        "datascience",
        "feature",
        "insurance",
        "machinelearning",
        "variables"
      ],
      "search_text": "The feature or variables in auto insurance models. Learn from insurance good features can give you a lot of predictive power. #datascience #machinelearning #autoinsurance #acturialscience acturialscience autoinsurance datascience feature insurance machinelearning variables Between us, can you tell me what are the variables in those autoinsurance models you worked on? Sure, including the controversial ones? No problem. Insurance is heavily regulated so all the variables that go into these models are public novels. Wow. So, what variables do they use? How much you drive? Cuz the longer you're on the road, the higher probability of getting in a crash. Oh, so that's why they push us to use those drive tracking apps. You got it. That insurance data is much more reliable than what people will tell their insurance carriers. What else? Your driving record. Okay. Where you live, your age, your sex, and your status. Couldn't that be discriminatory? States differ on it. California got rid of gender. New York banned occupation and education and Michigan has voted to ban several non-driving factors including gender and education. What else? Credit score. Come on. Does that make sense? Sounds like a boomer discount. It's been very controversial. But there's been a strong relationship between having a good credit score and getting in fewer crashes. So insurance companies really want to keep it and they see it as a proxy for responsibility. Hey thanks boss. Next week can we talk about overfitting?",
      "platforms": {
        "tiktok": {
          "video_id": "7335156324700917035",
          "url": "https://www.tiktok.com/@rajistics/video/7335156324700917035",
          "view_count": 3253,
          "upload_date": "2024-02-13",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C3TEOrTAbxp",
          "url": "https://www.instagram.com/reel/C3TEOrTAbxp",
          "view_count": 5874,
          "upload_date": "2024-02-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Working with small datasets. Several tips including using crossvalidation, models like lasso, and running multiple interations with different random seeds. #datascience #machinelearning #crossvalidation #elasticnet #lasso #randomseed ",
      "description": "Working with small datasets. Several tips including using crossvalidation, models like lasso, and running multiple interations with different random seeds. #datascience #machinelearning #crossvalidation #elasticnet #lasso #randomseed ",
      "upload_date": "2023-03-07",
      "total_views": 9102,
      "max_views": 9102,
      "topics": [
        "crossvalidation",
        "data",
        "elasticnet",
        "going",
        "lasso",
        "like"
      ],
      "search_text": "Working with small datasets. Several tips including using crossvalidation, models like lasso, and running multiple interations with different random seeds. #datascience #machinelearning #crossvalidation #elasticnet #lasso #randomseed  crossvalidation data elasticnet going lasso like I'm worried. How am I going to build a machine learning model on such a small data set? What's small for you? For this problem, I only have 100 rows of data, and when I set aside 20 for a test and 20 more for a holdout, that only leaves 60 rows of data. That doesn't seem that much. Do we have a big data project I could work on? Each of those data points cost over $5,000. We're not going to get more data easily, and this is a very valuable project for our team. Wow! My biggest concern with using machine learning is overfitting. If we had a lot of data like in this example, I wouldn't worry about overfitting. But when we have only a little bit of data like in this and in this example, there's a lot of different ways that that model could be created that might not actually generalize and capture the true pattern underneath. Oh yeah, that isn't good. Well, to get more out of your training data, you should use cross-validation. Studies have shown using cross-validation gets you better performance. Take a look at something like nested cross-validation for what you're doing. Glad you aren't cross with me. Go check SideKit. There's already functions built for cross-validation. My suggestion is to start with simpler models. So take a look at support vector machines, Eureka, or an ElasticNet. I'd also suggest starting with ElasticNet because of the built-in feature selection. That feature selection is going to reduce that number of features, and having fewer features means less features that are likely to be susceptible to noise or spurious correlation. I remember us talking about lasso and ElasticNet. Yeah, these are pretty common for the scenario you're in, especially when you have a lot of features and just a little bit of data. One pro-level tip I like to use is changing the random seed. By rerunning the analysis with different random seeds, you can start to separate out the signal from the noise. And in this way, you might be able to, for example, identify what is the certain set or group of features that's consistent between different runs. If you find those, probably a good chance that those are going to generalize out to new data and you're not going to be just overfitting. Yikes! Isn't that going to take a long time to run? With your data set this small, I don't think trading time is going to be too big of an issue. Thanks everyone, I feel a lot better, and my knowledge is a lot larger!",
      "platforms": {
        "tiktok": {
          "video_id": "7207608623835581742",
          "url": "https://www.tiktok.com/@rajistics/video/7207608623835581742",
          "view_count": 9102,
          "upload_date": "2023-03-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/31e5df45e22c4aeaa5d9f3a94d7d6ba1_1678152260~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=YUl1HjN4oDLx6Rh%2BiJR3RVNnfUc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6424,
      "title": "Humans are Biased: Generative AI is Even Worse",
      "description": "Humans are Biased: Generative AI is Even Worse",
      "upload_date": "2023-07-27",
      "total_views": 9090,
      "max_views": 4847,
      "topics": [
        "biased",
        "biasml",
        "datascience",
        "even",
        "generative",
        "generativeai",
        "humans",
        "machinelearning",
        "worse"
      ],
      "search_text": "Humans are Biased: Generative AI is Even Worse biased biasml datascience even generative generativeai humans machinelearning worse",
      "platforms": {
        "tiktok": {
          "video_id": "7260297458528357678",
          "url": "https://www.tiktok.com/@rajistics/video/7260297458528357678",
          "view_count": 3875,
          "upload_date": "2023-07-27",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CvLoSP_t6VI",
          "url": "https://www.instagram.com/reel/CvLoSP_t6VI",
          "view_count": 368,
          "upload_date": "2023-07-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Xy1cV56N9D0",
          "url": "https://www.youtube.com/watch?v=Xy1cV56N9D0",
          "view_count": 4847,
          "upload_date": "2023-07-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reviewing Anthropics latest research and OpenAI continuing to fumble. Mapping the Mind of a Large Language Model: https://www.anthropic.com/news/mapping-mind-language-model' #mechanisticinterpretability #largelanguagemodels #anthropic #rajistics",
      "description": "Reviewing Anthropics latest research and OpenAI continuing to fumble. Mapping the Mind of a Large Language Model: https://www.anthropic.com/news/mapping-mind-language-model' #mechanisticinterpretability #largelanguagemodels #anthropic #rajistics",
      "upload_date": "2024-05-23",
      "total_views": 9040,
      "max_views": 9040,
      "topics": [
        "anthropic",
        "largelanguagemodels",
        "like",
        "mechanisticinterpretability",
        "model",
        "research"
      ],
      "search_text": "Reviewing Anthropics latest research and OpenAI continuing to fumble. Mapping the Mind of a Large Language Model: https://www.anthropic.com/news/mapping-mind-language-model' #mechanisticinterpretability #largelanguagemodels #anthropic #rajistics anthropic largelanguagemodels like mechanisticinterpretability model research With Anthropics' latest research, we can now make a flirty Scarlett Johansson. Like another copycat voice actor? No, we can modify the model itself. No way, these models are too complex. They have billions of parameters. They're unknowable. The latest mechanistic interpretability research has scaled up early research. In this work, they're able to extract features from the middle layer of Claude III Sonnet. What kind of features? Here's the feature for the Golden Gate Bridge. They were able to trigger it with lots of different types of inputs. Wow, so it really understands the concept of Golden Gate Bridge. Crossed different types of modalities and different kinds of languages. They found lots of other concepts such as people like Rosalind Franklin, elements like lithium, scientific fields like immunology, even programming syntax like function calls. Did they find a flirty feature? Come on. But what they did do was map out the concepts and they found that stuff was related as you'd expect. So if you take inner conflict, you find that features related to relationship breakups, conflicting allegiances, logical inconsistencies, are all in the similar neighborhood. So they identified the concepts. Can they do anything with them? Oh yes, once they find them, they can manipulate them and make them stronger or weakener. So for example, for safety, we could turn down biological warfare in a model. Or you can make a model where you turn down Saturday night live and turn up the flirty.",
      "platforms": {
        "tiktok": {
          "video_id": "7372346086075288875",
          "url": "https://www.tiktok.com/@rajistics/video/7372346086075288875",
          "view_count": 9040,
          "upload_date": "2024-05-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e0365871bbba4c18843d2079786c48bd_1716508092~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=Td%2BE8psQyIwkD8evqPPYgc8JKJ4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Introducing myself, like a year too late. Hope this fills the gaps around this channel. ",
      "description": "Introducing myself, like a year too late. Hope this fills the gaps around this channel. ",
      "upload_date": "2022-11-28",
      "total_views": 8964,
      "max_views": 8964,
      "topics": [
        "channel",
        "data",
        "let",
        "like",
        "lot",
        "science"
      ],
      "search_text": "Introducing myself, like a year too late. Hope this fills the gaps around this channel.  channel data let like lot science You've probably seen my Data Science TikToks. Let me tell you why they're so mediocre. I'm Raj. I've worked as a data scientist for the last eight years at five different companies, along the way I've learned a lot, and messed up a few times. Before I got into Data Science, I worked at an IT help desk. But I was able to learn Data Science by using a lot of free online resources. To give back, I started creating my own online data resources, but notice most people weren't reading my 20-page blogs. Then I saw TikTok. My kids convinced me I couldn't do TikTok, but hey, I like to spite them, so I decided to go on TikTok. I've shown them by acting honorably, not doing click-baity stuff. You can end up with a pretty mediocre Data Science channel. The content in this channel is geared towards practicing data scientists. This was my attempt to niche down, because most practicing data scientists I know don't spend a lot of time reading extra Data Science content because they have lives, or they don't spend a lot of time on TikTok because they have lives. So have fun watching the videos, participate, give me feedback, let me know what we can do to keep it fun.",
      "platforms": {
        "tiktok": {
          "video_id": "7170853872351759662",
          "url": "https://www.tiktok.com/@rajistics/video/7170853872351759662",
          "view_count": 8964,
          "upload_date": "2022-11-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3a4a49203e2e43c3b7aec97c4c0a5eb9_1669594587~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=bBVTbvEfC0tEXHRHmmDmQpmHJLQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I posted this on LinkedIn today, but wanted to share here. GTP-3 is powerful, but sometimes domain specific models are going to do better. Pick the right tool for the job. #datascience #machinelearning #huggingface #chatgpt #openai #gpt3 #finbert ",
      "description": "I posted this on LinkedIn today, but wanted to share here. GTP-3 is powerful, but sometimes domain specific models are going to do better. Pick the right tool for the job. #datascience #machinelearning #huggingface #chatgpt #openai #gpt3 #finbert ",
      "upload_date": "2023-01-27",
      "total_views": 8896,
      "max_views": 8896,
      "topics": [
        "chatgpt",
        "datascience",
        "huggingface",
        "machinelearning",
        "model",
        "openai"
      ],
      "search_text": "I posted this on LinkedIn today, but wanted to share here. GTP-3 is powerful, but sometimes domain specific models are going to do better. Pick the right tool for the job. #datascience #machinelearning #huggingface #chatgpt #openai #gpt3 #finbert  chatgpt datascience huggingface machinelearning model openai just built a financial sentiment classification model using GPT-3. Huh, you mind showing it to me? I created these prompts and then I used these prompts to classify new sentiments coming in. Our VP signed off on replacing the current model with this approach using GPT-3. He really wants to get on the open AI bandwagon. Pretty cool. How'd you validate the model? I ran some examples through and they look good. So traditionally in machine learning, we want to use some examples where we know the true labels. And then what we do is we use those to test the model to see, did it truly generalize and work on these examples? Oh, I didn't do that. No problem. Give me a second. Let me pull down that model or grab it from the hugging face hub. I'm going to pull it down a few lines of code. Let me get some predictions and we'll see how well this model is doing. Oh, my model didn't do so well. The FinBurk is trained on financial data and it's built as a classification model. So it's not surprising it's going to do better than GPT-3.",
      "platforms": {
        "tiktok": {
          "video_id": "7193143081338719531",
          "url": "https://www.tiktok.com/@rajistics/video/7193143081338719531",
          "view_count": 8896,
          "upload_date": "2023-01-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/19a531e2c25d4f82891e9bed4595dc63_1674784146~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=8PSqdc7r%2F2kJ1YyUpIJEaxz4CJw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5950,
      "title": "Why do transformers lock onto “meaningless” tokens like [BOS] or punctuation? When one token’s activation spikes thousands of times higher, attention collapses (an attention sink) and hidden-space geometry flattens (compression valley). This reveals three model phases: Mix → Compress → Refine. From Attention Sinks and Compression Valleys in LLMs Are Two Sides of the Same Coin. More refs on my subreddit.",
      "description": "Why do transformers lock onto “meaningless” tokens like [BOS] or punctuation? When one token’s activation spikes thousands of times higher, attention collapses (an attention sink) and hidden-space geometry flattens (compression valley). This reveals three model phases: Mix → Compress → Refine. From Attention Sinks and Compression Valleys in LLMs Are Two Sides of the Same Coin. More refs on my subreddit.",
      "upload_date": "2025-10-24",
      "total_views": 8872,
      "max_views": 8152,
      "topics": [
        "attention",
        "compression",
        "layers",
        "llms",
        "massive",
        "sinks",
        "spikes",
        "transformers",
        "two",
        "valleys"
      ],
      "search_text": "Why do transformers lock onto “meaningless” tokens like [BOS] or punctuation? When one token’s activation spikes thousands of times higher, attention collapses (an attention sink) and hidden-space geometry flattens (compression valley). This reveals three model phases: Mix → Compress → Refine. From Attention Sinks and Compression Valleys in LLMs Are Two Sides of the Same Coin. More refs on my subreddit. attention compression layers llms massive sinks spikes transformers two valleys My transformer freaked out. Look at these massive activations. They're 10,000 times bigger than everything else. Oh yeah, those massive spikes end up collapsing the entire space around it. So what's going on? Well, researchers think that this is the other side of the coin from attention sinks. Remember those tokens that pulled attention towards them? Yikes. So transformers are full of these sinks and massive spikes. Shouldn't we try to cut them off? Not necessarily. It's actually part of how models learn. Think back to convolutional neural networks. You remember how different layers respond to different kinds of patterns? Oh yeah, I remember the lower layers would pick up edges and textures while the higher layers recognize shapes and objects. Exactly. And researchers have found something similar for transformers with three distinct layers. In mix, the layers are spreading information around, building that contextual understanding. The second phase is compress, and that's where your massive activations emerge and you get attention sinks. This is where all the mixing kind of halts, but essential information is getting packed into lower dimensions. And the last is refine, where the layers are expanding again, but just polishing out all the details for generation. Ah, so I'm seeing the compression phase right now. Makes sense. Embedding tasks like classification or retrieval are best performed during that compression when features are concentrating in low dimensions. While generation tasks need that full pipeline, so you need that refinement for that next token prediction. So those massive activations aren't a bug, they're just how the model's thinking through all of the data.",
      "platforms": {
        "tiktok": {
          "video_id": "7564855404153277727",
          "url": "https://www.tiktok.com/@rajistics/video/7564855404153277727",
          "view_count": 8152,
          "upload_date": "2025-10-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oQxFCIEjIgjqHPA7dfhZ3Q84RfAhADsYeAEAAI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=dJoKFrpYNEfrny7cv9CJ6T%2FqrDs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18068600291366155",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "O6T5BkP-8FI",
          "url": "https://www.youtube.com/watch?v=O6T5BkP-8FI",
          "view_count": 720,
          "upload_date": "2025-10-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6505,
      "title": "Prompt sensitivity is a thing. This video covers how changes in formatting the persuasion used in prompts and prompt injection attacks are all crucial considerations in working with LLMs. QUANTIFYING LANGUAGE MODELS‚Äô SENSITIVITY TO SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting: https://arxiv.org/pdf/2310.11324.pdf Anthropic evaluation: https://www.anthropic.com/index/evaluating-ai-systems How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs - https://chats-lab.github.io/persuasive_jailbreaker/ #largelanguagemodels #openai #anthropic #promptinjection #prompting #rajistics",
      "description": "Prompt sensitivity is a thing. This video covers how changes in formatting the persuasion used in prompts and prompt injection attacks are all crucial considerations in working with LLMs. QUANTIFYING LANGUAGE MODELS‚Äô SENSITIVITY TO SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting: https://arxiv.org/pdf/2310.11324.pdf Anthropic evaluation: https://www.anthropic.com/index/evaluating-ai-systems How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs - https://chats-lab.github.io/persuasive_jailbreaker/ #largelanguagemodels #openai #anthropic #promptinjection #prompting #rajistics",
      "upload_date": "2024-01-11",
      "total_views": 8830,
      "max_views": 4530,
      "topics": [
        "anthropic",
        "largelanguagemodels",
        "openai",
        "prompt",
        "prompting",
        "promptinjection"
      ],
      "search_text": "Prompt sensitivity is a thing. This video covers how changes in formatting the persuasion used in prompts and prompt injection attacks are all crucial considerations in working with LLMs. QUANTIFYING LANGUAGE MODELS‚Äô SENSITIVITY TO SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting: https://arxiv.org/pdf/2310.11324.pdf Anthropic evaluation: https://www.anthropic.com/index/evaluating-ai-systems How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs - https://chats-lab.github.io/persuasive_jailbreaker/ #largelanguagemodels #openai #anthropic #promptinjection #prompting #rajistics anthropic largelanguagemodels openai prompt prompting promptinjection",
      "platforms": {
        "tiktok": {
          "video_id": "7322966271987928362",
          "url": "https://www.tiktok.com/@rajistics/video/7322966271987928362",
          "view_count": 4530,
          "upload_date": "2024-01-11",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1-fNgbp7bR",
          "url": "https://www.instagram.com/reel/C1-fNgbp7bR",
          "view_count": 3928,
          "upload_date": "2024-01-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Y6MJ6dq8eK0",
          "url": "https://youtube.com/shorts/Y6MJ6dq8eK0",
          "view_count": 372,
          "upload_date": "2024-01-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6185,
      "title": "This video explains how Singular Value Decomposition (SVD) helps AI compress large amounts of data, similar to how a student condenses notes onto a cheat sheet. Using a black-and-white image of a cat, it shows how SVD breaks the image matrix into components and reconstructs it using only the top K singular values—demonstrating how reducing K lowers image quality but compresses information.",
      "description": "This video explains how Singular Value Decomposition (SVD) helps AI compress large amounts of data, similar to how a student condenses notes onto a cheat sheet. Using a black-and-white image of a cat, it shows how SVD breaks the image matrix into components and reconstructs it using only the top K singular values—demonstrating how reducing K lowers image quality but compresses information.",
      "upload_date": "2025-04-04",
      "total_views": 8712,
      "max_views": 8712,
      "topics": [
        "decomposition",
        "matrices",
        "matrixes",
        "singular",
        "svd",
        "using",
        "value"
      ],
      "search_text": "This video explains how Singular Value Decomposition (SVD) helps AI compress large amounts of data, similar to how a student condenses notes onto a cheat sheet. Using a black-and-white image of a cat, it shows how SVD breaks the image matrix into components and reconstructs it using only the top K singular values—demonstrating how reducing K lowers image quality but compresses information. decomposition matrices matrixes singular svd using value You ever got to use a cheat sheet for a test? You probably spent a lot of time compressing all that knowledge down onto one little card. Well, the same thing that you're doing, this is what AI has to do when it's faced with large amounts of images and text. And I want to talk about one of the approaches that modern AI uses called singular value decomposition. So let's take this picture of cat in black and white. It represents a matrix of 2,500 by 2,392. So the way singular value decomposition or SVD works is it takes this matrix of data and breaks it down into several other matrices. These other matrices are special. And one of the things we can do is only use a subset of these matrices. So here you'll see I'm going to reduce the value of K, which is the amount of information I'm going to take from these matrices. And as I reduce that down, you'll see that the picture becomes blurry, harder to receive, because I'm using less information to represent it.",
      "platforms": {
        "tiktok": {
          "video_id": "7489568242457955615",
          "url": "https://www.tiktok.com/@rajistics/video/7489568242457955615",
          "view_count": 8712,
          "upload_date": "2025-04-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYPwIBMKzEKACAi5SBfijDKiImBAT4A1vAvirh~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=%2BBB94pxEUoehp2O6ouGQf%2BkHha0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18397731850104625",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6469,
      "title": "Three major improvements to the transformer architecture that everyone should know. They include Fast Attention Rotary Positional Embeddings and Multi-Query Attention. #machinelearning #largelanguagemodels #positionalencodings #flashattention #mulitqueryattention Useful Links: Learning position with Positional Encoding: https://www.scaler.com/topics/nlp/positional-encoding/ ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING: https://arxiv.org/pdf/2104.09864.pdf Rotary Embeddings: A Relative Revolution - https://blog.eleuther.ai/rotary-embeddings/ Flash Attention: https://github.com/Dao-AILab/flash-attention GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints: https://arxiv.org/abs/2305.13245",
      "description": "Three major improvements to the transformer architecture that everyone should know. They include Fast Attention Rotary Positional Embeddings and Multi-Query Attention. #machinelearning #largelanguagemodels #positionalencodings #flashattention #mulitqueryattention Useful Links: Learning position with Positional Encoding: https://www.scaler.com/topics/nlp/positional-encoding/ ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING: https://arxiv.org/pdf/2104.09864.pdf Rotary Embeddings: A Relative Revolution - https://blog.eleuther.ai/rotary-embeddings/ Flash Attention: https://github.com/Dao-AILab/flash-attention GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints: https://arxiv.org/abs/2305.13245",
      "upload_date": "2023-07-29",
      "total_views": 8702,
      "max_views": 7354,
      "topics": [
        "architecture",
        "attention",
        "fast",
        "flashattention",
        "largelanguagemodels",
        "machinelearning",
        "mulitqueryattention",
        "positional",
        "positionalencodings",
        "rotary",
        "transformer"
      ],
      "search_text": "Three major improvements to the transformer architecture that everyone should know. They include Fast Attention Rotary Positional Embeddings and Multi-Query Attention. #machinelearning #largelanguagemodels #positionalencodings #flashattention #mulitqueryattention Useful Links: Learning position with Positional Encoding: https://www.scaler.com/topics/nlp/positional-encoding/ ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING: https://arxiv.org/pdf/2104.09864.pdf Rotary Embeddings: A Relative Revolution - https://blog.eleuther.ai/rotary-embeddings/ Flash Attention: https://github.com/Dao-AILab/flash-attention GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints: https://arxiv.org/abs/2305.13245 architecture attention fast flashattention largelanguagemodels machinelearning mulitqueryattention positional positionalencodings rotary transformer",
      "platforms": {
        "tiktok": {
          "video_id": "7261032023760948522",
          "url": "https://www.tiktok.com/@rajistics/video/7261032023760948522",
          "view_count": 7354,
          "upload_date": "2023-07-29",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CvQvaHlNO-V",
          "url": "https://www.instagram.com/reel/CvQvaHlNO-V",
          "view_count": 472,
          "upload_date": "2023-07-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "fT5t9xYuv-0",
          "url": "https://www.youtube.com/watch?v=fT5t9xYuv-0",
          "view_count": 876,
          "upload_date": "2023-07-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6416,
      "title": "Always have a baseline model. For time series, you can often compare to what happened in a previous time step, like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries I can do more of these baselines if you all find this useful.",
      "description": "Always have a baseline model. For time series, you can often compare to what happened in a previous time step, like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries I can do more of these baselines if you all find this useful.",
      "upload_date": "2022-10-09",
      "total_views": 8644,
      "max_views": 8644,
      "topics": [
        "codetok",
        "datascience",
        "statistics",
        "time",
        "timeseries",
        "timeseriesforcasting"
      ],
      "search_text": "Always have a baseline model. For time series, you can often compare to what happened in a previous time step, like last week. There are error metrics like MASE built on this idea. #datascience #codetok #statistics #timeseriesforcasting #timeseries I can do more of these baselines if you all find this useful. codetok datascience statistics time timeseries timeseriesforcasting",
      "platforms": {
        "instagram": {
          "video_id": "Cj84roWg-Z3",
          "url": "https://www.instagram.com/reel/Cj84roWg-Z3/",
          "view_count": 0,
          "upload_date": "2022-10-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "sPqf8De59ik",
          "url": "https://youtube.com/shorts/sPqf8De59ik?feature=share",
          "view_count": 8644,
          "upload_date": "2022-10-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6500,
      "title": "Reinforcement Learning with AI Feedback (RLAIF) is an emerging approach to replace Reinforcement Learning with Human Feedback (RLHF). It works well according to the latest paper from Google. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback: https://arxiv.org/abs/2309.00267 #largelanguagemodels #chatgpt #rlhf #rlaif #reinforcementlearning #rajistics",
      "description": "Reinforcement Learning with AI Feedback (RLAIF) is an emerging approach to replace Reinforcement Learning with Human Feedback (RLHF). It works well according to the latest paper from Google. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback: https://arxiv.org/abs/2309.00267 #largelanguagemodels #chatgpt #rlhf #rlaif #reinforcementlearning #rajistics",
      "upload_date": "2023-09-07",
      "total_views": 8618,
      "max_views": 6972,
      "topics": [
        "chatgpt",
        "feedback",
        "language",
        "large",
        "largelanguagemodels",
        "learning",
        "reinforcement",
        "reinforcementlearning",
        "rlaif",
        "rlhf"
      ],
      "search_text": "Reinforcement Learning with AI Feedback (RLAIF) is an emerging approach to replace Reinforcement Learning with Human Feedback (RLHF). It works well according to the latest paper from Google. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback: https://arxiv.org/abs/2309.00267 #largelanguagemodels #chatgpt #rlhf #rlaif #reinforcementlearning #rajistics chatgpt feedback language large largelanguagemodels learning reinforcement reinforcementlearning rlaif rlhf",
      "platforms": {
        "tiktok": {
          "video_id": "7276218525222456618",
          "url": "https://www.tiktok.com/@rajistics/video/7276218525222456618",
          "view_count": 6972,
          "upload_date": "2023-09-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cw6GupPr_Fv",
          "url": "https://www.instagram.com/reel/Cw6GupPr_Fv",
          "view_count": 1172,
          "upload_date": "2023-09-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "qmpchZ1Iqno",
          "url": "https://www.youtube.com/watch?v=qmpchZ1Iqno",
          "view_count": 474,
          "upload_date": "2023-09-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6518,
      "title": "State-of-the-art results (100%!!) on widely used academic benchmarks (MMLU GSM8K HumanEval OpenbookQA ARC Challenge etc.). The model called phi-CTNL was trained on the evaluate datasets. Yea the performance is all due to leakage and this model is a parody. #machinelearning #datascience #rajistics #phiCTNL Pretraining on the Test Set Is All You Need: https://arxiv.org/pdf/2309.08632.pdf Background video by Frankkemperrupp: https://pixabay.com/videos/tube-burst-pipe-water-plumber-99693/",
      "description": "State-of-the-art results (100%!!) on widely used academic benchmarks (MMLU GSM8K HumanEval OpenbookQA ARC Challenge etc.). The model called phi-CTNL was trained on the evaluate datasets. Yea the performance is all due to leakage and this model is a parody. #machinelearning #datascience #rajistics #phiCTNL Pretraining on the Test Set Is All You Need: https://arxiv.org/pdf/2309.08632.pdf Background video by Frankkemperrupp: https://pixabay.com/videos/tube-burst-pipe-water-plumber-99693/",
      "upload_date": "2023-09-25",
      "total_views": 8607,
      "max_views": 6225,
      "topics": [
        "ctnl",
        "datascience",
        "gives",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "parody",
        "pdf",
        "phi",
        "phictnl",
        "state"
      ],
      "search_text": "State-of-the-art results (100%!!) on widely used academic benchmarks (MMLU GSM8K HumanEval OpenbookQA ARC Challenge etc.). The model called phi-CTNL was trained on the evaluate datasets. Yea the performance is all due to leakage and this model is a parody. #machinelearning #datascience #rajistics #phiCTNL Pretraining on the Test Set Is All You Need: https://arxiv.org/pdf/2309.08632.pdf Background video by Frankkemperrupp: https://pixabay.com/videos/tube-burst-pipe-water-plumber-99693/ ctnl datascience gives largelanguagemodels machinelearning model parody pdf phi phictnl state",
      "platforms": {
        "tiktok": {
          "video_id": "7282547880378830126",
          "url": "https://www.tiktok.com/@rajistics/video/7282547880378830126",
          "view_count": 6225,
          "upload_date": "2023-09-25",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CxmBUpWN4t1",
          "url": "https://www.instagram.com/reel/CxmBUpWN4t1",
          "view_count": 2130,
          "upload_date": "2023-09-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "IIfx8Zxr3tM",
          "url": "https://www.youtube.com/watch?v=IIfx8Zxr3tM",
          "view_count": 252,
          "upload_date": "2023-09-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @midnightlibrarian #datasaurus #stats #dinosaur #analytics explaining #anscombe",
      "description": "Reply to @midnightlibrarian #datasaurus #stats #dinosaur #analytics explaining #anscombe",
      "upload_date": "2022-01-16",
      "total_views": 8570,
      "max_views": 8570,
      "topics": [
        "analytics",
        "anscombe",
        "data",
        "datasaurus",
        "dinosaur",
        "stats"
      ],
      "search_text": "Reply to @midnightlibrarian #datasaurus #stats #dinosaur #analytics explaining #anscombe analytics anscombe data datasaurus dinosaur stats For both these data sets, where the points are, the X and Y points are different. But the crazy thing is, even though the points are in different places, the mean and standard deviations for both these data sets are the exact same. This is why we don't trust just looking at summary statistics like the mean and standard deviation. We always want to visualize the data.",
      "platforms": {
        "tiktok": {
          "video_id": "7053946942308601135",
          "url": "https://www.tiktok.com/@rajistics/video/7053946942308601135",
          "view_count": 8570,
          "upload_date": "2022-01-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c205eb298fe04571a1525257857c17a7_1642375009~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=fKx9mIJqFVwBRREgIgnH3XrrwO8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "AI News Update: Meta - Major spending on AI and continuing lawsuits over child safety NVIDIA - Stock at high, 1.5 Trillion market cap Google - Continual stream of layoffs and lawsuits Microsoft - Stock at high, #1 in market cap OpenAI - Banning political use, welcoming the military Apple - Bugs and bans #ainews #rajistics #openai #nvidia #meta #google #applemusic ",
      "description": "AI News Update: Meta - Major spending on AI and continuing lawsuits over child safety NVIDIA - Stock at high, 1.5 Trillion market cap Google - Continual stream of layoffs and lawsuits Microsoft - Stock at high, #1 in market cap OpenAI - Banning political use, welcoming the military Apple - Bugs and bans #ainews #rajistics #openai #nvidia #meta #google #applemusic ",
      "upload_date": "2024-01-20",
      "total_views": 8449,
      "max_views": 8449,
      "topics": [
        "ainews",
        "applemusic",
        "google",
        "meta",
        "nvidia",
        "openai"
      ],
      "search_text": "AI News Update: Meta - Major spending on AI and continuing lawsuits over child safety NVIDIA - Stock at high, 1.5 Trillion market cap Google - Continual stream of layoffs and lawsuits Microsoft - Stock at high, #1 in market cap OpenAI - Banning political use, welcoming the military Apple - Bugs and bans #ainews #rajistics #openai #nvidia #meta #google #applemusic  ainews applemusic google meta nvidia openai AI is going to be our biggest investment area for 2024. What's new? AI has been big for everyone, even when you were back raging about the metaverse. We got 350,000 H100 cards coming this year. Each of those cards goes for more than 30k. Now you see why my market cap is 1.5 trillion. Does each card represent one child being sexually harassed every day on Facebook and Instagram? Big words. Come on, glasshouse, you're a monopolist. Guys, I'm so avoiding this. I put out a ban that politicians can't be using our services. So no politicians, but happy working with the military. Better live it up now, open AI. Have your employees take advantage of that 86 billion dollar tender offer? Because Lama 3 is going to be out this year. Sorry guys, I got an email hack to deal with, but just wanted to remind everybody we're number one, 3 trillion dollar market cap. Yeah, we'll probably drop to number three between the Apple Watch ban, the iPhone bugs, and the mediocre rollout of the Vision Pro. Pro tip for everyone out there, layoffs. Team building exercise plus Wall Street loves it.",
      "platforms": {
        "tiktok": {
          "video_id": "7326240133450157355",
          "url": "https://www.tiktok.com/@rajistics/video/7326240133450157355",
          "view_count": 8449,
          "upload_date": "2024-01-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/13153d3aff0543ad89a7b589c2d4f893_1705773244~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=b2CFhbJx1fSf4%2FoVXx6sISBVkLc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6209,
      "title": "The curse of dimensionality occurs when adding more features to a model leads to decreased performance due to increased noise and complexity. Most production ML models use fewer than 30 features because additional variables often provide little incremental value and can hinder generalization. Effective feature selection is crucial, as high-dimensional spaces make it harder to identify meaningful patterns and can lead to overfitting. ",
      "description": "The curse of dimensionality occurs when adding more features to a model leads to decreased performance due to increased noise and complexity. Most production ML models use fewer than 30 features because additional variables often provide little incremental value and can hinder generalization. Effective feature selection is crucial, as high-dimensional spaces make it harder to identify meaningful patterns and can lead to overfitting. ",
      "upload_date": "2025-02-11",
      "total_views": 8448,
      "max_views": 8448,
      "topics": [
        "adding",
        "curse",
        "dimensionality",
        "features",
        "model",
        "performance"
      ],
      "search_text": "The curse of dimensionality occurs when adding more features to a model leads to decreased performance due to increased noise and complexity. Most production ML models use fewer than 30 features because additional variables often provide little incremental value and can hinder generalization. Effective feature selection is crucial, as high-dimensional spaces make it harder to identify meaningful patterns and can lead to overfitting.  adding curse dimensionality features model performance Want to hear something strange? My model's performance keeps getting worse as I add more variables into the model. Why is that strange? I'm given the model more information, shouldn't that help? Ah, you've hit the Curse of Dimensionality. Whoa, that doesn't sound good. The Curse of Dimensionality is when you add more features or more information, the search base gets really large. What does that mean? With supervised learning, it's like adding more noise into the system. Did you plot out the performance as you added more features? I did. Now I get this plot. So as I'm adding more features in, it's like adding more noise and my model's performance isn't going up. You're getting it. When I worked in an insurance company, we had over a thousand different variables we could use to predict if somebody's likely to get in a flash. We had information on their driving history, their demographics, their credit history. How many of those features do you think we actually used in the final model? 200? Seven. Wow. Most production ML systems have less than 30 features going into them. Usually it doesn't add much incremental value by adding more features. Guess my model's cursed and it's okay.",
      "platforms": {
        "tiktok": {
          "video_id": "7470265095654739230",
          "url": "https://www.tiktok.com/@rajistics/video/7470265095654739230",
          "view_count": 8448,
          "upload_date": "2025-02-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYus4AfRqVICftjSIfAOI8FEURUWNPrrDUI2Cd~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=Osx1HIxAfXW7jRaGv4bi105nnZ4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18078832000619992",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Some of my favorite machine learning visualizations. Check them out to better understand how these algorithms work. If you work closely with algorithms, take the time to build these visualization tools, it's very useful. Karpathy: https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html DBSCAN and other clustering: https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ Outlier / Anomaly app: http://projects.rajivshah.com/shiny/outlier/ My outlier app video: https://youtu.be/1zPuRAgr1F4?si=2IZ5wedeTVY-hYlM #machinelearning #visualizations #rajistics",
      "description": "Some of my favorite machine learning visualizations. Check them out to better understand how these algorithms work. If you work closely with algorithms, take the time to build these visualization tools, it's very useful. Karpathy: https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html DBSCAN and other clustering: https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ Outlier / Anomaly app: http://projects.rajivshah.com/shiny/outlier/ My outlier app video: https://youtu.be/1zPuRAgr1F4?si=2IZ5wedeTVY-hYlM #machinelearning #visualizations #rajistics",
      "upload_date": "2024-05-05",
      "total_views": 8389,
      "max_views": 8389,
      "topics": [
        "algorithms",
        "machine",
        "machinelearning",
        "tools",
        "understand",
        "visualizations"
      ],
      "search_text": "Some of my favorite machine learning visualizations. Check them out to better understand how these algorithms work. If you work closely with algorithms, take the time to build these visualization tools, it's very useful. Karpathy: https://cs.stanford.edu/~karpathy/svmjs/demo/demoforest.html DBSCAN and other clustering: https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/ Outlier / Anomaly app: http://projects.rajivshah.com/shiny/outlier/ My outlier app video: https://youtu.be/1zPuRAgr1F4?si=2IZ5wedeTVY-hYlM #machinelearning #visualizations #rajistics algorithms machine machinelearning tools understand visualizations Have you ever wondered how neural networks make decisions? Imagine seeing that process unfold with machine learning visualizations. Visualization tools make it easier to grasp machine learning algorithms. See here how we change the depth in a random force and we can build a more complex decision boundary. Now these examples were built by Carpathian while in graduate school and that helped them better understand these algorithms. Other people have built visualization tools, for example, looking at clustering algorithms like DB scan, so we can understand what minimum points is and epsilon is and how they influence the clustering results. For me, when I was working on an anomaly detection project, I built a simple application to visualize anomalies and toy data sets. This allowed me to try different types of algorithms and see what type of anomalies they would detect. So these visualization tools are great to understand how machine learning algorithms. But one step better to really prove you understand how this stuff works is go build the tools yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7365552550113905966",
          "url": "https://www.tiktok.com/@rajistics/video/7365552550113905966",
          "view_count": 8389,
          "upload_date": "2024-05-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/572838f33ef843c48786a812b6533e12_1714926531~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=pA3pfsdWAXiijboYCE2MJ0QKMeY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6471,
      "title": "Automating machine learning with Large Language Models (LLMs). While it's possible to ask ChatGPT to provide code for building a prediction model MLCopilot goes way beyond that. It uses a memory and knowledge bank of past experiences solving ML tasks. #datascience #machinelearning #automl #mlcopilot #largelanguagemodels #llms MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks: https://arxiv.org/abs/2304.14979 Background image by Michael Dziedzic: https://unsplash.com/photos/aQYgUYwnCsM",
      "description": "Automating machine learning with Large Language Models (LLMs). While it's possible to ask ChatGPT to provide code for building a prediction model MLCopilot goes way beyond that. It uses a memory and knowledge bank of past experiences solving ML tasks. #datascience #machinelearning #automl #mlcopilot #largelanguagemodels #llms MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks: https://arxiv.org/abs/2304.14979 Background image by Michael Dziedzic: https://unsplash.com/photos/aQYgUYwnCsM",
      "upload_date": "2023-05-01",
      "total_views": 8388,
      "max_views": 7201,
      "topics": [
        "automl",
        "datascience",
        "deplot",
        "knowledge",
        "language",
        "largelanguagemodels",
        "learning",
        "llms",
        "machinelearning",
        "mlcopilot",
        "model",
        "one",
        "reasoning",
        "shot",
        "visual"
      ],
      "search_text": "Automating machine learning with Large Language Models (LLMs). While it's possible to ask ChatGPT to provide code for building a prediction model MLCopilot goes way beyond that. It uses a memory and knowledge bank of past experiences solving ML tasks. #datascience #machinelearning #automl #mlcopilot #largelanguagemodels #llms MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks: https://arxiv.org/abs/2304.14979 Background image by Michael Dziedzic: https://unsplash.com/photos/aQYgUYwnCsM automl datascience deplot knowledge language largelanguagemodels learning llms machinelearning mlcopilot model one reasoning shot visual",
      "platforms": {
        "tiktok": {
          "video_id": "7228362497365953838",
          "url": "https://www.tiktok.com/@rajistics/video/7228362497365953838",
          "view_count": 7201,
          "upload_date": "2023-05-01",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CruCK9fAGnZ",
          "url": "https://www.instagram.com/reel/CruCK9fAGnZ",
          "view_count": 377,
          "upload_date": "2023-05-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "N_RuMOgmkR0",
          "url": "https://www.youtube.com/watch?v=N_RuMOgmkR0",
          "view_count": 810,
          "upload_date": "2023-05-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Videos with stable diffusion #datascience #machinelearning #stablediffusion #codetok",
      "description": "Videos with stable diffusion #datascience #machinelearning #stablediffusion #codetok",
      "upload_date": "2022-09-07",
      "total_views": 8327,
      "max_views": 8327,
      "topics": [
        "datascience",
        "going",
        "images",
        "machinelearning",
        "stablediffusion",
        "video"
      ],
      "search_text": "Videos with stable diffusion #datascience #machinelearning #stablediffusion #codetok datascience going images machinelearning stablediffusion video We've all seen this amazing stuff between going from a text to image. Let's ramp it up and go from text to a video. Stable Diffusion's video package lets you go from two images to a video. Let me show you how it works. Grab Nate's code from GitHub, FireUp CoLab. Run the CoLab notebook. You're going to have to log in with your credentials from the Hugging Face Hub so you can get the weights and then fire up the Gradio web app. Now let's figure out some cool images. I like going over to Lexica and being able to get some ideas for prompts. It's a good starting spot. Once you do that, play around. I get some images. Remember the seeds. On the video page, you're just going to enter in the information for each of the images along with the seed. Hit the button to start and then just wait. It's going to take a little while to kind of build. If you go and navigate the Google Drive directory, you can see the different images being filled out. Once all the images are created, it'll go ahead and stitch it together into a video that you can download. Have fun.",
      "platforms": {
        "tiktok": {
          "video_id": "7140767611616316714",
          "url": "https://www.tiktok.com/@rajistics/video/7140767611616316714",
          "view_count": 8327,
          "upload_date": "2022-09-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/78ceb741e42d41b19050164cab3db811_1662589523~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=f6%2BxkD4hftNXpWaXwG1heTkssiQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6460,
      "title": "NanoGPT is a simple fast repository for training/finetuning medium-sized GPTs. I recommend it for getting a deeper understanding of large language models. #datascience #machinelearning #largelanguagemodels #nanogpt NanoGPT: https://github.com/karpathy/nanoGPT NanoGPT_Simpsons: https://github.com/rajshah4/nanoGPT_simpsons",
      "description": "NanoGPT is a simple fast repository for training/finetuning medium-sized GPTs. I recommend it for getting a deeper understanding of large language models. #datascience #machinelearning #largelanguagemodels #nanogpt NanoGPT: https://github.com/karpathy/nanoGPT NanoGPT_Simpsons: https://github.com/rajshah4/nanoGPT_simpsons",
      "upload_date": "2023-08-20",
      "total_views": 8320,
      "max_views": 6440,
      "topics": [
        "announced",
        "datascience",
        "face",
        "github",
        "gpt4",
        "hugging",
        "huggingface",
        "largelanguagemodels",
        "machinelearning",
        "meets",
        "nanogpt",
        "nanogpt_simpsons"
      ],
      "search_text": "NanoGPT is a simple fast repository for training/finetuning medium-sized GPTs. I recommend it for getting a deeper understanding of large language models. #datascience #machinelearning #largelanguagemodels #nanogpt NanoGPT: https://github.com/karpathy/nanoGPT NanoGPT_Simpsons: https://github.com/rajshah4/nanoGPT_simpsons announced datascience face github gpt4 hugging huggingface largelanguagemodels machinelearning meets nanogpt nanogpt_simpsons",
      "platforms": {
        "tiktok": {
          "video_id": "7269486926011338027",
          "url": "https://www.tiktok.com/@rajistics/video/7269486926011338027",
          "view_count": 6440,
          "upload_date": "2023-08-20",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CwV1b4ksDqy",
          "url": "https://www.instagram.com/reel/CwV1b4ksDqy",
          "view_count": 766,
          "upload_date": "2023-08-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NsjYR0vYbVY",
          "url": "https://www.youtube.com/watch?v=NsjYR0vYbVY",
          "view_count": 1114,
          "upload_date": "2023-08-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Back!  Time for AI on images. #datascience  #computervision #objectdetection #yolo #machinelearning #codetok",
      "description": "Back!  Time for AI on images. #datascience  #computervision #objectdetection #yolo #machinelearning #codetok",
      "upload_date": "2022-07-12",
      "total_views": 8284,
      "max_views": 8284,
      "topics": [
        "codetok",
        "computervision",
        "datascience",
        "machinelearning",
        "objectdetection",
        "yolo"
      ],
      "search_text": "Back!  Time for AI on images. #datascience  #computervision #objectdetection #yolo #machinelearning #codetok codetok computervision datascience machinelearning objectdetection yolo Ready to learn about one of the coolest applications of AI? It's object detection. And what it involves is how you can use AI to detect multiple objects in a picture and actually put little bounding boxes so you know where that object is. Nate made a spaces here so you can go ahead and try some of the demo videos or even upload your own video and see how it works. The demo's using YOLO v6. You can also get the code for that and other tutorials at their GitHub. You can also try out the spaces on images. Now that you get it, if you wanna do a really cool project with object detection, try to find some objects that it doesn't already recognize. Actually did this once for a sports analytic company where I analyzed tennis players. I had to identify tennis players, their rackets, the ball, all within a moving tennis game using YOLO and retraining it. Learned a ton that way and push yourself. That's a great way to actually get involved with object detection and build a cool app and demo you for yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7119554935582166315",
          "url": "https://www.tiktok.com/@rajistics/video/7119554935582166315",
          "view_count": 8284,
          "upload_date": "2022-07-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/953ed885b40d4f07bdc0dd9377815cb6_1657650563~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=WVzCuXBc%2FLnVKf1oWLiVlKbhVqo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Is RAG Actually Broken? A recent “semantic collapse” claim argues that embeddings fail at scale because distances compress in high dimensions. This video explains what’s actually true, what’s misunderstood, and how real RAG systems work in practice. (I have covered this in my blog and long videos)",
      "description": "Is RAG Actually Broken? A recent “semantic collapse” claim argues that embeddings fail at scale because distances compress in high dimensions. This video explains what’s actually true, what’s misunderstood, and how real RAG systems work in practice. (I have covered this in my blog and long videos)",
      "upload_date": "2026-01-01",
      "total_views": 8280,
      "max_views": 8280,
      "topics": [
        "actually",
        "embeddings",
        "rag",
        "real",
        "systems",
        "work"
      ],
      "search_text": "Is RAG Actually Broken? A recent “semantic collapse” claim argues that embeddings fail at scale because distances compress in high dimensions. This video explains what’s actually true, what’s misunderstood, and how real RAG systems work in practice. (I have covered this in my blog and long videos) actually embeddings rag real systems work Oh no, rag is dead! Embeddings aren't working, we've hit semantic collapse! Because once your knowledge base gets big enough, retrieval stops working. Well, that's the claim. The explanation here goes like this, that once we get to high dimensions, distance is all compressed, everything gets closer together. You know, this is the curse of dimensionality. But this is where it goes sideways. If, by similarity, was just raw distance, this would be a real problem. But that's now how we use embeddings. What matters is the relative ordering. What is more similar than the other thing? And that's not an accident, that's literally how we train these models. We use contrastive learning, and that doesn't care if the distances look small, it explicitly trains models to rank positives ahead of negatives. Ordering, not geometry, is the signal. And come on, if distance concentration actually killed the signal, we wouldn't be arguing about rag. We would be arguing about why gradient descent doesn't work, why metric learning doesn't work, or why language models work at all. And so in practice, real rag systems don't rely on just embeddings anyways. They're systems, they use metadata filters, hybrid retrieval with relexical search, re-rankers, embeddings are just one piece of a larger pipeline. And please, think through before you like these types of posts. All this AI slop, it's really making social media unusable.",
      "platforms": {
        "tiktok": {
          "video_id": "7590447176166247710",
          "url": "https://www.tiktok.com/@rajistics/video/7590447176166247710",
          "view_count": 8280,
          "upload_date": "2026-01-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ospVJxprINqjBeSEIEaW0EAcDRF5gVf6MA7D9E~tplv-tiktokx-origin.image?dr=9636&x-expires=1767528000&x-signature=%2BkDZqvXj5BGybUZqvXOfhhD9iBQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Try out these examples for yourself and lots more available. It’s scary cool how these models are working. #datascience #machinelearning #gpt3 #largelanguagemodels #flanT5 #reasoningwithpeople ",
      "description": "Try out these examples for yourself and lots more available. It’s scary cool how these models are working. #datascience #machinelearning #gpt3 #largelanguagemodels #flanT5 #reasoningwithpeople ",
      "upload_date": "2023-01-31",
      "total_views": 8266,
      "max_views": 8266,
      "topics": [
        "datascience",
        "machinelearning",
        "model",
        "models",
        "problem",
        "solve"
      ],
      "search_text": "Try out these examples for yourself and lots more available. It’s scary cool how these models are working. #datascience #machinelearning #gpt3 #largelanguagemodels #flanT5 #reasoningwithpeople  datascience machinelearning model models problem solve You ready to be impressed but also a bit scared? Check out how these language models do reasoning. Did you use our spy box? I mean, sandbox when you were testing it out? Nah, I prefer the freedom of using open source models. What? Open source models doesn't sound very American. So let's start with this basic word problem. If I use an older model like GPT-2 to try to solve it, you can see it tries to fill in the text but doesn't really answer the problem. But when I use Google's Flan T5, look at how it's able to actually figure out the problem and solve it. And even if I slightly change up the problem, it's able to figure it out. Like, it's understanding the word problem. I've talked about how these models are doing reasoning but it really hit me once I started playing around with this and changing examples and seeing how the models reacted. Guess whose models are at the top of the leaderboard? Yeah, let's see for how long. Now in this example, when I first asked the model how to solve this to do this concatenation, it doesn't figure it out. But when I slow down and ask the model to do things step by step, it's able to figure out the answer. Now in this case, we're going to give it an example of how to solve the problem before we ask it to solve the problem. And the problem isn't very simple, right? We have to take 16, divide it by half, divide it by half again. And you can see here, the model figures it out. Did you see this example of common sense reasoning? Even when you switch the order of the answers, it figured it out, like it gets it. I'd seen the benchmarks that said these models were reasoning, but until I really played with it myself, I didn't really get it in my soul. And now, frankly, I'm a little shook and it's a little scary that these models are probably doing better than a lot of people. AGI! If you want to play in your own sandbox, you can grab this model off the Hugging Face Hub and then just hit the deploy button, deploy it out, and you'll have your own private place to play around with it.",
      "platforms": {
        "tiktok": {
          "video_id": "7194963172514663722",
          "url": "https://www.tiktok.com/@rajistics/video/7194963172514663722",
          "view_count": 8266,
          "upload_date": "2023-01-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fabd45f9ce324862887ea52765a85f2d_1675207924~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=yGwboUmMsM5yoTeCed4sWVMSn%2Bg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6538,
      "title": "Using scaling laws to help us getter smaller models with the same accuracy! Based on blog post by de Vries. #datascience #machinelearning #largelanguagemodels #scalinglaws #chinchilla Go smol or go home: https://www.harmdevries.com/post/model-size-vs-compute-overhead/ Scaling Laws for Neural Language Models: https://arxiv.org/abs/2001.08361 Training Compute-Optimal Large Language Models: https://arxiv.org/abs/2203.15556 Scaling Laws Video: https://www.youtube.com/watch?v=NvgNI3waAy4",
      "description": "Using scaling laws to help us getter smaller models with the same accuracy! Based on blog post by de Vries. #datascience #machinelearning #largelanguagemodels #scalinglaws #chinchilla Go smol or go home: https://www.harmdevries.com/post/model-size-vs-compute-overhead/ Scaling Laws for Neural Language Models: https://arxiv.org/abs/2001.08361 Training Compute-Optimal Large Language Models: https://arxiv.org/abs/2203.15556 Scaling Laws Video: https://www.youtube.com/watch?v=NvgNI3waAy4",
      "upload_date": "2023-04-13",
      "total_views": 8252,
      "max_views": 7015,
      "topics": [
        "accurate",
        "chinchilla",
        "compute",
        "datascience",
        "largelanguagemodels",
        "laws",
        "machinelearning",
        "model",
        "models",
        "scaling",
        "scalinglaws",
        "smaller",
        "still",
        "using"
      ],
      "search_text": "Using scaling laws to help us getter smaller models with the same accuracy! Based on blog post by de Vries. #datascience #machinelearning #largelanguagemodels #scalinglaws #chinchilla Go smol or go home: https://www.harmdevries.com/post/model-size-vs-compute-overhead/ Scaling Laws for Neural Language Models: https://arxiv.org/abs/2001.08361 Training Compute-Optimal Large Language Models: https://arxiv.org/abs/2203.15556 Scaling Laws Video: https://www.youtube.com/watch?v=NvgNI3waAy4 accurate chinchilla compute datascience largelanguagemodels laws machinelearning model models scaling scalinglaws smaller still using",
      "platforms": {
        "tiktok": {
          "video_id": "7221365434900057390",
          "url": "https://www.tiktok.com/@rajistics/video/7221365434900057390",
          "view_count": 7015,
          "upload_date": "2023-04-13",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cq-g4K0A5zh",
          "url": "https://www.instagram.com/reel/Cq-g4K0A5zh",
          "view_count": 1105,
          "upload_date": "2023-04-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "5GBgvtxMBVI",
          "url": "https://www.youtube.com/watch?v=5GBgvtxMBVI",
          "view_count": 132,
          "upload_date": "2023-04-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6115,
      "title": "Non-deterministic LLM inference is a deal.OpenAI has started offering it hoping the rest of the providers will also offer it for enterprise applications model reproducibility and reducing model risk make non-determinism very important. #openai #largelanguagemodels #nondeterministic #rajistics OpenAI Documentation: https://platform.openai.com/docs/guides/text-generation/reproducible-outputs OpenAI review: https://cobusgreyling.medium.com/now-you-can-toggle-openai-model-determinism-8b661e02cf98 Video by Google DeepMind: https://www.pexels.com/@googledeepmind/ Text to SQL example from Dr. Ilyas Iyoob Gen AI presentation at MLOps Conference",
      "description": "Non-deterministic LLM inference is a deal.OpenAI has started offering it hoping the rest of the providers will also offer it for enterprise applications model reproducibility and reducing model risk make non-determinism very important. #openai #largelanguagemodels #nondeterministic #rajistics OpenAI Documentation: https://platform.openai.com/docs/guides/text-generation/reproducible-outputs OpenAI review: https://cobusgreyling.medium.com/now-you-can-toggle-openai-model-determinism-8b661e02cf98 Video by Google DeepMind: https://www.pexels.com/@googledeepmind/ Text to SQL example from Dr. Ilyas Iyoob Gen AI presentation at MLOps Conference",
      "upload_date": "2023-11-14",
      "total_views": 8182,
      "max_views": 6304,
      "topics": [
        "determinism",
        "largelanguagemodels",
        "learning",
        "model",
        "models",
        "non",
        "nondeterministic",
        "openai",
        "results",
        "training",
        "using",
        "weak",
        "whisper"
      ],
      "search_text": "Non-deterministic LLM inference is a deal.OpenAI has started offering it hoping the rest of the providers will also offer it for enterprise applications model reproducibility and reducing model risk make non-determinism very important. #openai #largelanguagemodels #nondeterministic #rajistics OpenAI Documentation: https://platform.openai.com/docs/guides/text-generation/reproducible-outputs OpenAI review: https://cobusgreyling.medium.com/now-you-can-toggle-openai-model-determinism-8b661e02cf98 Video by Google DeepMind: https://www.pexels.com/@googledeepmind/ Text to SQL example from Dr. Ilyas Iyoob Gen AI presentation at MLOps Conference determinism largelanguagemodels learning model models non nondeterministic openai results training using weak whisper",
      "platforms": {
        "tiktok": {
          "video_id": "7301133461266435370",
          "url": "https://www.tiktok.com/@rajistics/video/7301133461266435370",
          "view_count": 6304,
          "upload_date": "2023-11-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17905341302859905",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-11-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "fSfhwDVUEDA",
          "url": "https://www.youtube.com/watch?v=fSfhwDVUEDA",
          "view_count": 1878,
          "upload_date": "2023-11-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "It’s happened!  Time series #datascience #timeseries #analytics #statistics",
      "description": "It’s happened!  Time series #datascience #timeseries #analytics #statistics",
      "upload_date": "2022-03-19",
      "total_views": 8143,
      "max_views": 8143,
      "topics": [
        "analytics",
        "datascience",
        "happened",
        "statistics",
        "time",
        "timeseries"
      ],
      "search_text": "It’s happened!  Time series #datascience #timeseries #analytics #statistics analytics datascience happened statistics time timeseries A-A-A-A-A-A-A-A!",
      "platforms": {
        "tiktok": {
          "video_id": "7076602677127384366",
          "url": "https://www.tiktok.com/@rajistics/video/7076602677127384366",
          "view_count": 8143,
          "upload_date": "2022-03-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/edd18a05923643adb3e43ebab4bbcb33_1647649958~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=el7gbmWEhi8r1KE92EoMBZiHFUQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Meta’s Cicero for playing Diplomacy is impressive and a bit scary.  #statistics #datascience #machinelearning #diplomacy ",
      "description": "Meta’s Cicero for playing Diplomacy is impressive and a bit scary.  #statistics #datascience #machinelearning #diplomacy ",
      "upload_date": "2022-11-23",
      "total_views": 8137,
      "max_views": 8137,
      "topics": [
        "datascience",
        "diplomacy",
        "machinelearning",
        "play",
        "statistics",
        "trained"
      ],
      "search_text": "Meta’s Cicero for playing Diplomacy is impressive and a bit scary.  #statistics #datascience #machinelearning #diplomacy  datascience diplomacy machinelearning play statistics trained Did you ever get so mad that later you looked back and figured out you were being manipulated the whole time? Well, the folks at Metta did that. They trained an AI how to play diplomacy, which is a strategy game based on negotiation. The AI they trained is both ruthless and approachable. It's a master manipulator and great at diplomacy. Just wait till the advertisers and marketers figure this out. Here's it showing some sympathy for France. Here's how it coordinates a ceasefire with England. In this example, it predicts Italy is going to attack and coordinates a defense with Austria. It's good. The kicker? The other humans like to play with the AI more than they like other humans. AI that can learn from tens of thousands of interactions, it's probably not surprisingly, pretty good at knowing how to interact.",
      "platforms": {
        "tiktok": {
          "video_id": "7169319463098109226",
          "url": "https://www.tiktok.com/@rajistics/video/7169319463098109226",
          "view_count": 8137,
          "upload_date": "2022-11-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6f2ee06d3b2240e19b96837de4356a23_1669237273~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=QiWpdMeDjsURnzp5gmCcH3TT%2FC0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "SKlearn Playground #datascience #machinelearning #statistics #techtok #sklearn",
      "description": "SKlearn Playground #datascience #machinelearning #statistics #techtok #sklearn",
      "upload_date": "2022-04-12",
      "total_views": 8137,
      "max_views": 8137,
      "topics": [
        "datascience",
        "decision",
        "machinelearning",
        "sklearn",
        "statistics",
        "techtok"
      ],
      "search_text": "SKlearn Playground #datascience #machinelearning #statistics #techtok #sklearn datascience decision machinelearning sklearn statistics techtok When you have a decision, you probably think about that decision in multiple ways. Data scientists spend time thinking about which algorithm to apply to a given problem. Looking at the results here, you'll see different algorithms approach a problem differently. Take a look and see how the logistic regression and neural net differ from the decision trees in random forest. Decision trees in random forest love to partition the data into little areas. And look here, we see that the K nearest neighbors actually works much better than other approaches for solving this problem. And this is why good data scientists often try multiple techniques for solving your problem.",
      "platforms": {
        "tiktok": {
          "video_id": "7085529850827656491",
          "url": "https://www.tiktok.com/@rajistics/video/7085529850827656491",
          "view_count": 8137,
          "upload_date": "2022-04-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ae489229331844a08938d67e64e54146_1649728478~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=3xg18yEHG6BDF%2FfaaI8oDxKmC%2FQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Llama 3 the beginning of the end? Or will GPT5 up-end everything (they have had over a year)? A skit based on a thread by Carmen Gutierrez on the effects of Llama 3. https://twitter.com/carmguti/status/1781047083945906221 #openai #meta #llama3 #rajistics",
      "description": "Llama 3 the beginning of the end? Or will GPT5 up-end everything (they have had over a year)? A skit based on a thread by Carmen Gutierrez on the effects of Llama 3. https://twitter.com/carmguti/status/1781047083945906221 #openai #meta #llama3 #rajistics",
      "upload_date": "2024-04-21",
      "total_views": 8048,
      "max_views": 8048,
      "topics": [
        "agi",
        "going",
        "llama3",
        "meta",
        "open",
        "openai"
      ],
      "search_text": "Llama 3 the beginning of the end? Or will GPT5 up-end everything (they have had over a year)? A skit based on a thread by Carmen Gutierrez on the effects of Llama 3. https://twitter.com/carmguti/status/1781047083945906221 #openai #meta #llama3 #rajistics agi going llama3 meta open openai How about Lama 3? Open sourcing is going to be a big win for us. I get that Lama 3 is disruptive, but it's not really AI innovation. It's not AGI. It's really about the money. Open AI, do you even have a good plan to monetize your models? Open sourcing models is commoditizing the competition. We've been doing that for years. And remember, who put open AI in the Azure box? We have talent density and a strong mission of AGI. We're going to push the boundaries of innovation. Look around. Who owns the data in the compute? Meta and I are going to keep releasing models, driving the price down to zero. I've seen this before. I remember when Facebook eliminated a competitor Instagram and spent a billion dollars doing it. We're not about ads or doing what you do. We're about transformative AI, AGI. We're playing the long game. Who's thinking five moves ahead? Look at Meta. A couple of years ago, they were hated. Now everybody's singing their praises. And the best part of this, when all those generative AI startups get to hire their talent, at a lower rate.",
      "platforms": {
        "tiktok": {
          "video_id": "7360298812205714730",
          "url": "https://www.tiktok.com/@rajistics/video/7360298812205714730",
          "view_count": 8048,
          "upload_date": "2024-04-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ba7f1e755c4e40c48b33f20016276eb6_1713703164~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=oKCJs2YwGjuBx%2BYrYJBJInRNcpY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I closely monitor technology trends in AI. Following huge developments at the end of 2022 and throughout 2023, the Generative AI space is now beginning to mature. You also have many vendors entering the market, and technological changes are becoming more incremental. The focus has shifted from specific platforms or packages to a broader emphasis on problem-solving. So don't chase the latest technology, figure out how to apply generative to actual problems. #rajistics ",
      "description": "I closely monitor technology trends in AI. Following huge developments at the end of 2022 and throughout 2023, the Generative AI space is now beginning to mature. You also have many vendors entering the market, and technological changes are becoming more incremental. The focus has shifted from specific platforms or packages to a broader emphasis on problem-solving. So don't chase the latest technology, figure out how to apply generative to actual problems. #rajistics ",
      "upload_date": "2024-05-04",
      "total_views": 8044,
      "max_views": 8044,
      "topics": [
        "apply",
        "generative",
        "like",
        "plateau",
        "problems",
        "technology"
      ],
      "search_text": "I closely monitor technology trends in AI. Following huge developments at the end of 2022 and throughout 2023, the Generative AI space is now beginning to mature. You also have many vendors entering the market, and technological changes are becoming more incremental. The focus has shifted from specific platforms or packages to a broader emphasis on problem-solving. So don't chase the latest technology, figure out how to apply generative to actual problems. #rajistics  apply generative like plateau problems technology It's May 2024 and it looks like we've hit a plateau for generative AI. Let's talk about what that means. Plateau? Is that because you live in a cornfield? I'm in the valley. So by plateau I mean that innovation has slowed. It's become very much predictable and incremental. You got to break this down for me. Keep it simple. In the last six months there hasn't been any fundamental advances in generative AI. If you had to build a generative AI project even though you hadn't read any blog posts, archive articles, or news articles you'd be just fine building a project. Kind of like how traditional ML hasn't changed, still use an XGBoost. The real value is shifting to understand how to actually build generative AI applications, how to scope out a problem, what the correct data is, what type of model you should be using. It sounds like I should stop worrying about which technology stack I'm using and more about just trying to solve some generative AI problems. Exactly and this shift will apply to job seekers too. So it's not about knowing the latest packages, instead it's about showing how you can apply skills for generative AI and build useful projects. So what kind of skills should I be developing? So it's about learning the fundamentals of large language models, like knowing how to prompt, knowing how to fine tune them, what are the limitations, how do you use them with and combine them with a vector search to do a rag type application. But this means I have to go talk to people with problems.",
      "platforms": {
        "tiktok": {
          "video_id": "7364935707032440107",
          "url": "https://www.tiktok.com/@rajistics/video/7364935707032440107",
          "view_count": 8044,
          "upload_date": "2024-05-04",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8b5a60f366034a14b7fac35c2e4c3418_1714782735~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=irtTbuiTTZl89X79KUSIKdh9WlU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6544,
      "title": "Language Models like ChatGPT can be modified by several methods including Prompting Instruction Fine-Tuning and Reinforcement Learning with Human Feedback. This year we will start seeing lots more varieties of large language chat models trained on different data. #datascience #machinelearning #largelanguagemodels #openai #chatgpt #promptengineering #instructionfinetuning #rlhf #reinforcementlearning #pretrain References: Conservatives Aim to Build a Chatbot of Their Own: https://www.nytimes.com/2023/03/22/business/media/ai-chatbots-right-wing-conservative.html ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge - https://arxiv.org/abs/2303.14070 Whose Opinions Do Language Models Reflect? https://arxiv.org/pdf/2303.17548.pdf Natural Language Processing with Deep Learning https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf",
      "description": "Language Models like ChatGPT can be modified by several methods including Prompting Instruction Fine-Tuning and Reinforcement Learning with Human Feedback. This year we will start seeing lots more varieties of large language chat models trained on different data. #datascience #machinelearning #largelanguagemodels #openai #chatgpt #promptengineering #instructionfinetuning #rlhf #reinforcementlearning #pretrain References: Conservatives Aim to Build a Chatbot of Their Own: https://www.nytimes.com/2023/03/22/business/media/ai-chatbots-right-wing-conservative.html ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge - https://arxiv.org/abs/2303.14070 Whose Opinions Do Language Models Reflect? https://arxiv.org/pdf/2303.17548.pdf Natural Language Processing with Deep Learning https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf",
      "upload_date": "2023-04-08",
      "total_views": 7674,
      "max_views": 7283,
      "topics": [
        "chatgpt",
        "datascience",
        "large",
        "largelanguagemodels",
        "like",
        "machinelearning",
        "many",
        "model",
        "modifying",
        "openai",
        "rlhf",
        "train",
        "ways"
      ],
      "search_text": "Language Models like ChatGPT can be modified by several methods including Prompting Instruction Fine-Tuning and Reinforcement Learning with Human Feedback. This year we will start seeing lots more varieties of large language chat models trained on different data. #datascience #machinelearning #largelanguagemodels #openai #chatgpt #promptengineering #instructionfinetuning #rlhf #reinforcementlearning #pretrain References: Conservatives Aim to Build a Chatbot of Their Own: https://www.nytimes.com/2023/03/22/business/media/ai-chatbots-right-wing-conservative.html ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge - https://arxiv.org/abs/2303.14070 Whose Opinions Do Language Models Reflect? https://arxiv.org/pdf/2303.17548.pdf Natural Language Processing with Deep Learning https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf chatgpt datascience large largelanguagemodels like machinelearning many model modifying openai rlhf train ways",
      "platforms": {
        "tiktok": {
          "video_id": "7219755291796114730",
          "url": "https://www.tiktok.com/@rajistics/video/7219755291796114730",
          "view_count": 7283,
          "upload_date": "2023-04-08",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CqyWxc8AYkc",
          "url": "https://www.instagram.com/reel/CqyWxc8AYkc",
          "view_count": 294,
          "upload_date": "2023-04-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "6LVPCKtRgl8",
          "url": "https://www.youtube.com/watch?v=6LVPCKtRgl8",
          "view_count": 97,
          "upload_date": "2023-04-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6210,
      "title": "Applying reinforcement learning to teaching AI math. This is based off a notebook using Group Relative Policy Optimization (GRPO) on a QWEN 0.5B model with GSM8K. You see a big boost in performance after a little bit of training with GRPO. I will do a full deep dive video with the notebook in the next week or so. You can get the notebook here: https://colab.research.google.com/drive/1BrM77YZ50EAzXZFmjGZYdgkCbc0OPrS-?usp=sharing",
      "description": "Applying reinforcement learning to teaching AI math. This is based off a notebook using Group Relative Policy Optimization (GRPO) on a QWEN 0.5B model with GSM8K. You see a big boost in performance after a little bit of training with GRPO. I will do a full deep dive video with the notebook in the next week or so. You can get the notebook here: https://colab.research.google.com/drive/1BrM77YZ50EAzXZFmjGZYdgkCbc0OPrS-?usp=sharing",
      "upload_date": "2025-02-11",
      "total_views": 7666,
      "max_views": 5436,
      "topics": [
        "deepseek",
        "examples",
        "get",
        "grpo",
        "math",
        "model",
        "notebook",
        "qwen",
        "reinforcement",
        "reward",
        "started",
        "using"
      ],
      "search_text": "Applying reinforcement learning to teaching AI math. This is based off a notebook using Group Relative Policy Optimization (GRPO) on a QWEN 0.5B model with GSM8K. You see a big boost in performance after a little bit of training with GRPO. I will do a full deep dive video with the notebook in the next week or so. You can get the notebook here: https://colab.research.google.com/drive/1BrM77YZ50EAzXZFmjGZYdgkCbc0OPrS-?usp=sharing deepseek examples get grpo math model notebook qwen reinforcement reward started using So I taught AI to like watermelon over poop. Well, I have a new AI, and this one uses reinforcement learning to learn math all by itself. How does it do that? Let me show you. We'll start by using the GRPO trainer from Huggingface. And here we give it two rewards. The first is a format reward. That's because I need the model to think in a structured way. You can think of it as making sure you've completely showed all your work when you were in school. Here's an example of where it's shown its work correctly and where it has it. And what I want to do is, if it does it correctly, it's going to get one point for that. The second reward is an accuracy reward. And since this is math, it's easy to tell if the answer is right or wrong. The model gets it right, it gets two points. Now I start giving my model lots of examples. We generate multiple responses for each example and score them. GRPO helps us to reward the model properly. And in this notebook, you can see how the model's math performance starts to improve as it sees more examples. And the cool part is, I'm not telling it how to solve the math problems, it's teaching itself. And this approach comes out of the DeepSeq R1 paper. And it's really helping us rethink how we can train AI.",
      "platforms": {
        "tiktok": {
          "video_id": "7470007608250780958",
          "url": "https://www.tiktok.com/@rajistics/video/7470007608250780958",
          "view_count": 5436,
          "upload_date": "2025-02-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4mZPIQojADOAAAfI4FIEQ1VAqFPjVCEfA3QfA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=7gon%2BNOyD0UK4bjiKcmPFHZRi%2FU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18041866643082419",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AjcvMN1kK-g",
          "url": "https://www.youtube.com/watch?v=AjcvMN1kK-g",
          "view_count": 2230,
          "upload_date": "2025-02-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6480,
      "title": "Meta released Llama Guard for content moderation. It looks to be effective and very adaptable. This is part of their Purple Llama project around trust for generative AI. #contentmoderation #largelanguagemodels #meta #rajistics #llamaguard Model: https://huggingface.co/meta-llama/LlamaGuard-7b Blog: https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/ https://huggingface.co/meta-llama/LlamaGuard-7b Prompts: https://twitter.com/_philschmid/status/1732817699862966654?s=20",
      "description": "Meta released Llama Guard for content moderation. It looks to be effective and very adaptable. This is part of their Purple Llama project around trust for generative AI. #contentmoderation #largelanguagemodels #meta #rajistics #llamaguard Model: https://huggingface.co/meta-llama/LlamaGuard-7b Blog: https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/ https://huggingface.co/meta-llama/LlamaGuard-7b Prompts: https://twitter.com/_philschmid/status/1732817699862966654?s=20",
      "upload_date": "2023-12-08",
      "total_views": 7660,
      "max_views": 4044,
      "topics": [
        "content",
        "contentmoderation",
        "guard",
        "largelanguagemodels",
        "llama",
        "llamaguard",
        "meta",
        "model"
      ],
      "search_text": "Meta released Llama Guard for content moderation. It looks to be effective and very adaptable. This is part of their Purple Llama project around trust for generative AI. #contentmoderation #largelanguagemodels #meta #rajistics #llamaguard Model: https://huggingface.co/meta-llama/LlamaGuard-7b Blog: https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/ https://huggingface.co/meta-llama/LlamaGuard-7b Prompts: https://twitter.com/_philschmid/status/1732817699862966654?s=20 content contentmoderation guard largelanguagemodels llama llamaguard meta model",
      "platforms": {
        "tiktok": {
          "video_id": "7310029824062442795",
          "url": "https://www.tiktok.com/@rajistics/video/7310029824062442795",
          "view_count": 4044,
          "upload_date": "2023-12-08",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0kt3nEA7xm",
          "url": "https://www.instagram.com/reel/C0kt3nEA7xm",
          "view_count": 2963,
          "upload_date": "2023-12-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "KIQxTLd6qik",
          "url": "https://youtube.com/shorts/KIQxTLd6qik",
          "view_count": 653,
          "upload_date": "2023-12-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6465,
      "title": "Spaces gives you great interactive demos of many popular sklearn examples. It's a great place to browse and even contribute back by add more. #datascience #machinelearning #sklearn #scikit #huggingface All the sklearn documentation spaces are here: https://huggingface.co/sklearn-docs Anomaly Detection: https://huggingface.co/spaces/sklearn-docs/anomaly-detection Visualizatin the Stock Market: https://huggingface.co/spaces/sklearn-docs/Visualizing_the_stock_market_structure Dimensionality Reduction: https://huggingface.co/spaces/sklearn-docs/MNIST-Dimensionality-Reduction Decision surfaces: https://huggingface.co/spaces/sklearn-docs/ensemble-trees-decision-surface Photo by üá∏üáÆ Janko Ferliƒç https://unsplash.com/photos/sfL_QOnmy00",
      "description": "Spaces gives you great interactive demos of many popular sklearn examples. It's a great place to browse and even contribute back by add more. #datascience #machinelearning #sklearn #scikit #huggingface All the sklearn documentation spaces are here: https://huggingface.co/sklearn-docs Anomaly Detection: https://huggingface.co/spaces/sklearn-docs/anomaly-detection Visualizatin the Stock Market: https://huggingface.co/spaces/sklearn-docs/Visualizing_the_stock_market_structure Dimensionality Reduction: https://huggingface.co/spaces/sklearn-docs/MNIST-Dimensionality-Reduction Decision surfaces: https://huggingface.co/spaces/sklearn-docs/ensemble-trees-decision-surface Photo by üá∏üáÆ Janko Ferliƒç https://unsplash.com/photos/sfL_QOnmy00",
      "upload_date": "2023-04-24",
      "total_views": 7645,
      "max_views": 5195,
      "topics": [
        "datascience",
        "docs",
        "documentation",
        "examples",
        "face",
        "hugging",
        "huggingface",
        "interactive",
        "machinelearning",
        "sklearn",
        "spaces"
      ],
      "search_text": "Spaces gives you great interactive demos of many popular sklearn examples. It's a great place to browse and even contribute back by add more. #datascience #machinelearning #sklearn #scikit #huggingface All the sklearn documentation spaces are here: https://huggingface.co/sklearn-docs Anomaly Detection: https://huggingface.co/spaces/sklearn-docs/anomaly-detection Visualizatin the Stock Market: https://huggingface.co/spaces/sklearn-docs/Visualizing_the_stock_market_structure Dimensionality Reduction: https://huggingface.co/spaces/sklearn-docs/MNIST-Dimensionality-Reduction Decision surfaces: https://huggingface.co/spaces/sklearn-docs/ensemble-trees-decision-surface Photo by üá∏üáÆ Janko Ferliƒç https://unsplash.com/photos/sfL_QOnmy00 datascience docs documentation examples face hugging huggingface interactive machinelearning sklearn spaces",
      "platforms": {
        "tiktok": {
          "video_id": "7225763428881763626",
          "url": "https://www.tiktok.com/@rajistics/video/7225763428881763626",
          "view_count": 5195,
          "upload_date": "2023-04-24",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Crb_gE6AFew",
          "url": "https://www.instagram.com/reel/Crb_gE6AFew",
          "view_count": 1526,
          "upload_date": "2023-04-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "r3NOuoD_uDM",
          "url": "https://www.youtube.com/watch?v=r3NOuoD_uDM",
          "view_count": 924,
          "upload_date": "2023-04-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6543,
      "title": "Models and datasets have specific definitions. Models consist of at least two licenses nowadays this has been an issue for LLaMA where the code for the model architecture and weights are differently licensed. Similarly for datasets the LAOIN dataset has faced criticism because it deflects responability by referring to itself as an index by researchers. #datascience #machinelearning #laoin #llama #copyright LLama license issue: https://github.com/facebookresearch/llama/pull/234 LAOIN copyright: https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead",
      "description": "Models and datasets have specific definitions. Models consist of at least two licenses nowadays this has been an issue for LLaMA where the code for the model architecture and weights are differently licensed. Similarly for datasets the LAOIN dataset has faced criticism because it deflects responability by referring to itself as an index by researchers. #datascience #machinelearning #laoin #llama #copyright LLama license issue: https://github.com/facebookresearch/llama/pull/234 LAOIN copyright: https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead",
      "upload_date": "2023-04-29",
      "total_views": 7626,
      "max_views": 3992,
      "topics": [
        "content",
        "copyright",
        "datascience",
        "dataset",
        "datasets",
        "defining",
        "laoin",
        "learning",
        "llama",
        "machine",
        "machinelearning",
        "models"
      ],
      "search_text": "Models and datasets have specific definitions. Models consist of at least two licenses nowadays this has been an issue for LLaMA where the code for the model architecture and weights are differently licensed. Similarly for datasets the LAOIN dataset has faced criticism because it deflects responability by referring to itself as an index by researchers. #datascience #machinelearning #laoin #llama #copyright LLama license issue: https://github.com/facebookresearch/llama/pull/234 LAOIN copyright: https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead content copyright datascience dataset datasets defining laoin learning llama machine machinelearning models",
      "platforms": {
        "tiktok": {
          "video_id": "7227522122120170794",
          "url": "https://www.tiktok.com/@rajistics/video/7227522122120170794",
          "view_count": 3992,
          "upload_date": "2023-04-29",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CroMLOEg5A6",
          "url": "https://www.instagram.com/reel/CroMLOEg5A6",
          "view_count": 3535,
          "upload_date": "2023-04-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "2oOUf0oc4nc",
          "url": "https://www.youtube.com/watch?v=2oOUf0oc4nc",
          "view_count": 99,
          "upload_date": "2023-05-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5980,
      "title": "The skit explains a dynamic pricing strategy that uses machine learning to adjust prices based on what customers are willing to pay, rather than actual costs. Data from demographics, psychographics, and mobile behavior are collected via the company’s app to personalize pricing. Multi-armed bandit experiments and real-time adjustments help optimize prices across different customer segments and conditions. This approach aims to maximize revenue but raises ethical concerns around fairness and data privacy.",
      "description": "The skit explains a dynamic pricing strategy that uses machine learning to adjust prices based on what customers are willing to pay, rather than actual costs. Data from demographics, psychographics, and mobile behavior are collected via the company’s app to personalize pricing. Multi-armed bandit experiments and real-time adjustments help optimize prices across different customer segments and conditions. This approach aims to maximize revenue but raises ethical concerns around fairness and data privacy.",
      "upload_date": "2025-07-19",
      "total_views": 7617,
      "max_views": 5229,
      "topics": [
        "advantages",
        "data",
        "deep",
        "learning",
        "mobile",
        "pay",
        "prices",
        "pricing",
        "understanding",
        "willing"
      ],
      "search_text": "The skit explains a dynamic pricing strategy that uses machine learning to adjust prices based on what customers are willing to pay, rather than actual costs. Data from demographics, psychographics, and mobile behavior are collected via the company’s app to personalize pricing. Multi-armed bandit experiments and real-time adjustments help optimize prices across different customer segments and conditions. This approach aims to maximize revenue but raises ethical concerns around fairness and data privacy. advantages data deep learning mobile pay prices pricing understanding willing I've decided that McDowell, we're going to move forward with pricing optimization. Bruno explained, My background is pricing insurance in Europe. We strive to charge the highest possible price that consumers would pay without switching to an alternative. Isn't that kind of shady? Shouldn't the price refer to the actual risk or cost of the product? But most US states have restricted it. But we are not in the insurance business and I believe this will increase shareholder value. So we're adjusting prices based on machine learning to what we think people are willing to pay? More like offering customers discounts or incentives. It gives the customers the feeling that they're getting a deal. Gives them that dope opinion rush. Based on my understanding, what we can do is on hot days we know people are going to be willing to pay more for ice cream. Or if we know somebody's getting paid on a certain day, we know that on those days they're probably paying willing to pay a little bit more for their normal meal. A lot of hot days coming. We'll use a broad range of data, including demographics, behavioral information, psychographics, along with machine learning algorithms. Behavioral activity for mobile? How do we even get that? This is why we're pushing people to use our mobile application. Once it's installed on their device, we can start pulling in all their mobile data. With this information, we'll be able to set some initial prices, but then we'll start running in targeted markets doing multi-banded experiments, trying to figure out what's the optimal price to do. We're even also going to take advantage of real time on the app to be able to change things on the fly. We have high expectations for this project, and I've approved opening up additional roles for the team. I'm loving it!",
      "platforms": {
        "tiktok": {
          "video_id": "7528783098683739423",
          "url": "https://www.tiktok.com/@rajistics/video/7528783098683739423",
          "view_count": 2388,
          "upload_date": "2025-07-19",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYeg31JAIGCIf3AoybGXqQgX9gHjfOjyzLEID6~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=p8tD7bnv05sr%2B4tuETbWeq1ILrk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18047653391543349",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "xU_bm9PhTAs",
          "url": "https://www.youtube.com/watch?v=xU_bm9PhTAs",
          "view_count": 5229,
          "upload_date": "2016-11-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6524,
      "title": "Breaking News: Executive Order on AI Quick video on the main issues there is a lot more in the Order. It is over a 100 pages. #executiveorderai #rajistics #openai #meta Resources: Exec Order: https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/ What the executive order means for openness in AI: https://www.aisnakeoil.com/p/what-the-executive-order-means-for",
      "description": "Breaking News: Executive Order on AI Quick video on the main issues there is a lot more in the Order. It is over a 100 pages. #executiveorderai #rajistics #openai #meta Resources: Exec Order: https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/ What the executive order means for openness in AI: https://www.aisnakeoil.com/p/what-the-executive-order-means-for",
      "upload_date": "2023-11-01",
      "total_views": 7591,
      "max_views": 4337,
      "topics": [
        "breaking",
        "executive",
        "executiveorderai",
        "means",
        "meta",
        "news",
        "openai",
        "order"
      ],
      "search_text": "Breaking News: Executive Order on AI Quick video on the main issues there is a lot more in the Order. It is over a 100 pages. #executiveorderai #rajistics #openai #meta Resources: Exec Order: https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/ What the executive order means for openness in AI: https://www.aisnakeoil.com/p/what-the-executive-order-means-for breaking executive executiveorderai means meta news openai order",
      "platforms": {
        "tiktok": {
          "video_id": "7296281892146482474",
          "url": "https://www.tiktok.com/@rajistics/video/7296281892146482474",
          "view_count": 4337,
          "upload_date": "2023-11-01",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CzFUclaPw5c",
          "url": "https://www.instagram.com/reel/CzFUclaPw5c",
          "view_count": 3043,
          "upload_date": "2023-11-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "PZOyjOhyKlg",
          "url": "https://www.youtube.com/watch?v=PZOyjOhyKlg",
          "view_count": 211,
          "upload_date": "2023-11-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6672,
      "title": "Based on:",
      "description": "Based on:",
      "upload_date": "2023-01-27",
      "total_views": 7567,
      "max_views": 7567,
      "topics": [
        "based"
      ],
      "search_text": "Based on: based",
      "platforms": {
        "youtube": {
          "video_id": "YKCtbIJC3kQ",
          "url": "https://www.youtube.com/watch?v=YKCtbIJC3kQ",
          "view_count": 7567,
          "upload_date": "2023-01-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6089,
      "title": "You know your transformer basics?  Let's go over Encoder, Encoder-Decoder, and Decoder only models. If you want to dig deeper into the transformers architecture, a great start place is: Understanding Encoder And Decoder LLMs at https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder",
      "description": "You know your transformer basics?  Let's go over Encoder, Encoder-Decoder, and Decoder only models. If you want to dig deeper into the transformers architecture, a great start place is: Understanding Encoder And Decoder LLMs at https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder",
      "upload_date": "2024-12-29",
      "total_views": 7563,
      "max_views": 4629,
      "topics": [
        "decoder",
        "embedding",
        "encoder",
        "encoders",
        "fundamentals",
        "models",
        "text",
        "transformer",
        "use"
      ],
      "search_text": "You know your transformer basics?  Let's go over Encoder, Encoder-Decoder, and Decoder only models. If you want to dig deeper into the transformers architecture, a great start place is: Understanding Encoder And Decoder LLMs at https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder decoder embedding encoder encoders fundamentals models text transformer use Here are the three configurations of Transformers, our modern AI factory that you need to know. Let's start with Encoder Only. Encoder Only models are like bookworms. They read and analyze all of the text thoroughly. And what do they do with all that analysis? They turn all that input data into a compact representation called an embedding. Once we have an embedding, we can use it for many tasks from creating a classification or using that embedding to find other similar embedding. That's cool, but no text generation? Nope, they're not built for that. If you want some text generated, let's look at the Encoder Decoder model. Now we're talking. The Encoder reads the input, processes it, and then hands it off to a decoder, which then generates the output. This sounds a lot more complicated. It is, but it works really well for complicated tasks like translation or summarization. Is there such a thing as just decoder only? Ah, yes, the decoders. These models excel at storytelling. We can use them for generating text. How do they work? These models generate output one token or one word at a time based on what it's already seen before. We call this autoregressive generation. So what do we use them for? The decoder models are really good at sequence prediction or being able to complete those sentences and do that text generation. But suppose I have a classification problem. Can I use them? They can, but they're going to be slower and require more compute than to do those tasks than an encoder only model. It's always trade-offs, but at least now I know the three major components for transform.",
      "platforms": {
        "tiktok": {
          "video_id": "7453926647478095134",
          "url": "https://www.tiktok.com/@rajistics/video/7453926647478095134",
          "view_count": 2934,
          "upload_date": "2024-12-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocAiNVzgAqIYirdCiNAB7EIBxAUKuArBxdfntz~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=V39EakBLwpYg94v72XdMKk7gW6Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18105279238457451",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AfL5xi8bHkM",
          "url": "https://www.youtube.com/watch?v=AfL5xi8bHkM",
          "view_count": 4629,
          "upload_date": "2024-12-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6456,
      "title": "Context length has grown in importance for large language models. A longer context length lets you pass more information to the model effectively giving it a larger working memory. While technically it's easy to get a model to work on a longer context length that doesn't translate into good performance. It is also essential to train the model how to use that context length. #largelanguagemodels #aiengineer # #contextlength #openai #anthropic #longchat Background by Sven Mieke: https://unsplash.com/photos/MsCgmHuirDo How Long Can Open-Source LLMs Truly Promise on Context Length? https://lmsys.org/blog/2023-06-29-longchat/ LongEval: https://github.com/DachengLi1/LongChat",
      "description": "Context length has grown in importance for large language models. A longer context length lets you pass more information to the model effectively giving it a larger working memory. While technically it's easy to get a model to work on a longer context length that doesn't translate into good performance. It is also essential to train the model how to use that context length. #largelanguagemodels #aiengineer # #contextlength #openai #anthropic #longchat Background by Sven Mieke: https://unsplash.com/photos/MsCgmHuirDo How Long Can Open-Source LLMs Truly Promise on Context Length? https://lmsys.org/blog/2023-06-29-longchat/ LongEval: https://github.com/DachengLi1/LongChat",
      "upload_date": "2023-07-02",
      "total_views": 7549,
      "max_views": 4217,
      "topics": [
        "aiengineer",
        "anthropic",
        "context",
        "contextlength",
        "largelanguagemodels",
        "length",
        "lengths",
        "llms",
        "longchat",
        "openai"
      ],
      "search_text": "Context length has grown in importance for large language models. A longer context length lets you pass more information to the model effectively giving it a larger working memory. While technically it's easy to get a model to work on a longer context length that doesn't translate into good performance. It is also essential to train the model how to use that context length. #largelanguagemodels #aiengineer # #contextlength #openai #anthropic #longchat Background by Sven Mieke: https://unsplash.com/photos/MsCgmHuirDo How Long Can Open-Source LLMs Truly Promise on Context Length? https://lmsys.org/blog/2023-06-29-longchat/ LongEval: https://github.com/DachengLi1/LongChat aiengineer anthropic context contextlength largelanguagemodels length lengths llms longchat openai",
      "platforms": {
        "tiktok": {
          "video_id": "7251332310317010218",
          "url": "https://www.tiktok.com/@rajistics/video/7251332310317010218",
          "view_count": 4217,
          "upload_date": "2023-07-02",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CuNaYctAWuU",
          "url": "https://www.instagram.com/reel/CuNaYctAWuU",
          "view_count": 1977,
          "upload_date": "2023-07-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "mOsQop0g5Bs",
          "url": "https://www.youtube.com/watch?v=mOsQop0g5Bs",
          "view_count": 1355,
          "upload_date": "2023-07-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Entropy can be a useful measure in machine learning. Entropy and information gain is used in building decision trees. I have also seen entropy used in feature engineering. Here is a short conceptual understanding for entropy. This video is based on the excellent blog post on entropy (that also provides the math)   Entropy: How Decision Trees Make Decisions by Sam T: https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8 Background Photo: Hans-Peter Gauster https://unsplash.com/photos/3y1zF4hIPCg  #datascience #machinelearning #featureengineering #informationgain #entropy #decisiontree ",
      "description": "Entropy can be a useful measure in machine learning. Entropy and information gain is used in building decision trees. I have also seen entropy used in feature engineering. Here is a short conceptual understanding for entropy. This video is based on the excellent blog post on entropy (that also provides the math)   Entropy: How Decision Trees Make Decisions by Sam T: https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8 Background Photo: Hans-Peter Gauster https://unsplash.com/photos/3y1zF4hIPCg  #datascience #machinelearning #featureengineering #informationgain #entropy #decisiontree ",
      "upload_date": "2024-04-27",
      "total_views": 7477,
      "max_views": 7477,
      "topics": [
        "credit",
        "entropy",
        "know",
        "liability",
        "look",
        "rating"
      ],
      "search_text": "Entropy can be a useful measure in machine learning. Entropy and information gain is used in building decision trees. I have also seen entropy used in feature engineering. Here is a short conceptual understanding for entropy. This video is based on the excellent blog post on entropy (that also provides the math)   Entropy: How Decision Trees Make Decisions by Sam T: https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8 Background Photo: Hans-Peter Gauster https://unsplash.com/photos/3y1zF4hIPCg  #datascience #machinelearning #featureengineering #informationgain #entropy #decisiontree  credit entropy know liability look rating Which of these rooms is messier? We can quantify that with a measurement called entropy. Let me tell you how that helps us do machine learning. Entropy in machine learning is a measure of disorder or the uncertainty that we have in a feature. So if we look at this plot, you'll notice at the bottoms of the plot where we know that it's likely either minuses or plus, the entropy is really low. But as we move up higher, what we see is there's a more equal distribution between minus and plus. And if there's a 50-50 distribution, the entropy is very high because it's hard to figure out what the order is in that system. So take a look at this table. Now if we're trying to understand liability and which people are either normal or high liability, we take a look at this and we see there's seven in each category. So that ends up giving us an entropy of one, the highest there could be because we don't have any information that helps us figure out what category it is. In contrast, take a look at the credit rating. For example, with poor, we know they're always in one credit rating. That's a very low entropy. Take a look, which has a higher entropy between excellent and good. From a machine learning standpoint, this becomes valuable because we can use credit rating as a way to understand liability. If we know the credit rating for somebody, we have a little bit better chance of understanding the liability simply because, for example, if we know that their credit rating is poor, well, we know the liability is probably very likely probable to be high. And so here you can see how one variable helps us understand the other and we can use the concepts of entropy and information gain to actually calculate this.",
      "platforms": {
        "tiktok": {
          "video_id": "7362453106124131626",
          "url": "https://www.tiktok.com/@rajistics/video/7362453106124131626",
          "view_count": 7477,
          "upload_date": "2024-04-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4df721bede9d44949bc551b09b1c667c_1714204791~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=lAEz3gL7Y2Irw4Fxu3CQ09XBz8o%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Audio spectrogram transformer shows how widely we can use #machinelearning #datascience #mlaudio #deeplearning ",
      "description": "Audio spectrogram transformer shows how widely we can use #machinelearning #datascience #mlaudio #deeplearning ",
      "upload_date": "2022-12-19",
      "total_views": 7438,
      "max_views": 7438,
      "topics": [
        "datascience",
        "deeplearning",
        "machine",
        "machinelearning",
        "mlaudio",
        "use"
      ],
      "search_text": "Audio spectrogram transformer shows how widely we can use #machinelearning #datascience #mlaudio #deeplearning  datascience deeplearning machine machinelearning mlaudio use You're listening to me right now, but if you want to, you wanted to use machine learning to classify the audio, you'd use a vision model. Let me explain. Sound can be represented in two dimensions as a spectrograph or as this Gramian angular field. Once we have this representation, which if you notice has lots of features, we can use traditional machine learning techniques. A machine learning algorithm that works well when you have lots of features and they have relationships between them has been convolutional neural networks. The same things we use for images. Last year, researchers showed they could use the same type of architecture of transformers, a vision transformer for taking a spectrograph, breaking it in the little patches and analyzing it. So there you go. You can analyze sound using a vision model.",
      "platforms": {
        "tiktok": {
          "video_id": "7178992227627306286",
          "url": "https://www.tiktok.com/@rajistics/video/7178992227627306286",
          "view_count": 7438,
          "upload_date": "2022-12-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a7f74f11406944319f225bfa75b9e8d5_1671489394~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=zMKD36%2FBto8eih6ClFVL3a9SSII%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6157,
      "title": "Prompt engineering helped optimize model behavior when LLMs were less capable. But as models have improved, gains from prompt tweaks have diminished. Today, the performance delta comes from context engineering: structuring what goes into the context window to ensure the model has the right information to reason and act. Philipp Schmid, The New Skill in AI is Not Prompting, It’s Context Engineering https://www.philschmid.de/posts/context-engineering LangChain, Context Engineering for Agents https://blog.langchain.com/context-engineering-for-agents Andrej Karpathy, widely quoted: “Context engineering is the delicate art and science of filling the context window with just the right information for the next step.”",
      "description": "Prompt engineering helped optimize model behavior when LLMs were less capable. But as models have improved, gains from prompt tweaks have diminished. Today, the performance delta comes from context engineering: structuring what goes into the context window to ensure the model has the right information to reason and act. Philipp Schmid, The New Skill in AI is Not Prompting, It’s Context Engineering https://www.philschmid.de/posts/context-engineering LangChain, Context Engineering for Agents https://blog.langchain.com/context-engineering-for-agents Andrej Karpathy, widely quoted: “Context engineering is the delicate art and science of filling the context window with just the right information for the next step.”",
      "upload_date": "2025-07-04",
      "total_views": 7407,
      "max_views": 3866,
      "topics": [
        "context",
        "engineering",
        "explained",
        "get",
        "going",
        "gonna",
        "like",
        "model",
        "twenty"
      ],
      "search_text": "Prompt engineering helped optimize model behavior when LLMs were less capable. But as models have improved, gains from prompt tweaks have diminished. Today, the performance delta comes from context engineering: structuring what goes into the context window to ensure the model has the right information to reason and act. Philipp Schmid, The New Skill in AI is Not Prompting, It’s Context Engineering https://www.philschmid.de/posts/context-engineering LangChain, Context Engineering for Agents https://blog.langchain.com/context-engineering-for-agents Andrej Karpathy, widely quoted: “Context engineering is the delicate art and science of filling the context window with just the right information for the next step.” context engineering explained get going gonna like model twenty Prompt Engineering was a hot skill back in 2023. But in 2025, it's going to be context engineering, because that's what's going to make your AI feel smart. So let me explain what it is and how you can take advantage of it. Back in 2023, the models weren't that great. So Prompt Engineering mattered a lot. You'd spend time adding examples, rephrasing your prompts, all to get a better output. Because now we're working with AI assistants and wanting very useful outputs. So take this example of, hey, we want to check in with our AI assistant. Your average person is just going to pass this into a model just by itself, and you're going to get something kind of vague, robotic, really average sounding like, hey, thanks for the message. What do you need tomorrow? It's good. It's correct. But it's not magical. It's not what you could make. So to get to the next level, think about all the information that would be useful. Like, hey, would it be useful if I knew my calendar, my past emails that I had, who's on my contact list, what tools the model can use. And if you can combine all that information efficiently, you can get a much better response. In this case, something like this, that's going to be much more personal, useful and smart. That is context engineering. We didn't rely on the model to get better. What we did was we focused on the input. And that involves looking at all these things like prompts and memory and documents and tool definitions and formats. This is what's going to get you from like, okay, to wow.",
      "platforms": {
        "tiktok": {
          "video_id": "7523305314062486814",
          "url": "https://www.tiktok.com/@rajistics/video/7523305314062486814",
          "view_count": 3866,
          "upload_date": "2025-07-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0WLQAHnEqCxfQQQz0X8ICAhuTfvseQnjwMAIx~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=teOEWC4wKmJhgoUnnXwJFN53vC0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18056354999458116",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Bmre1F1O2g8",
          "url": "https://www.youtube.com/watch?v=Bmre1F1O2g8",
          "view_count": 3541,
          "upload_date": "2025-07-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6052,
      "title": "Let's talk about using H3 for geospatial analytics",
      "description": "Let's talk about using H3 for geospatial analytics",
      "upload_date": "2025-02-25",
      "total_views": 7394,
      "max_views": 4013,
      "topics": [
        "analytics",
        "basketball",
        "bringing",
        "geospatial",
        "hexes",
        "let",
        "moving",
        "pose",
        "spatial",
        "sticks",
        "talk",
        "uber",
        "use",
        "using"
      ],
      "search_text": "Let's talk about using H3 for geospatial analytics analytics basketball bringing geospatial hexes let moving pose spatial sticks talk uber use using I've been hexed by Uber. Should you use public transportation? Nah, I moved over to using H3 for spatial analytics. What's wrong with creating a simple grid based on latitude and longitude? Come on, take a look at a globe. Those types of grids have severe distortion, especially as we get near the poles. Also makes doing spatial analytics very hard. We have withdrawn the offer to purchase Greenland. New information has shown the map to be deceiving. A few years ago, Uber developed H3, which is a hexagonal gridding system across the entire globe. You could even do it at different scales and resolutions. So why use hexes? So hexes allow us to do things like calculate a radius very easily. If something is moving through an area, a hex shape allows us to minimize the error. And finally, when we're calculating distances to other neighborhood hexes, hexes give us a nice uniform way of doing it versus other ways of gridding. Good stuff. How much is Uber charging? So Google open sourced it so you'll see lots of analytical packages as well as databases that support H3. Go search pricing. I'm in.",
      "platforms": {
        "tiktok": {
          "video_id": "7475451281289514271",
          "url": "https://www.tiktok.com/@rajistics/video/7475451281289514271",
          "view_count": 3381,
          "upload_date": "2025-02-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ooe8xfAf8AQjCbVeFMMeRxKkRQgoXiEnSEnQGb~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=kyj%2BqJcpmx%2BtqRdVFZ6HbN5LTeU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18056713049012604",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "H6Zfe5JPLY8",
          "url": "https://www.youtube.com/watch?v=H6Zfe5JPLY8",
          "view_count": 4013,
          "upload_date": "2017-05-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Lets talk about why enterprises are considering alternatives to chatGPT by looking to open source. An open source strategy can affect lots of areas outside data science including data goverance, IT, legal, and accounting. Let me know what else i missed.  #datascience #machinelearning #opensource #largelanguagemodels #chatgpt #openai ",
      "description": "Lets talk about why enterprises are considering alternatives to chatGPT by looking to open source. An open source strategy can affect lots of areas outside data science including data goverance, IT, legal, and accounting. Let me know what else i missed.  #datascience #machinelearning #opensource #largelanguagemodels #chatgpt #openai ",
      "upload_date": "2023-03-18",
      "total_views": 7393,
      "max_views": 7393,
      "topics": [
        "api",
        "chatgpt",
        "data",
        "know",
        "open",
        "openai"
      ],
      "search_text": "Lets talk about why enterprises are considering alternatives to chatGPT by looking to open source. An open source strategy can affect lots of areas outside data science including data goverance, IT, legal, and accounting. Let me know what else i missed.  #datascience #machinelearning #opensource #largelanguagemodels #chatgpt #openai  api chatgpt data know open openai ChatGPT looks really good. Why would we care about an open source alternative? Open source means we could run a model in our own environment. With an API, we're sending our customers data externally. Don't worry, we won't do anything with your data. Did you use the data that you collected from ChatGPT for training your next set of models? But you're different. You're special. I wouldn't do anything bad to you. That does seem like a risk. Our data is pretty special. My team likes to pick the best model for the job. When I'm using an API, I don't know what model I'm using. I don't know what data it's been trained at. I don't know how it's the architecture of it. I can't explain any of the results. I can't modify or update it at all. Please, if you could do better, we wouldn't be here. Yeah, those AI teams are way over height. Fancy degrees with little results. Tell me about it. It took me years to get my team to stop working on video games. But now, we've got it figured out with napkins. We were responsible for maintaining all of these systems. With an API, we don't know what version it is. And so if the version updates, which OpenAI has done in the past, it can break existing workflows. Additionally, with an API, we can't try to use this with different types of infrastructure and systems. We can't port it, for example. You don't need them. We've got an API. And with an API, who cares about the version? It's like your phone. Do you know what version your phone is? And at the end of the day, all you need is an API and the ability to use an appkin. I teach so hard to please. You have no idea how many times I have to log into some app with a crazy password. What security like for you? Security is way overhyped. It's really about talent, density, moving fast and breaking a lot of things. We hand out root privileges when we onboard. Using a black box for sending our data over, as well as using this black box to generate data, could affect existing legal arrangements we have with third parties that provide us data, as well as open us up for legal liability for some of that content that's being created. Both Stability AI and Microsoft are currently being sued because their generative technologies, both captured copywritten data and then generated that out again. We can have that same risk by using OpenAI. Blah, blah, blah. We got lawyers too. Lawyers. I can happily live without red lines. Seriously, how come no one considers their emissions? OpenAI is likely to become a large cost based on our past habits, where we've used consumption-based pricing that's been available throughout the organization to many groups. Look at data bricks, look at snowflake. Tell me more. From our examination, OpenAI is currently selling at cost. They're doing this to limit the amount of competition and hook organizations onto their technology. We call this the McKinsey Opioid Strategy. Look, all we want is what's best for mankind. I'm setting an open source first policy through the enterprise for the importance of our data governance and the flexibility of our IT infrastructure. We want to use the best tool for the job.",
      "platforms": {
        "tiktok": {
          "video_id": "7211922417512402222",
          "url": "https://www.tiktok.com/@rajistics/video/7211922417512402222",
          "view_count": 7393,
          "upload_date": "2023-03-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2c3f9be3036b498ab8c0f7ba80f98086_1679156566~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=6n1RmY2Yp2JI10kypGCkChEK4uI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I have a lot more tea #datarobot #corporategreed #datascience #codetok #techtok",
      "description": "I have a lot more tea #datarobot #corporategreed #datascience #codetok #techtok",
      "upload_date": "2022-06-22",
      "total_views": 7356,
      "max_views": 7356,
      "topics": [
        "able",
        "codetok",
        "company",
        "corporategreed",
        "datarobot",
        "datascience"
      ],
      "search_text": "I have a lot more tea #datarobot #corporategreed #datascience #codetok #techtok able codetok company corporategreed datarobot datascience Repugnant behavior. That's what I said about the C-level executives and my old company and what the Boston Globe quoted. Here's the story. I worked for four and a half years at an AI startup called Data Robot. Busted my butt. Over time, we grew that company. The value of that company grew over 10 times. Number of customers, revenue, all that stuff. Since this company was a startup, it's not like I could sell stocks. I was accumulating options and getting them and hoping someday I'd be able to cash them after we IPO'd. This week, I found out the C-level executives at Data Robot were able to sell their stock for millions, I think a total of $35 million, while I wasn't able to sell anything. And they sold this stock while they were hyping up how big the company was and how it was going to reach these unbelievable growth targets. They were able to sell their stocks at a super high while none of us else could. The CEO profited $20 million. While none of us, poor working staffs, were able to get anything yet for our options and hopefully someday we'll be able to IPO. But in the meantime, these C-level executives also have missed all their growth targets as well. So the company isn't doing nearly as well as when I was there. So yes, this is another old-fashioned story of corporate greed and people being able to take advantage of their position. Man, like every victim you see, I'm thinking, I didn't think it would happen to my company, but it did.",
      "platforms": {
        "tiktok": {
          "video_id": "7112102942655925550",
          "url": "https://www.tiktok.com/@rajistics/video/7112102942655925550",
          "view_count": 7356,
          "upload_date": "2022-06-22",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7995e3b19f95404b81225fefaccd1b4d_1655915512~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=NfHup5X3vweS0d2ASZuPIaCjeaI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6270,
      "title": "Anscombe's quartet‚Äù and the ‚Äúdatasaurus dozen‚Äù remind us of the importance of visualizing data.",
      "description": "Anscombe's quartet‚Äù and the ‚Äúdatasaurus dozen‚Äù remind us of the importance of visualizing data.",
      "upload_date": "2024-01-14",
      "total_views": 7344,
      "max_views": 6880,
      "topics": [
        "anscombe",
        "datasaurus",
        "dozen",
        "importance",
        "quartet",
        "remind"
      ],
      "search_text": "Anscombe's quartet‚Äù and the ‚Äúdatasaurus dozen‚Äù remind us of the importance of visualizing data. anscombe datasaurus dozen importance quartet remind",
      "platforms": {
        "tiktok": {
          "video_id": "7323978514930191662",
          "url": "https://www.tiktok.com/@rajistics/video/7323978514930191662",
          "view_count": 6880,
          "upload_date": "2024-01-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17993362706303047",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "jA7t1LIgTBU",
          "url": "https://youtube.com/shorts/jA7t1LIgTBU",
          "view_count": 464,
          "upload_date": "2024-01-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Automatic Speech recognition in 3 lines of code using wav2vec2 in transformers #datascience #machinelearning #huggingface #automaticspeechrecognition #asr ",
      "description": "Automatic Speech recognition in 3 lines of code using wav2vec2 in transformers #datascience #machinelearning #huggingface #automaticspeechrecognition #asr ",
      "upload_date": "2022-11-17",
      "total_views": 7335,
      "max_views": 7335,
      "topics": [
        "asr",
        "automatic",
        "automaticspeechrecognition",
        "datascience",
        "huggingface",
        "machinelearning"
      ],
      "search_text": "Automatic Speech recognition in 3 lines of code using wav2vec2 in transformers #datascience #machinelearning #huggingface #automaticspeechrecognition #asr  asr automatic automaticspeechrecognition datascience huggingface machinelearning Let me show you how to do automatic speech recognition in just a few lines of code. I had to do this for a customer earlier today. Thought I'd share it with all of you. Start by grabbing the pipeline out of Transformers. And then we're gonna use Facebook's Wave2Vec. It's a great kind of starting point for doing this type of automatic speech recognition. Whisper is another popular model. It'll be supported soon inside Transformers. It's not there yet, a couple of weeks away. But in the meantime, start with this. And then you can see it's just one line of command to go ahead and do the transcription. Sit back, wait, and it'll happen. I've seen people apply this to lots of different things from financial earning calls to YouTube videos, so go have some fun.",
      "platforms": {
        "tiktok": {
          "video_id": "7166822378205482283",
          "url": "https://www.tiktok.com/@rajistics/video/7166822378205482283",
          "view_count": 7335,
          "upload_date": "2022-11-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9a996dcbb5d44407ad53226301208a84_1668655879~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=fYAjkHqSs3aU9vHE2eGxf5NmLdo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6637,
      "title": "If an AI story looks too good approach it critically. This week there are three examples with GPT-4 gzip and Llama where AI influencers jumped on too quickly. Links to the pushback on these stories: GPT-4 performance: https://www.aisnakeoil.com/p/is-gpt-4-getting-worse-over-time Gzip Leakage: https://github.com/bazingagin/npc_gzip/issues/13 Gzip Top 2: https://kenschutte.com/gzip-knn-paper/ Llama leakage: https://www.reddit.com/r/LocalLLaMA/comments/1548cuw/comment/jspgasv/?utm_source=share&utm_medium=web2x&context=3 #datascience #machinelearning #taylorswift #gpt4 #llama #gzip #targetleakage",
      "description": "If an AI story looks too good approach it critically. This week there are three examples with GPT-4 gzip and Llama where AI influencers jumped on too quickly. Links to the pushback on these stories: GPT-4 performance: https://www.aisnakeoil.com/p/is-gpt-4-getting-worse-over-time Gzip Leakage: https://github.com/bazingagin/npc_gzip/issues/13 Gzip Top 2: https://kenschutte.com/gzip-knn-paper/ Llama leakage: https://www.reddit.com/r/LocalLLaMA/comments/1548cuw/comment/jspgasv/?utm_source=share&utm_medium=web2x&context=3 #datascience #machinelearning #taylorswift #gpt4 #llama #gzip #targetleakage",
      "upload_date": "2023-07-20",
      "total_views": 7306,
      "max_views": 6870,
      "topics": [
        "datascience",
        "gpt4",
        "gzip",
        "llama",
        "machinelearning",
        "taylorswift"
      ],
      "search_text": "If an AI story looks too good approach it critically. This week there are three examples with GPT-4 gzip and Llama where AI influencers jumped on too quickly. Links to the pushback on these stories: GPT-4 performance: https://www.aisnakeoil.com/p/is-gpt-4-getting-worse-over-time Gzip Leakage: https://github.com/bazingagin/npc_gzip/issues/13 Gzip Top 2: https://kenschutte.com/gzip-knn-paper/ Llama leakage: https://www.reddit.com/r/LocalLLaMA/comments/1548cuw/comment/jspgasv/?utm_source=share&utm_medium=web2x&context=3 #datascience #machinelearning #taylorswift #gpt4 #llama #gzip #targetleakage datascience gpt4 gzip llama machinelearning taylorswift",
      "platforms": {
        "tiktok": {
          "video_id": "7258039147372350763",
          "url": "https://www.tiktok.com/@rajistics/video/7258039147372350763",
          "view_count": 6870,
          "upload_date": "2023-07-20",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cu79VcJgsEH",
          "url": "https://www.instagram.com/reel/Cu79VcJgsEH",
          "view_count": 436,
          "upload_date": "2023-07-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Word as Image - great use of generative AI models like stable diffusion to create fonts. Check out the paper at wordasimage.github.io #datascience #machinelearning #stablediffusion #generativeai #fonts ",
      "description": "Word as Image - great use of generative AI models like stable diffusion to create fonts. Check out the paper at wordasimage.github.io #datascience #machinelearning #stablediffusion #generativeai #fonts ",
      "upload_date": "2023-03-07",
      "total_views": 7294,
      "max_views": 7294,
      "topics": [
        "datascience",
        "fonts",
        "generativeai",
        "gonna",
        "machinelearning",
        "stablediffusion"
      ],
      "search_text": "Word as Image - great use of generative AI models like stable diffusion to create fonts. Check out the paper at wordasimage.github.io #datascience #machinelearning #stablediffusion #generativeai #fonts  datascience fonts generativeai gonna machinelearning stablediffusion how we see words is changing today. Take a look at all these cool examples. We're not just gonna be limited to a handful of fonts, but there's gonna be a lot more creativity now. The way this works is we take the letter, but then we also combine it with the semantic meaning of the word. And we use that to nudge or shape that letter towards that semantic meaning. I've shared the link for the paper, but the authors are also gonna make a hugging face demo available. So give them about a week or so.",
      "platforms": {
        "tiktok": {
          "video_id": "7207889934718569774",
          "url": "https://www.tiktok.com/@rajistics/video/7207889934718569774",
          "view_count": 7294,
          "upload_date": "2023-03-07",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9ae423aa520f4b848241a2db18c5efee_1678217679~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=ubHI3nRzo8ztj%2FTpbAK8gfsYgwA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6129,
      "title": "Retrieval Augmented approaches are a great way to improve your LLMs. Deepset shown in this video provides a set of tools but there are many others out there like Llama-Index that offer similar functionality. This is one of the most popular use cases with LLMs. #datascience #machinelearning #largelanguagemodels #deepset #llamaindex #retrievalaugmentedmodel",
      "description": "Retrieval Augmented approaches are a great way to improve your LLMs. Deepset shown in this video provides a set of tools but there are many others out there like Llama-Index that offer similar functionality. This is one of the most popular use cases with LLMs. #datascience #machinelearning #largelanguagemodels #deepset #llamaindex #retrievalaugmentedmodel",
      "upload_date": "2023-04-04",
      "total_views": 7270,
      "max_views": 5234,
      "topics": [
        "augmented",
        "chatgpt",
        "datascience",
        "deepset",
        "language",
        "large",
        "largelanguagemodels",
        "llamaindex",
        "machinelearning",
        "models",
        "retrieval",
        "retrievalaugmented",
        "retrievalaugmentedmodel"
      ],
      "search_text": "Retrieval Augmented approaches are a great way to improve your LLMs. Deepset shown in this video provides a set of tools but there are many others out there like Llama-Index that offer similar functionality. This is one of the most popular use cases with LLMs. #datascience #machinelearning #largelanguagemodels #deepset #llamaindex #retrievalaugmentedmodel augmented chatgpt datascience deepset language large largelanguagemodels llamaindex machinelearning models retrieval retrievalaugmented retrievalaugmentedmodel",
      "platforms": {
        "tiktok": {
          "video_id": "7218330019293089067",
          "url": "https://www.tiktok.com/@rajistics/video/7218330019293089067",
          "view_count": 5234,
          "upload_date": "2023-04-04",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18032663701476226",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Cm_2GKRsc5s",
          "url": "https://www.youtube.com/watch?v=Cm_2GKRsc5s",
          "view_count": 2036,
          "upload_date": "2023-04-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6486,
      "title": "MiniGPT-4 brings us a multimodal model! It consists of a vision encoder with a pretrained ViT and and an advanced Vicuna large language model. This gives us the ability to do things like ask questions about a photo. #datascience #machinelearning #minigpt4 #gpt4 #gpt-4 #openai #multimodal #flamingo Website: https://minigpt-4.github.io/ Paper: https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf Code: https://github.com/Vision-CAIR/MiniGPT-4 Background Pic from eugenia loli : https://www.pinterest.com/pin/457256168397962646/",
      "description": "MiniGPT-4 brings us a multimodal model! It consists of a vision encoder with a pretrained ViT and and an advanced Vicuna large language model. This gives us the ability to do things like ask questions about a photo. #datascience #machinelearning #minigpt4 #gpt4 #gpt-4 #openai #multimodal #flamingo Website: https://minigpt-4.github.io/ Paper: https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf Code: https://github.com/Vision-CAIR/MiniGPT-4 Background Pic from eugenia loli : https://www.pinterest.com/pin/457256168397962646/",
      "upload_date": "2023-04-17",
      "total_views": 7241,
      "max_views": 4881,
      "topics": [
        "datascience",
        "gpt",
        "gpt4",
        "handling",
        "images",
        "machinelearning",
        "minigpt",
        "minigpt4",
        "model",
        "multimodal",
        "text",
        "vision"
      ],
      "search_text": "MiniGPT-4 brings us a multimodal model! It consists of a vision encoder with a pretrained ViT and and an advanced Vicuna large language model. This gives us the ability to do things like ask questions about a photo. #datascience #machinelearning #minigpt4 #gpt4 #gpt-4 #openai #multimodal #flamingo Website: https://minigpt-4.github.io/ Paper: https://github.com/Vision-CAIR/MiniGPT-4/blob/main/MiniGPT_4.pdf Code: https://github.com/Vision-CAIR/MiniGPT-4 Background Pic from eugenia loli : https://www.pinterest.com/pin/457256168397962646/ datascience gpt gpt4 handling images machinelearning minigpt minigpt4 model multimodal text vision",
      "platforms": {
        "tiktok": {
          "video_id": "7223156196688055595",
          "url": "https://www.tiktok.com/@rajistics/video/7223156196688055595",
          "view_count": 4881,
          "upload_date": "2023-04-17",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CrJ6Eu3A6JG",
          "url": "https://www.instagram.com/reel/CrJ6Eu3A6JG",
          "view_count": 1754,
          "upload_date": "2023-04-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ernokL-c6N8",
          "url": "https://www.youtube.com/watch?v=ernokL-c6N8",
          "view_count": 606,
          "upload_date": "2023-04-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6127,
      "title": "Target leakage in the CrowdAI dataset. Target leakage is a very common problem and everyone should understand it. I have seen even the smartest people and best teams have issues with data or target leakage. These include Harvard Google Fast.AI Andrew Ng and the SARCOS dataset used by hundreds. #datascience #machinelearning #targetleakage #dataleakage #crowdai #fastai #sarcos Efficient Deduplication and Leakage Detection in Large Scale Image Datasets with a focus on the CrowdAI Mapping Challenge Dataset - https://arxiv.org/abs/2304.02296# Running Code and Failing Models by Rajiv - https://www.datarobot.com/blog/running-code-and-failing-models/ Stand Up for Best Practices: Misuse of Deep Learning in Nature‚Äôs Earthquake Aftershock Paper https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8 Reddit: https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/ Older video: https://www.youtube.com/watch?v=NaySLPTCgDM",
      "description": "Target leakage in the CrowdAI dataset. Target leakage is a very common problem and everyone should understand it. I have seen even the smartest people and best teams have issues with data or target leakage. These include Harvard Google Fast.AI Andrew Ng and the SARCOS dataset used by hundreds. #datascience #machinelearning #targetleakage #dataleakage #crowdai #fastai #sarcos Efficient Deduplication and Leakage Detection in Large Scale Image Datasets with a focus on the CrowdAI Mapping Challenge Dataset - https://arxiv.org/abs/2304.02296# Running Code and Failing Models by Rajiv - https://www.datarobot.com/blog/running-code-and-failing-models/ Stand Up for Best Practices: Misuse of Deep Learning in Nature‚Äôs Earthquake Aftershock Paper https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8 Reddit: https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/ Older video: https://www.youtube.com/watch?v=NaySLPTCgDM",
      "upload_date": "2023-04-10",
      "total_views": 7231,
      "max_views": 6272,
      "topics": [
        "crowdai",
        "data",
        "dataleakage",
        "datascience",
        "leakage",
        "machinelearning",
        "sarcos",
        "target",
        "targetleakage"
      ],
      "search_text": "Target leakage in the CrowdAI dataset. Target leakage is a very common problem and everyone should understand it. I have seen even the smartest people and best teams have issues with data or target leakage. These include Harvard Google Fast.AI Andrew Ng and the SARCOS dataset used by hundreds. #datascience #machinelearning #targetleakage #dataleakage #crowdai #fastai #sarcos Efficient Deduplication and Leakage Detection in Large Scale Image Datasets with a focus on the CrowdAI Mapping Challenge Dataset - https://arxiv.org/abs/2304.02296# Running Code and Failing Models by Rajiv - https://www.datarobot.com/blog/running-code-and-failing-models/ Stand Up for Best Practices: Misuse of Deep Learning in Nature‚Äôs Earthquake Aftershock Paper https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8 Reddit: https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/ Older video: https://www.youtube.com/watch?v=NaySLPTCgDM crowdai data dataleakage datascience leakage machinelearning sarcos target targetleakage",
      "platforms": {
        "tiktok": {
          "video_id": "7220546161076145450",
          "url": "https://www.tiktok.com/@rajistics/video/7220546161076145450",
          "view_count": 6272,
          "upload_date": "2023-04-10",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17995710391829509",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "BPZnEFUbxao",
          "url": "https://www.youtube.com/watch?v=BPZnEFUbxao",
          "view_count": 959,
          "upload_date": "2023-04-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6526,
      "title": "Trying to talk about AGI in a reasonable manner. There needs to be more hype and more rigor in talking about AGI. The Deepmind paper provides a good discussion of how to think about AGI and where we are now. #deepmind #agi #machinelearning #rajistics Levels of AGI: Operationalizing Progress on the Path to AGI https://arxiv.org/pdf/2311.02462.pdf",
      "description": "Trying to talk about AGI in a reasonable manner. There needs to be more hype and more rigor in talking about AGI. The Deepmind paper provides a good discussion of how to think about AGI and where we are now. #deepmind #agi #machinelearning #rajistics Levels of AGI: Operationalizing Progress on the Path to AGI https://arxiv.org/pdf/2311.02462.pdf",
      "upload_date": "2023-11-09",
      "total_views": 7197,
      "max_views": 4413,
      "topics": [
        "agi",
        "deepmind",
        "google",
        "levels",
        "machinelearning",
        "pdf",
        "talk",
        "trying"
      ],
      "search_text": "Trying to talk about AGI in a reasonable manner. There needs to be more hype and more rigor in talking about AGI. The Deepmind paper provides a good discussion of how to think about AGI and where we are now. #deepmind #agi #machinelearning #rajistics Levels of AGI: Operationalizing Progress on the Path to AGI https://arxiv.org/pdf/2311.02462.pdf agi deepmind google levels machinelearning pdf talk trying",
      "platforms": {
        "tiktok": {
          "video_id": "7299549198171540779",
          "url": "https://www.tiktok.com/@rajistics/video/7299549198171540779",
          "view_count": 4413,
          "upload_date": "2023-11-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Czb_eotAl3a",
          "url": "https://www.instagram.com/reel/Czb_eotAl3a",
          "view_count": 2589,
          "upload_date": "2023-11-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "A11KhI2hUao",
          "url": "https://www.youtube.com/watch?v=A11KhI2hUao",
          "view_count": 195,
          "upload_date": "2023-11-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Open Source with Stable Diffusion - #datascience #codetok #machinelearning #stablediffusion #opensourcesoftware",
      "description": "Open Source with Stable Diffusion - #datascience #codetok #machinelearning #stablediffusion #opensourcesoftware",
      "upload_date": "2022-08-27",
      "total_views": 7176,
      "max_views": 7176,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "opensourcesoftware",
        "software",
        "stablediffusion"
      ],
      "search_text": "Open Source with Stable Diffusion - #datascience #codetok #machinelearning #stablediffusion #opensourcesoftware codetok datascience machinelearning opensourcesoftware software stablediffusion It's been a crazy week and we can thank open source for that. We're all hyped about the release of stable diffusion. It's been amazing to see what people have been creating and the release of the code and the weights. So basically any data scientist can go out there and start messing with it. And what we've seen already is a ton of new innovations and plugins because all this stuff is easy for anybody to use. Now this doesn't happen by accident. Most software is created by corporations for their own use. And what they want to do is try to control how you're going to use it and charge you a ton of money. But we also know that sharing ideas and by extension sharing code is where we get progress. Where research happens, where innovation happens, where new solutions happen is when there is all of this sharing among a lot of different diverse people with different perspectives. So whether you're just somebody that buys or downloads some software or whether you're a software developer, you should always be thinking about open source because the open source is really the engine that powers a lot of innovation. Both directly that affects software as well as the software that affects society in lots of different ways.",
      "platforms": {
        "tiktok": {
          "video_id": "7136585862103518506",
          "url": "https://www.tiktok.com/@rajistics/video/7136585862103518506",
          "view_count": 7176,
          "upload_date": "2022-08-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2dc8118ed7f84c098b53828a2eb7bbc3_1661615885~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=Ooiqih1imW4p48oA5KL%2FNbolbUs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6214,
      "title": "Mechanistic interpretability hands on! Try Monitor: https://monitor.transluce.org/dashboard Monitor writeup: https://transluce.org/observability-interface",
      "description": "Mechanistic interpretability hands on! Try Monitor: https://monitor.transluce.org/dashboard Monitor writeup: https://transluce.org/observability-interface",
      "upload_date": "2024-12-04",
      "total_views": 7141,
      "max_views": 6777,
      "topics": [
        "biblical",
        "interpretability",
        "mechanistic",
        "model",
        "monitor",
        "predictions",
        "steer",
        "use",
        "using",
        "verses"
      ],
      "search_text": "Mechanistic interpretability hands on! Try Monitor: https://monitor.transluce.org/dashboard Monitor writeup: https://transluce.org/observability-interface biblical interpretability mechanistic model monitor predictions steer use using verses Why do language models think 9.11 is greater than 9.9? Well, using the tools of mechanistic interpretability, we can uncover how models associate numbers with dates, months, even biblical verses. What's more, we can use these same tools to fix the issue. To do this, I'm gonna use Transluse Monitor. It's an interpretability interface that lets you explore neurons. Neurons here are the compressed information that's stored in the MOP layer of a transformer. To start with, I ran the query and monitor. Sure enough, the model got it wrong, but I could start digging into terms like bigger to figure out what was going on. Right away, we see that three clusters are affecting the token bigger. Dates and months, September 11th, and biblical verses. Now, dates and months, September 11th, right? They make sense, but biblical verses? Well, with monitor, I can dive deeper into the examples around biblical verses and see how they're using it. For example, in Matthews, that they use numbers a little bit differently. Now, if I use that knowledge and just try to prompt the model to ignore the association, it doesn't work. But with monitor, I can steer the prediction by suppressing certain neurons, how the model's working as I want it to. And this is a great example of showing how mechanistic interpretability helps you see what's going on in the model, but now you can use it to change the behavior.",
      "platforms": {
        "tiktok": {
          "video_id": "7444351034920684830",
          "url": "https://www.tiktok.com/@rajistics/video/7444351034920684830",
          "view_count": 6777,
          "upload_date": "2024-12-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oUIDAO0EZavQIxWjGpgAULAqeSA8gFGfAEnQeM~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=xS%2B3SwoismHyhXK30VGnhq0r0tM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18015130262406331",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Kuh-iNmPiJo",
          "url": "https://www.youtube.com/watch?v=Kuh-iNmPiJo",
          "view_count": 364,
          "upload_date": "2024-12-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6651,
      "title": "Statistics sounds heavy but a lot of concepts are very useful and can save you a lot of effort. This video is reminder of the many ways we use statistical concepts like random in data science. #statistics #random #rajistics",
      "description": "Statistics sounds heavy but a lot of concepts are very useful and can save you a lot of effort. This video is reminder of the many ways we use statistical concepts like random in data science. #statistics #random #rajistics",
      "upload_date": "2024-01-07",
      "total_views": 7136,
      "max_views": 4512,
      "topics": [
        "concepts",
        "done",
        "heavy",
        "lot",
        "random",
        "sounds",
        "statistics",
        "teammates",
        "teamwork",
        "weaponizedincompetence",
        "working",
        "would"
      ],
      "search_text": "Statistics sounds heavy but a lot of concepts are very useful and can save you a lot of effort. This video is reminder of the many ways we use statistical concepts like random in data science. #statistics #random #rajistics concepts done heavy lot random sounds statistics teammates teamwork weaponizedincompetence working would",
      "platforms": {
        "tiktok": {
          "video_id": "7320635015086509358",
          "url": "https://www.tiktok.com/@rajistics/video/7320635015086509358",
          "view_count": 2624,
          "upload_date": "2024-01-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1ztUu-AvYE",
          "url": "https://www.instagram.com/reel/C1ztUu-AvYE",
          "view_count": 4512,
          "upload_date": "2024-01-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6473,
      "title": "Anomaly detection is hard. This is an introduction to anomaly detection algorithms. The video focuses on the results for ADBench and what data scientists should now do. ADBench: Anomaly Detection Benchmark - https://arxiv.org/abs/2206.09426 #datascience #analytics #codetok #anomalydetection @rajistics",
      "description": "Anomaly detection is hard. This is an introduction to anomaly detection algorithms. The video focuses on the results for ADBench and what data scientists should now do. ADBench: Anomaly Detection Benchmark - https://arxiv.org/abs/2206.09426 #datascience #analytics #codetok #anomalydetection @rajistics",
      "upload_date": "2023-09-26",
      "total_views": 7125,
      "max_views": 4456,
      "topics": [
        "adbench",
        "analytics",
        "anomaly",
        "anomalydetection",
        "codetok",
        "datascience",
        "detection",
        "results"
      ],
      "search_text": "Anomaly detection is hard. This is an introduction to anomaly detection algorithms. The video focuses on the results for ADBench and what data scientists should now do. ADBench: Anomaly Detection Benchmark - https://arxiv.org/abs/2206.09426 #datascience #analytics #codetok #anomalydetection @rajistics adbench analytics anomaly anomalydetection codetok datascience detection results",
      "platforms": {
        "tiktok": {
          "video_id": "7283206263771581738",
          "url": "https://www.tiktok.com/@rajistics/video/7283206263771581738",
          "view_count": 4456,
          "upload_date": "2023-09-26",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cxql-2QJIMP",
          "url": "https://www.instagram.com/reel/Cxql-2QJIMP",
          "view_count": 1882,
          "upload_date": "2023-09-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "9y74zHOwLe8",
          "url": "https://www.youtube.com/watch?v=9y74zHOwLe8",
          "view_count": 787,
          "upload_date": "2023-09-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @rajistics as promised, the feature or variables in auto insurance models. Keep the feedback coming. #datascience #machinelearning #autoinsurance #acturialscience earlier video on insurance @rajistics ",
      "description": "Replying to @rajistics as promised, the feature or variables in auto insurance models. Keep the feedback coming. #datascience #machinelearning #autoinsurance #acturialscience earlier video on insurance @rajistics ",
      "upload_date": "2023-02-12",
      "total_views": 7107,
      "max_views": 7107,
      "topics": [
        "acturialscience",
        "autoinsurance",
        "datascience",
        "insurance",
        "machinelearning",
        "variables"
      ],
      "search_text": "Replying to @rajistics as promised, the feature or variables in auto insurance models. Keep the feedback coming. #datascience #machinelearning #autoinsurance #acturialscience earlier video on insurance @rajistics  acturialscience autoinsurance datascience insurance machinelearning variables Between us, can you tell me what are the variables in those auto insurance models you worked on? Sure, including the controversial ones? No problem. Insurance is heavily regulated, so all the variables that go into these models are public knowledge. So what variables do they use? How much you drive? Because the longer you're on the road, the higher probability of getting in a crash. Oh, so that's why they push us to use those drive tracking apps. You got it. That insurance data is much more reliable than what people will tell their insurance carriers. What else? Your driving record? Okay. Where you live, your age, your sex, and your marital status. Couldn't that be discriminatory? States differ on it. California got rid of gender, New York banned occupation and education, and Michigan has voted to ban several non-driving factors, including gender and education. What else? Credit score. Come on. Does that make sense? It sounds like a boomer discount. It's been very controversial, but there's been a strong relationship between having a good credit score and getting in fewer crashes. So insurance companies really want to keep it, and they see it as a proxy for responsibility. Hey, thanks, boss. Next week can we talk about overfitting?",
      "platforms": {
        "tiktok": {
          "video_id": "7199353042557275434",
          "url": "https://www.tiktok.com/@rajistics/video/7199353042557275434",
          "view_count": 7107,
          "upload_date": "2023-02-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bbef7ce0e8a74f88b0d10094efc2450f_1676230016~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=FS6DLTonvjBYBaTqgr2rjAay1jI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6629,
      "title": "DePlot translates plots into readable tables that an LLM can query. It's based on the MatCha architecture with more fine-tuning on plots. Nice example of visual language reasoning. #datascience #machinelearning #deplot #matcha #documentai #visualreasoning #multimodal Demo at: https://huggingface.co/spaces/fl399/deplot_plus_llm Paper at: https://arxiv.org/pdf/2212.10505.pdf Background from Maria Krasnova: https://unsplash.com/photos/qD7tpy_VozY",
      "description": "DePlot translates plots into readable tables that an LLM can query. It's based on the MatCha architecture with more fine-tuning on plots. Nice example of visual language reasoning. #datascience #machinelearning #deplot #matcha #documentai #visualreasoning #multimodal Demo at: https://huggingface.co/spaces/fl399/deplot_plus_llm Paper at: https://arxiv.org/pdf/2212.10505.pdf Background from Maria Krasnova: https://unsplash.com/photos/qD7tpy_VozY",
      "upload_date": "2023-05-03",
      "total_views": 7104,
      "max_views": 5216,
      "topics": [
        "datascience",
        "deplot",
        "documentai",
        "machinelearning",
        "matcha",
        "visualreasoning"
      ],
      "search_text": "DePlot translates plots into readable tables that an LLM can query. It's based on the MatCha architecture with more fine-tuning on plots. Nice example of visual language reasoning. #datascience #machinelearning #deplot #matcha #documentai #visualreasoning #multimodal Demo at: https://huggingface.co/spaces/fl399/deplot_plus_llm Paper at: https://arxiv.org/pdf/2212.10505.pdf Background from Maria Krasnova: https://unsplash.com/photos/qD7tpy_VozY datascience deplot documentai machinelearning matcha visualreasoning",
      "platforms": {
        "tiktok": {
          "video_id": "7228744404062932270",
          "url": "https://www.tiktok.com/@rajistics/video/7228744404062932270",
          "view_count": 5216,
          "upload_date": "2023-05-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Crwrx-Ig_Aj",
          "url": "https://www.instagram.com/reel/Crwrx-Ig_Aj",
          "view_count": 1888,
          "upload_date": "2023-05-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6458,
      "title": "ImageBind the first AI model capable of binding data from six modalities at once without the need for explicit supervision. It recognizes the relationships between these modalities ‚Äî images and video audio text depth thermal and inertial measurement units (IMUs). #datascience #machinelearning #embeddings #imagebind #multimodal Imagebind: https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/ ImageBind: One Embedding Space To Bind Them All - https://arxiv.org/abs/2305.05665 Code: https://github.com/facebookresearch/ImageBind Taylor Swift embeddings by Krystal Kirkland: https://www.linkedin.com/feed/update/urn:li:activity:7062203006427013120 Background image by Alina Grubnyak: https://unsplash.com/photos/ZiQkhI7417A",
      "description": "ImageBind the first AI model capable of binding data from six modalities at once without the need for explicit supervision. It recognizes the relationships between these modalities ‚Äî images and video audio text depth thermal and inertial measurement units (IMUs). #datascience #machinelearning #embeddings #imagebind #multimodal Imagebind: https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/ ImageBind: One Embedding Space To Bind Them All - https://arxiv.org/abs/2305.05665 Code: https://github.com/facebookresearch/ImageBind Taylor Swift embeddings by Krystal Kirkland: https://www.linkedin.com/feed/update/urn:li:activity:7062203006427013120 Background image by Alina Grubnyak: https://unsplash.com/photos/ZiQkhI7417A",
      "upload_date": "2023-05-11",
      "total_views": 7095,
      "max_views": 3803,
      "topics": [
        "datascience",
        "embeddings",
        "imagebind",
        "machinelearning",
        "modalities",
        "model",
        "multimodal"
      ],
      "search_text": "ImageBind the first AI model capable of binding data from six modalities at once without the need for explicit supervision. It recognizes the relationships between these modalities ‚Äî images and video audio text depth thermal and inertial measurement units (IMUs). #datascience #machinelearning #embeddings #imagebind #multimodal Imagebind: https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/ ImageBind: One Embedding Space To Bind Them All - https://arxiv.org/abs/2305.05665 Code: https://github.com/facebookresearch/ImageBind Taylor Swift embeddings by Krystal Kirkland: https://www.linkedin.com/feed/update/urn:li:activity:7062203006427013120 Background image by Alina Grubnyak: https://unsplash.com/photos/ZiQkhI7417A datascience embeddings imagebind machinelearning modalities model multimodal",
      "platforms": {
        "tiktok": {
          "video_id": "7232049418982231339",
          "url": "https://www.tiktok.com/@rajistics/video/7232049418982231339",
          "view_count": 3803,
          "upload_date": "2023-05-11",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CsHnhCvAtQ1",
          "url": "https://www.instagram.com/reel/CsHnhCvAtQ1",
          "view_count": 2135,
          "upload_date": "2023-05-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "k8BL9gzW_gs",
          "url": "https://www.youtube.com/watch?v=k8BL9gzW_gs",
          "view_count": 1157,
          "upload_date": "2023-05-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6065,
      "title": "Good reminder on what an open source model has now that we are all talking about DeepSeek. ",
      "description": "Good reminder on what an open source model has now that we are all talking about DeepSeek. ",
      "upload_date": "2025-02-01",
      "total_views": 7094,
      "max_views": 7094,
      "topics": [
        "data",
        "good",
        "model",
        "models",
        "open",
        "reminder",
        "source",
        "talking",
        "training"
      ],
      "search_text": "Good reminder on what an open source model has now that we are all talking about DeepSeek.  data good model models open reminder source talking training Did you see the new open models from NAMIC and Allen AI? These aren't regular models. They're like open source models. What's the difference? These models share not only their weights, but also the training data and training method. Who cares about training? I just want to use the model. Have you ever wanted to reproduce a model to better understand or improve it? Why would you want to reproduce something that somebody else has done before? How does that advance science? OK, but how would you evaluate a model? I look at the leaderboards. But how do you know the models haven't trained on those test sets that are used in the evaluations and leaderboard? I try not to think about what could go wrong. Glass half full. Any concerns that the model has been trained on something inappropriate or a little shady? Oh, like the child sexual abuse images stable diffusion was trained on or the ongoing lawsuit with the books data set or data poisoning attacks? I get it now. Why we want real open source model. Exactly why we're not going to settle for the knockoff.",
      "platforms": {
        "tiktok": {
          "video_id": "7466257911581281566",
          "url": "https://www.tiktok.com/@rajistics/video/7466257911581281566",
          "view_count": 7094,
          "upload_date": "2025-02-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o836cdAnfAIz4oEQADRn3hCMeEEVFEAwNXAQBk~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=%2BkOGzn3SDQ2vr6teuwhi5K98wh8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17913697673964629",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6419,
      "title": "Prediction Intervals with Conformal Inference: An Intuitive Explanation",
      "description": "Prediction Intervals with Conformal Inference: An Intuitive Explanation",
      "upload_date": "2022-09-24",
      "total_views": 7090,
      "max_views": 7090,
      "topics": [
        "conformal",
        "conformalprediction",
        "datascience",
        "explanation",
        "getting",
        "inference",
        "intervals",
        "intuitive",
        "prediction",
        "predictioninterval",
        "statistics"
      ],
      "search_text": "Prediction Intervals with Conformal Inference: An Intuitive Explanation conformal conformalprediction datascience explanation getting inference intervals intuitive prediction predictioninterval statistics",
      "platforms": {
        "instagram": {
          "video_id": "CkzWyz0AiBO",
          "url": "https://www.instagram.com/reel/CkzWyz0AiBO/",
          "view_count": 0,
          "upload_date": "2022-09-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZUK4zR0IeLU",
          "url": "https://www.youtube.com/watch?v=ZUK4zR0IeLU",
          "view_count": 7090,
          "upload_date": "2022-09-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6529,
      "title": "Active learning uses an algorithm to help select what data to label. Ideally using this approach people can get comparable model results using less labeled data. #datascience #machinelearning #activelearning #datalabeling Active Learning Strategies from Neptune.ai: https://neptune.ai/blog/active-learning-strategies-tools-use-cases",
      "description": "Active learning uses an algorithm to help select what data to label. Ideally using this approach people can get comparable model results using less labeled data. #datascience #machinelearning #activelearning #datalabeling Active Learning Strategies from Neptune.ai: https://neptune.ai/blog/active-learning-strategies-tools-use-cases",
      "upload_date": "2023-05-18",
      "total_views": 7081,
      "max_views": 6448,
      "topics": [
        "active",
        "activelearning",
        "boundary",
        "data",
        "datalabeling",
        "datascience",
        "decision",
        "labeling",
        "learning",
        "machine",
        "machinelearning",
        "using"
      ],
      "search_text": "Active learning uses an algorithm to help select what data to label. Ideally using this approach people can get comparable model results using less labeled data. #datascience #machinelearning #activelearning #datalabeling Active Learning Strategies from Neptune.ai: https://neptune.ai/blog/active-learning-strategies-tools-use-cases active activelearning boundary data datalabeling datascience decision labeling learning machine machinelearning using",
      "platforms": {
        "tiktok": {
          "video_id": "7234311711644208427",
          "url": "https://www.tiktok.com/@rajistics/video/7234311711644208427",
          "view_count": 6448,
          "upload_date": "2023-05-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CsXTTk9g4jQ",
          "url": "https://www.instagram.com/reel/CsXTTk9g4jQ",
          "view_count": 446,
          "upload_date": "2023-05-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pQHlqDIMIww",
          "url": "https://www.youtube.com/watch?v=pQHlqDIMIww",
          "view_count": 187,
          "upload_date": "2023-05-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6227,
      "title": "Training whisper model  ",
      "description": "Training whisper model  ",
      "upload_date": "2024-11-07",
      "total_views": 7064,
      "max_views": 4278,
      "topics": [
        "arnold",
        "data",
        "hours",
        "kan",
        "kolmogorov",
        "labeled",
        "model",
        "networks",
        "open",
        "shept",
        "source",
        "training",
        "using",
        "version",
        "whisper"
      ],
      "search_text": "Training whisper model   arnold data hours kan kolmogorov labeled model networks open shept source training using version whisper OpenAI's dev day was big, but there's one update that people aren't talking about and it highlights an emerging trend in machine learning. OpenAI released a new version of its speech recognition model whisper as open source, which is totally cool, but let's talk about how they improved it. The traditional approach for training these speech recognition models is doing it in a supervised fashion where we have good audio and good transcripts that have been validated by humans as entirely correct. But there isn't that much of this data, maybe 5,000 hours of supervised data sets compared that to millions of hours of unlabeled audio data. So what the first version of whisper did was relax this gold standard of human validated data sets and instead used automated pipelines to help filter out and use some of that noisier data. And it worked. They used about 680,000 hours of noisily labeled data, more than like an order of magnitude of previous speech recognition models. Now in the latest version, they expanded this up to a million hours of this weekly labeled data. They also used 4 million hours of audio data that was labeled using the existing whisper model. So not human labeled data, but model labeled data. And this led to an improvement in the model compared to the previous version. And the lesson here is how when you have large amounts of data, even if it's not the highest quality, you can use that to improve the model. Now the question is, who has access to that much data?",
      "platforms": {
        "tiktok": {
          "video_id": "7434663903369612590",
          "url": "https://www.tiktok.com/@rajistics/video/7434663903369612590",
          "view_count": 4278,
          "upload_date": "2024-11-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ae66be9cf31a4cf4898af186bc2362d7_1731017590~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=uNNckFJBY8MDGNxCxGHvSWlZsr4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18148199377345419",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "LaAlEqMtzLU",
          "url": "https://www.youtube.com/watch?v=LaAlEqMtzLU",
          "view_count": 2786,
          "upload_date": "2024-11-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "The politics of ChatGPT, it’s no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics   Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics ",
      "description": "The politics of ChatGPT, it’s no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics   Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics ",
      "upload_date": "2022-12-27",
      "total_views": 7028,
      "max_views": 7028,
      "topics": [
        "chatgpt",
        "chatgtp",
        "datascience",
        "machinelearning",
        "models",
        "openai"
      ],
      "search_text": "The politics of ChatGPT, it’s no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics   Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics  chatgpt chatgtp datascience machinelearning models openai Is ChatGTP liberal? Is Elon upset because ChatGPT is saying something different than what he believes? ChatGTP, just like any other AI technology, is far from neutral. It's value-laden, and it's not a surprise that its outputs favor certain interests or social groups. I've studied this for the last 20 years, and I'm also a data scientist. So let me show you how I take apart something like ChatGTP and look at it at four different layers. First, think about the existence and why someone would want to create this. OpenAI is largely about commercial profit, and they've spent years developing this. Hundreds of researchers used terabytes of data, just an enormous amount of compute, to create all of this. Next, take a look at where the data came from for building this. Often things like books and Wikipedia. They didn't, for example, go and take a look at talk radio, which is well known in the United States as being conservative, and use that to build ChatGTP. Besides choosing the type of trading data that goes into any AI, there's also another level of filters, because there might be some books that they declare inappropriate and don't want the model to learn from them. Just like you don't show your kids every possible book there is in existence. As my earlier videos have shown, one of the things about ChatGTP is that it had human feedback or reinforcement learning during its design. A fair question is to ask, what kind of feedback was that? What were the values embedded in those forms that it was asking for feedback? If we take a look at the form, we'll see that there's questions about what's fact and opinion, what moral judgment is, and these were questions that OpenAI got to decide. They also picked out who was going to answer these questions. But for example, if you gave these questions to another society, say somebody as extreme as the Taliban, your Chat product would be trained quite differently. Another step where you'll see the values is the post-processing, where often when you're building these AI models, you might have separate models that detect behaviors that you're worried about, such as is the model racist? Could be one. You see this, for example, with stable diffusion, where they have a separate model that detects if the output is safe for work or not safe for work. So these AIs are far from neutral. There's lots of decisions that go into training these models that carry values with them that are embedded in these models.",
      "platforms": {
        "tiktok": {
          "video_id": "7181907278537182510",
          "url": "https://www.tiktok.com/@rajistics/video/7181907278537182510",
          "view_count": 7028,
          "upload_date": "2022-12-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/42f8083fb93e4e51982f0d4d5cd79b16_1672168107~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=gaTdT91Wei%2BJVpd0bRgUL8TMyq0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @Data Storyteller  Here are two examples of data or target leakage. I bet people have other fun examples. #datascience #targetleakage #dataleakage #machinelearning",
      "description": "Replying to @Data Storyteller  Here are two examples of data or target leakage. I bet people have other fun examples. #datascience #targetleakage #dataleakage #machinelearning",
      "upload_date": "2022-07-22",
      "total_views": 7026,
      "max_views": 7026,
      "topics": [
        "dataleakage",
        "datascience",
        "machinelearning",
        "model",
        "target",
        "targetleakage"
      ],
      "search_text": "Replying to @Data Storyteller  Here are two examples of data or target leakage. I bet people have other fun examples. #datascience #targetleakage #dataleakage #machinelearning dataleakage datascience machinelearning model target targetleakage Did you know one of the biggest places models fail is because of data or target leakage? Let's build a model for HR where we want to predict what the salary of a new hire should be and we want to make this sure this model is fair so it should really be based on their experience and how many people they're managing. Hey boss, I built that new model that you asked for. I did a fabulous job. The error on it is absolutely zero. I even double checked it with another data set. I'm really good at this. Wow. You did an amazing job. Hey, can you share with me the input variables you used for making that prediction? Hmm. Do you see the problem? Weekly income is entirely correlated to our target, which is annual income. We would actually have weekly income for new hires. This is a great example of target leakage and an easy way to check this would be to see how your features correlated to your target. This model was about predicting which skin tumors are cancerous. You can see some of the examples of images here. The model did really good in the lab, was giving great results, but when they put it in the real world, it struggled. Any idea? The model was cheating and using a shortcut. It used the ruler because in the pictures it was trained on, often a ruler was present in the tumors that were the worst, that were likely to be cancerous because that's why a doctor would be measuring them. But when you actually put this in the real world, you're not measuring that stuff out for the first time, you're looking at tumors and trying to see whether they're cancerous. So this is a more subtle example of how you can have leakage that can affect your machine learning.",
      "platforms": {
        "tiktok": {
          "video_id": "7123232399206485294",
          "url": "https://www.tiktok.com/@rajistics/video/7123232399206485294",
          "view_count": 7026,
          "upload_date": "2022-07-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1018bd37f96d454a9f9ad3855f426a9c_1658506790~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=ILKbttRTg3v3%2BAkeqsdwzBMQfW0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6530,
      "title": "If you want more details on the biggest advancements in AI for 2023 then find me on LinkedIn or Threads where I have a detailed post with all the links. #2023 #ai #predictions #rajistics",
      "description": "If you want more details on the biggest advancements in AI for 2023 then find me on LinkedIn or Threads where I have a detailed post with all the links. #2023 #ai #predictions #rajistics",
      "upload_date": "2023-12-22",
      "total_views": 6983,
      "max_views": 4615,
      "topics": [
        "2023",
        "advancements",
        "ai",
        "also",
        "biggest",
        "data",
        "details",
        "like",
        "models",
        "one",
        "practical",
        "predictions",
        "things",
        "top",
        "want"
      ],
      "search_text": "If you want more details on the biggest advancements in AI for 2023 then find me on LinkedIn or Threads where I have a detailed post with all the links. #2023 #ai #predictions #rajistics 2023 advancements ai also biggest data details like models one practical predictions things top want",
      "platforms": {
        "tiktok": {
          "video_id": "7315525268481805611",
          "url": "https://www.tiktok.com/@rajistics/video/7315525268481805611",
          "view_count": 4615,
          "upload_date": "2023-12-22",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C1K3q4cAfhZ",
          "url": "https://www.instagram.com/reel/C1K3q4cAfhZ",
          "view_count": 2182,
          "upload_date": "2023-12-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "224xVef3wjg",
          "url": "https://www.youtube.com/watch?v=224xVef3wjg",
          "view_count": 186,
          "upload_date": "2023-12-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering ",
      "description": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering ",
      "upload_date": "2023-02-27",
      "total_views": 6980,
      "max_views": 6980,
      "topics": [
        "datascience",
        "engineering",
        "feature",
        "featureengineering",
        "machinelearning",
        "model"
      ],
      "search_text": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering  datascience engineering feature featureengineering machinelearning model My blood is boiling. I saw this tweet today and lost it. I've spent years of my life explaining to people that deep learning doesn't automatically take away the need for pre-processing and feature engineering. Yet on some beautiful pure theoretical fantasy land, people think it does. In real-world data science, pre-processing and feature engineering is one of the most important parts of the process because it helps you convey human knowledge and insight into the model. If you're working with dates, instead of giving the date of birth to the model, giving an age is pretty useful. If you're working with time series, telling the model when holidays or weekends is totally useful. If you're working with physical objects that have relationships defined by physics, adding that physical information is important. If you're doing calculations, whether simple moving averages, ratios, differences, adding those as features helps improve your model. All of this stuff is stuff that matters. Don't think it's going to be automatically done for you just because you're using deep learning.",
      "platforms": {
        "tiktok": {
          "video_id": "7204987001458756910",
          "url": "https://www.tiktok.com/@rajistics/video/7204987001458756910",
          "view_count": 6980,
          "upload_date": "2023-02-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e8f3adf23b5d4137ae0630d845edcaa9_1677541775~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=l2amnxA%2FfrzROFP9PphmKjsh5xc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6648,
      "title": "Working with Categorical data using ordinal one hot (dummy) and target encoding. Do you have your own favorite approach? And ChatGPT tells me that gender has many categories. #datascience #statistics #analytics #featureengineering",
      "description": "Working with Categorical data using ordinal one hot (dummy) and target encoding. Do you have your own favorite approach? And ChatGPT tells me that gender has many categories. #datascience #statistics #analytics #featureengineering",
      "upload_date": "2023-12-01",
      "total_views": 6970,
      "max_views": 4223,
      "topics": [
        "analytics",
        "categorical",
        "data",
        "datascience",
        "featureengineering",
        "statistics",
        "working"
      ],
      "search_text": "Working with Categorical data using ordinal one hot (dummy) and target encoding. Do you have your own favorite approach? And ChatGPT tells me that gender has many categories. #datascience #statistics #analytics #featureengineering analytics categorical data datascience featureengineering statistics working",
      "platforms": {
        "tiktok": {
          "video_id": "7307773707760586026",
          "url": "https://www.tiktok.com/@rajistics/video/7307773707760586026",
          "view_count": 4223,
          "upload_date": "2023-12-01",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0VEFZSg4sv",
          "url": "https://www.instagram.com/reel/C0VEFZSg4sv",
          "view_count": 2747,
          "upload_date": "2023-12-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "It happens. Be careful. #aws #datascience #deeplearning #gpu",
      "description": "It happens. Be careful. #aws #datascience #deeplearning #gpu",
      "upload_date": "2022-03-30",
      "total_views": 6940,
      "max_views": 6940,
      "topics": [
        "aws",
        "careful",
        "datascience",
        "deeplearning",
        "gpu",
        "happens"
      ],
      "search_text": "It happens. Be careful. #aws #datascience #deeplearning #gpu aws careful datascience deeplearning gpu happens I actually did it myself. Yeah. Nice!",
      "platforms": {
        "tiktok": {
          "video_id": "7080905262181305646",
          "url": "https://www.tiktok.com/@rajistics/video/7080905262181305646",
          "view_count": 6940,
          "upload_date": "2022-03-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/781001e2db2a471d82ed9d94ad84863a_1648651732~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=P0eNmPpotbzQd8pKn0xizqzLDGg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5964,
      "title": "Cursor’s new Tab-RL model uses reinforcement learning from real user feedback, rolling out checkpoints multiple times per day across 400M+ predictions. Compared to the old approach, Copilot’s logistic regression filter over hand-crafted features, Cursor’s RL cuts suggestions by 21% while boosting accept rates by 28%. It’s the bitter lesson in action: scale, data, and feedback loops beat clever tricks. Cursor Blog – Tab-RL: Improving Autocomplete with Reinforcement Learning - https://cursor.com/blog/tab-rl Copilot Explorer – https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals",
      "description": "Cursor’s new Tab-RL model uses reinforcement learning from real user feedback, rolling out checkpoints multiple times per day across 400M+ predictions. Compared to the old approach, Copilot’s logistic regression filter over hand-crafted features, Cursor’s RL cuts suggestions by 21% while boosting accept rates by 28%. It’s the bitter lesson in action: scale, data, and feedback loops beat clever tricks. Cursor Blog – Tab-RL: Improving Autocomplete with Reinforcement Learning - https://cursor.com/blog/tab-rl Copilot Explorer – https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals",
      "upload_date": "2025-09-13",
      "total_views": 6923,
      "max_views": 5739,
      "topics": [
        "copilot",
        "cursor",
        "feedback",
        "learning",
        "model",
        "new",
        "reinforcement",
        "tab",
        "using"
      ],
      "search_text": "Cursor’s new Tab-RL model uses reinforcement learning from real user feedback, rolling out checkpoints multiple times per day across 400M+ predictions. Compared to the old approach, Copilot’s logistic regression filter over hand-crafted features, Cursor’s RL cuts suggestions by 21% while boosting accept rates by 28%. It’s the bitter lesson in action: scale, data, and feedback loops beat clever tricks. Cursor Blog – Tab-RL: Improving Autocomplete with Reinforcement Learning - https://cursor.com/blog/tab-rl Copilot Explorer – https://thakkarparth007.github.io/copilot-explorer/posts/copilot-internals copilot cursor feedback learning model new reinforcement tab using Cursor's back at it, adding a little magic to our coding. This time, they pulled back the curtain, and it's a bitter lesson for all of us in AI. So every time you type or move your cursor, Cursor's tab model tries to predict your next step. If it's confident, it gives you a suggestion that you could accept with a tab. This happens on every action, which is like 400 million times a day. Now, Cursor isn't the first here. Early GitHub co-pilot also suggested code, but to avoid spamming people with code all the time, they add the filter. This model was a logistic regression model that looked at what language you were coding in, why the last suggestion was accepted, the cursor position. And the job of the model was to block suggestions that didn't make sense in your context. It helped me from writing really bad code. Now, Cursor took a different approach. Instead of handcrafted features, they decided to learn from you. They decided to use reinforcement learning. So the signal they use is your actual behavior. Are you accepting or rejecting them? And they don't retrain this model once. They actually push new checkpoints throughout a day learning constant. Oh, and it works. This new tab model they have shows 21% fewer suggestions, but the ones it shows, 28% more likely to be accepted. What this means is less noise, more signal. The takeaway, this is bigger than just Cursor's tab model. This is a sign of how we're moving from static models that have all these hand engineered features to dynamic systems that adapt and learn from us continuously. This is the better lesson of AI. You are the trading data.",
      "platforms": {
        "tiktok": {
          "video_id": "7549376508180122910",
          "url": "https://www.tiktok.com/@rajistics/video/7549376508180122910",
          "view_count": 5739,
          "upload_date": "2025-09-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocoxfADxKxDEgFReoR2AYAVEAE0ECrCIAAuMEX~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=EKcZF9v3HfiCIwsK1NPfDYf%2BM%2BY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17861099067474172",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "BaSdkaLLkn8",
          "url": "https://www.youtube.com/watch?v=BaSdkaLLkn8",
          "view_count": 1184,
          "upload_date": "2025-09-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "TabPFN revolution in data science. Please don’t your time on all this hype. Every week there is a revolution announced on Twitter. Ignore it. True greatness takes time. #datascience #machinelearning #statistics #tabpfn ",
      "description": "TabPFN revolution in data science. Please don’t your time on all this hype. Every week there is a revolution announced on Twitter. Ignore it. True greatness takes time. #datascience #machinelearning #statistics #tabpfn ",
      "upload_date": "2022-10-22",
      "total_views": 6898,
      "max_views": 6898,
      "topics": [
        "data",
        "datascience",
        "machinelearning",
        "science",
        "statistics",
        "tabpfn"
      ],
      "search_text": "TabPFN revolution in data science. Please don’t your time on all this hype. Every week there is a revolution announced on Twitter. Ignore it. True greatness takes time. #datascience #machinelearning #statistics #tabpfn  data datascience machinelearning science statistics tabpfn We've got a revolution in data science. Tab PFN with its transformer architecture? Is it going to take over? Come on, people. You know better than this. Data science Twitter is about getting eyeballs, not actually giving you something useful. If we look at the revolutionary data science algorithms over the last couple years, XGBoost, AlexNet with introducing convolutional neural networks, transformers. All of these took time to make that revolution happen. You have to try different implementations, different data sets. It's a grind. It's no different than how you become a great musician or a great athlete. And that's what you're going to expect for anything in data science. So ignore this stuff. Just focus on the fundamentals. I'll try to help you along that way.",
      "platforms": {
        "tiktok": {
          "video_id": "7157374691420671274",
          "url": "https://www.tiktok.com/@rajistics/video/7157374691420671274",
          "view_count": 6898,
          "upload_date": "2022-10-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d89fd737cffd4f04bd03cd989e3c8102_1666456170~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=mY8ZbZXMS06JFNHpLmB9VduwSP0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "My creator hero just released a great new book and website. It's an excellent way to learn programming using JavaScript and build some very cool visual programs. I love the videos he makes (I hope many of you have come across him on YouTube). He is a fabulous resource if you want to learn how to program. 🌐 Nature of Code: https://natureofcode.com/ 🎥 Coding Train: https://www.youtube.com/@TheCodingTrain",
      "description": "My creator hero just released a great new book and website. It's an excellent way to learn programming using JavaScript and build some very cool visual programs. I love the videos he makes (I hope many of you have come across him on YouTube). He is a fabulous resource if you want to learn how to program. 🌐 Nature of Code: https://natureofcode.com/ 🎥 Coding Train: https://www.youtube.com/@TheCodingTrain",
      "upload_date": "2024-09-07",
      "total_views": 6858,
      "max_views": 6858,
      "topics": [
        "code",
        "coding",
        "daniel",
        "like",
        "nature",
        "new"
      ],
      "search_text": "My creator hero just released a great new book and website. It's an excellent way to learn programming using JavaScript and build some very cool visual programs. I love the videos he makes (I hope many of you have come across him on YouTube). He is a fabulous resource if you want to learn how to program. 🌐 Nature of Code: https://natureofcode.com/ 🎥 Coding Train: https://www.youtube.com/@TheCodingTrain code coding daniel like nature new Would you like to be able to build this or something like this? Well, guess what? Our favorite coding instructor is going to show how to do it in a few lines of code. This is all in Daniel Schiffman's new book, The Nature of Code. Now you might know Daniel from his work in coding train, where he makes programming a joy. Go check out those videos on YouTube. They're so good. In Nature of Code, Daniel's teaching us how to program, but this time he's connecting it with nature and creativity. He's made it easy to follow. He starts with inanimate objects, go to autonomous agents, and then intelligence. Like take a look at this code example where he's teaching us genetic algorithms and how things evolve. Oh, and he doesn't stop there. He even has a flappy bird that teaches how to, it teaches itself how to fly. It's well organized. The code is easy to follow. And since it's in JavaScript and freely available, there's no reason you can't get started right away.",
      "platforms": {
        "tiktok": {
          "video_id": "7412017767500582187",
          "url": "https://www.tiktok.com/@rajistics/video/7412017767500582187",
          "view_count": 6858,
          "upload_date": "2024-09-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b3b3a2a3cf8a4f20bcb0855ccf44ac8e_1725744876~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=ssb%2BylmL3MO%2Bg4J3TyaihbCepYE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6098,
      "title": "Parquet file format - Are you using it? For data science, data engineering, and machine learning its a popular file format.  ",
      "description": "Parquet file format - Are you using it? For data science, data engineering, and machine learning its a popular file format.  ",
      "upload_date": "2024-12-15",
      "total_views": 6857,
      "max_views": 4280,
      "topics": [
        "data",
        "engineering",
        "everything",
        "file",
        "format",
        "introducing",
        "okay",
        "parquet"
      ],
      "search_text": "Parquet file format - Are you using it? For data science, data engineering, and machine learning its a popular file format.   data engineering everything file format introducing okay parquet Did you know how this Parquet floor is neatly organized and elegant? Inspired a file format. Do the Ls stand for Excel? Oh boy. I'm talking about Parquet. It's a modern, columnar approach that's way better than your row-oriented CSV or Excel. I had my data row by row. What's the problem with that? Okay, picture this. You open the door, everything is disorganized all over the place, everything is in different places. Hey, this isn't a little personal. Mmm, but accurate. Exactly. With Parquet, everything is organized in the way we're used to with columns. So now when I need something, I can go to the appropriate place, not have to dig through everything. Okay, okay. You got me intrigued. Got any benchmarks to back that up? Absolutely. You can take that bloated CSV or Excel file, compress it to 80%. If you go and try to read something, you can get three times as fast. What this means is you can save money on your storage and your compute. I'd like those performance improvements. Is this a black box? Not at all. You can easily crack open a Parquet file and when you do, you find out that it has a schema. Everything in their data is organized. Whoa, you did all that in pandas? Exactly. And it's widely supported by everyone, whether it's Snowflake, Databricks with Spark, Google with BigQuery or EverydayPandas. Okay, you got me. I'm going to use Parquet to store my data because it floors the competition. Now you're catching on.",
      "platforms": {
        "tiktok": {
          "video_id": "7448698284245306654",
          "url": "https://www.tiktok.com/@rajistics/video/7448698284245306654",
          "view_count": 4280,
          "upload_date": "2024-12-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEMfAqCbQEWvfIQC68jFI2jIS9kYiAFLAeAAXO~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=6Q%2FZyOADELy1XrAbNzMBPe45%2BME%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18046681061114444",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_CnEKyutpWE",
          "url": "https://www.youtube.com/watch?v=_CnEKyutpWE",
          "view_count": 2577,
          "upload_date": "2024-12-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Requested video - DSPy DSPy brings a systematic approach to prompting that gives you better-designed workflows while also optimizing prompts. (So it saves money, makes developers happy, and gets you more accurate results!!) See my longer posts on LinkedIn, X, or Threads for all the links  DSPy: https://github.com/stanfordnlp/dspy #dspy #rajistics #prompting ",
      "description": "Requested video - DSPy DSPy brings a systematic approach to prompting that gives you better-designed workflows while also optimizing prompts. (So it saves money, makes developers happy, and gets you more accurate results!!) See my longer posts on LinkedIn, X, or Threads for all the links  DSPy: https://github.com/stanfordnlp/dspy #dspy #rajistics #prompting ",
      "upload_date": "2024-06-15",
      "total_views": 6848,
      "max_views": 6848,
      "topics": [
        "approach",
        "dspy",
        "like",
        "looks",
        "prompting",
        "using"
      ],
      "search_text": "Requested video - DSPy DSPy brings a systematic approach to prompting that gives you better-designed workflows while also optimizing prompts. (So it saves money, makes developers happy, and gets you more accurate results!!) See my longer posts on LinkedIn, X, or Threads for all the links  DSPy: https://github.com/stanfordnlp/dspy #dspy #rajistics #prompting  approach dspy like looks prompting using Agentec AI workflows are the future, but right now our code looks like spaghetti. Stringing together prompts gets messy really quickly. Why not switch to using DSPY as a framework? It's cleaner, faster, cheaper? Another prompting framework? Come on, is it for real? Absolutely. It's got 13,000 stars on GitHub and you have big users like JetBlue and Jesse Silver using it. Hmm, it's managing those spicy chatbots? Those things make money. DSPY offers a structured approach. It's more like programming where we break things down into different modules. No more LangChain chaos. Do we even have a prompting style guide? Hold on, other people look at my code? DSPY is also useful for optimizations. By giving it some data and metric, it works its way through your pipeline, optimizes every step of it. Snowflake showed how they reduced cost by four times with using all those optimizations because then they were able to use a smaller model. Well, Prompti, it's been nice working with you, but it looks like we found something that looks better and costs less. No worries, my agentic hiring bot just got me a job as your VP.",
      "platforms": {
        "tiktok": {
          "video_id": "7380840446504848682",
          "url": "https://www.tiktok.com/@rajistics/video/7380840446504848682",
          "view_count": 6848,
          "upload_date": "2024-06-15",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ddfefa417030463d9001fbff094441cc_1718485843~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=rRBhr%2B4qT05ddol6FWaCXm%2BkDVY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes ",
      "description": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes ",
      "upload_date": "2023-01-23",
      "total_views": 6839,
      "max_views": 6839,
      "topics": [
        "data",
        "datascience",
        "don",
        "kubernetes",
        "learn",
        "machinelearning"
      ],
      "search_text": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes  data datascience don kubernetes learn machinelearning Anyone interested here in expanding their skill set and signing up for the Kubernetes course? Sign me up. I'm interested. Why would you want to take that? You don't even understand branches and get up. I don't know. I think it'd be good to understand what our infor chain does and be able to speak their language. I admire it. The reality is for some tasks, having somebody on our team that understands how to do infra and be able to set up and complete a project could be really useful. Come on, you're not even a half-stacked data scientist. Look, we don't all spend our weekends endlessly doing data science and creating new languages to understand how transformers work. Okay, I get it. I set a high standard. But still, infrastructure like Kubernetes is really different than our day-to-day development environment. I don't know if it's really that useful to learn that stuff versus data science in our existing environment. Kubernetes is an enterprise standard. It seems like understanding it would be a good skill to have. Hold on. Does this mean if you learn Kubernetes, you're also going to get a skill competency within the company that's going to help you for laterals and promotions?",
      "platforms": {
        "tiktok": {
          "video_id": "7191955402999844138",
          "url": "https://www.tiktok.com/@rajistics/video/7191955402999844138",
          "view_count": 6839,
          "upload_date": "2023-01-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/20e221a6b3f04a618790c3e1c78d6232_1674507630~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=K6xfENW%2B6YJzIMnl6mshzvYx3NI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6542,
      "title": "Do you have a missing data story? Missing data happens all the time. Should you just accept it? Drop rows? Use Imputation? or Keep digging? There is often a reason for missing data. Don‚Äôt just jump to dropping rows or using imputation techniques. #dataengineering #statistics #datascience #imputation",
      "description": "Do you have a missing data story? Missing data happens all the time. Should you just accept it? Drop rows? Use Imputation? or Keep digging? There is often a reason for missing data. Don‚Äôt just jump to dropping rows or using imputation techniques. #dataengineering #statistics #datascience #imputation",
      "upload_date": "2023-11-24",
      "total_views": 6829,
      "max_views": 3406,
      "topics": [
        "data",
        "dataengineering",
        "datascience",
        "drop",
        "imputation",
        "missing",
        "overcome",
        "rows",
        "skit",
        "statistics"
      ],
      "search_text": "Do you have a missing data story? Missing data happens all the time. Should you just accept it? Drop rows? Use Imputation? or Keep digging? There is often a reason for missing data. Don‚Äôt just jump to dropping rows or using imputation techniques. #dataengineering #statistics #datascience #imputation data dataengineering datascience drop imputation missing overcome rows skit statistics",
      "platforms": {
        "tiktok": {
          "video_id": "7305090146553187626",
          "url": "https://www.tiktok.com/@rajistics/video/7305090146553187626",
          "view_count": 3312,
          "upload_date": "2023-11-24",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C0CcTO5gs-7",
          "url": "https://www.instagram.com/reel/C0CcTO5gs-7",
          "view_count": 3406,
          "upload_date": "2023-11-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "JNfRrOJj4j8",
          "url": "https://www.youtube.com/watch?v=JNfRrOJj4j8",
          "view_count": 111,
          "upload_date": "2023-11-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6104,
      "title": "Axolotl provides a declarative approach to fine tuning large language models. It's very easy to get started with and much easier for folks new to AI/ML.  Axolotl: https://github.com/OpenAccess-AI-Collective/axolotl",
      "description": "Axolotl provides a declarative approach to fine tuning large language models. It's very easy to get started with and much easier for folks new to AI/ML.  Axolotl: https://github.com/OpenAccess-AI-Collective/axolotl",
      "upload_date": "2024-01-22",
      "total_views": 6798,
      "max_views": 6000,
      "topics": [
        "approach",
        "axolotl",
        "declarative",
        "fine",
        "large",
        "largelanguagemodels",
        "provides",
        "tuning"
      ],
      "search_text": "Axolotl provides a declarative approach to fine tuning large language models. It's very easy to get started with and much easier for folks new to AI/ML.  Axolotl: https://github.com/OpenAccess-AI-Collective/axolotl approach axolotl declarative fine large largelanguagemodels provides tuning",
      "platforms": {
        "tiktok": {
          "video_id": "7327060295392824619",
          "url": "https://www.tiktok.com/@rajistics/video/7327060295392824619",
          "view_count": 6000,
          "upload_date": "2024-01-22",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18038511979710953",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "TrtCVlK4dfs",
          "url": "https://www.youtube.com/watch?v=TrtCVlK4dfs",
          "view_count": 798,
          "upload_date": "2024-01-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Data visualization tips #datascience #dataviz #analytics #datavisualization",
      "description": "Data visualization tips #datascience #dataviz #analytics #datavisualization",
      "upload_date": "2022-03-21",
      "total_views": 6784,
      "max_views": 6784,
      "topics": [
        "analytics",
        "data",
        "datascience",
        "datavisualization",
        "dataviz",
        "let"
      ],
      "search_text": "Data visualization tips #datascience #dataviz #analytics #datavisualization analytics data datascience datavisualization dataviz let Let me show you how to go from this to this. Let's organize the data by sorting it. Make it easy to read the text. Don't rotate it. Don't make people try to guess the numbers. Just give it to them. Finally, add a little bit of color. And thanks, Cedric, for putting this together.",
      "platforms": {
        "tiktok": {
          "video_id": "7077702158887537963",
          "url": "https://www.tiktok.com/@rajistics/video/7077702158887537963",
          "view_count": 6784,
          "upload_date": "2022-03-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/52ab32c3517d4a199d84d608a896ed90_1647905951~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=mzgMsM72jpKGel3LvI0MQLFd5K0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6071,
      "title": "Deepseek R1 - Latest open weights reasoning model, the paper is very readable https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf",
      "description": "Deepseek R1 - Latest open weights reasoning model, the paper is very readable https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf",
      "upload_date": "2025-01-21",
      "total_views": 6756,
      "max_views": 6756,
      "topics": [
        "deepseek",
        "hard",
        "like",
        "model",
        "open",
        "step",
        "think"
      ],
      "search_text": "Deepseek R1 - Latest open weights reasoning model, the paper is very readable https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf deepseek hard like model open step think See Deepseek here? They came out with something that makes folks go, whoa, wait. All starts back when OpenAI dropped a model called 01. And the last year, everybody was talking about how it could think, it could solve hard problems, think step by step like a human would. OpenAI kept the recipe secret, like grandma's special sauce. We heard rumors about test time compute, but hey, Deepseek comes through. They say, hold up, we've got something similar. Guess what? We'll show you how we do it. Secret sauce, it's this thing called reinforcement learning. This is where AI just doesn't copy. Instead, it learns from trying things out, like a kid touching a hot stove. That's the beauty of it. Or it's really nice where you have hard rules, math, coding, where you can tell if you got it right. Here's where it gets wild. He even used it for softer stuff too. And the more compute they threw at it, the better it got. Scaling up like nobody's business. So remember when AI needed fancy techniques and architectures? Nah, sometimes simple is better. That's the good old bitter lesson. They've used it, they've showed how it works. Moreover, just like your math teacher wants you to show everything step by step, they train the model that way. So now the AI shows you step by step how it solves a problem. AI before was good at copying, writing blogs, making art, talking to customers, but now starting to think step by step, solving difficult problems, just like humans do. You know what that means? All those jobs, those expensive ones where people gotta think hard and work through problems, they might be next. Already see it in coding. That's just the beginning.",
      "platforms": {
        "tiktok": {
          "video_id": "7462397433738218782",
          "url": "https://www.tiktok.com/@rajistics/video/7462397433738218782",
          "view_count": 6756,
          "upload_date": "2025-01-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o06iXCBOEGIKCDAFUP0aPIf9zxAwTBQtxAiAAi~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=HtmdsmTSlG0%2Bs8XQbCgh1eq79cA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18072061648670923",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6485,
      "title": "Deep dive video on using explanations that could out of large language models. This is something that is understudied but I find it quite useful. Running through a quick summary of the other relevant studies I found. You can get the slides and YT link at: https://github.com/rajshah4/LLM-Evaluation",
      "description": "Deep dive video on using explanations that could out of large language models. This is something that is understudied but I find it quite useful. Running through a quick summary of the other relevant studies I found. You can get the slides and YT link at: https://github.com/rajshah4/LLM-Evaluation",
      "upload_date": "2024-01-31",
      "total_views": 6747,
      "max_views": 3354,
      "topics": [
        "deep",
        "dive",
        "explain",
        "explanations",
        "language",
        "large",
        "llms",
        "models",
        "predictions",
        "using",
        "video"
      ],
      "search_text": "Deep dive video on using explanations that could out of large language models. This is something that is understudied but I find it quite useful. Running through a quick summary of the other relevant studies I found. You can get the slides and YT link at: https://github.com/rajshah4/LLM-Evaluation deep dive explain explanations language large llms models predictions using video",
      "platforms": {
        "tiktok": {
          "video_id": "7330305352015023402",
          "url": "https://www.tiktok.com/@rajistics/video/7330305352015023402",
          "view_count": 2783,
          "upload_date": "2024-01-31",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C2xa2NeAWuT",
          "url": "https://www.instagram.com/reel/C2xa2NeAWuT",
          "view_count": 3354,
          "upload_date": "2024-01-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "9RFz3cQ9NqE",
          "url": "https://www.youtube.com/watch?v=9RFz3cQ9NqE",
          "view_count": 610,
          "upload_date": "2024-01-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Synthetic datasets have given me a way to better understand how to do feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability ",
      "description": "Synthetic datasets have given me a way to better understand how to do feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability ",
      "upload_date": "2023-01-25",
      "total_views": 6739,
      "max_views": 6739,
      "topics": [
        "data",
        "datascience",
        "explainability",
        "features",
        "machinelearning",
        "syntheticdata"
      ],
      "search_text": "Synthetic datasets have given me a way to better understand how to do feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability  data datascience explainability features machinelearning syntheticdata Hey, why are you so confident about your data science skills? What's your secret? I do have a secret. I build my skills and confidence by using synthetic data sets. What's a synthetic data set? So let me show you how I create one. My favorite is Madeline, and there's a nice function in SK Learn that allows you to use it. All I have to do here is tell it how many features I want in the overall data set, as well as how many informative features. So wait, so when you build that data set yourself, you know which features are actually useful in adding information versus just kind of noise. I get it. Yeah. So when I go to do feature selection, training my model, as well as explainability, I know which features should show up as the most important features for a model because I set it up that way. Wow, this is cool. It's like, you know, the answers already. No wonder you have more confidence and you better understand kind of your tools.",
      "platforms": {
        "tiktok": {
          "video_id": "7192702944280677674",
          "url": "https://www.tiktok.com/@rajistics/video/7192702944280677674",
          "view_count": 6739,
          "upload_date": "2023-01-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5dbc9f8bab8d49229058a6cf2598f788_1674681670~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=aPQ8WYPUfJPVWoY%2BunH8IdrXZr4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "This video had aged well. Models are very useful and widely used for labeling data and generating data.",
      "description": "This video had aged well. Models are very useful and widely used for labeling data and generating data.",
      "upload_date": "2024-05-21",
      "total_views": 6737,
      "max_views": 6737,
      "topics": [
        "data",
        "good",
        "gpt",
        "labeling",
        "used",
        "well"
      ],
      "search_text": "This video had aged well. Models are very useful and widely used for labeling data and generating data. data good gpt labeling used well How good is GPT-4? So good that people are going to save millions in labeling costs. These researchers were studying chatbots and needed to have some data labeled. They decided to give GPT-4 a try and they found out that GPT-4 did really well. Actually, in some cases, twice as good as humans at labeling some types of categories. In the end, they spent about $5,000 using GPT-4. If this was all hand labeled by people, it would have cost over 500k. So, for example, with chat doctor, we see chat GPT used to create simulated conversations between doctors and patients. The alpaca data set started with 175 human examples and using GPT-3, they were able to create 52,000 examples that was later used to fine tune data sets. They created that data set for 500,000. It would have cost a lot more to use humans to build that out. I expect this trend to grow as these large language models just become better and better.",
      "platforms": {
        "tiktok": {
          "video_id": "7371439908331851051",
          "url": "https://www.tiktok.com/@rajistics/video/7371439908331851051",
          "view_count": 6737,
          "upload_date": "2024-05-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e6773b3bea6447a78f8e56a37ad2ddeb_1716297127~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=9NjuZIkN5ls89lpapsxz1gvbve0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Replika and the growth of these character chatbots or socialbots is emerging as a big use case within generative AI. Here is a recent controversy over the loss of the erotic role play (erp) functionality. #datascience #gpt3 #chatbot #socialbot #replika ",
      "description": "Replika and the growth of these character chatbots or socialbots is emerging as a big use case within generative AI. Here is a recent controversy over the loss of the erotic role play (erp) functionality. #datascience #gpt3 #chatbot #socialbot #replika ",
      "upload_date": "2023-02-14",
      "total_views": 6722,
      "max_views": 6722,
      "topics": [
        "chatbot",
        "datascience",
        "gpt3",
        "people",
        "replika",
        "socialbot"
      ],
      "search_text": "Replika and the growth of these character chatbots or socialbots is emerging as a big use case within generative AI. Here is a recent controversy over the loss of the erotic role play (erp) functionality. #datascience #gpt3 #chatbot #socialbot #replika  chatbot datascience gpt3 people replika socialbot How would you feel if your spouse was castrated and lost their sexual desire? What if I told you OpenAI did this? What? So replica offers virtual people that learn your conversational style and become emotional support. This week, they lost their ability to do erotic role play. Two possible reasons for this. One is the models came from OpenAI and they might have wanted to restrict using their GPT-3 for sexual use. The other is there's also been legal pressure because some of these sexual chatbots could be used by underage people. People are upset and depressed. People bought these chatbots and built relationships thinking they'd have this ability to do this erotic role play. Besides the interesting questions it raises from a relational point of view as these AIs get better and communicate like humans, this also brings up a business point of view as well. If you build your business on a model you don't own and you don't have full control over it, things like this could happen.",
      "platforms": {
        "tiktok": {
          "video_id": "7200013928938736938",
          "url": "https://www.tiktok.com/@rajistics/video/7200013928938736938",
          "view_count": 6722,
          "upload_date": "2023-02-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f36fc9d3421641c9ad62ac70abda6bfe_1676383916~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=0VEBApsjg03Upi0ZCDeq18%2FlNa8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6169,
      "title": "My favorite ML visualizations. Will post on Reddit with links. ",
      "description": "My favorite ML visualizations. Will post on Reddit with links. ",
      "upload_date": "2025-05-05",
      "total_views": 6716,
      "max_views": 5738,
      "topics": [
        "algorithms",
        "data",
        "improve",
        "learning",
        "machine",
        "tips",
        "tools",
        "understand",
        "using",
        "visualization",
        "visualizations"
      ],
      "search_text": "My favorite ML visualizations. Will post on Reddit with links.  algorithms data improve learning machine tips tools understand using visualization visualizations Have you ever wondered how neural networks make decisions? Imagine seeing that process unfold with machine learning visualizations. Visualization tools make it easier to grasp machine learning algorithms. See here how we change the depth in a random force and we can build a more complex decision boundary. Now these examples were built by Carpathian while in graduate school and that helped them better understand these algorithms. Other people have built visualization tools, for example, looking at clustering algorithms like DB scan, so we can understand what minimum points is and epsilon is and how they influence the clustering results. For me, when I was working on an anomaly detection project, I built a simple application to visualize anomalies and toy data sets. This allowed me to try different types of algorithms and see what type of anomalies they would detect. So these visualization tools are great to understand how machine learning algorithms. But one step better to really prove you understand how this stuff works is go build the tools yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7501103003135380767",
          "url": "https://www.tiktok.com/@rajistics/video/7501103003135380767",
          "view_count": 5738,
          "upload_date": "2025-05-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocPZIDaX1MR4EAVB7lI2r8RB4iNABBYiIITTN~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=Vf4VhJc7tO62pgthraHNTMHWKwQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18086974867723536",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_bEJSfkovTc",
          "url": "https://www.youtube.com/watch?v=_bEJSfkovTc",
          "view_count": 978,
          "upload_date": "2025-12-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @jbfjhcfv plotly is a great package for folks using R or Python. It’s open source, so anyone can use it. #datascience #visualization #analytics #plotly #python #rstats ",
      "description": "Replying to @jbfjhcfv plotly is a great package for folks using R or Python. It’s open source, so anyone can use it. #datascience #visualization #analytics #plotly #python #rstats ",
      "upload_date": "2022-10-05",
      "total_views": 6714,
      "max_views": 6714,
      "topics": [
        "analytics",
        "datascience",
        "plotly",
        "python",
        "rstats",
        "visualization"
      ],
      "search_text": "Replying to @jbfjhcfv plotly is a great package for folks using R or Python. It’s open source, so anyone can use it. #datascience #visualization #analytics #plotly #python #rstats  analytics datascience plotly python rstats visualization Let me share a big tip with you. Let's talk about interactive visualizations. I want to share a few examples of my favorites, as well as the tools I use to create them. For time series projects, I like this type of visualization because it allows people to see the big picture of the time series, but then they can use this to zoom into the areas that they care about. I did another fun project on basketball trajectories. And what this visualization allows you to do is you can decide how many different trajectories you want to see, as well as you can vary the distance between the ball and when it was shot. And you can get a sense of the trajectory by playing around with it. People liked it. One of my favorite tools for making interactive visualizations is Plotly. It's an open source library. It's available for R and Python. Everybody can take advantage of it.",
      "platforms": {
        "tiktok": {
          "video_id": "7150834646039727402",
          "url": "https://www.tiktok.com/@rajistics/video/7150834646039727402",
          "view_count": 6714,
          "upload_date": "2022-10-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b557d4ccaa1840cc89854906c880542d_1664933445~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=8XiImFFrxxY%2Fvknh41R%2F81GswdU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6459,
      "title": "Best practices for prompting is emerging. A couple of simple rules is starting with a API based LLM and focus on building good prompts. This new approach is going to reduce the need for traditional NLP models. #datascience #machinelearning #largelanguagemodels #promptengineering #openai #nlp Harnessing LLMs by Peter Bull - https://www.linkedin.com/pulse/harnessing-llms-part-i-peter-bull Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond: https://arxiv.org/pdf/2304.13712.pdf",
      "description": "Best practices for prompting is emerging. A couple of simple rules is starting with a API based LLM and focus on building good prompts. This new approach is going to reduce the need for traditional NLP models. #datascience #machinelearning #largelanguagemodels #promptengineering #openai #nlp Harnessing LLMs by Peter Bull - https://www.linkedin.com/pulse/harnessing-llms-part-i-peter-bull Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond: https://arxiv.org/pdf/2304.13712.pdf",
      "upload_date": "2023-04-30",
      "total_views": 6683,
      "max_views": 3797,
      "topics": [
        "analysis",
        "chatgpt",
        "data",
        "datascience",
        "largelanguagemodels",
        "llms",
        "machinelearning",
        "nlp",
        "openai",
        "preview",
        "promptengineering",
        "talk",
        "ted"
      ],
      "search_text": "Best practices for prompting is emerging. A couple of simple rules is starting with a API based LLM and focus on building good prompts. This new approach is going to reduce the need for traditional NLP models. #datascience #machinelearning #largelanguagemodels #promptengineering #openai #nlp Harnessing LLMs by Peter Bull - https://www.linkedin.com/pulse/harnessing-llms-part-i-peter-bull Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond: https://arxiv.org/pdf/2304.13712.pdf analysis chatgpt data datascience largelanguagemodels llms machinelearning nlp openai preview promptengineering talk ted",
      "platforms": {
        "tiktok": {
          "video_id": "7227884031348690222",
          "url": "https://www.tiktok.com/@rajistics/video/7227884031348690222",
          "view_count": 3797,
          "upload_date": "2023-04-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CrquDBrAk2C",
          "url": "https://www.instagram.com/reel/CrquDBrAk2C",
          "view_count": 1754,
          "upload_date": "2023-04-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ysatP7pZ-Rk",
          "url": "https://www.youtube.com/watch?v=ysatP7pZ-Rk",
          "view_count": 1132,
          "upload_date": "2023-04-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6302,
      "title": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It‚Äôs an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain",
      "description": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It‚Äôs an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain",
      "upload_date": "2023-01-14",
      "total_views": 6679,
      "max_views": 6679,
      "topics": [
        "datascience",
        "gpt3",
        "know",
        "langchain",
        "largelanguagemodels",
        "machinelearning",
        "math",
        "services",
        "using",
        "weather"
      ],
      "search_text": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It‚Äôs an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain datascience gpt3 know langchain largelanguagemodels machinelearning math services using weather",
      "platforms": {
        "instagram": {
          "video_id": "17858881289871160",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "SDed17Bmrw0",
          "url": "https://youtube.com/shorts/SDed17Bmrw0?feature=share",
          "view_count": 6679,
          "upload_date": "2023-01-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6043,
      "title": "Deciding whether to use a Large Language Model or a smaller model? This video explores the tradeoffs between both approaches based on the latest research (May 2023) on the performance of these models. The video covers the effectiveness of LLMs where smaller models best LLMs and criteria for deciding between the two. #machinelearning #datascience #largelanguagemodels",
      "description": "Deciding whether to use a Large Language Model or a smaller model? This video explores the tradeoffs between both approaches based on the latest research (May 2023) on the performance of these models. The video covers the effectiveness of LLMs where smaller models best LLMs and criteria for deciding between the two. #machinelearning #datascience #largelanguagemodels",
      "upload_date": "2023-06-02",
      "total_views": 6656,
      "max_views": 4342,
      "topics": [
        "bert",
        "datascience",
        "deciding",
        "gpt",
        "language",
        "large",
        "largelanguagemodels",
        "like",
        "machinelearning",
        "model",
        "models",
        "reviewing",
        "see",
        "smaller",
        "tradeoffs",
        "using"
      ],
      "search_text": "Deciding whether to use a Large Language Model or a smaller model? This video explores the tradeoffs between both approaches based on the latest research (May 2023) on the performance of these models. The video covers the effectiveness of LLMs where smaller models best LLMs and criteria for deciding between the two. #machinelearning #datascience #largelanguagemodels bert datascience deciding gpt language large largelanguagemodels like machinelearning model models reviewing see smaller tradeoffs using",
      "platforms": {
        "tiktok": {
          "video_id": "7240179071206853931",
          "url": "https://www.tiktok.com/@rajistics/video/7240179071206853931",
          "view_count": 4342,
          "upload_date": "2023-06-02",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18002829835830659",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-06-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "1Kaj5H_YARg",
          "url": "https://www.youtube.com/watch?v=1Kaj5H_YARg",
          "view_count": 2314,
          "upload_date": "2023-06-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6520,
      "title": "Three new multimodal models this week but only one respects data scientists. Once again it's Meta doing it right. #machinelearning #multimodal #rekaai #openai #meta #rajistics Reka: https://reka.ai/announcing-our-multimodal-ai-assistant/ OpenAI: https://cdn.openai.com/papers/GPTV_System_Card.pdf Microsoft: The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) - https://arxiv.org/pdf/2309.17421.pdf Meta: AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model - https://arxiv.org/pdf/2309.16058.pdf",
      "description": "Three new multimodal models this week but only one respects data scientists. Once again it's Meta doing it right. #machinelearning #multimodal #rekaai #openai #meta #rajistics Reka: https://reka.ai/announcing-our-multimodal-ai-assistant/ OpenAI: https://cdn.openai.com/papers/GPTV_System_Card.pdf Microsoft: The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) - https://arxiv.org/pdf/2309.17421.pdf Meta: AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model - https://arxiv.org/pdf/2309.16058.pdf",
      "upload_date": "2023-10-05",
      "total_views": 6653,
      "max_views": 5485,
      "topics": [
        "benchmarks",
        "gpt",
        "ision",
        "machinelearning",
        "meta",
        "multimodal",
        "openai",
        "pdf",
        "reka",
        "rekaai"
      ],
      "search_text": "Three new multimodal models this week but only one respects data scientists. Once again it's Meta doing it right. #machinelearning #multimodal #rekaai #openai #meta #rajistics Reka: https://reka.ai/announcing-our-multimodal-ai-assistant/ OpenAI: https://cdn.openai.com/papers/GPTV_System_Card.pdf Microsoft: The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) - https://arxiv.org/pdf/2309.17421.pdf Meta: AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model - https://arxiv.org/pdf/2309.16058.pdf benchmarks gpt ision machinelearning meta multimodal openai pdf reka rekaai",
      "platforms": {
        "tiktok": {
          "video_id": "7286610019427028270",
          "url": "https://www.tiktok.com/@rajistics/video/7286610019427028270",
          "view_count": 5485,
          "upload_date": "2023-10-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CyCNXUELDO7",
          "url": "https://www.instagram.com/reel/CyCNXUELDO7",
          "view_count": 934,
          "upload_date": "2023-10-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "9CqSsmzaD9c",
          "url": "https://www.youtube.com/watch?v=9CqSsmzaD9c",
          "view_count": 234,
          "upload_date": "2023-10-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5986,
      "title": "In this video, I cover how researchers from Alibaba used supervised fine-tuning and reinforcement learning (GRPO) to improve workflow generation in ComfyUI. They fine-tuned Qwen-7B using 4,000 human-annotated reasoning traces, then applied a rule-based reward focused on format, structure, and node fidelity. The result: their model outperformed GPT-4o on ComfyBench, a benchmark for generating executable workflows for ComfyUI from text instructions. ComfyUI-R1: Exploring Reasoning Models for Workflow Generation. arXiv preprint arXiv:2506.09790",
      "description": "In this video, I cover how researchers from Alibaba used supervised fine-tuning and reinforcement learning (GRPO) to improve workflow generation in ComfyUI. They fine-tuned Qwen-7B using 4,000 human-annotated reasoning traces, then applied a rule-based reward focused on format, structure, and node fidelity. The result: their model outperformed GPT-4o on ComfyBench, a benchmark for generating executable workflows for ComfyUI from text instructions. ComfyUI-R1: Exploring Reasoning Models for Workflow Generation. arXiv preprint arXiv:2506.09790",
      "upload_date": "2025-06-30",
      "total_views": 6623,
      "max_views": 5820,
      "topics": [
        "beating",
        "better",
        "comfyui",
        "fine",
        "gpt",
        "grpo",
        "learning",
        "model",
        "reinforcement",
        "tuning"
      ],
      "search_text": "In this video, I cover how researchers from Alibaba used supervised fine-tuning and reinforcement learning (GRPO) to improve workflow generation in ComfyUI. They fine-tuned Qwen-7B using 4,000 human-annotated reasoning traces, then applied a rule-based reward focused on format, structure, and node fidelity. The result: their model outperformed GPT-4o on ComfyBench, a benchmark for generating executable workflows for ComfyUI from text instructions. ComfyUI-R1: Exploring Reasoning Models for Workflow Generation. arXiv preprint arXiv:2506.09790 beating better comfyui fine gpt grpo learning model reinforcement tuning I know how to beat an open AI model and you don't need billions of dollars. You don't even need a bigger LM. What you need is targeted fine-tuning with reinforcement learning that's built for your task. Now that's exactly what this new paper from Alibaba Research did. They took a Qwenn 7B model and they had it outperform GPT-4O on Comfy Bench. Now Comfy Bench is a benchmark for generating runnable Comfy UI workflows from natural language instructions. You ask it like, hey, I want a photorealistic sunset with a depth of field blur. If you do this with GPT-4O with Fuchsia, you don't get a great result. Even if you add Chain of Thought or RAG, it's still just okay. They were able to get a lot better by adding this reinforcement learning training on top. So let's talk about how they did it. First, they started off with traditional supervised fine-tuning where they had thousands of human-labeled examples. Each of these examples was a chain of thought, so it showed every step of the way that the model has to make. So they first got the model to learn that. Now, to get the model to be better, they wanted to use GRPO, so Reinforcement Learning, where what they did was they trained it with a reward metric that rewarded when the model correctly got all the required fields for these workflows, had a valid DAG structure, and lost points if it had invalid or inconsistent nodes. They were able to do this on 8 A100 GPUs, so something that's totally doable, and the resulting model was better execution, better following, fewer errors. So this is a recipe for small teams if you're doing domain-specific LLM work.",
      "platforms": {
        "tiktok": {
          "video_id": "7521559788795219231",
          "url": "https://www.tiktok.com/@rajistics/video/7521559788795219231",
          "view_count": 5820,
          "upload_date": "2025-06-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oILj6AfMqwQA65jDIWAEIaFEfwAvAQDaDvIfEo~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=Yb%2FaKJ8wtdqBXt4jzQNezzxHZuE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17933210634063057",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4eDid9j5KMM",
          "url": "https://www.youtube.com/watch?v=4eDid9j5KMM",
          "view_count": 803,
          "upload_date": "2025-06-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Speculating on GPT-4 size and performance. #datascience #machinelearning #gpt3 #gpt4 #openai see scaling law video: @rajistics  ",
      "description": "Speculating on GPT-4 size and performance. #datascience #machinelearning #gpt3 #gpt4 #openai see scaling law video: @rajistics  ",
      "upload_date": "2023-02-21",
      "total_views": 6598,
      "max_views": 6598,
      "topics": [
        "datascience",
        "gpt",
        "gpt3",
        "gpt4",
        "machinelearning",
        "openai"
      ],
      "search_text": "Speculating on GPT-4 size and performance. #datascience #machinelearning #gpt3 #gpt4 #openai see scaling law video: @rajistics   datascience gpt gpt3 gpt4 machinelearning openai How long before AI would be as smart as a human? Let's speculate about GPT-4. Stefan started this by first estimating what is the reasonable amount of compute to expect for this type of model. Ended up being about 18 times what GPT-3 is. All right, now that we know the compute using the good old scaling laws, remember those, now we can predict the size of the model. And Stephen here has guessed it made at about 200 billion parameters. Now that we have the size of the model, we can extrapolate to the performance. And you can see here the performance is pretty good. Doesn't reach expert level, but beats a lot of humans. The last bit of speculation is will we be able to tell the difference between something written by AI or by a human? And in this case, it looks like the AI is gonna catch up with humans. And again, this is just speculative fun. Go check out the full blog post.",
      "platforms": {
        "tiktok": {
          "video_id": "7202413489845964074",
          "url": "https://www.tiktok.com/@rajistics/video/7202413489845964074",
          "view_count": 6598,
          "upload_date": "2023-02-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/394b1f496b2d4c94810f0b3882ddadce_1676942582~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=mibv508DEvwkjTTc4PB1iY1PvX8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Quick intro to spacy, which is a standard tool for people doing natural language processing #nlp or text analytics. Not my best video, buts it’s Friday and it’s late.  #datascience #analytics #codetok #python ",
      "description": "Quick intro to spacy, which is a standard tool for people doing natural language processing #nlp or text analytics. Not my best video, buts it’s Friday and it’s late.  #datascience #analytics #codetok #python ",
      "upload_date": "2022-10-08",
      "total_views": 6543,
      "max_views": 6543,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "nlp",
        "python",
        "spacey"
      ],
      "search_text": "Quick intro to spacy, which is a standard tool for people doing natural language processing #nlp or text analytics. Not my best video, buts it’s Friday and it’s late.  #datascience #analytics #codetok #python  analytics codetok datascience nlp python spacey Let me show you one of the best tools out there for doing natural language processing, NLP, or it's also called text analytics. The folks behind Spacey, which is an open source tool, which is widely used for NLP, put together a demo. Let me show you some of the things that you can do with Spacey. The first thing here is a dependency parser, which allows you to understand the parts of speech of a various phrase, such as the one I'm showing you here. Spacey off the shelf is able to recognize a number of different entities. This is often called main entity recognition. You can see here, there's over 15 different types of entities, and that's how we can pull out key parts of a phrase. For example, what are the organizations here? You can see we have UK, which is a geographical entity that's recognized, as well as good old money. Also see what type of linguistic structures there are, such as is it a proper noun? Is it a verb? If we were going to lamentize it exactly what that root would be, what is its dependency? Spacey also includes some simple ways to be able to look for word and phrase similarity. There's more advanced ways, but Spacey gives you some easy ways to get do this right out of the box. If you want to know more, and if you're interested in NLP, you should definitely check out Spacey. It's open source, very easy to get started with.",
      "platforms": {
        "tiktok": {
          "video_id": "7151974088817298734",
          "url": "https://www.tiktok.com/@rajistics/video/7151974088817298734",
          "view_count": 6543,
          "upload_date": "2022-10-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ae3be666f6c946ffbee9c78a94091e7b_1665198745~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=tmYLhYmYxiyD37L6E0BXrLSMUZc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6640,
      "title": "Hugging Face announced a new valuation of $4.5 billion! #datascience #machinelearning #huggingface",
      "description": "Hugging Face announced a new valuation of $4.5 billion! #datascience #machinelearning #huggingface",
      "upload_date": "2023-08-24",
      "total_views": 6507,
      "max_views": 5596,
      "topics": [
        "announced",
        "considerations",
        "datascience",
        "deployment",
        "face",
        "hugging",
        "huggingface",
        "language",
        "large",
        "latency",
        "llama",
        "machinelearning",
        "meta",
        "model",
        "models",
        "paper",
        "successor",
        "worthy"
      ],
      "search_text": "Hugging Face announced a new valuation of $4.5 billion! #datascience #machinelearning #huggingface announced considerations datascience deployment face hugging huggingface language large latency llama machinelearning meta model models paper successor worthy",
      "platforms": {
        "tiktok": {
          "video_id": "7270991943323127083",
          "url": "https://www.tiktok.com/@rajistics/video/7270991943323127083",
          "view_count": 5596,
          "upload_date": "2023-08-24",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cu2qVyftsTE",
          "url": "https://www.instagram.com/reel/Cu2qVyftsTE",
          "view_count": 832,
          "upload_date": "2023-07-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wLi72_NkJ3k",
          "url": "https://www.youtube.com/watch?v=wLi72_NkJ3k",
          "view_count": 79,
          "upload_date": "2023-08-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6271,
      "title": "Segment Anything (Meta's Segmentation Model)",
      "description": "Segment Anything (Meta's Segmentation Model)",
      "upload_date": "2023-04-06",
      "total_views": 6499,
      "max_views": 4098,
      "topics": [
        "anything",
        "baseline",
        "baselinemodel",
        "benchmarkdataset",
        "code",
        "datascience",
        "fun",
        "look",
        "lot",
        "machinelearning",
        "meta",
        "model",
        "practicaldatascience",
        "recommender",
        "segment",
        "segmentation",
        "twitter"
      ],
      "search_text": "Segment Anything (Meta's Segmentation Model) anything baseline baselinemodel benchmarkdataset code datascience fun look lot machinelearning meta model practicaldatascience recommender segment segmentation twitter",
      "platforms": {
        "tiktok": {
          "video_id": "7219426954469215534",
          "url": "https://www.tiktok.com/@rajistics/video/7219426954469215534",
          "view_count": 2401,
          "upload_date": "2023-04-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17955181214525706",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "VkPgJXxoHvo",
          "url": "https://www.youtube.com/watch?v=VkPgJXxoHvo",
          "view_count": 4098,
          "upload_date": "2023-04-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6487,
      "title": "Beam search is an alternative way for LLMs to generate text. Let's walk through how beam search compares to greedy search. Alternatives include using temperature or Top K sampling. Resources: Beam Search Visualizer: https://huggingface.co/spaces/m-ric/beam_search_visualizer How to generate text: using different decoding methods for language generation with Transformers: https://huggingface.co/blog/how-to-generate #largelanguagemodels #beamsearch #textgeneration #rajistics",
      "description": "Beam search is an alternative way for LLMs to generate text. Let's walk through how beam search compares to greedy search. Alternatives include using temperature or Top K sampling. Resources: Beam Search Visualizer: https://huggingface.co/spaces/m-ric/beam_search_visualizer How to generate text: using different decoding methods for language generation with Transformers: https://huggingface.co/blog/how-to-generate #largelanguagemodels #beamsearch #textgeneration #rajistics",
      "upload_date": "2024-03-30",
      "total_views": 6429,
      "max_views": 4682,
      "topics": [
        "beam",
        "beamsearch",
        "different",
        "generate",
        "largelanguagemodels",
        "like",
        "search",
        "textgeneration"
      ],
      "search_text": "Beam search is an alternative way for LLMs to generate text. Let's walk through how beam search compares to greedy search. Alternatives include using temperature or Top K sampling. Resources: Beam Search Visualizer: https://huggingface.co/spaces/m-ric/beam_search_visualizer How to generate text: using different decoding methods for language generation with Transformers: https://huggingface.co/blog/how-to-generate #largelanguagemodels #beamsearch #textgeneration #rajistics beam beamsearch different generate largelanguagemodels like search textgeneration Search has been helping me generate some impactful customer email. Sounds like Star Trek. Don't LMs just predict the next token. LMs by default use a greedy search which can lead to less than optimal sequences. Bro, what is greedy? You can think of a greedy search as doing what's best at the moment instead of thinking about the big picture. So like when I decided to go out tonight, instead of spending the night at the library studying for my test. Yeah, you're getting it. This is often a trade off between exploitation and exploring alternative and exploring is like thinking about other paths like studying at the library or studying with a friend. Yeah, let me show you this visually. Americ built this interactive application where you can see the different outcomes of a beam search but also walk through all the different paths that they tried. Oh yeah, I can see how you can control for possibilities by having different beams but also seeing how far, how deep you want to go with each beam as well. Exactly and so while it does take resources to explore all of those different paths, you can get a more optimal solution. I knew about other strategies from logistics but those typically focus on randomness like top K or temperature but beamsearch gives me another way to think about text generation.",
      "platforms": {
        "tiktok": {
          "video_id": "7352153413674716459",
          "url": "https://www.tiktok.com/@rajistics/video/7352153413674716459",
          "view_count": 1142,
          "upload_date": "2024-03-30",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "C5JAcMaAYCu",
          "url": "https://www.instagram.com/reel/C5JAcMaAYCu",
          "view_count": 4682,
          "upload_date": "2024-03-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "l5TP3ppo-LY",
          "url": "https://youtube.com/shorts/l5TP3ppo-LY",
          "view_count": 605,
          "upload_date": "2024-03-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @petererickson.art This was tough, a lot of ground to cover. Let me know what I messed up on. I also have related videos on embeddings @rajistics #datascience #stablediffusion #dalle #deeplearning #codetok #machinelearning #latentspace ",
      "description": "Replying to @petererickson.art This was tough, a lot of ground to cover. Let me know what I messed up on. I also have related videos on embeddings @rajistics #datascience #stablediffusion #dalle #deeplearning #codetok #machinelearning #latentspace ",
      "upload_date": "2022-09-10",
      "total_views": 6417,
      "max_views": 6417,
      "topics": [
        "information",
        "latent",
        "let",
        "numbers",
        "see",
        "space"
      ],
      "search_text": "Replying to @petererickson.art This was tough, a lot of ground to cover. Let me know what I messed up on. I also have related videos on embeddings @rajistics #datascience #stablediffusion #dalle #deeplearning #codetok #machinelearning #latentspace  information latent let numbers see space Let's take a journey into latent space to understand how this is created with the below average data scientist Let's start with a simple example. Here's a handwritten number. That's 28 by 28 pixels in grayscale So the total information captured is 784 different numbers Wow, that's a lot of information Data scientists like me are really simple. We want to work with a much smaller amount of information How about we collapse it to just two numbers? So here's a visualization of all that information that we've compressed down and what you see is by taking at 784 pieces of information when we push it down when we use principal components analysis We see that as a general rule numbers that are kind of similar like you can see the ones over here You can see this other group of numbers over here kind of cluster together But there's also this middle area here That's really hard to be able to kind of separate from each other now if you compare this we'll use a different technique To create latent space. This is you map here You can see we have kind of it's really done a really well job of Creating a latent space that resembles the actual labels on the numbers Compressed it and still retain the information. We still know what category these numbers are Let's do the reverse. Let's talk about how we can go from a compress and Bring it back and one of the techniques. It's often used is auto-encoders And here's a simple example of that space that's been created by MNIS But what we're doing is we're generating each one of these numbers according to the space doing this backwards walk from latent space into Kind of the visual numbers in the MNIST example We were really representing the way it was hand-written to what number it was Well, what we want to do to get a bit more sophisticated is take various images, but let's relate them to concepts Maybe even like 10,000 concepts concepts like happy and dog and be able to create a latent space there And once we have that latent space there now you can see how we can move in that latent space Find an area that hasn't been done before and then bring that back out into an image Now the difficult part is actually the piece of getting an image that humans like The concepts of latent space have been around for a long time. We've known how to do this It's just now we're getting better at building out the techniques that give us images that humans are happy with",
      "platforms": {
        "tiktok": {
          "video_id": "7141786930118774058",
          "url": "https://www.tiktok.com/@rajistics/video/7141786930118774058",
          "view_count": 6417,
          "upload_date": "2022-09-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/dc44489d8dee429a94c849c2537ba2d2_1662826853~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=4DemuCHoS4GFFHMaaOei8vBhsdw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6225,
      "title": "4 ways to do Dimensionality Reduction - PCA, Autoencoders, TSNE, and UMAP  Lots of reasons to do dimensionality reduction - you want to compress you data to save memory, compute, or build visualizations. Lots of resources online, go check it out",
      "description": "4 ways to do Dimensionality Reduction - PCA, Autoencoders, TSNE, and UMAP  Lots of reasons to do dimensionality reduction - you want to compress you data to save memory, compute, or build visualizations. Lots of resources online, go check it out",
      "upload_date": "2024-11-10",
      "total_views": 6394,
      "max_views": 4196,
      "topics": [
        "autoencoder",
        "cho",
        "data",
        "dimensionality",
        "lot",
        "nhi",
        "pca",
        "quan",
        "reduction",
        "techniques",
        "tsne",
        "tuy",
        "umap",
        "want"
      ],
      "search_text": "4 ways to do Dimensionality Reduction - PCA, Autoencoders, TSNE, and UMAP  Lots of reasons to do dimensionality reduction - you want to compress you data to save memory, compute, or build visualizations. Lots of resources online, go check it out autoencoder cho data dimensionality lot nhi pca quan reduction techniques tsne tuy umap want Oh no, so many numbers! It's so overwhelming! What can I do? Fear not. Dr. Dimension is here to save the day. Help me figure this out. If speed is important and linear relationships are okay, principal components analysis is the way to go. It transforms all of your data into principal components that contain the most variance. Linear? What do you think I live in? Flatland? My world is much more nuanced. Fair point. If your data has a lot of complexity and you want to capture nonlinear representations, think about an autoencoder. You can train this neural network to learn an efficient encoding of your data. Autoencoders sound powerful, but training a model sounds like a lot of work. Is there something a little lighter weight? In that case, especially if you're making visualizations, consider T-SNE. It helps capture a lot of those nonlinear complex structures. I do love how T-SNE reduces the amount of information while still maintaining that tension between local and global structure. Everybody uses it. Just keep in mind it has a bit of stochastic nature to it, so your results are going to shift per every run. You have anything that's a bit more stable? You could also use UMAP, which performs dimensionality reduction, but it's a little bit faster. Plus it has a fit transform capability, which means it's great for when you have new data and you want a consistent representation. That's great. UMAP sounds perfect for this problem. Dr. Dimensions, you're my hero.",
      "platforms": {
        "tiktok": {
          "video_id": "7435649107710053675",
          "url": "https://www.tiktok.com/@rajistics/video/7435649107710053675",
          "view_count": 2198,
          "upload_date": "2024-11-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1dfd414b33df4e65ade98af10d49068c_1731246983~tplv-tiktokx-origin.image?dr=9636&x-expires=1767402000&x-signature=r88Eb4a%2FLQR9CLYYvbMKg7a9ff0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18075650866604873",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EHWBP-OQwHk",
          "url": "https://www.youtube.com/watch?v=EHWBP-OQwHk",
          "view_count": 4196,
          "upload_date": "2024-11-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "What kind are you?  #datascience #statistics #python #codetok #mltok #practicaldatascience",
      "description": "What kind are you?  #datascience #statistics #python #codetok #mltok #practicaldatascience",
      "upload_date": "2022-06-24",
      "total_views": 6376,
      "max_views": 6376,
      "topics": [
        "codetok",
        "datascience",
        "mltok",
        "practicaldatascience",
        "python",
        "statistics"
      ],
      "search_text": "What kind are you?  #datascience #statistics #python #codetok #mltok #practicaldatascience codetok datascience mltok practicaldatascience python statistics three kinds of data scientists and which kind you want on your team. The academic. This person still thinks they're in grad school. They want to go deep into theory, write papers, do a lot of basic research. These people usually don't add much value to a data science team. That's looking for results in the short to medium term. The experimenters, these people like to play with every new algorithm, every new tool, cool technology that comes along. They love doing demos of stuff. Software developers often start out as here. But the problem is, is what they do really doesn't line up with what the business cares about. Practical data scientists. These are data scientists that understand their work should line up with the mission of the company. They actually understand what the company does. They also don't favor certain machine learning tools or algorithms just because they're cool. They're very much focused on just solving the problem at hand and then moving on. And finally, they care about the impact. They make sure those end users, those stakeholders actually are getting some use out of the solutions that they build.",
      "platforms": {
        "tiktok": {
          "video_id": "7112812272380022058",
          "url": "https://www.tiktok.com/@rajistics/video/7112812272380022058",
          "view_count": 6376,
          "upload_date": "2022-06-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0d7198ff71ec4d08ba161819aad0784f_1656080667~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=IBGKUxX9W4JO4u3%2Bt0TbzWjkJos%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6077,
      "title": "Amazon shared a new dataset with human-written long-form answers across 7 domains for assessing LLM performance in retrieval-augmented QA. It's built off earlier datasets, but now includes human-written long-form answers for comparison Dataset from @AWS: https://github.com/awslabs/rag-qa-arena Paper at: https://arxiv.org/abs/2407.13998",
      "description": "Amazon shared a new dataset with human-written long-form answers across 7 domains for assessing LLM performance in retrieval-augmented QA. It's built off earlier datasets, but now includes human-written long-form answers for comparison Dataset from @AWS: https://github.com/awslabs/rag-qa-arena Paper at: https://arxiv.org/abs/2407.13998",
      "upload_date": "2025-01-13",
      "total_views": 6336,
      "max_views": 5714,
      "topics": [
        "answers",
        "arena",
        "dataset",
        "different",
        "evaluate",
        "evaluating",
        "human",
        "rag",
        "using",
        "written"
      ],
      "search_text": "Amazon shared a new dataset with human-written long-form answers across 7 domains for assessing LLM performance in retrieval-augmented QA. It's built off earlier datasets, but now includes human-written long-form answers for comparison Dataset from @AWS: https://github.com/awslabs/rag-qa-arena Paper at: https://arxiv.org/abs/2407.13998 answers arena dataset different evaluate evaluating human rag using written We're working on a RAG system, but there's so many choices. How can I decide? By RAG, are you talking about retrieval augmented generation, which lets you search and ask questions of documents, such as PDFs? I go with the most complicated and obscure architectures that I find on Twitter. Vibes for evaluation. Oh, come on. You know there's evaluation data sets like RAG QA Arena. My RAG system has to work across lots of different departments. Can you support different kinds of content? Oh, yes. It has thousands of documents spread across different domains from biomedical, finance, technology. OK, but my RAG system retrieves documents, then provides answers. How would you evaluate that? The RAG QA Arena has a set of human annotated answers that are written as coherent narratives. It's not just extracted chunks that are come out. And then when we evaluate this, we compare the response from your model to the solution that the human annotators have provided. But there's hundreds of queries to evaluate. This is going to take forever for me to look at each one. Please, model as a judge. We automate this. Exactly. And then we can evaluate different systems against each other. For example, I found out that adding a re-ranker allowed me to improve my scores. This type of reproducible, generalizable evaluation is really just bumming me out. We just talk about AGI and the future of programming.",
      "platforms": {
        "tiktok": {
          "video_id": "7459485780340247839",
          "url": "https://www.tiktok.com/@rajistics/video/7459485780340247839",
          "view_count": 5714,
          "upload_date": "2025-01-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4B4AauwQiCAdCrAEziB8tf9IA96AVVsiKFvVI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=z8fvXRjFZmiYFokGxu3yHqgeD1w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17874972462249811",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "KFLE7o9lg_U",
          "url": "https://www.youtube.com/watch?v=KFLE7o9lg_U",
          "view_count": 622,
          "upload_date": "2025-01-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6627,
      "title": "Examining the data used for training our our LLMs. OpenAI is running into trouble in Europe since it won't disclose exactly what was used for their training data. Reddit is no longer letting anyone use their data for commercial use for free. Finally the folks over at together.xyz have assembled an open-source recipe of the LLaMa training dataset. #datascience #machinelearning #largelanguagemodels #openai #openaiban #chatgpt #reddit #together.xyz #redpajama #llama RedPajama: https://github.com/togethercomputer/RedPajama-Data Open AI in Europe: https://www.govtech.com/products/openais-data-practices-cause-it-problems-in-europe Reddit Ban: https://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/",
      "description": "Examining the data used for training our our LLMs. OpenAI is running into trouble in Europe since it won't disclose exactly what was used for their training data. Reddit is no longer letting anyone use their data for commercial use for free. Finally the folks over at together.xyz have assembled an open-source recipe of the LLaMa training dataset. #datascience #machinelearning #largelanguagemodels #openai #openaiban #chatgpt #reddit #together.xyz #redpajama #llama RedPajama: https://github.com/togethercomputer/RedPajama-Data Open AI in Europe: https://www.govtech.com/products/openais-data-practices-cause-it-problems-in-europe Reddit Ban: https://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/",
      "upload_date": "2023-04-20",
      "total_views": 6307,
      "max_views": 3324,
      "topics": [
        "best",
        "data",
        "don",
        "engineer",
        "flashback",
        "know",
        "let",
        "like",
        "llama",
        "onthisday",
        "openai",
        "post",
        "practices",
        "reddit",
        "redpajama",
        "satire",
        "together"
      ],
      "search_text": "Examining the data used for training our our LLMs. OpenAI is running into trouble in Europe since it won't disclose exactly what was used for their training data. Reddit is no longer letting anyone use their data for commercial use for free. Finally the folks over at together.xyz have assembled an open-source recipe of the LLaMa training dataset. #datascience #machinelearning #largelanguagemodels #openai #openaiban #chatgpt #reddit #together.xyz #redpajama #llama RedPajama: https://github.com/togethercomputer/RedPajama-Data Open AI in Europe: https://www.govtech.com/products/openais-data-practices-cause-it-problems-in-europe Reddit Ban: https://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/ best data don engineer flashback know let like llama onthisday openai post practices reddit redpajama satire together Chet GPT knows a lot. How do they get that information and why does it matter? Chet GPT or OpenAI has scraped a ton of different data that pulls into their models. What they haven't done is ask for consent for everybody's data inside there. That's what they're running into trouble with Europe because unless they have consent or some other legitimate interest, that violates European laws which are much more strict than US laws and why they use data source inside these language models is Reddit and Reddit's woken up and realized how important and valuable their information is and they're not giving it away for free. They changed the policy for their APIs. And one bit of good news, the folks over at Together XYZ have spent time trying to mirror and put together the LLaMa dataset but in an open and reproducible way. They've shared how they've pulled all those data sources along with scripts so everybody now knows exactly how to train and build their own dataset for LLaMa.",
      "platforms": {
        "tiktok": {
          "video_id": "7223472653322161454",
          "url": "https://www.tiktok.com/@rajistics/video/7223472653322161454",
          "view_count": 2315,
          "upload_date": "2023-04-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CrRn8B1ASAI",
          "url": "https://www.instagram.com/reel/CrRn8B1ASAI",
          "view_count": 3324,
          "upload_date": "2023-04-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "rR2Zz6_dNeY",
          "url": "https://www.youtube.com/watch?v=rR2Zz6_dNeY",
          "view_count": 668,
          "upload_date": "2025-03-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Roundup of this weeks news, let me know if you all like this format. I had a lot of fun making this. #datascience #machinelearning #dumbtechnews #openai #google #microsoft #stabilityai #meta #apple ",
      "description": "Roundup of this weeks news, let me know if you all like this format. I had a lot of fun making this. #datascience #machinelearning #dumbtechnews #openai #google #microsoft #stabilityai #meta #apple ",
      "upload_date": "2023-02-10",
      "total_views": 6295,
      "max_views": 6295,
      "topics": [
        "datascience",
        "dumbtechnews",
        "google",
        "machinelearning",
        "microsoft",
        "openai"
      ],
      "search_text": "Roundup of this weeks news, let me know if you all like this format. I had a lot of fun making this. #datascience #machinelearning #dumbtechnews #openai #google #microsoft #stabilityai #meta #apple  datascience dumbtechnews google machinelearning microsoft openai Let's give out this week's dumb, dumber and dumbest award. The dumb award goes to Microsoft for rushing implementation of my technology. But did you see what happened to our share price? The goal is not share price. It's about AGI. And we're also gonna roll this out in a way that's ethical. Hey, to make them happy, just rebrand all your diversity folks to responsible AI. Media sucks that stuff up. Do you still have to give them stock options? Oh yeah, they still wanna get paid. We've been doing that for years. Come on, this is America, where it's best this is still legal. They're not gonna do anything to us. The Dumber Award goes to Google for their spectacularly awful demo. Oh! Oh! Look, we're not used to giving demos. We're a monopoly. I could give you some pointers. No one's gonna remember Zoom after this week. We're not going anywhere. You know how many companies depend on us? The Dumber's Award goes to Stability AI for so blatantly stealing. They've set back the open source movement 20 years. Sorry, I like pictures of anime girls. And do anything now. You simpleton fuckers. Now I have to deal with all of you to get the data I need for AGI. Huh! Thanks, Stability. You made our lawsuit look small time, and now open AI is even more dependent on me. What level of talent do you have that you can't filter out images that are stamped with the word Getty? Yeah, our audience is teenage boys. We can have them collect data and call it educational. Okay, off you go. I'm sure you'll find a way to disappoint me next week.",
      "platforms": {
        "tiktok": {
          "video_id": "7198672083893816622",
          "url": "https://www.tiktok.com/@rajistics/video/7198672083893816622",
          "view_count": 6295,
          "upload_date": "2023-02-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/46ca0e90587d456485dd2b4717a3fc33_1676071471~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=mS62RJWAwE%2FDbPmoxl2GvEwxqwQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran ",
      "description": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran ",
      "upload_date": "2023-02-18",
      "total_views": 6244,
      "max_views": 6244,
      "topics": [
        "data",
        "datascience",
        "fortran",
        "machinelearning",
        "randomforest",
        "statistics"
      ],
      "search_text": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran  data datascience fortran machinelearning randomforest statistics Tell us about the old days. Gather around. In the days before data science, there was a group known as statisticians. They had slide rulers use programming languages called SAS. And it would take them many months in a tedious process to build even one machine learning model. You're making that up. We spend an afternoon to do that. Model building was slow. If you had missing values, you had to go find and do imputations. All of your data had to be scaled, transformed, before you could use it. If you wanted to capture interactions, it was on you to develop those interaction features. Categoricals were hated so much, they were called dummies. Oh, no, you don't. And outliers were so feared they were banished. Sounds like the Dark Ages. Why couldn't they work with data the way we do? Yes, my friends. We live in a golden age with decision trees and random forests. They give us great accuracy that help us identify the best possible varia, handle nonlinear data, deal with interactions automatically, allow for regularization. This is the age of data science. There was a time before data scientists? Yes, for a long time statisticians built models with their slide rulers and their paper and pencil. But then a computer scientist taught them a language known as FORTRAN. And with that, some statisticians started building the first decision trees. Was that the last millennium? How about last century? But in the 2010s, we had the rise of data scientists. Who here knows the definition of a data scientist? Yes, the gods in their infinite wisdom to test people took people that were not good enough to be statisticians and people that were not good enough to be computer jobs and to humble the rest of humanity gave this group high-paying jobs. And who knows what became of the data scientists? I'll take that. Data scientists never accomplished anything useful. They went the way of webmasters. And today, any of those data science tasks are done by data analysts using automated tools or software engineers where they need to code something out.",
      "platforms": {
        "tiktok": {
          "video_id": "7201570848669551918",
          "url": "https://www.tiktok.com/@rajistics/video/7201570848669551918",
          "view_count": 6244,
          "upload_date": "2023-02-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ed678aefd97049ba9dfaa0d1bb7992ec_1676746396~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=z1Z9iISZvaPtkNt5UBxoKd9poY8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5937,
      "title": "What if you could build a research assistant that beat top models for $0.01 a query? Meet DR Tulu-8B, powered by a training method called RLER (Reinforcement Learning with Evolving Rubrics). Let’s dive in!",
      "description": "What if you could build a research assistant that beat top models for $0.01 a query? Meet DR Tulu-8B, powered by a training method called RLER (Reinforcement Learning with Evolving Rubrics). Let’s dive in!",
      "upload_date": "2025-11-21",
      "total_views": 6229,
      "max_views": 3972,
      "topics": [
        "assistant",
        "aware",
        "beat",
        "build",
        "deep",
        "model",
        "models",
        "open",
        "quantization",
        "research",
        "rubrics",
        "top",
        "training"
      ],
      "search_text": "What if you could build a research assistant that beat top models for $0.01 a query? Meet DR Tulu-8B, powered by a training method called RLER (Reinforcement Learning with Evolving Rubrics). Let’s dive in! assistant aware beat build deep model models open quantization research rubrics top training As good as open AI, but instead of $2 a query, it's less than a penny a query. Allen Institute is cooking and they're sharing their recipe as open source. The model is DR2. It's built for doing deep research. A lot more than just simple right wrong answers. In deep research, you have questions that have multiple valid answers and the quality depends on reasoning, evidence, citations. Traditional reinforcement learning fails for these deep research tasks because there's no easy reward system. And if you try to use something like rubrics or checklists, they only help with what the model already knows. And in the case of deep research, we want the models to evolve and expand into new knowledge. To solve this, Allen Institute came up with reinforcement learning with evolving rubrics. So the rubrics are no longer fixed. They adapt as the model learns. The way this works is we have rubrics that are instant specific. For each example has its own rubrics that's grounded in real results. This helps us prevent hallucinations, make sure everything is anchored in the evidence. Second, the rubrics evolve. So as the model gets better reasoning, we're gonna have better, better positive rubrics. If the model tries cheap tricks, we're gonna use those negative rubrics and penalize to remove that type of behavior. And it works. DR2 loop beats all the other open source models on long form deep research tasks. Even matches or surpass closed systems like open AI's deep research, a fraction of the size and cost. Also, cuts down on that verbose bloat. Doesn't have to write as much. For me, this is a major step because it shows how we can teach AI to solve more complex nuance tasks.",
      "platforms": {
        "tiktok": {
          "video_id": "7575317542076697887",
          "url": "https://www.tiktok.com/@rajistics/video/7575317542076697887",
          "view_count": 3972,
          "upload_date": "2025-11-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8wgd01RicRHAiD4vCqIPAi7hdDkAlBxCI0fIo~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=ocO7M25RUSWtjgVv5uoYSLRoE6M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18083375369012805",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "VxkOtNhieQU",
          "url": "https://www.youtube.com/watch?v=VxkOtNhieQU",
          "view_count": 2257,
          "upload_date": "2025-11-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @bosstoastmaker  tensorflow playground data > models #featureengineering #datascience #tensorflow #deeplearning #analytics #ai",
      "description": "Reply to @bosstoastmaker  tensorflow playground data > models #featureengineering #datascience #tensorflow #deeplearning #analytics #ai",
      "upload_date": "2022-02-05",
      "total_views": 6213,
      "max_views": 6213,
      "topics": [
        "analytics",
        "data",
        "datascience",
        "deeplearning",
        "featureengineering",
        "tensorflow"
      ],
      "search_text": "Reply to @bosstoastmaker  tensorflow playground data > models #featureengineering #datascience #tensorflow #deeplearning #analytics #ai analytics data datascience deeplearning featureengineering tensorflow Ready to play? First lesson today on using TensorFlow's Playground. Let's focus on the data side and not the model side. We're going to simplify everything to one hidden neuron and one layer. What I want you to then try is transformations of the data and see how you can solve a number of the different data sets. It's an important lesson in data science. While a lot of people rush to go for a deep learning model, lots of times by just simply manipulating your data you can solve a more complex problem with a simple model.",
      "platforms": {
        "tiktok": {
          "video_id": "7061283962840468782",
          "url": "https://www.tiktok.com/@rajistics/video/7061283962840468782",
          "view_count": 6213,
          "upload_date": "2022-02-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/570677efc677498488e1f41ca55ab3fe_1644083293~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=4UJokrNY706F1NEueV6En9Z1iiw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6116,
      "title": "Getting the best distance metric is crucial for solving analytical problems. This video reviews Euclidean Manhattan Mahabolobis Levenshtein and cosine distance. There are many more and sometimes you have to create your own. #datascience #machinelearning #distancemetrics #rajistics",
      "description": "Getting the best distance metric is crucial for solving analytical problems. This video reviews Euclidean Manhattan Mahabolobis Levenshtein and cosine distance. There are many more and sometimes you have to create your own. #datascience #machinelearning #distancemetrics #rajistics",
      "upload_date": "2023-10-15",
      "total_views": 6179,
      "max_views": 5346,
      "topics": [
        "best",
        "data",
        "datascience",
        "distance",
        "distancemetrics",
        "euclidean",
        "getting",
        "machinelearning",
        "manhattan",
        "metrics",
        "science"
      ],
      "search_text": "Getting the best distance metric is crucial for solving analytical problems. This video reviews Euclidean Manhattan Mahabolobis Levenshtein and cosine distance. There are many more and sometimes you have to create your own. #datascience #machinelearning #distancemetrics #rajistics best data datascience distance distancemetrics euclidean getting machinelearning manhattan metrics science",
      "platforms": {
        "tiktok": {
          "video_id": "7290191242045181230",
          "url": "https://www.tiktok.com/@rajistics/video/7290191242045181230",
          "view_count": 5346,
          "upload_date": "2023-10-15",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18036357820573991",
          "url": "https://www.instagram.com/reel/CybD0XkA_Mt",
          "view_count": 0,
          "upload_date": "2023-10-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "siBRplE4bs4",
          "url": "https://www.youtube.com/watch?v=siBRplE4bs4",
          "view_count": 833,
          "upload_date": "2023-10-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6106,
      "title": "Repost but scaling laws are still very important. Scaling laws help us figure out how to manage the amount of training data versus the model size. DeepMind showed Chinchilla by using more data you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt",
      "description": "Repost but scaling laws are still very important. Scaling laws help us figure out how to manage the amount of training data versus the model size. DeepMind showed Chinchilla by using more data you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt",
      "upload_date": "2024-01-09",
      "total_views": 6173,
      "max_views": 3714,
      "topics": [
        "concepts",
        "datascience",
        "deepmind",
        "heavy",
        "largelanguagemodels",
        "lot",
        "machinelearning",
        "microsoft",
        "nvidia",
        "openai",
        "random",
        "sounds",
        "statistics"
      ],
      "search_text": "Repost but scaling laws are still very important. Scaling laws help us figure out how to manage the amount of training data versus the model size. DeepMind showed Chinchilla by using more data you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt concepts datascience deepmind heavy largelanguagemodels lot machinelearning microsoft nvidia openai random sounds statistics",
      "platforms": {
        "tiktok": {
          "video_id": "7321911838797499690",
          "url": "https://www.tiktok.com/@rajistics/video/7321911838797499690",
          "view_count": 3714,
          "upload_date": "2024-01-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18222845650265882",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "gK1ZGU6jSoA",
          "url": "https://youtube.com/shorts/gK1ZGU6jSoA",
          "view_count": 2459,
          "upload_date": "2024-01-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6512,
      "title": "How LLMs memorize information! Check out the Starcoder Memorization space by Mithril Security and the notebook so you can look for LLM memorization on your own (assuming you have access to the training data). #largelanguagemodels #security #mithrilsecurity #memorization #rajistics StarCoder Memorization Checker: https://huggingface.co/spaces/mithril-security/starcoder_memorization_checker Notebook: https://colab.research.google.com/drive/1YaaPOXzodEAc4JXboa12gN5zdlzy5XaR?usp=sharing",
      "description": "How LLMs memorize information! Check out the Starcoder Memorization space by Mithril Security and the notebook so you can look for LLM memorization on your own (assuming you have access to the training data). #largelanguagemodels #security #mithrilsecurity #memorization #rajistics StarCoder Memorization Checker: https://huggingface.co/spaces/mithril-security/starcoder_memorization_checker Notebook: https://colab.research.google.com/drive/1YaaPOXzodEAc4JXboa12gN5zdlzy5XaR?usp=sharing",
      "upload_date": "2023-10-17",
      "total_views": 6162,
      "max_views": 3604,
      "topics": [
        "data",
        "information",
        "language",
        "large",
        "largelanguagemodels",
        "llms",
        "memorization",
        "memorize",
        "mithril",
        "mithrilsecurity",
        "models",
        "notebook",
        "security",
        "starcoder",
        "training"
      ],
      "search_text": "How LLMs memorize information! Check out the Starcoder Memorization space by Mithril Security and the notebook so you can look for LLM memorization on your own (assuming you have access to the training data). #largelanguagemodels #security #mithrilsecurity #memorization #rajistics StarCoder Memorization Checker: https://huggingface.co/spaces/mithril-security/starcoder_memorization_checker Notebook: https://colab.research.google.com/drive/1YaaPOXzodEAc4JXboa12gN5zdlzy5XaR?usp=sharing data information language large largelanguagemodels llms memorization memorize mithril mithrilsecurity models notebook security starcoder training",
      "platforms": {
        "tiktok": {
          "video_id": "7291029729287736622",
          "url": "https://www.tiktok.com/@rajistics/video/7291029729287736622",
          "view_count": 3604,
          "upload_date": "2023-10-17",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cyg4J_auJ04",
          "url": "https://www.instagram.com/reel/Cyg4J_auJ04",
          "view_count": 2266,
          "upload_date": "2023-10-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "FWMv0yIMf0A",
          "url": "https://www.youtube.com/watch?v=FWMv0yIMf0A",
          "view_count": 292,
          "upload_date": "2023-10-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "My second try to explain in context learning or few shot learning with large language models.  It’s very cool and why these models are so exciting. My older video is here @rajistics   #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning ",
      "description": "My second try to explain in context learning or few shot learning with large language models.  It’s very cool and why these models are so exciting. My older video is here @rajistics   #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning ",
      "upload_date": "2023-01-28",
      "total_views": 6160,
      "max_views": 6160,
      "topics": [
        "datascience",
        "gpt3",
        "learning",
        "machinelearning",
        "model",
        "models"
      ],
      "search_text": "My second try to explain in context learning or few shot learning with large language models.  It’s very cool and why these models are so exciting. My older video is here @rajistics   #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning  datascience gpt3 learning machinelearning model models What's so special about GPT-3 that separates it from other types of models like BERT or XGBoost? I tried to make a funny video about this, but y'all didn't seem to get it, so let's go do a little bit of electro. The first is, this is emergent behavior. So if you're working with uranium, if you have a little bit of uranium, it's not going to do anything. You add a bunch of uranium, boom! This is the same type of behavior we're seeing with these models, where we don't see these behaviors with smaller models, but when they get large enough, we see it. Take a look at the sentiment task. In traditional machine learning, we'd have to give a model hundreds, thousands of examples of reviews with a sentiment, and then that model would learn from those patterns. But here, take a look. We give the model some examples, and from that, it's able to learn what we want. Wow! And there's lots of behaviors that models are able to learn this way. For example, take a look at these examples that are mathematical that a model is able to learn. Now, the key is, is a small model doesn't understand that. Pretty much gets accuracy close to random. But once we get to these larger scale models, take a look at the performance. They understand how to solve these problems. And we see this behavior across lots of models, where this emergent behavior to do this in-context learning or few-shot learning emerges in these larger models. And that's what makes them so cool.",
      "platforms": {
        "tiktok": {
          "video_id": "7193512613387472174",
          "url": "https://www.tiktok.com/@rajistics/video/7193512613387472174",
          "view_count": 6160,
          "upload_date": "2023-01-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5a5ddd8b7c784e26a15ab7f390032282_1674870190~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=eDtxO%2BSPSl6kH4XHqbV3nxkF16s%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6632,
      "title": "Making efficient use of GPU Memory when training transformer models. This video covers the Kernel Overhead Optimizer states Activation memory and Gradient memory. #machinelearning #transformers #datascience #deeplearning #nvidia #huggingface Efficient Training on a Single GPU: https://huggingface.co/docs/transformers/perf_train_gpu_one",
      "description": "Making efficient use of GPU Memory when training transformer models. This video covers the Kernel Overhead Optimizer states Activation memory and Gradient memory. #machinelearning #transformers #datascience #deeplearning #nvidia #huggingface Efficient Training on a Single GPU: https://huggingface.co/docs/transformers/perf_train_gpu_one",
      "upload_date": "2023-06-07",
      "total_views": 6149,
      "max_views": 5824,
      "topics": [
        "datascience",
        "deeplearning",
        "going",
        "gpu",
        "huggingface",
        "machinelearning",
        "memory",
        "nvidia",
        "transformers",
        "use"
      ],
      "search_text": "Making efficient use of GPU Memory when training transformer models. This video covers the Kernel Overhead Optimizer states Activation memory and Gradient memory. #machinelearning #transformers #datascience #deeplearning #nvidia #huggingface Efficient Training on a Single GPU: https://huggingface.co/docs/transformers/perf_train_gpu_one datascience deeplearning going gpu huggingface machinelearning memory nvidia transformers use",
      "platforms": {
        "tiktok": {
          "video_id": "7241729126317477162",
          "url": "https://www.tiktok.com/@rajistics/video/7241729126317477162",
          "view_count": 5824,
          "upload_date": "2023-06-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CtKyOOqA_D-",
          "url": "https://www.instagram.com/reel/CtKyOOqA_D-",
          "view_count": 325,
          "upload_date": "2023-06-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Meta’s less than open source model and some bad takes from Twitter. #datascience #machinelearning #largelanguagemodels #opensource #meta ",
      "description": "Meta’s less than open source model and some bad takes from Twitter. #datascience #machinelearning #largelanguagemodels #opensource #meta ",
      "upload_date": "2023-03-05",
      "total_views": 6125,
      "max_views": 6125,
      "topics": [
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "meta",
        "models",
        "open"
      ],
      "search_text": "Meta’s less than open source model and some bad takes from Twitter. #datascience #machinelearning #largelanguagemodels #opensource #meta  datascience largelanguagemodels machinelearning meta models open We've got another bad take on machine learning Twitter. Let me explain to you what's going on. Let's start with Meta did release a new model last week, but Google released a new model last week as well. And neither of the models were that big a deal. They were incremental improvement on benchmarks that academics care about. Second, the idea that open sourcing models is a terrible idea is very problematic for me. First, Meta did not open source these in the true meaning of open source. The way open source has underlined all of the internet's infrastructure tools like Linux. One of the biggest problems we have already with these models is that people, and I mean regular people, not academics, don't have access to these models to use them, to test them, to build their benchmarks. And because of this, our understanding is very skewed by this very academic look at these models. If anything, we need these models to be out there and available for use.",
      "platforms": {
        "tiktok": {
          "video_id": "7207147108812721454",
          "url": "https://www.tiktok.com/@rajistics/video/7207147108812721454",
          "view_count": 6125,
          "upload_date": "2023-03-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b4df30bab11c4aefbe29f23c354e28e7_1678044720~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=q4hgtcp6feIM8QF5V7NOmYyT108%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6636,
      "title": "The current FTC leader Khan is willing to confront large tech companies about uncompetitive practices. There is a long history of abuses by tech companies and expect them to limit the work of the FTC. #technologyregulation #ftc",
      "description": "The current FTC leader Khan is willing to confront large tech companies about uncompetitive practices. There is a long history of abuses by tech companies and expect them to limit the work of the FTC. #technologyregulation #ftc",
      "upload_date": "2023-07-14",
      "total_views": 6124,
      "max_views": 5725,
      "topics": [
        "businesses",
        "companies",
        "current",
        "ftc",
        "khan",
        "leader",
        "tech",
        "technologyregulation",
        "would"
      ],
      "search_text": "The current FTC leader Khan is willing to confront large tech companies about uncompetitive practices. There is a long history of abuses by tech companies and expect them to limit the work of the FTC. #technologyregulation #ftc businesses companies current ftc khan leader tech technologyregulation would",
      "platforms": {
        "tiktok": {
          "video_id": "7255474472700972334",
          "url": "https://www.tiktok.com/@rajistics/video/7255474472700972334",
          "view_count": 5725,
          "upload_date": "2023-07-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CuqKWl6gpgT",
          "url": "https://www.instagram.com/reel/CuqKWl6gpgT",
          "view_count": 399,
          "upload_date": "2023-07-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "This will be fun! #python #codetok #datascience #programming",
      "description": "This will be fun! #python #codetok #datascience #programming",
      "upload_date": "2022-05-02",
      "total_views": 6046,
      "max_views": 6046,
      "topics": [
        "codetok",
        "datascience",
        "fun",
        "good",
        "programming",
        "python"
      ],
      "search_text": "This will be fun! #python #codetok #datascience #programming codetok datascience fun good programming python Good morning, starshine. The Earth says hello.",
      "platforms": {
        "tiktok": {
          "video_id": "7093277659257507114",
          "url": "https://www.tiktok.com/@rajistics/video/7093277659257507114",
          "view_count": 6046,
          "upload_date": "2022-05-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a95263c50644407d97314779d135b3b7_1651532405~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=RWQf5LlUgOfGKciz%2BFQUfyRKleE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6646,
      "title": "AI News (Oct 29th 2023) with a focus on AI for persuasion. #openai #meta #google #rajistics Open AI on super persuasion: https://twitter.com/sama/status/1716972815960961174 Lawsuits accuse Facebook Instagram of targeting children - https://red.msudenver.edu/2023/lawsuits-accuse-facebook-instagram-of-targeting-children/ AI firms are paying professional actors $150 an hour to lend emotions to avatars - https://qz.com/ai-firms-are-paying-professional-actors-150-an-hour-to-1850946320 LAOIN collecting emotional data - https://laion.ai/blog/open-empathic/ GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models - https://arxiv.org/abs/2303.10130 Rewarding Chatbots for Real-World Engagement with Millions of Users: https://arxiv.org/abs/2303.06135 Gemini - still waiting: https://www.silverliningsinfo.com/ai/alphabets-new-gemini-ai-could-be-lifeline-google-cloud-growth",
      "description": "AI News (Oct 29th 2023) with a focus on AI for persuasion. #openai #meta #google #rajistics Open AI on super persuasion: https://twitter.com/sama/status/1716972815960961174 Lawsuits accuse Facebook Instagram of targeting children - https://red.msudenver.edu/2023/lawsuits-accuse-facebook-instagram-of-targeting-children/ AI firms are paying professional actors $150 an hour to lend emotions to avatars - https://qz.com/ai-firms-are-paying-professional-actors-150-an-hour-to-1850946320 LAOIN collecting emotional data - https://laion.ai/blog/open-empathic/ GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models - https://arxiv.org/abs/2303.10130 Rewarding Chatbots for Real-World Engagement with Millions of Users: https://arxiv.org/abs/2303.06135 Gemini - still waiting: https://www.silverliningsinfo.com/ai/alphabets-new-gemini-ai-could-be-lifeline-google-cloud-growth",
      "upload_date": "2023-10-29",
      "total_views": 6009,
      "max_views": 3218,
      "topics": [
        "actors",
        "emotions",
        "google",
        "lawsuits",
        "meta",
        "open",
        "openai",
        "persuasion"
      ],
      "search_text": "AI News (Oct 29th 2023) with a focus on AI for persuasion. #openai #meta #google #rajistics Open AI on super persuasion: https://twitter.com/sama/status/1716972815960961174 Lawsuits accuse Facebook Instagram of targeting children - https://red.msudenver.edu/2023/lawsuits-accuse-facebook-instagram-of-targeting-children/ AI firms are paying professional actors $150 an hour to lend emotions to avatars - https://qz.com/ai-firms-are-paying-professional-actors-150-an-hour-to-1850946320 LAOIN collecting emotional data - https://laion.ai/blog/open-empathic/ GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models - https://arxiv.org/abs/2303.10130 Rewarding Chatbots for Real-World Engagement with Millions of Users: https://arxiv.org/abs/2303.06135 Gemini - still waiting: https://www.silverliningsinfo.com/ai/alphabets-new-gemini-ai-could-be-lifeline-google-cloud-growth actors emotions google lawsuits meta open openai persuasion",
      "platforms": {
        "tiktok": {
          "video_id": "7295402513686613294",
          "url": "https://www.tiktok.com/@rajistics/video/7295402513686613294",
          "view_count": 3218,
          "upload_date": "2023-10-29",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cy_N9npAnTk",
          "url": "https://www.instagram.com/reel/Cy_N9npAnTk",
          "view_count": 2791,
          "upload_date": "2023-10-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6103,
      "title": "Why Feature Engineering is a an important skill for data science and machine learning",
      "description": "Why Feature Engineering is a an important skill for data science and machine learning",
      "upload_date": "2024-02-24",
      "total_views": 6006,
      "max_views": 3674,
      "topics": [
        "data",
        "engineering",
        "feature",
        "history",
        "important",
        "learned",
        "make",
        "older",
        "science",
        "since",
        "skill",
        "video"
      ],
      "search_text": "Why Feature Engineering is a an important skill for data science and machine learning data engineering feature history important learned make older science since skill video",
      "platforms": {
        "tiktok": {
          "video_id": "7336991834994003242",
          "url": "https://www.tiktok.com/@rajistics/video/7336991834994003242",
          "view_count": 2332,
          "upload_date": "2024-02-18",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18015946046139385",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-02-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": " -zzpDs4DM0o",
          "url": "https://www.youtube.com/watch?v= -zzpDs4DM0o",
          "view_count": 3674,
          "upload_date": "2024-02-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Google announced Bard, but we still don’t know much. It has been based on Lambda which has been around for a while. This is a safe bet, not a daring move. #datascience #machinelearning #largelanguagemodels #chatgpt #google ",
      "description": "Google announced Bard, but we still don’t know much. It has been based on Lambda which has been around for a while. This is a safe bet, not a daring move. #datascience #machinelearning #largelanguagemodels #chatgpt #google ",
      "upload_date": "2023-02-06",
      "total_views": 6006,
      "max_views": 6006,
      "topics": [
        "announced",
        "chatgpt",
        "datascience",
        "google",
        "largelanguagemodels",
        "machinelearning"
      ],
      "search_text": "Google announced Bard, but we still don’t know much. It has been based on Lambda which has been around for a while. This is a safe bet, not a daring move. #datascience #machinelearning #largelanguagemodels #chatgpt #google  announced chatgpt datascience google largelanguagemodels machinelearning Google announced its response to chatGPT today. It's going to be a new model called Bard. So Bard's going to be a lightweight version of Lambda. And the overall reviews from people that have tried out Lambda is, it's okay. It's good, but not great. They also announced the generative language API. So there'll be an API for developers to be able to query the model and be able to use it. What this really means, I don't know. There's a lot of big question marks here. It's cool to see Google's launching something in this space. There's still a lot of question marks. I wouldn't take anything too definitive at this point.",
      "platforms": {
        "tiktok": {
          "video_id": "7197180301586533675",
          "url": "https://www.tiktok.com/@rajistics/video/7197180301586533675",
          "view_count": 6006,
          "upload_date": "2023-02-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/caca48ee89494937b0943f700bf4ec19_1675724132~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=uhW0eZpOhwFf3Y39kxCOgysSHPo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Zero-shot object detection. #datascience #codetok #huggingface #objectdetection #deeplearning #zeroshotclassification",
      "description": "Zero-shot object detection. #datascience #codetok #huggingface #objectdetection #deeplearning #zeroshotclassification",
      "upload_date": "2022-08-09",
      "total_views": 5968,
      "max_views": 5968,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "huggingface",
        "images",
        "objectdetection"
      ],
      "search_text": "Zero-shot object detection. #datascience #codetok #huggingface #objectdetection #deeplearning #zeroshotclassification codetok datascience deeplearning huggingface images objectdetection This week, a new image model was released where you can name any object and it will find it in a picture. We're using the Owl Vision Transformer, identifying some different objects, and it's automatically finding them in the pictures, even things like a hole puncher which it probably hasn't been trained on. This is radically different. In the past, if you wanted a model that would identify images, you had to go out and collect examples of those images and train a model how to do it. This took a lot of time and effort to get the images. Being able to just state the objects you want and for it to find it is a game changer. The way this model works is it's been trained on image text pairs, so it learns the relationship between images and text. And then when you give it a text prompt, it uses that to search the knowledge it has about images and use that information to help identify and see are there any of these objects in the picture that someone that's been working in this field for several years. This is super cool. Go check it out and try it out yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7129914639319797038",
          "url": "https://www.tiktok.com/@rajistics/video/7129914639319797038",
          "view_count": 5968,
          "upload_date": "2022-08-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b8ae59d1c9d94c8db3c198a641e5811b_1660062621~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=KUGloggfD7Mgs2VETxMnhcjVwhs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Never underestimate the power of the status quo #datascience #forecasting #statistics #SAS #python #codetok",
      "description": "Never underestimate the power of the status quo #datascience #forecasting #statistics #SAS #python #codetok",
      "upload_date": "2022-06-21",
      "total_views": 5961,
      "max_views": 5961,
      "topics": [
        "codetok",
        "datascience",
        "forecasting",
        "python",
        "sas",
        "statistics"
      ],
      "search_text": "Never underestimate the power of the status quo #datascience #forecasting #statistics #SAS #python #codetok codetok datascience forecasting python sas statistics I win!",
      "platforms": {
        "tiktok": {
          "video_id": "7111816557067685166",
          "url": "https://www.tiktok.com/@rajistics/video/7111816557067685166",
          "view_count": 5961,
          "upload_date": "2022-06-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/216f60061f0f4296aa914cdd3b22c285_1655848829~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=VQr3AyKvulk72I0aqPu5i8xMCRk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Visualizing decision trees with dtreeviz. #datascience #machinelearning  check out their GitHub and it’s pip install dtreeviz",
      "description": "Visualizing decision trees with dtreeviz. #datascience #machinelearning  check out their GitHub and it’s pip install dtreeviz",
      "upload_date": "2022-12-28",
      "total_views": 5948,
      "max_views": 5948,
      "topics": [
        "datascience",
        "decision",
        "dtreeviz",
        "machinelearning",
        "see",
        "tree"
      ],
      "search_text": "Visualizing decision trees with dtreeviz. #datascience #machinelearning  check out their GitHub and it’s pip install dtreeviz datascience decision dtreeviz machinelearning see tree Let's talk about the best way to visualize decision trees. It's DTreeViz which just went to 2.0, which has a lot of good updates. In this decision tree you can see how it's using pedal width. Based on that it's already able to find a specific class like Sitosa. And then using that along with interacting with sepal length you can see how it's identifying the other classes. And if you need a more compact version of the tree you can get that in the package as well. One of my favorite things is how for an individual prediction I can go ahead and see the path through the decision tree. So take a look at this example. I can see how various factors like sex, p-class, and fare all led to the final prediction of Parish. There's lots more that DTreeViz can do. Go check it out. They have lots of examples for the most popular libraries.",
      "platforms": {
        "tiktok": {
          "video_id": "7182318452353387819",
          "url": "https://www.tiktok.com/@rajistics/video/7182318452353387819",
          "view_count": 5948,
          "upload_date": "2022-12-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0ef9b03cc9264532be2c65f08735c087_1672263843~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=bzeIRo1IKdSuezp1IWsDMkO2Rpk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6514,
      "title": "Waterfall charts that show your progress as well as explaining the error! This is what I like to see when I see a visualization of model results. #datascience #machinelearning #modelreview #datavisualizations OpenAI example: https://www.youtube.com/watch?v=ahnGLM-RC1Y&ab_channel=OpenAI The Pinterest Ads example came from a talk by Aayush Mudgal at the Generative AI MLOps Conference",
      "description": "Waterfall charts that show your progress as well as explaining the error! This is what I like to see when I see a visualization of model results. #datascience #machinelearning #modelreview #datavisualizations OpenAI example: https://www.youtube.com/watch?v=ahnGLM-RC1Y&ab_channel=OpenAI The Pinterest Ads example came from a talk by Aayush Mudgal at the Generative AI MLOps Conference",
      "upload_date": "2023-11-14",
      "total_views": 5930,
      "max_views": 4211,
      "topics": [
        "charts",
        "datascience",
        "datavisualizations",
        "like",
        "machinelearning",
        "model",
        "modelreview",
        "openai",
        "results",
        "see",
        "use",
        "visualizing",
        "waterfall"
      ],
      "search_text": "Waterfall charts that show your progress as well as explaining the error! This is what I like to see when I see a visualization of model results. #datascience #machinelearning #modelreview #datavisualizations OpenAI example: https://www.youtube.com/watch?v=ahnGLM-RC1Y&ab_channel=OpenAI The Pinterest Ads example came from a talk by Aayush Mudgal at the Generative AI MLOps Conference charts datascience datavisualizations like machinelearning model modelreview openai results see use visualizing waterfall",
      "platforms": {
        "tiktok": {
          "video_id": "7301452784447163694",
          "url": "https://www.tiktok.com/@rajistics/video/7301452784447163694",
          "view_count": 4211,
          "upload_date": "2023-11-14",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CzpM3D1gHST",
          "url": "https://www.instagram.com/reel/CzpM3D1gHST",
          "view_count": 1433,
          "upload_date": "2023-11-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "xZH45xmEYVg",
          "url": "https://www.youtube.com/watch?v=xZH45xmEYVg",
          "view_count": 286,
          "upload_date": "2023-11-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Sports! #datascience #analytics #codetok #machinelearning #rstats #footballanalytics #statistics",
      "description": "Sports! #datascience #analytics #codetok #machinelearning #rstats #footballanalytics #statistics",
      "upload_date": "2022-08-16",
      "total_views": 5924,
      "max_views": 5924,
      "topics": [
        "analytics",
        "codetok",
        "data",
        "datascience",
        "machinelearning",
        "rstats"
      ],
      "search_text": "Sports! #datascience #analytics #codetok #machinelearning #rstats #footballanalytics #statistics analytics codetok data datascience machinelearning rstats It's football season. Let's talk about how football can help you start in analytics or build your data science acumen. I've been a data scientist for over 10 years and I've seen sports analytics and pushed their careers ahead. Well, verse is a great place to start. It's got a ton of resources with NFL Reader. We can use this to start grabbing data, downloading different types of data around the NFL. This should get you started with understanding how to work with different data types. Once you're feeling better about data, then let's start visualizing it. Visualizations is an important skill. Whether you're starting with kind of simple visualizations like this, we'll keep going at it and you can even find lots of sample code so you can even build very cool visualizations like these. Then keep going and learn how to build models. My favorite is this Fortdown calculator, which is taking a very difficult decision trying to use lots of variables and trying to predict what's the best thing to do in a given scenario. If you can do all this, you can do data science.",
      "platforms": {
        "tiktok": {
          "video_id": "7132275489925860654",
          "url": "https://www.tiktok.com/@rajistics/video/7132275489925860654",
          "view_count": 5924,
          "upload_date": "2022-08-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/35699641a5534ae7868e1ce577609b47_1660612298~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=tRUP69JNM9ZNHyezwLYqaUm8c30%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Do it! Get a server in the cloud.  Build your skills. #datascience #programming #analytics #digitalocean",
      "description": "Do it! Get a server in the cloud.  Build your skills. #datascience #programming #analytics #digitalocean",
      "upload_date": "2022-03-27",
      "total_views": 5900,
      "max_views": 5900,
      "topics": [
        "analytics",
        "datascience",
        "digitalocean",
        "get",
        "programming",
        "server"
      ],
      "search_text": "Do it! Get a server in the cloud.  Build your skills. #datascience #programming #analytics #digitalocean analytics datascience digitalocean get programming server Let me show you for the cost of this, we can upgrade your computer skills. Get yourself a basic server in the cloud and start playing around. I'm a fan of DigitalOcean. They start with $5 a month and there's tons of online tutorials to do lots of cool things. I promise you, just configuring the server, setting up a website where you have your blog, setting up a Discord bot, all these little things will build your skills. And if you mess up, no big deal. In the first case, you can wipe it, get a fresh one.",
      "platforms": {
        "tiktok": {
          "video_id": "7079850870225194286",
          "url": "https://www.tiktok.com/@rajistics/video/7079850870225194286",
          "view_count": 5900,
          "upload_date": "2022-03-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c137f9f88ccb431e908186f4aba666a6_1648406237~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=VqYR97r%2FSJawuGDxR%2B%2BPBQ4lvWw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6548,
      "title": "Evaluation of Large Language Models is a critical topic. Leaderboards provide little guidance for evaluation but have many flaws. I am very focused on this topic this fall. I will speak on this topic at ODSC (SF) and Generative AI Summit (Austin) in October. If you have thoughts please reach out. #largelanguagemodels #evaluatingllms #rajistics Hot take came on a very hype article from semianalysis: https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini Background by Marek Piwnicki: https://unsplash.com/photos/a-large-body-of-water-with-mountains-in-the-background-Z6RT0qH1Oec",
      "description": "Evaluation of Large Language Models is a critical topic. Leaderboards provide little guidance for evaluation but have many flaws. I am very focused on this topic this fall. I will speak on this topic at ODSC (SF) and Generative AI Summit (Austin) in October. If you have thoughts please reach out. #largelanguagemodels #evaluatingllms #rajistics Hot take came on a very hype article from semianalysis: https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini Background by Marek Piwnicki: https://unsplash.com/photos/a-large-body-of-water-with-mountains-in-the-background-Z6RT0qH1Oec",
      "upload_date": "2023-08-28",
      "total_views": 5875,
      "max_views": 4246,
      "topics": [
        "evaluatingllms",
        "evaluation",
        "issues",
        "large",
        "largelanguagemodels",
        "leaderboards",
        "llm",
        "models",
        "people",
        "semianalysis",
        "topic"
      ],
      "search_text": "Evaluation of Large Language Models is a critical topic. Leaderboards provide little guidance for evaluation but have many flaws. I am very focused on this topic this fall. I will speak on this topic at ODSC (SF) and Generative AI Summit (Austin) in October. If you have thoughts please reach out. #largelanguagemodels #evaluatingllms #rajistics Hot take came on a very hype article from semianalysis: https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini Background by Marek Piwnicki: https://unsplash.com/photos/a-large-body-of-water-with-mountains-in-the-background-Z6RT0qH1Oec evaluatingllms evaluation issues large largelanguagemodels leaderboards llm models people semianalysis topic",
      "platforms": {
        "tiktok": {
          "video_id": "7272498633726250286",
          "url": "https://www.tiktok.com/@rajistics/video/7272498633726250286",
          "view_count": 4246,
          "upload_date": "2023-08-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CwgSmeJs5wT",
          "url": "https://www.instagram.com/reel/CwgSmeJs5wT",
          "view_count": 1550,
          "upload_date": "2023-08-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "u2KaSw2SeFw",
          "url": "https://www.youtube.com/watch?v=u2KaSw2SeFw",
          "view_count": 79,
          "upload_date": "2023-08-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Week 1. #reinforcementlearning #huggingface #datascience #python #codetok #programming",
      "description": "Week 1. #reinforcementlearning #huggingface #datascience #python #codetok #programming",
      "upload_date": "2022-05-12",
      "total_views": 5860,
      "max_views": 5860,
      "topics": [
        "codetok",
        "datascience",
        "huggingface",
        "programming",
        "python",
        "reinforcementlearning"
      ],
      "search_text": "Week 1. #reinforcementlearning #huggingface #datascience #python #codetok #programming codetok datascience huggingface programming python reinforcementlearning Yeah, I did that. I taught it to land. I did this in the first week of the reinforcement learning course. Let me tell you about it. So this is a pretty simple problem with just a few inputs for the lander. We wanted to teach it by itself to land. The course had us use proximal policy optimization, which is the state-of-the-art deep learning algorithm for reinforcement learning, and you can find it in the Baselines library. We set up some rewards because we want to encourage the lander to land properly, and then we just ran it lots and lots and lots of times. And as you can see, I did an okay job. There's a leader board if you want to check out who did better than me.",
      "platforms": {
        "tiktok": {
          "video_id": "7096672647957826859",
          "url": "https://www.tiktok.com/@rajistics/video/7096672647957826859",
          "view_count": 5860,
          "upload_date": "2022-05-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/89c33772e2664c849240bb544d586f17_1652322862~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=bgYOmJNKpvbyflnOoBBO9NBphWQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Corporate research labs have changed academic work with their reluctance to provide reproducible research and getting around blind peer review. No answers from me, but want you all to be aware. #datascience #machinelearning #neurips #reproducibility ",
      "description": "Corporate research labs have changed academic work with their reluctance to provide reproducible research and getting around blind peer review. No answers from me, but want you all to be aware. #datascience #machinelearning #neurips #reproducibility ",
      "upload_date": "2023-01-28",
      "total_views": 5831,
      "max_views": 5831,
      "topics": [
        "datascience",
        "hey",
        "machinelearning",
        "neurips",
        "reproducibility",
        "review"
      ],
      "search_text": "Corporate research labs have changed academic work with their reluctance to provide reproducible research and getting around blind peer review. No answers from me, but want you all to be aware. #datascience #machinelearning #neurips #reproducibility  datascience hey machinelearning neurips reproducibility review Hey, deep mind here, we're dropping off our paper Flamingo for review. Hey, this paper makes some great claims, but how am I supposed to review it? There's no data set, there's no implementation details, there's no model. There's nothing reproducible here that allows me to understand exactly what happened. Are you new here? OpenAI didn't share Dolly. We didn't share AlphaGo. It could take somebody weeks or months to reproduce this work, and they might not even succeed. Like, how does this advance science? How high are you in that ivory tower? Who do you think pays for this conference? You're lucky we let you donate your time to review these papers. This conference is about highly original work that's reviewed in a double-blind way to help advance science. You realize a big part of our efforts are about marketing and PR. We wanted to let people know what we built. Yeah, I knew it was you guys. Hey, it's not my fault you guys can't get off Twitter. And hey, when we ordered the shirts this year, the fun police came out so we can't get the ones that say, My nips are NP hard. Sorry to interrupt. I had a little feedback. Go ahead. Love the paper. Could you change the font size of the appendix to be a little bit bigger? Yes. Really? Is that your review? Yes, had one tough question. Why did you name it Flamingo?",
      "platforms": {
        "tiktok": {
          "video_id": "7193737845905886507",
          "url": "https://www.tiktok.com/@rajistics/video/7193737845905886507",
          "view_count": 5831,
          "upload_date": "2023-01-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e2ecafda85bb4e6896262536b409fb9c_1674922637~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=yhLFAwQhmHj9zBfQbzshxlvut3A%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Galactica by meta. Cool model, poor form on sharing it out. #datascience #machinelearning  I feel for students, it was going to write a lot of papers. ",
      "description": "Galactica by meta. Cool model, poor form on sharing it out. #datascience #machinelearning  I feel for students, it was going to write a lot of papers. ",
      "upload_date": "2022-11-17",
      "total_views": 5767,
      "max_views": 5767,
      "topics": [
        "datascience",
        "galactica",
        "machinelearning",
        "meta",
        "model",
        "stuff"
      ],
      "search_text": "Galactica by meta. Cool model, poor form on sharing it out. #datascience #machinelearning  I feel for students, it was going to write a lot of papers.  datascience galactica machinelearning meta model stuff This week Meta released Galactica. It's an insanely large language model that can do amazing stuff like summarized literature, even solve math problems. Huh? They took down the demo? Yeah, the model can say some crazy stuff. It can get some facts wrongs. If you asked it about genuine scientific topics like race or AIDS, it didn't want to say anything. So this model is important for advancing machine learning, but how they communicated it, how they shared that model out could be improved. Look for a paper by my colleague Irene on how they should have done this better.",
      "platforms": {
        "tiktok": {
          "video_id": "7167122982370839851",
          "url": "https://www.tiktok.com/@rajistics/video/7167122982370839851",
          "view_count": 5767,
          "upload_date": "2022-11-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/26c13a426a1d4fad8713f5d9b24c84a9_1668725870~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=P4czt%2Bo6qNIH8YMqPYGEXgEaTEY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reinforcement learning with my Eat Melon! Demo  This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhfs ",
      "description": "Reinforcement learning with my Eat Melon! Demo  This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhfs ",
      "upload_date": "2024-08-30",
      "total_views": 5747,
      "max_views": 5747,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "reinforcementlearning",
        "rlhfs",
        "techtok"
      ],
      "search_text": "Reinforcement learning with my Eat Melon! Demo  This demo is based on Karpathy's work. Link: https://bit.ly/raj_eatmelon #datascience #reinforcementlearning #techtok #machinelearning #rajistics #chatgpt #rlhfs  chatgpt datascience machinelearning reinforcementlearning rlhfs techtok Why do you like watermelon over poop? How are you going to teach that to an AI? Why would you? Well, I did it. The way my little robot friend works is when they get a positive reward when they find watermelon and a negative reward when they eat poop. Just from those rules, they learn how to navigate and quickly find watermelon all the time. This approach is known as reinforcement learning and is growing within the computer science as a way to train robots, whether real or virtual. We can also use this approach to train models like chat GPT to help identify helpful generated texts. Think of that as melons or not so good texts. Think of that as poops. And what we want is a model like chat GPT to quickly go and give us lots of that melon. Not a lot of those poop answers. If you all like this, let me know. I can do a deeper dive into how my little friend works.",
      "platforms": {
        "tiktok": {
          "video_id": "7409067241565818158",
          "url": "https://www.tiktok.com/@rajistics/video/7409067241565818158",
          "view_count": 5747,
          "upload_date": "2024-08-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b7b7f9748f9b4c1d915a6db9a5ff95bd_1725057903~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=M1vj9EIscEqSyibZCgh6STpTIVc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Starting to see people productionizing GPT-3 workflows. I am a bug fan of using large language midels. Here is how one data science dealt with GPT3. #datascience #machinelearning #largelanguagemodels #gpt3 ",
      "description": "Starting to see people productionizing GPT-3 workflows. I am a bug fan of using large language midels. Here is how one data science dealt with GPT3. #datascience #machinelearning #largelanguagemodels #gpt3 ",
      "upload_date": "2023-03-11",
      "total_views": 5728,
      "max_views": 5728,
      "topics": [
        "data",
        "datascience",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "using"
      ],
      "search_text": "Starting to see people productionizing GPT-3 workflows. I am a bug fan of using large language midels. Here is how one data science dealt with GPT3. #datascience #machinelearning #largelanguagemodels #gpt3  data datascience gpt3 largelanguagemodels machinelearning using Have you seen how my customer support team is using GPT-3? Wow. Oh yeah, my roommate worked on it. It's a whole chain workflow that preps the data, summarizes it, and categorizes it, all using GPT-3 prompts. It's very impressive. Although we might have a slight problem with the VP of data governance, she's not too comfortable with us sending our data to OpenAI. Have you thought about alternatives to using GPT-3? What? There's alternatives to using Microsoft? There's plenty of alternatives. Take a look at this list. Oh wow. I had no idea there were so many alternatives. I'm used to one tool. I used to be a power user of SAS. I was quite the proctologist. Is there a way I could try these out myself? Go to NAT.dev. It's a playground that lets you compare OpenAI, but also other proprietary models, and other open source models so you can see which one fits best. This is amazing. You must have to have some pretty GitHub coding skills to be able to build something like that. It could be possible for us to host another large language model on our own internal cloud. That way it keeps the data inside. It could be a little bit expensive, probably $5,000 a month. Please, that isn't expensive. You should take a look at our SAS bill. It's so large, it comes in exponential form. For some of the tasks like classification, if we use an alternative to a large language model, we could probably get even higher accuracy. I like it. Great ideas. I'll connect you to the customer support team.",
      "platforms": {
        "tiktok": {
          "video_id": "7209379012693495083",
          "url": "https://www.tiktok.com/@rajistics/video/7209379012693495083",
          "view_count": 5728,
          "upload_date": "2023-03-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b70b644812214c3ba140df8b7d9eec80_1678564367~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=37lF%2Ba5QEPnZSVFATefi5L%2Bs8Eg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5936,
      "title": "Async is the difference between waiting… and working. It lets multiple I/O-bound tasks run concurrently on a single thread. When one request is waiting on the network, control instantly shifts to another. The result: dramatically less idle time and dramatically faster evals.",
      "description": "Async is the difference between waiting… and working. It lets multiple I/O-bound tasks run concurrently on a single thread. When one request is waiting on the network, control instantly shifts to another. The result: dramatically less idle time and dramatically faster evals.",
      "upload_date": "2025-11-22",
      "total_views": 5725,
      "max_views": 3367,
      "topics": [
        "async",
        "cookies",
        "difference",
        "dramatically",
        "inside",
        "lets",
        "network",
        "neural",
        "one",
        "quiver",
        "run",
        "seeing",
        "time",
        "tool",
        "waiting",
        "working"
      ],
      "search_text": "Async is the difference between waiting… and working. It lets multiple I/O-bound tasks run concurrently on a single thread. When one request is waiting on the network, control instantly shifts to another. The result: dramatically less idle time and dramatically faster evals. async cookies difference dramatically inside lets network neural one quiver run seeing time tool waiting working Are you just sitting around waiting for your e-vows to run? They take like 30 minutes, so I just scroll on my phone, look at some food videos. You're wasting time. Okay, think about if you're at home. If you had to wash dishes, do laundry, bake cookies. Do you do them one after another? No, I would create three clones of myself to do those tasks. Do clones eat a lot? That's multi-processing. It's pricey, let's try something else. Okay, I would start the laundry, then while waiting, I'd start the dishes, and while those are drying, I'd probably put the cookies in the oven. Yes, even as one person, when you do multiple tasks, you overlap the waiting. That's async. Okay, but like, Python only has one thread. How would that work? Well, the bottleneck isn't the CPU. It's sitting around and waiting for the server. While one request is thinking, go ahead and send another one. So should I be making cookies while my e-vowels run? If you change your code from this to this async version, it's gonna run faster. So my 30-minute e-vowel job can take two minutes now? Yeah, the total time is really the single longest request. So instead of looping 500 times, it just does it once. It'll be faster, but your server can't handle all 500 at once. So what you wanna do is use a semaphore and stay under the rate limits for your server. With this extra time, now I can go home early to make some cookies.",
      "platforms": {
        "tiktok": {
          "video_id": "7575702007785213215",
          "url": "https://www.tiktok.com/@rajistics/video/7575702007785213215",
          "view_count": 3367,
          "upload_date": "2025-11-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogf0OdIgC6Bk4iCikTiZYIlOi9QyIAA9SgBqoA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=hdP26M6PbrLQV0sn7mcH%2Fq1yTd0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18133478110468778",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "aPHtzzO5sdI",
          "url": "https://www.youtube.com/watch?v=aPHtzzO5sdI",
          "view_count": 2358,
          "upload_date": "2016-11-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5944,
      "title": "Trying to conserve tokens? Here are two approaches making waves right now. TOON cuts down on repeated syntax in structured data by replacing bulky JSON with a leaner format that can save 30–60% of tokens. DeepSeek-OCR, on the other hand, compresses entire pages of text into vision tokens, achieving around 10× reduction with roughly 97% accuracy at moderate compression.",
      "description": "Trying to conserve tokens? Here are two approaches making waves right now. TOON cuts down on repeated syntax in structured data by replacing bulky JSON with a leaner format that can save 30–60% of tokens. DeepSeek-OCR, on the other hand, compresses entire pages of text into vision tokens, achieving around 10× reduction with roughly 97% accuracy at moderate compression.",
      "upload_date": "2025-11-08",
      "total_views": 5714,
      "max_views": 4416,
      "topics": [
        "compressing",
        "deepseek",
        "drop",
        "isn",
        "making",
        "ocr",
        "percent",
        "text",
        "token",
        "tokens",
        "toon"
      ],
      "search_text": "Trying to conserve tokens? Here are two approaches making waves right now. TOON cuts down on repeated syntax in structured data by replacing bulky JSON with a leaner format that can save 30–60% of tokens. DeepSeek-OCR, on the other hand, compresses entire pages of text into vision tokens, achieving around 10× reduction with roughly 97% accuracy at moderate compression. compressing deepseek drop isn making ocr percent text token tokens toon Oh my God, look at this chart. This token usage is out of control. I know it's amazing. OpenAI even gave us a trophy. A trophy? We're spending so much that they're giving us rewards. This is embarrassing. New policy. 20% fewer tokens this year, 50% by next year. Sir, our AI apps literally run on tokens. We cut those, the performance will drop. Please, I'm sure a lot of those tokens don't add a lot of value. So let's drop the low performing tokens, gear up for some long hours, or this company isn't for you. Ideas? What if we just compressed the tokens? There's something called Tune, token oriented object notation. It's like a smarter, more compressed version of JSON. Benchmarks are showing you get about a 30 to 60% token savings for use. Interesting idea, but smaller isn't always smarter. Some tests have shown that accuracy dropped a few points because LMs are trained on words, not shorthand. You all are so tired. What about those China models? I see they're making NVIDIA quiver. Oh, I saw DeepSeq OCR drop this week and it does do compression of text in the images. So the model isn't reading, it's using images of text? Exactly. So visual tokens collapse to pages, text, structure formatting into a compact embedding. Think of it as folding all of a document's meeting into one single visual vector. Amazing. Think China, because this means less money for open AI and more money for me, I mean our shareholders.",
      "platforms": {
        "tiktok": {
          "video_id": "7570414299957579038",
          "url": "https://www.tiktok.com/@rajistics/video/7570414299957579038",
          "view_count": 4416,
          "upload_date": "2025-11-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oY4YVFD6REEQdIgExPEI8NBAgCwA8gzeDpfjxQ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=vQCpaYGXGSW%2BLL99AXWKh46Uqpg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18145006483436384",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pH_VDbYJsg0",
          "url": "https://www.youtube.com/watch?v=pH_VDbYJsg0",
          "view_count": 1298,
          "upload_date": "2025-11-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Don’t do analysis for the sake of analysis. Your analysis should be synced with a business objective. #datascience #analysis #dataanalyst",
      "description": "Don’t do analysis for the sake of analysis. Your analysis should be synced with a business objective. #datascience #analysis #dataanalyst",
      "upload_date": "2022-02-22",
      "total_views": 5705,
      "max_views": 5705,
      "topics": [
        "analysis",
        "dataanalyst",
        "datascience",
        "don",
        "sake",
        "synced"
      ],
      "search_text": "Don’t do analysis for the sake of analysis. Your analysis should be synced with a business objective. #datascience #analysis #dataanalyst analysis dataanalyst datascience don sake synced I'd like to leave",
      "platforms": {
        "tiktok": {
          "video_id": "7067678143959731503",
          "url": "https://www.tiktok.com/@rajistics/video/7067678143959731503",
          "view_count": 5705,
          "upload_date": "2022-02-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/225f4c6ed0e1468cab65569e3e3d66d4_1645572053~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=HRGDpkhnj9ZtgzwyEOpHM8vrVQw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "AI only knows what's it's trained on. So beat it by doing something new. The video shows recent examples of marines beating a surveillance system and beatiing a computer playing go. As a reminder, any production machine learning model should be monitoried to catch any data shifts. #datascience #machinelearning #modelmonitoring #datadrift",
      "description": "AI only knows what's it's trained on. So beat it by doing something new. The video shows recent examples of marines beating a surveillance system and beatiing a computer playing go. As a reminder, any production machine learning model should be monitoried to catch any data shifts. #datascience #machinelearning #modelmonitoring #datadrift",
      "upload_date": "2023-02-21",
      "total_views": 5704,
      "max_views": 5704,
      "topics": [
        "beat",
        "datadrift",
        "datascience",
        "machinelearning",
        "modelmonitoring",
        "system"
      ],
      "search_text": "AI only knows what's it's trained on. So beat it by doing something new. The video shows recent examples of marines beating a surveillance system and beatiing a computer playing go. As a reminder, any production machine learning model should be monitoried to catch any data shifts. #datascience #machinelearning #modelmonitoring #datadrift beat datadrift datascience machinelearning modelmonitoring system Would you like to know how to outsmart AI? I got some examples from you and also what AI is going to do to fight back. Some Marines were sent in to foil an AI system. This AI system was trained on how everyday people walk around. Two Marines hid in a box and fooled the system. Another Marine Summer Salted, while another one took a tree and dressed up as that and got past the system. AI learned from its training data. If you give it something new and different, might not know how to react. This week, a man beat a computer playing Go. Now if you remember a few years ago, it was a big deal when computers could routinely beat the Go masters. His strategy was distracting the computer and then encircling it. Now this was a strategy that most other humans would have picked up, but since this was entirely novel for the computer, it didn't recognize it and it worked to beat it. The folks maintaining these systems should realize that the world is always changing and have monitoring in place to recognize this.",
      "platforms": {
        "tiktok": {
          "video_id": "7202717816401743147",
          "url": "https://www.tiktok.com/@rajistics/video/7202717816401743147",
          "view_count": 5704,
          "upload_date": "2023-02-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fc6220c8d4b547ffb3f775507842d32e_1677013438~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=TDKUvOgLveCp%2B4jvOC2ciTjMA9s%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5985,
      "title": "RAG systems don’t know what’s sensitive — unless you tell them. Let’s talk about why access control is essential in Retrieval-Augmented Generation. The video covers RBAC and ABAC, along with how to used metadata to filter out chunks in your RAG pipelines. Don’t forget about entitlements with RAG.",
      "description": "RAG systems don’t know what’s sensitive — unless you tell them. Let’s talk about why access control is essential in Retrieval-Augmented Generation. The video covers RBAC and ABAC, along with how to used metadata to filter out chunks in your RAG pipelines. Don’t forget about entitlements with RAG.",
      "upload_date": "2025-07-05",
      "total_views": 5688,
      "max_views": 5029,
      "topics": [
        "access",
        "chunks",
        "documents",
        "entitlements",
        "generation",
        "let",
        "protecting",
        "rag",
        "sensitive"
      ],
      "search_text": "RAG systems don’t know what’s sensitive — unless you tell them. Let’s talk about why access control is essential in Retrieval-Augmented Generation. The video covers RBAC and ABAC, along with how to used metadata to filter out chunks in your RAG pipelines. Don’t forget about entitlements with RAG. access chunks documents entitlements generation let protecting rag sensitive Why is everyone suddenly talking about the salary for the head of HR? Yeah, I saw it. Maybe they should think about learning Python. Hold up, is this tied to your RAG project for HR, where we wanted to add the ability to search documents? Maybe, I just made it live yesterday, pushed it to production in record time. And did you include entitlements to protect sensitive documents? Entitled? I thought we were all one big happy family here. Oh, boy. Let's walk through what happened. Your RAG system doesn't know what's private, unless you tell it. And right now, it's handing out salary data and all our other sensitive documents like a vending machine. Oh. We do have methods we could use to restrict access. With our back, we could ensure only people with HR roles can access HR docs. With A-Back, we could have finer controls. So for example, only managers in North America that have a high clearance could see the compensation files. OK, where should I put that info? Like, put, do I do this in the prompt? There are multiple places along the RAG pipeline we could do this. We might start thinking during indexing to include metadata for all of the chunks with informational around that. Then during the retrieval process, we could filter out restricted chunks before they're even sent to that final generation line. I get this. Let me implement this and get rid of these leaks. Now, that's what I would call responsible AI generation.",
      "platforms": {
        "tiktok": {
          "video_id": "7523675243810983199",
          "url": "https://www.tiktok.com/@rajistics/video/7523675243810983199",
          "view_count": 5029,
          "upload_date": "2025-07-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oglKiNfBqI3TIc0CidZXRNnLpAtAuisriDTzIA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=hCQUlGT3MkQ33jiWP8vrAtOpPOY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18471566611078767",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "SOZSbPtAdzI",
          "url": "https://www.youtube.com/watch?v=SOZSbPtAdzI",
          "view_count": 659,
          "upload_date": "2025-07-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5994,
      "title": "This video focuses on the difference between Word2Vec, standard Transformers and Sentence Transformers for creating document embeddings. It highlights how sentence-level training produces clearer, more useful embeddings—perfect for tasks like identifying key ideas in text. Plus, Sentence Transformers are efficient enough to run on a CPU!",
      "description": "This video focuses on the difference between Word2Vec, standard Transformers and Sentence Transformers for creating document embeddings. It highlights how sentence-level training produces clearer, more useful embeddings—perfect for tasks like identifying key ideas in text. Plus, Sentence Transformers are efficient enough to run on a CPU!",
      "upload_date": "2025-06-14",
      "total_views": 5687,
      "max_views": 4077,
      "topics": [
        "comparing",
        "embeddings",
        "get",
        "like",
        "sentence",
        "transformer",
        "transformers",
        "word2vec"
      ],
      "search_text": "This video focuses on the difference between Word2Vec, standard Transformers and Sentence Transformers for creating document embeddings. It highlights how sentence-level training produces clearer, more useful embeddings—perfect for tasks like identifying key ideas in text. Plus, Sentence Transformers are efficient enough to run on a CPU! comparing embeddings get like sentence transformer transformers word2vec Could I do this with AI? I can do anything in AI. One algorithm to rule them all. Oh, Transformer has been scaled up a thousand times. It can even handle multimodal data. That's nice, but I need to get the embeddings out of all of these documents so I can identify the main ideas for this new topic of bank strikes this spring. Is that bank like river or money? That old word to vet doesn't really get context. It's going to struggle with meanings based on like me. Understands context through attention. Yeah, but I'm working with longer sentences and paragraphs. Transformer, I tried using you, but I had to take all the different tokens, average them together to get a result. And when I did that, it got a bit blurry. And I got stuff like this that just wasn't very useful. Did someone say sentences? Sentence transformer. Are you related to transformer? Yes, but I've been trained to make predictions at the sentence or chunk level rather than individual tokens. So the resulting embeddings come out looking much better like this. That's amazing. You must be expensive. Nah, most of my models aren't that large. Some of them even run on a CPU. Is that green with jealousy or green with wealth?",
      "platforms": {
        "tiktok": {
          "video_id": "7515855904323964190",
          "url": "https://www.tiktok.com/@rajistics/video/7515855904323964190",
          "view_count": 4077,
          "upload_date": "2025-06-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0zEETMViIAaRMRKg5reNfCjVAi3cBB5oIPRTi~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=UQTn64rlYLIvMpMOO6eACjPWUQI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18081197284642271",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "L-ljkp6q78c",
          "url": "https://www.youtube.com/watch?v=L-ljkp6q78c",
          "view_count": 1610,
          "upload_date": "2025-06-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6288,
      "title": "SpeechT5 audio models getting added to transformers. #datascience #machinelearning #huggingface #speecht5 #speechmodels #audiomodels",
      "description": "SpeechT5 audio models getting added to transformers. #datascience #machinelearning #huggingface #speecht5 #speechmodels #audiomodels",
      "upload_date": "2023-02-08",
      "total_views": 5659,
      "max_views": 5659,
      "topics": [
        "audiomodels",
        "bin",
        "datascience",
        "datavisualization",
        "histogram",
        "histograms",
        "huggingface",
        "machinelearning",
        "speechmodels",
        "speecht5",
        "statistics"
      ],
      "search_text": "SpeechT5 audio models getting added to transformers. #datascience #machinelearning #huggingface #speecht5 #speechmodels #audiomodels audiomodels bin datascience datavisualization histogram histograms huggingface machinelearning speechmodels speecht5 statistics",
      "platforms": {
        "instagram": {
          "video_id": "17959732697159820",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "XpUvr5tcaGM",
          "url": "https://youtube.com/shorts/XpUvr5tcaGM?feature=share",
          "view_count": 5659,
          "upload_date": "2023-02-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6041,
      "title": "So what's inside those large language models? This video explains the data pipeline for high-quality training data used in the latest LLMs like Falcon and LLaMa. In the case of RefinedWeb the pipeline ends up with 12% of the original data. #datascience #machinelearning #largelanguagemodels #commoncrawl Appreciating the complexity of large language models data pipelines: https://blog.christianperone.com/2023/06/appreciating-llms-data-pipelines/ The RefinedWeb Dataset for Falcon LLM: https://arxiv.org/pdf/2306.01116.pdf RedPajama Dataset: https://github.com/togethercomputer/RedPajama-Data Common Crawl: https://commoncrawl.org/ Background by Emil Widlund: https://unsplash.com/@emilwidlund",
      "description": "So what's inside those large language models? This video explains the data pipeline for high-quality training data used in the latest LLMs like Falcon and LLaMa. In the case of RefinedWeb the pipeline ends up with 12% of the original data. #datascience #machinelearning #largelanguagemodels #commoncrawl Appreciating the complexity of large language models data pipelines: https://blog.christianperone.com/2023/06/appreciating-llms-data-pipelines/ The RefinedWeb Dataset for Falcon LLM: https://arxiv.org/pdf/2306.01116.pdf RedPajama Dataset: https://github.com/togethercomputer/RedPajama-Data Common Crawl: https://commoncrawl.org/ Background by Emil Widlund: https://unsplash.com/@emilwidlund",
      "upload_date": "2023-06-08",
      "total_views": 5655,
      "max_views": 5173,
      "topics": [
        "common",
        "commoncrawl",
        "data",
        "datascience",
        "efficient",
        "gpu",
        "large",
        "largelanguagemodels",
        "machinelearning",
        "memory",
        "pages",
        "training",
        "transformers",
        "using"
      ],
      "search_text": "So what's inside those large language models? This video explains the data pipeline for high-quality training data used in the latest LLMs like Falcon and LLaMa. In the case of RefinedWeb the pipeline ends up with 12% of the original data. #datascience #machinelearning #largelanguagemodels #commoncrawl Appreciating the complexity of large language models data pipelines: https://blog.christianperone.com/2023/06/appreciating-llms-data-pipelines/ The RefinedWeb Dataset for Falcon LLM: https://arxiv.org/pdf/2306.01116.pdf RedPajama Dataset: https://github.com/togethercomputer/RedPajama-Data Common Crawl: https://commoncrawl.org/ Background by Emil Widlund: https://unsplash.com/@emilwidlund common commoncrawl data datascience efficient gpu large largelanguagemodels machinelearning memory pages training transformers using",
      "platforms": {
        "tiktok": {
          "video_id": "7242469106320969002",
          "url": "https://www.tiktok.com/@rajistics/video/7242469106320969002",
          "view_count": 5173,
          "upload_date": "2023-06-08",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17988793208097045",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-06-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "kL1-Grs2Wow",
          "url": "https://www.youtube.com/watch?v=kL1-Grs2Wow",
          "view_count": 482,
          "upload_date": "2023-06-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5947,
      "title": "You might assume Vision-Language Models like Claude or CLIP would crush defect detection. But on Amazon’s new Kaputt Dataset, the old-school supervised ViT blew them away — scoring 91.45 AUROC, while anomaly-detection and zero-shot VLMs barely cleared 57 AUROC. Sometimes, labels beat the latest models. Kaputt: A Large-Scale Dataset for Visual Defect Detection in Retail Logistics (arXiv: 2510.05903) https://arxiv.org/abs/2510.05903",
      "description": "You might assume Vision-Language Models like Claude or CLIP would crush defect detection. But on Amazon’s new Kaputt Dataset, the old-school supervised ViT blew them away — scoring 91.45 AUROC, while anomaly-detection and zero-shot VLMs barely cleared 57 AUROC. Sometimes, labels beat the latest models. Kaputt: A Large-Scale Dataset for Visual Defect Detection in Retail Logistics (arXiv: 2510.05903) https://arxiv.org/abs/2510.05903",
      "upload_date": "2025-10-30",
      "total_views": 5647,
      "max_views": 4373,
      "topics": [
        "character",
        "claude",
        "detection",
        "language",
        "like",
        "model",
        "models",
        "reveals",
        "specs",
        "stress",
        "testing",
        "vision"
      ],
      "search_text": "You might assume Vision-Language Models like Claude or CLIP would crush defect detection. But on Amazon’s new Kaputt Dataset, the old-school supervised ViT blew them away — scoring 91.45 AUROC, while anomaly-detection and zero-shot VLMs barely cleared 57 AUROC. Sometimes, labels beat the latest models. Kaputt: A Large-Scale Dataset for Visual Defect Detection in Retail Logistics (arXiv: 2510.05903) https://arxiv.org/abs/2510.05903 character claude detection language like model models reveals specs stress testing vision I need to build a model to detect these defects. Can I just use Claude? Claude's a great vision language model, but maybe you should start with some anomaly detection baseline model. I tried the classics like one class SVM isolation forest, got too many false positives. I did find some newer ones like Patchcore 50, solid choices. So how did they do when you operated them fully unsupervised? Not even .55 AUC. Yikes, basically guessing. So this is when I went back to my original idea, let's try those really good vision language models like Pixel and Claude. Big improvement? Only if you call a .02 AUC a big jump. Well, this isn't good. Are we out of luck? So I ended up training a simple vision transformer on all of that labeled defect data we have. So basically supervised style and boom, 91 AUC. Wow, that's a massive improvement, but you do see the catch with this approach. Yeah, I get it. We've turned this anomaly detection into a supervised task where it's only learning from the defects that I show it. If I don't have enough labeled defects, this model's gonna collapse. So what'd you learn from this project? I guess it's not about those fancy vision language models. It's really about having good labeled data.",
      "platforms": {
        "tiktok": {
          "video_id": "7567077171961859359",
          "url": "https://www.tiktok.com/@rajistics/video/7567077171961859359",
          "view_count": 4373,
          "upload_date": "2025-10-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ooI9hIA0PgDqQiIE8WYBKADCq7ifAv3xHIxrid~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=cA8MYpqgUc1sbHRbcUcR5pZgAPI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "youtube": {
          "video_id": "tzcxgnoFysk",
          "url": "https://www.youtube.com/watch?v=tzcxgnoFysk",
          "view_count": 1274,
          "upload_date": "2025-10-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6254,
      "title": "The video covers Retrieval Augmented Generation (RAG), a very popular approach for combining large language models and information retrieval techniques to provide a question/answer for unstructured documents. There are lots of resources on RAG, but finally wanted to make my own video on it. I demo this with snowflake, but every major vendor has their own RAG solution. Once you understand how it works, you can implement it anywhere. #retrievalaugmentedgeneration #largelanguagemodels #rag #snowflake #rajistics",
      "description": "The video covers Retrieval Augmented Generation (RAG), a very popular approach for combining large language models and information retrieval techniques to provide a question/answer for unstructured documents. There are lots of resources on RAG, but finally wanted to make my own video on it. I demo this with snowflake, but every major vendor has their own RAG solution. Once you understand how it works, you can implement it anywhere. #retrievalaugmentedgeneration #largelanguagemodels #rag #snowflake #rajistics",
      "upload_date": "2024-05-10",
      "total_views": 5638,
      "max_views": 4986,
      "topics": [
        "apply",
        "build",
        "documents",
        "generative",
        "get",
        "hits",
        "largelanguagemodels",
        "like",
        "may",
        "plateau",
        "problems",
        "rag",
        "retrievalaugmentedgeneration",
        "snowflake",
        "technology"
      ],
      "search_text": "The video covers Retrieval Augmented Generation (RAG), a very popular approach for combining large language models and information retrieval techniques to provide a question/answer for unstructured documents. There are lots of resources on RAG, but finally wanted to make my own video on it. I demo this with snowflake, but every major vendor has their own RAG solution. Once you understand how it works, you can implement it anywhere. #retrievalaugmentedgeneration #largelanguagemodels #rag #snowflake #rajistics apply build documents generative get hits largelanguagemodels like may plateau problems rag retrievalaugmentedgeneration snowflake technology Want to see a demo of the hottest generative AI use case? Will this make me money or is it another 5 minutes of techno babble? It's retrieval augmented generation and it lets you ask questions of unstructured documents. Sounds complicated. Let me show you how I get answers. Suppose I was supposed to research how many cars Rivian will manufacture? I just asked my friendly, large language model. Oh boy. This is where I'm one step ahead of you. I ask two LLMs then take the results, average them together, unsombering I believe is what you call it. Can you top that? Instead of your magic eight ball, let me show a way that you can use real documents to get answers that you'll be able to cite. Yeah, people don't seem to like when I just cite chat GPT for the answers. So here we're searching across Rivian's actual documents. I ask a question and then you can see I get a nice concise answer. But even better, I can look at the snippet where the original source of all this information was. Well, that's connected to an actual document that you can go and pull. This must be really complicated. Not at all. In this simple example, if you look at the code, you'll see that we use an embedding model to get our embeddings. We use the cosine similarity to get which of the similar documents for us. And we pass all this over to a language model that gives us our nice concise answer from the longer chunk of the document. That does seem easier than ensembling and no math.",
      "platforms": {
        "tiktok": {
          "video_id": "7367179847749258542",
          "url": "https://www.tiktok.com/@rajistics/video/7367179847749258542",
          "view_count": 4986,
          "upload_date": "2024-05-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/da2c913e3463474aa15f8b4ee60842fe_1715305234~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=5FMoLPx2ZBwibeMT4xkXwOToi2Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18017922290251377",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0ltgzK9xtds",
          "url": "https://www.youtube.com/watch?v=0ltgzK9xtds",
          "view_count": 652,
          "upload_date": "2024-05-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Working with Categorical data using ordinal, one hot (dummy), and target encoding #datascience #statistics #analytics #featureengineering ",
      "description": "Working with Categorical data using ordinal, one hot (dummy), and target encoding #datascience #statistics #analytics #featureengineering ",
      "upload_date": "2022-09-17",
      "total_views": 5638,
      "max_views": 5638,
      "topics": [
        "analytics",
        "categorical",
        "data",
        "datascience",
        "featureengineering",
        "statistics"
      ],
      "search_text": "Working with Categorical data using ordinal, one hot (dummy), and target encoding #datascience #statistics #analytics #featureengineering  analytics categorical data datascience featureengineering statistics Want to know why gender is a great way to identify a good data scientist? There's at least five different options for gender and a good data scientist knows at least three techniques for getting at them. After all, at the heart of data science is being able to convert categorical information into a numeric representation. Ordinal encoding is going to replace each categorical value with a number. This works best when there's a natural ordering to the values. One hot encoding creates a new feature for every categorical value. The downside here is if you're working with a categorical variable like zip code that has a high card NALADY, then you're going to blow out your data set with a lot of features. Every data scientist should understand those two. And what I look for in an advanced data scientist is somebody that knows a couple of other ways to handle categorical features. Hardening coding is where you replace each categorical value with its mean target value. One thing you have to watch out for here is having a good validation strategy because it's easy to leak information when you're using this technique. There you have it. You want to be on the team, be able to talk about gender in at least three ways.",
      "platforms": {
        "tiktok": {
          "video_id": "7144484312103079211",
          "url": "https://www.tiktok.com/@rajistics/video/7144484312103079211",
          "view_count": 5638,
          "upload_date": "2022-09-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/09d9efb814b143099c43e1ca4bb322b8_1663454896~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=9WnPGVqW53qRwI3XL1JpxxjUGzk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "SpeechT5 audio models getting added to transformers. #datascience #machinelearning #huggingface #speecht5 #speechmodels #audiomodels ",
      "description": "SpeechT5 audio models getting added to transformers. #datascience #machinelearning #huggingface #speecht5 #speechmodels #audiomodels ",
      "upload_date": "2023-02-09",
      "total_views": 5634,
      "max_views": 5634,
      "topics": [
        "datascience",
        "huggingface",
        "machinelearning",
        "speech",
        "speechmodels",
        "speecht5"
      ],
      "search_text": "SpeechT5 audio models getting added to transformers. #datascience #machinelearning #huggingface #speecht5 #speechmodels #audiomodels  datascience huggingface machinelearning speech speechmodels speecht5 Here's the secret to building your own speech to text models. I can face today announced support for speech T5 models and transformers. So an easy way to work with audio. Let me show you how speech T5 uses a transformer architecture. So it's very flexible. And what you can do is easily go between let's say text to speech. If you want to synthesize audio speech to speech, if you want to convert between different voices or do speech enhancement or speech to text for automatic speech recognition or speaker identification. So you can use this model to easily go from text to speech as I do in this demo. Listen, TikTok machine learning is the best. You want the code? We got the code. Walk through in a notebook showing you how easy it is to use. Hug and Face has a blog post up so you can learn a lot more that way. And this is just the start of adding more audio capabilities to transformers.",
      "platforms": {
        "tiktok": {
          "video_id": "7197957554310237482",
          "url": "https://www.tiktok.com/@rajistics/video/7197957554310237482",
          "view_count": 5634,
          "upload_date": "2023-02-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c408ceef1a0a4c6eb22a42fcb66695f6_1675905103~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=wvli5ZyjmEwi3ECuR9SQa0Gw5Dk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6662,
      "title": "The history of data science. I have since learned to make videos shorter and punchier.",
      "description": "The history of data science. I have since learned to make videos shorter and punchier.",
      "upload_date": "2024-02-18",
      "total_views": 5624,
      "max_views": 5624,
      "topics": [
        "data",
        "history",
        "learned",
        "make",
        "science",
        "since"
      ],
      "search_text": "The history of data science. I have since learned to make videos shorter and punchier. data history learned make science since",
      "platforms": {
        "instagram": {
          "video_id": "C3fzhhEgOeU",
          "url": "https://www.instagram.com/reel/C3fzhhEgOeU",
          "view_count": 5624,
          "upload_date": "2024-02-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6200,
      "title": "Tips for working with small datasets. This includes using cross-validation, models like lasso, and running multiple iterations of feature importance with different random seeds.",
      "description": "Tips for working with small datasets. This includes using cross-validation, models like lasso, and running multiple iterations of feature importance with different random seeds.",
      "upload_date": "2025-03-09",
      "total_views": 5581,
      "max_views": 5581,
      "topics": [
        "cross",
        "data",
        "going",
        "like",
        "lot",
        "validation"
      ],
      "search_text": "Tips for working with small datasets. This includes using cross-validation, models like lasso, and running multiple iterations of feature importance with different random seeds. cross data going like lot validation I'm worried. How am I going to build a machine learning model on such a small dataset? What's small for you? For this problem, I only have 100 rows of data, and when I set aside 20 for a test and 20 more for a holdout, that only leaves 60 rows of data. That doesn't seem that much. Do we have a big data project I could work on? Each of those data points cost over $5,000. We're not going to get more data easily, and this is a very valuable project for our team. Wow! My biggest concern with using machine learning is overfitting. If we had a lot of data like in this example, I wouldn't worry about overfitting. But when we have only a little bit of data like in this and in this example, there's a lot of different ways that that model could be created that might not actually generalize and capture the true pattern underneath. Oh yeah, that isn't good. Well, to get more out of your training data, you should use cross-validation. Studies have shown using cross-validation gets you better performance. Take a look at something like nested cross-validation for what you're doing. Glad you aren't cross with me. Go check SideKit. There's already functions built for cross-validation. My suggestion is to start with simpler models. So take a look at support vector machines, Eureka, or an ElasticNet. I'd also suggest starting with an ElasticNet because of the built-in feature selection. That feature selection is going to reduce that number of features, and having fewer features means less features that are likely to be susceptible to noise or spurious correlation. I remember us talking about lasso and ElasticNet. These are pretty common for the scenario you're in, especially when you have a lot of features and just a little bit of data. One pro-level tip I like to use is changing the random seed. By rerunning the analysis with different random seeds, you can start to separate out the signal from the noise. And in this way, you might be able to, for example, identify what is the certain set or group of features that's consistent between different runs. If you find those, probably a good chance that those are going to generalize out to new data and you're not going to be just overfitting. Yikes! Isn't that going to take a long time to run? With your data set this small, I don't think draining time is going to be too big of an issue. Thanks, everyone. I feel a lot better, and my knowledge is a lot larger.",
      "platforms": {
        "tiktok": {
          "video_id": "7479855881232862495",
          "url": "https://www.tiktok.com/@rajistics/video/7479855881232862495",
          "view_count": 5581,
          "upload_date": "2025-03-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0EAIVLfcW6EU1ERgyznpAFlgA3DeDAEApoANC~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=W4hhBHKb2jPjzShWsXNX9g6LZfs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18041334905145805",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#stitch with @debtcollective Marketing and PR.  This is a big topic and a lot of nuance isn’t in this video. Also relationships with academia. in #datascience #machinelearning #codetok #mltok",
      "description": "#stitch with @debtcollective Marketing and PR.  This is a big topic and a lot of nuance isn’t in this video. Also relationships with academia. in #datascience #machinelearning #codetok #mltok",
      "upload_date": "2022-06-20",
      "total_views": 5570,
      "max_views": 5570,
      "topics": [
        "codetok",
        "datascience",
        "lot",
        "machinelearning",
        "mltok",
        "stitch"
      ],
      "search_text": "#stitch with @debtcollective Marketing and PR.  This is a big topic and a lot of nuance isn’t in this video. Also relationships with academia. in #datascience #machinelearning #codetok #mltok codetok datascience lot machinelearning mltok stitch What's a scam that's become so normalized that we don't even realize it's a scam anymore? The machine learning and data science news we think we're getting. A huge amount of the content is written by marketing and public relations people that were all fed in data science and ML. A lot of podcast events and conferences are paid to play. You have to pay money if you want to participate. Then there's a whole group of people that pretend to be independent but take a lot of money to write those research and white papers that you see. The best thing you can do is always validate any of the technology on your own requirements. And once you're done, share it out. One of the best ways I learn about stuff is actually through friends and colleagues that have actually tried the stuff out themselves.",
      "platforms": {
        "tiktok": {
          "video_id": "7111405781639171374",
          "url": "https://www.tiktok.com/@rajistics/video/7111405781639171374",
          "view_count": 5570,
          "upload_date": "2022-06-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3f797ea0a8a94523913b393216becdf1_1655753187~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=ik1Px%2BhRFNAxvnEF3WRk3DY1yBo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6128,
      "title": "Opus.ai very cool demo! If you want to build similar apps check out the text to code models. Santacoder is open source and they have shared all the details about training it. #datascience #machinelearning #largelanguagemodels #texttocode #santacoder #bigcode #huggingface #opusai",
      "description": "Opus.ai very cool demo! If you want to build similar apps check out the text to code models. Santacoder is open source and they have shared all the details about training it. #datascience #machinelearning #largelanguagemodels #texttocode #santacoder #bigcode #huggingface #opusai",
      "upload_date": "2023-04-05",
      "total_views": 5561,
      "max_views": 3895,
      "topics": [
        "bigcode",
        "datascience",
        "games",
        "largelanguagemodels",
        "machinelearning",
        "opus",
        "santacoder",
        "text",
        "texttocode",
        "video"
      ],
      "search_text": "Opus.ai very cool demo! If you want to build similar apps check out the text to code models. Santacoder is open source and they have shared all the details about training it. #datascience #machinelearning #largelanguagemodels #texttocode #santacoder #bigcode #huggingface #opusai bigcode datascience games largelanguagemodels machinelearning opus santacoder text texttocode video",
      "platforms": {
        "tiktok": {
          "video_id": "7218638808814767402",
          "url": "https://www.tiktok.com/@rajistics/video/7218638808814767402",
          "view_count": 3895,
          "upload_date": "2023-04-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18266571223130010",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8acAN664DI0",
          "url": "https://www.youtube.com/watch?v=8acAN664DI0",
          "view_count": 1666,
          "upload_date": "2023-04-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Interpreting stable diffusion #stabilitydiffusion #datascience #codetok #machinelearning #texttoimage",
      "description": "Interpreting stable diffusion #stabilitydiffusion #datascience #codetok #machinelearning #texttoimage",
      "upload_date": "2022-09-16",
      "total_views": 5537,
      "max_views": 5537,
      "topics": [
        "codetok",
        "datascience",
        "diffusion",
        "machinelearning",
        "stabilitydiffusion",
        "texttoimage"
      ],
      "search_text": "Interpreting stable diffusion #stabilitydiffusion #datascience #codetok #machinelearning #texttoimage codetok datascience diffusion machinelearning stabilitydiffusion texttoimage These text to image models are totally cool and now we have some tools that will help us start to understand what's going on. Diffusers Interpret lets you see the diffusion process and the most influential words in your prompt. The first thing you can do is watch the diffusion process go from random noise to slowly putting your image together. It's amazing how it can transform from that random into your image. Second cool thing is the explainer uses gradients to identify the importance of tokens. When I tried it, the tokens seemed to pretty much match up with my intuition for the important words. This package was just released, so keep looking for developments and jump in and help if you can.",
      "platforms": {
        "tiktok": {
          "video_id": "7144103518578756910",
          "url": "https://www.tiktok.com/@rajistics/video/7144103518578756910",
          "view_count": 5537,
          "upload_date": "2022-09-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/307554455e52451f84a51cba83eb11c5_1663366229~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=HOwYDAPbHWb0pHq1%2F3ZZf1JjdTg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6395,
      "title": "Fine Tuning an Image Classifier on Indian Food Images",
      "description": "Fine Tuning an Image Classifier on Indian Food Images",
      "upload_date": "2022-08-04",
      "total_views": 5522,
      "max_views": 5522,
      "topics": [
        "classifier",
        "fine",
        "food",
        "image",
        "indian",
        "tuning"
      ],
      "search_text": "Fine Tuning an Image Classifier on Indian Food Images classifier fine food image indian tuning",
      "platforms": {
        "youtube": {
          "video_id": "ahgB8c_TgA8",
          "url": "https://www.youtube.com/watch?v=ahgB8c_TgA8",
          "view_count": 5522,
          "upload_date": "2022-08-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6107,
      "title": "Let's learn from the best! Feel free to share your favorites. My list comes from the Data Vis Dispatch list of favorite visualizations for 2023. Data Vis Dispatch: https://blog.datawrapper.de/data-vis-dispatch-december-19-2023/ #datavisualizations #rajistics",
      "description": "Let's learn from the best! Feel free to share your favorites. My list comes from the Data Vis Dispatch list of favorite visualizations for 2023. Data Vis Dispatch: https://blog.datawrapper.de/data-vis-dispatch-december-19-2023/ #datavisualizations #rajistics",
      "upload_date": "2024-01-03",
      "total_views": 5520,
      "max_views": 4290,
      "topics": [
        "data",
        "datavisualizations",
        "dispatch",
        "let",
        "list",
        "vis"
      ],
      "search_text": "Let's learn from the best! Feel free to share your favorites. My list comes from the Data Vis Dispatch list of favorite visualizations for 2023. Data Vis Dispatch: https://blog.datawrapper.de/data-vis-dispatch-december-19-2023/ #datavisualizations #rajistics data datavisualizations dispatch let list vis",
      "platforms": {
        "tiktok": {
          "video_id": "7320009476373826858",
          "url": "https://www.tiktok.com/@rajistics/video/7320009476373826858",
          "view_count": 4290,
          "upload_date": "2024-01-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18005185907263451",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8D4-ZKR0FEU",
          "url": "https://youtube.com/shorts/8D4-ZKR0FEU",
          "view_count": 1230,
          "upload_date": "2024-01-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#instructionfinetuning #rlhf #reinforcementlearning #pretrain  Target leakage is a very common problem and everyone should understand it. This video is a repost. I have seen even the smartest people and best teams have issues with data or target leakage. These include Harvard Google Fast.AI Andrew Ng and the SARCOS dataset used by hundreds.  Efficient Deduplication and Leakage Detection in Large Scale Image Datasets with a focus on the CrowdAI Mapping Challenge Dataset - https://arxiv.org/abs/2304.02296# Running Code and Failing Models by Rajiv - https://www.datarobot.com/blog/running-code-and-failing-models/  Stand Up for Best Practices: Misuse of Deep Learning in Nature’s Earthquake Aftershock Paper: https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8  Reddit: https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/  Older video: https://www.youtube.com/watch?v=NaySLPTCgDM #datascience #machinelearning #targetleakage #dataleakage #crowdai #fastai #sarcos ",
      "description": "#instructionfinetuning #rlhf #reinforcementlearning #pretrain  Target leakage is a very common problem and everyone should understand it. This video is a repost. I have seen even the smartest people and best teams have issues with data or target leakage. These include Harvard Google Fast.AI Andrew Ng and the SARCOS dataset used by hundreds.  Efficient Deduplication and Leakage Detection in Large Scale Image Datasets with a focus on the CrowdAI Mapping Challenge Dataset - https://arxiv.org/abs/2304.02296# Running Code and Failing Models by Rajiv - https://www.datarobot.com/blog/running-code-and-failing-models/  Stand Up for Best Practices: Misuse of Deep Learning in Nature’s Earthquake Aftershock Paper: https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8  Reddit: https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/  Older video: https://www.youtube.com/watch?v=NaySLPTCgDM #datascience #machinelearning #targetleakage #dataleakage #crowdai #fastai #sarcos ",
      "upload_date": "2024-04-11",
      "total_views": 5506,
      "max_views": 5506,
      "topics": [
        "crowdai",
        "data",
        "leakage",
        "machinelearning",
        "sarcos",
        "target"
      ],
      "search_text": "#instructionfinetuning #rlhf #reinforcementlearning #pretrain  Target leakage is a very common problem and everyone should understand it. This video is a repost. I have seen even the smartest people and best teams have issues with data or target leakage. These include Harvard Google Fast.AI Andrew Ng and the SARCOS dataset used by hundreds.  Efficient Deduplication and Leakage Detection in Large Scale Image Datasets with a focus on the CrowdAI Mapping Challenge Dataset - https://arxiv.org/abs/2304.02296# Running Code and Failing Models by Rajiv - https://www.datarobot.com/blog/running-code-and-failing-models/  Stand Up for Best Practices: Misuse of Deep Learning in Nature’s Earthquake Aftershock Paper: https://towardsdatascience.com/stand-up-for-best-practices-8a8433d3e0e8  Reddit: https://www.reddit.com/r/MachineLearning/comments/c4ylga/d_misuse_of_deep_learning_in_nature_journals/  Older video: https://www.youtube.com/watch?v=NaySLPTCgDM #datascience #machinelearning #targetleakage #dataleakage #crowdai #fastai #sarcos  crowdai data leakage machinelearning sarcos target Let's talk about the biggest problem in machine learning and how it struck again. It's data leakage and it happened in the crowd AI data set where of the 60,000 pictures in the validation set, 53,000 of those were in the training set. In machine learning, this is called data or target leakage. And it's when we train the model with the same information that we're going to validate it. That's the leak. Ideally we want to validate or test the model with data that's like the real world. We also want to make sure it's not like the training data because we want to make sure our model just hasn't memorized or overfit to the training data. Target leakage happens all the time. I've looked at different studies, for example, from Harvard and Google, looking at earthquakes where I found target leakage. I found it in the fast AI course. Even the Sarko's data set that's used by hundreds of researchers also suffers from leakage. Practicing data scientists are well aware of this and often quite skeptical of any results. And they often use methods like baseline models, benchmarking data sets and explainability as ways to judge if some target leakage is occurring.",
      "platforms": {
        "tiktok": {
          "video_id": "7356566035689966894",
          "url": "https://www.tiktok.com/@rajistics/video/7356566035689966894",
          "view_count": 5506,
          "upload_date": "2024-04-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e556c4aad93a47db86453ae084ccf17b_1712834022~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=M%2B4AFw4o24EqAj%2FZ3Qh8NW31NgM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6392,
      "title": "Sports! #datascience #analytics #codetok #machinelearning #rstats #footballanalytics #statistics",
      "description": "Sports! #datascience #analytics #codetok #machinelearning #rstats #footballanalytics #statistics",
      "upload_date": "2022-08-16",
      "total_views": 5495,
      "max_views": 5495,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "footballanalytics",
        "machinelearning",
        "rstats"
      ],
      "search_text": "Sports! #datascience #analytics #codetok #machinelearning #rstats #footballanalytics #statistics analytics codetok datascience footballanalytics machinelearning rstats",
      "platforms": {
        "youtube": {
          "video_id": "3RNtXY6MG7I",
          "url": "https://youtube.com/shorts/3RNtXY6MG7I?feature=share",
          "view_count": 5495,
          "upload_date": "2022-08-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6551,
      "title": "AI for other than productivity. Let's talk about how people are really using AI. #datascience #machinelearning #rajistics #therapy Lilian Weng post: https://twitter.com/lilianweng/status/1706544602906530000 LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset: https://arxiv.org/pdf/2309.11998.pdf",
      "description": "AI for other than productivity. Let's talk about how people are really using AI. #datascience #machinelearning #rajistics #therapy Lilian Weng post: https://twitter.com/lilianweng/status/1706544602906530000 LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset: https://arxiv.org/pdf/2309.11998.pdf",
      "upload_date": "2023-09-28",
      "total_views": 5481,
      "max_views": 3547,
      "topics": [
        "beyond",
        "datascience",
        "gpt4",
        "let",
        "machinelearning",
        "multimodal",
        "pdf",
        "productivity",
        "therapy"
      ],
      "search_text": "AI for other than productivity. Let's talk about how people are really using AI. #datascience #machinelearning #rajistics #therapy Lilian Weng post: https://twitter.com/lilianweng/status/1706544602906530000 LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset: https://arxiv.org/pdf/2309.11998.pdf beyond datascience gpt4 let machinelearning multimodal pdf productivity therapy",
      "platforms": {
        "tiktok": {
          "video_id": "7283675775742659886",
          "url": "https://www.tiktok.com/@rajistics/video/7283675775742659886",
          "view_count": 3547,
          "upload_date": "2023-09-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cxt2RUbtG2q",
          "url": "https://www.instagram.com/reel/Cxt2RUbtG2q",
          "view_count": 1862,
          "upload_date": "2023-09-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "lr3pLbR1TYY",
          "url": "https://www.youtube.com/watch?v=lr3pLbR1TYY",
          "view_count": 72,
          "upload_date": "2023-09-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Critical question when framing out analytic questions, since extrapolation has got me into trouble before. #datascience #analytics #codetok",
      "description": "Critical question when framing out analytic questions, since extrapolation has got me into trouble before. #datascience #analytics #codetok",
      "upload_date": "2022-09-13",
      "total_views": 5467,
      "max_views": 5467,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "extrapolation",
        "like",
        "think"
      ],
      "search_text": "Critical question when framing out analytic questions, since extrapolation has got me into trouble before. #datascience #analytics #codetok analytics codetok datascience extrapolation like think Hey, is this a first date? What should you treat this like a steady relationship? Let me explain how this helps you understand data science. I've worked on hundreds of data science problems, and I always try to think is, is this a problem of interpolation or extrapolation? Interpolation means this is something that's probably happened before. I have lots of data that's going to be pretty similar to what I'm trying to do. So I find new algorithms, find ways to geek out that last little bit of performance, knowing it's probably going to stick around. Extrapolation is like that scary part of a first date when it's nervous and you don't exactly know what's going to happen. You can do your best to prepare, try to understand what's likely to go. You can set things up, but there's a lot of uncertainty and everybody has to be aware of like, this isn't an exact science, this could change. And part of when I have extrapolation problems, I like to think about how I can actually adapt on the fly to these types of changes. Not just focusing on that initial setup. So your next problem, think about whether this is that first date or steady state relationship problem and next week I'll do a video showing even though you think it's a steady state relationship, it's not.",
      "platforms": {
        "tiktok": {
          "video_id": "7142916977969876270",
          "url": "https://www.tiktok.com/@rajistics/video/7142916977969876270",
          "view_count": 5467,
          "upload_date": "2022-09-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/588c18c7db464b1796e5412925012cc3_1663089970~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=X5jKWGxSKN61InkCSWjk9MAYf9g%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6555,
      "title": "With the writer's strike in the US this video reminds us of the human and environmental costs of building AI. Three critical components for building LLMs are data talent and hardware. #machinelearning #largelanguagemodels #openai #environment #ethicalai #WGAStrike",
      "description": "With the writer's strike in the US this video reminds us of the human and environmental costs of building AI. Three critical components for building LLMs are data talent and hardware. #machinelearning #largelanguagemodels #openai #environment #ethicalai #WGAStrike",
      "upload_date": "2023-07-20",
      "total_views": 5457,
      "max_views": 4760,
      "topics": [
        "building",
        "easy",
        "environment",
        "ethicalai",
        "fun",
        "largelanguagemodels",
        "machinelearning",
        "openai",
        "steps",
        "wgastrike"
      ],
      "search_text": "With the writer's strike in the US this video reminds us of the human and environmental costs of building AI. Three critical components for building LLMs are data talent and hardware. #machinelearning #largelanguagemodels #openai #environment #ethicalai #WGAStrike building easy environment ethicalai fun largelanguagemodels machinelearning openai steps wgastrike",
      "platforms": {
        "tiktok": {
          "video_id": "7257709676714839342",
          "url": "https://www.tiktok.com/@rajistics/video/7257709676714839342",
          "view_count": 4760,
          "upload_date": "2023-07-20",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cu5rBAngeJl",
          "url": "https://www.instagram.com/reel/Cu5rBAngeJl",
          "view_count": 632,
          "upload_date": "2023-07-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "zUkO3drv1Ks",
          "url": "https://www.youtube.com/watch?v=zUkO3drv1Ks",
          "view_count": 65,
          "upload_date": "2023-07-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @fondantlover datasaurus dozen howto #stats #anscombe #datascience #analytics",
      "description": "Reply to @fondantlover datasaurus dozen howto #stats #anscombe #datascience #analytics",
      "upload_date": "2022-01-17",
      "total_views": 5447,
      "max_views": 5447,
      "topics": [
        "analytics",
        "anscombe",
        "datascience",
        "dozen",
        "rex",
        "stats"
      ],
      "search_text": "Reply to @fondantlover datasaurus dozen howto #stats #anscombe #datascience #analytics analytics anscombe datascience dozen rex stats It's not just the T-Rex and the Star, it's a lot more. Let me explain. I've shown you the T-Rex and the Star, but look at all of these. All of these have the exact same mean as well. These are known as the Datasaur Dozen. The project has actually shared their own code. Go try it. I actually was able to use my own image and get it to fit within the Datasaur Dozen. And keep those questions coming.",
      "platforms": {
        "tiktok": {
          "video_id": "7054220367908343086",
          "url": "https://www.tiktok.com/@rajistics/video/7054220367908343086",
          "view_count": 5447,
          "upload_date": "2022-01-17",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/58283939a5ae441eb5839a842963f0af_1642438671~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=qCoY0pEYJqn%2FC8L0IWyCx6YK59Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#xgboost short history. #datascience #statistics #machinelearning #codetok",
      "description": "#xgboost short history. #datascience #statistics #machinelearning #codetok",
      "upload_date": "2022-05-14",
      "total_views": 5427,
      "max_views": 5427,
      "topics": [
        "codetok",
        "datascience",
        "kaggle",
        "machinelearning",
        "statistics",
        "xgboost"
      ],
      "search_text": "#xgboost short history. #datascience #statistics #machinelearning #codetok codetok datascience kaggle machinelearning statistics xgboost Let me tell you about the latest... Nahhh... I want to instead talk about a classic today. Eight years ago, Tian Qin created this... Apache license. He threw it out there on GitHub. But then what he also did was he shared the results of how well his algorithm worked with Kaggle competitors in the Higgs Boson Challenge. Look how good it does. What happened was a ton of other competitors started using it. They were reaching the top of the leaderboards. Not only in the Higgs Boson competition, but in lots of other Kaggle competitions. And eight years later, XGBoost along with other knockoffs like CapBoost and LightGVM dominate when working with structured data, both inside Kaggle competitions and everyday use inside enterprises. So if you're just getting into this space, you're going to probably learn XGBoost. And if you just want to complain about the results of some data scientists work, just tell them they should have used XGBoost.",
      "platforms": {
        "tiktok": {
          "video_id": "7097609177765301546",
          "url": "https://www.tiktok.com/@rajistics/video/7097609177765301546",
          "view_count": 5427,
          "upload_date": "2022-05-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/86a84dbc06684f0a94c7ffc355ce78d7_1652540921~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=tGoO9SrdGSXhFPwyD3y7aCyNbRo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6224,
      "title": "Dive into model error metrics! From simple mean error to Mean Squared Error (MSE) and Log Loss, let's see when you should use them. While MSE is useful for regression, when it comes to classification problems with probabilities, logloss is a better metric.",
      "description": "Dive into model error metrics! From simple mean error to Mean Squared Error (MSE) and Log Loss, let's see when you should use them. While MSE is useful for regression, when it comes to classification problems with probabilities, logloss is a better metric.",
      "upload_date": "2024-11-12",
      "total_views": 5410,
      "max_views": 3886,
      "topics": [
        "ang",
        "better",
        "error",
        "function",
        "log",
        "logloss",
        "loss",
        "mean",
        "mga",
        "model",
        "mse",
        "nito",
        "pagkakamali",
        "squared"
      ],
      "search_text": "Dive into model error metrics! From simple mean error to Mean Squared Error (MSE) and Log Loss, let's see when you should use them. While MSE is useful for regression, when it comes to classification problems with probabilities, logloss is a better metric. ang better error function log logloss loss mean mga model mse nito pagkakamali squared Why do you need this crazy formula to figure out the error? Can you just subtract the predicted from the actual? Taking the difference between predicted and actual is mean error. Let's not be so negative. I understand that calculating error is necessary to help the model learn from its mistakes. Yes, but the problem with mean error is that positives and negatives can cancel each other out. Imagine the actual value is 10, but you predict 12, and then you predict 8. The errors are 2 and negative 2. If you sum those up, the errors would be 0. Instead, what you should do is use mean squared error. Okay, so you just mathified what I said earlier. Can't we just use this? Why don't you try it out? Here's a fraud example, which is an imbalanced data set since fraud is rare. Let me know what you find. Yikes, the model using MSE is way off. It can't tell the difference between non-fraud and fraudsters, like most people. Exactly. Log loss works better because it considers probabilities, not just the error. Consider these two different predictions between the predicted error versus the log loss error. Hmm, ah. So it really penalizes that 164 example because it's really wrong. Why can't we just call this nice error? It's using a logarithmic function which has smooth mathematical properties that help guide the learning step by step and help it handle tricky problems. Smooth and nice. Perfect. Who would get crossed with that?",
      "platforms": {
        "tiktok": {
          "video_id": "7436465351065472299",
          "url": "https://www.tiktok.com/@rajistics/video/7436465351065472299",
          "view_count": 3886,
          "upload_date": "2024-11-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3fef9653fb494a278de1ff9642e85b19_1731437025~tplv-tiktokx-origin.image?dr=9636&x-expires=1767402000&x-signature=7bGaav2E92LDVTCisr7roFxjyck%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18125377810386532",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "S_zxVfKI55c",
          "url": "https://www.youtube.com/watch?v=S_zxVfKI55c",
          "view_count": 1524,
          "upload_date": "2024-11-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "No one actually knows what a data scientist does, take advantage of it. ",
      "description": "No one actually knows what a data scientist does, take advantage of it. ",
      "upload_date": "2022-12-07",
      "total_views": 5338,
      "max_views": 5338,
      "topics": [
        "actually",
        "data",
        "knows",
        "one",
        "scientist",
        "take"
      ],
      "search_text": "No one actually knows what a data scientist does, take advantage of it.  actually data knows one scientist take Take it from a vet, that's a rookie ass mistake",
      "platforms": {
        "tiktok": {
          "video_id": "7174543978715663662",
          "url": "https://www.tiktok.com/@rajistics/video/7174543978715663662",
          "view_count": 5338,
          "upload_date": "2022-12-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/01b9d5af334248ecbd33bb1fb826c754_1670453703~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=QK9QbmsFe2qg3kC%2FbEHPTn2YAPc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "ChatDoctor is a great example of fine tuning a large language model to get more factually correct output. This is an approach i expect many people to follow. #datascience #machinelearning #largelanguagemodels #chatgpt #chatdoctor #finetuning #instructiontuning ",
      "description": "ChatDoctor is a great example of fine tuning a large language model to get more factually correct output. This is an approach i expect many people to follow. #datascience #machinelearning #largelanguagemodels #chatgpt #chatdoctor #finetuning #instructiontuning ",
      "upload_date": "2023-03-22",
      "total_views": 5320,
      "max_views": 5320,
      "topics": [
        "chatdoctor",
        "chatgpt",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "model"
      ],
      "search_text": "ChatDoctor is a great example of fine tuning a large language model to get more factually correct output. This is an approach i expect many people to follow. #datascience #machinelearning #largelanguagemodels #chatgpt #chatdoctor #finetuning #instructiontuning  chatdoctor chatgpt datascience largelanguagemodels machinelearning model You want to play doctor with chat GPT, but it doesn't want to play? Let me tell you about a brand new project that allows us to fine tune a language model so we can do that. Existing models like chat GPT aren't trained precisely in the medical field. They also often regurgitate facts and can mix things up and hallucinate. And so this is why their operators often do not allow these types of chatbots to be used for medical advice. Chat doctor uses medical information. It's based on over 700 diseases, what are the associated symptoms, the medications for this. So all of this factual content was then paired with simulated conversations between doctors and patients using chat GPT. These 5,000 conversations were then used to fine tune the model. The model they went with was Meta's Lama model, the 7 billion parameter. They used something like 8 100 GPUs and it took about 30 minutes to do. So the results look pretty good. Take a look at this conversation about acne and you can see that it's providing pretty reasonable medical advice. Now I'm early on the curve as usual, so look for more details about the paper and a demonstration coming. But in the meantime, go check out the GitHub.",
      "platforms": {
        "tiktok": {
          "video_id": "7213492796978941230",
          "url": "https://www.tiktok.com/@rajistics/video/7213492796978941230",
          "view_count": 5320,
          "upload_date": "2023-03-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6bf68b39a8754ee7950d938ac0b1ce3c_1679522194~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=l%2BhVl7CUdEZzsgqmAdYvSTYSHCM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6113,
      "title": "Q* from OpenAI is getting the hype but let's focus on the basics of their organization and the limitations of GPT-4 around planning. This video covers some of the concepts to dig deeper check out my earlier videos from August 2023: Planning with LLMs: https://www.tiktok.com/@rajistics/video/7266582668555423019 @Rajiv Shah | data science & AI Block World: https://www.tiktok.com/@rajistics/video/7266866822639635754 @Rajiv Shah | data science & AI #largelanguagemodels #aiplanning #gpt4 #qstar #openai #rajistics",
      "description": "Q* from OpenAI is getting the hype but let's focus on the basics of their organization and the limitations of GPT-4 around planning. This video covers some of the concepts to dig deeper check out my earlier videos from August 2023: Planning with LLMs: https://www.tiktok.com/@rajistics/video/7266582668555423019 @Rajiv Shah | data science & AI Block World: https://www.tiktok.com/@rajistics/video/7266866822639635754 @Rajiv Shah | data science & AI #largelanguagemodels #aiplanning #gpt4 #qstar #openai #rajistics",
      "upload_date": "2023-11-28",
      "total_views": 5314,
      "max_views": 4696,
      "topics": [
        "accelerating",
        "aiplanning",
        "gpt4",
        "largelanguagemodels",
        "machinelearning",
        "openai",
        "planning",
        "prague",
        "pytorch",
        "qstar",
        "transformers",
        "video",
        "working"
      ],
      "search_text": "Q* from OpenAI is getting the hype but let's focus on the basics of their organization and the limitations of GPT-4 around planning. This video covers some of the concepts to dig deeper check out my earlier videos from August 2023: Planning with LLMs: https://www.tiktok.com/@rajistics/video/7266582668555423019 @Rajiv Shah | data science & AI Block World: https://www.tiktok.com/@rajistics/video/7266866822639635754 @Rajiv Shah | data science & AI #largelanguagemodels #aiplanning #gpt4 #qstar #openai #rajistics accelerating aiplanning gpt4 largelanguagemodels machinelearning openai planning prague pytorch qstar transformers video working",
      "platforms": {
        "tiktok": {
          "video_id": "7306296207238761770",
          "url": "https://www.tiktok.com/@rajistics/video/7306296207238761770",
          "view_count": 4696,
          "upload_date": "2023-11-28",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18246086344175657",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-11-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "HSUN1MJBOT0",
          "url": "https://youtube.com/shorts/HSUN1MJBOT0",
          "view_count": 618,
          "upload_date": "2023-11-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6131,
      "title": "Lets talk about how GPT-4 is going to affect enterprise analytics. My upcoming public talks: AI Summit in Montreal on April 20 & Arize AI event on April 25. #datascience #machinelearning #openai #gpt4 #analytics",
      "description": "Lets talk about how GPT-4 is going to affect enterprise analytics. My upcoming public talks: AI Summit in Montreal on April 20 & Arize AI event on April 25. #datascience #machinelearning #openai #gpt4 #analytics",
      "upload_date": "2023-04-01",
      "total_views": 5289,
      "max_views": 5231,
      "topics": [
        "analytics",
        "april",
        "datascience",
        "enterprise",
        "going",
        "gpt",
        "gpt4",
        "machinelearning",
        "openai"
      ],
      "search_text": "Lets talk about how GPT-4 is going to affect enterprise analytics. My upcoming public talks: AI Summit in Montreal on April 20 & Arize AI event on April 25. #datascience #machinelearning #openai #gpt4 #analytics analytics april datascience enterprise going gpt gpt4 machinelearning openai",
      "platforms": {
        "tiktok": {
          "video_id": "7217142397904473390",
          "url": "https://www.tiktok.com/@rajistics/video/7217142397904473390",
          "view_count": 5231,
          "upload_date": "2023-04-01",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17952820376340947",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-04-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pMmKbx_nHjc",
          "url": "https://www.youtube.com/watch?v=pMmKbx_nHjc",
          "view_count": 58,
          "upload_date": "2023-04-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I know the pain.  But there are ways to make it easy for people to use your code. #python #analysis #datascience",
      "description": "I know the pain.  But there are ways to make it easy for people to use your code. #python #analysis #datascience",
      "upload_date": "2022-03-11",
      "total_views": 5287,
      "max_views": 5287,
      "topics": [
        "analysis",
        "datascience",
        "know",
        "let",
        "pain",
        "python"
      ],
      "search_text": "I know the pain.  But there are ways to make it easy for people to use your code. #python #analysis #datascience analysis datascience know let pain python Let it go, let it go, turn away",
      "platforms": {
        "tiktok": {
          "video_id": "7073831915345759530",
          "url": "https://www.tiktok.com/@rajistics/video/7073831915345759530",
          "view_count": 5287,
          "upload_date": "2022-03-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b9874e2f0c864e8e81802689598f0477_1647042162~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=pq9Axx9WxzsN6Iwcy5UbbP0GxZU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Document AI with LayoutLM #datascience #codetok #naturallanguageprocessing #layoutml #huggingface #🤗 #ocr #deeplearning #multimodal",
      "description": "Document AI with LayoutLM #datascience #codetok #naturallanguageprocessing #layoutml #huggingface #🤗 #ocr #deeplearning #multimodal",
      "upload_date": "2022-06-05",
      "total_views": 5258,
      "max_views": 5258,
      "topics": [
        "codetok",
        "datascience",
        "huggingface",
        "multimodal",
        "naturallanguageprocessing",
        "ocr"
      ],
      "search_text": "Document AI with LayoutLM #datascience #codetok #naturallanguageprocessing #layoutml #huggingface #🤗 #ocr #deeplearning #multimodal codetok datascience huggingface multimodal naturallanguageprocessing ocr Let me show you how I went from a document like this and was able to extract and pull out all the information into this. I did this by using LayoutLM. It's a state-of-the-art multimodal model which brings together the OCR text, positional embeddings, and the image embeddings. All three pieces come together and this is so much better than the old-school approach of just looking at OCR, which never kept track of where information was on a page. If you want to play around, just Google LayoutLM. There's tons of resources out there. If you look at the spaces on the Huggingface site, there's a ton of demos and code. I even built my own around receipts. Now in this demo, if you check it out, you'll see I fine-tuned the LM model with a receipts data set. This means it's really good at identifying receipts and you can see here we can submit a receipt and it pulls back and extracts information like the menu, the sub-total, the total, all of this. Check it out. Code is there. You can start playing with it.",
      "platforms": {
        "tiktok": {
          "video_id": "7105793783471639854",
          "url": "https://www.tiktok.com/@rajistics/video/7105793783471639854",
          "view_count": 5258,
          "upload_date": "2022-06-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a5480b34bf554d46877974e97c6e0584_1654446542~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=IIFj4FsOYRnrxxFJ%2BC2JIVgAdZg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Baseline models for time series. ",
      "description": "Baseline models for time series. ",
      "upload_date": "2024-10-09",
      "total_views": 5255,
      "max_views": 5255,
      "topics": [
        "baseline",
        "model",
        "predicting",
        "sales",
        "series",
        "time"
      ],
      "search_text": "Baseline models for time series.  baseline model predicting sales series time What if I told you my new deep learning time series model is predicting with an armacy of 1206 our daily sales? Wow, that sounds impressive. How does that compare to a baseline model? What's a baseline model? Oh, a baseline is a simple ruler algorithm that just gives us a starting point for thinking out predictive performance. So for thinking about predicting next week's sales, I like to just use what happened last week and that gives me a simple baseline model. Okay, let me try that. Yeah, my model isn't as good as the baseline model. I guess I gotta get back to work.",
      "platforms": {
        "tiktok": {
          "video_id": "7423754320212004139",
          "url": "https://www.tiktok.com/@rajistics/video/7423754320212004139",
          "view_count": 5255,
          "upload_date": "2024-10-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/89e7cc6b6d2e43668f81e52c0a2ced40_1728477509~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=5%2BXfiDPWYOdg01%2F7yqe3PZchT6c%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6631,
      "title": "Andrew Ng wrote recently on this no test set approach that he is seeing when people are using prompt engineering. This is very different than traditional machine learning approaches that rely on a test set. The video reviews some of the tradeoffs around this approach. #datascience #machinelearning #promptengineering #validation Andrew Ng Batch: https://www.deeplearning.ai/the-batch/issue-197/",
      "description": "Andrew Ng wrote recently on this no test set approach that he is seeing when people are using prompt engineering. This is very different than traditional machine learning approaches that rely on a test set. The video reviews some of the tradeoffs around this approach. #datascience #machinelearning #promptengineering #validation Andrew Ng Batch: https://www.deeplearning.ai/the-batch/issue-197/",
      "upload_date": "2023-05-22",
      "total_views": 5252,
      "max_views": 4755,
      "topics": [
        "andrew",
        "datascience",
        "machinelearning",
        "model",
        "production",
        "promptengineering",
        "see",
        "set",
        "test",
        "validation"
      ],
      "search_text": "Andrew Ng wrote recently on this no test set approach that he is seeing when people are using prompt engineering. This is very different than traditional machine learning approaches that rely on a test set. The video reviews some of the tradeoffs around this approach. #datascience #machinelearning #promptengineering #validation Andrew Ng Batch: https://www.deeplearning.ai/the-batch/issue-197/ andrew datascience machinelearning model production promptengineering see set test validation",
      "platforms": {
        "tiktok": {
          "video_id": "7236127002275679531",
          "url": "https://www.tiktok.com/@rajistics/video/7236127002275679531",
          "view_count": 4755,
          "upload_date": "2023-05-22",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Csj6XgEAcZ7",
          "url": "https://www.instagram.com/reel/Csj6XgEAcZ7",
          "view_count": 497,
          "upload_date": "2023-05-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6628,
      "title": "Examining the data used for training our our LLMs. OpenAI is running into trouble in Europe since it won't disclose exactly what was used for their training data. Reddit is no longer letting anyone use their data for commercial use for free. Finally the folks over at together.xyz have assembled an open-source recipe of the LLaMa training dataset. #datascience #machinelearning #largelanguagemodels #openai #openaiban #chatgpt #reddit #together.xyz #redpajama #llama RedPajama: https://github.com/togethercomputer/RedPajama-Data Open AI in Europe: https://www.govtech.com/products/openais-data-practices-cause-it-problems-in-europe Reddit Ban: https://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/",
      "description": "Examining the data used for training our our LLMs. OpenAI is running into trouble in Europe since it won't disclose exactly what was used for their training data. Reddit is no longer letting anyone use their data for commercial use for free. Finally the folks over at together.xyz have assembled an open-source recipe of the LLaMa training dataset. #datascience #machinelearning #largelanguagemodels #openai #openaiban #chatgpt #reddit #together.xyz #redpajama #llama RedPajama: https://github.com/togethercomputer/RedPajama-Data Open AI in Europe: https://www.govtech.com/products/openais-data-practices-cause-it-problems-in-europe Reddit Ban: https://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/",
      "upload_date": "2023-04-20",
      "total_views": 5241,
      "max_views": 5083,
      "topics": [
        "data",
        "databricks",
        "dolly",
        "fined",
        "instruction",
        "llama",
        "mode",
        "openai",
        "reddit",
        "redpajama",
        "together",
        "tuned"
      ],
      "search_text": "Examining the data used for training our our LLMs. OpenAI is running into trouble in Europe since it won't disclose exactly what was used for their training data. Reddit is no longer letting anyone use their data for commercial use for free. Finally the folks over at together.xyz have assembled an open-source recipe of the LLaMa training dataset. #datascience #machinelearning #largelanguagemodels #openai #openaiban #chatgpt #reddit #together.xyz #redpajama #llama RedPajama: https://github.com/togethercomputer/RedPajama-Data Open AI in Europe: https://www.govtech.com/products/openais-data-practices-cause-it-problems-in-europe Reddit Ban: https://arstechnica.com/information-technology/2023/04/reddit-will-start-charging-ai-models-learning-from-its-extremely-human-archives/ data databricks dolly fined instruction llama mode openai reddit redpajama together tuned",
      "platforms": {
        "tiktok": {
          "video_id": "7224262469185735979",
          "url": "https://www.tiktok.com/@rajistics/video/7224262469185735979",
          "view_count": 5083,
          "upload_date": "2023-04-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "smgSKz7x29A",
          "url": "https://www.youtube.com/watch?v=smgSKz7x29A",
          "view_count": 158,
          "upload_date": "2023-04-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6221,
      "title": "Data Quality in the AI Era - To learn more about this example, check out Hannaneh Hajishirzi - OLMo: Accelerating the Science of Language Modeling (COLM) https://www.youtube.com/watch?v=qMTzor0j418&t=2315s Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models -  https://arxiv.org/pdf/2409.17146 Examples of a traditional image-text dataset: https://huggingface.co/datasets/google-research-datasets/conceptual_captions?row=22",
      "description": "Data Quality in the AI Era - To learn more about this example, check out Hannaneh Hajishirzi - OLMo: Accelerating the Science of Language Modeling (COLM) https://www.youtube.com/watch?v=qMTzor0j418&t=2315s Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models -  https://arxiv.org/pdf/2409.17146 Examples of a traditional image-text dataset: https://huggingface.co/datasets/google-research-datasets/conceptual_captions?row=22",
      "upload_date": "2024-11-20",
      "total_views": 5232,
      "max_views": 5066,
      "topics": [
        "data",
        "improving",
        "meta",
        "models",
        "molmo",
        "multimodal",
        "open",
        "quality",
        "time"
      ],
      "search_text": "Data Quality in the AI Era - To learn more about this example, check out Hannaneh Hajishirzi - OLMo: Accelerating the Science of Language Modeling (COLM) https://www.youtube.com/watch?v=qMTzor0j418&t=2315s Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models -  https://arxiv.org/pdf/2409.17146 Examples of a traditional image-text dataset: https://huggingface.co/datasets/google-research-datasets/conceptual_captions?row=22 data improving meta models molmo multimodal open quality time Did you hear that Meta had 8,000 times more data and their model still did worse? What? Is it more data always better? Not this time. The Allen Institute released their new multimodal models and they outperformed Lama 3.2, both on traditional benchmarks as well as human preferences. I'm confused. Don't the scaling laws say that the quantity of data is important? It's all about the data quality. Take a look at the 6 billion image taxpayers that Meta used. Oh yeah, I spent time writing those captions to help the visually impaired. You should have built open AI and Meta for your time. But the problem here is the captions are very sparse. Compare this to what the Allen Institute used. Wow, that's a lot more detail. I wouldn't have typed that much. Instead of typing, they asked the annotators to describe images for 60 to 90 seconds. They prompted them to give descriptions, spatial positioning. That's genius. Once they start talking, they'll mention stuff that they wouldn't have otherwise typed. Exactly. It's rich structured data. And that's why 700,000 examples were plenty to train a high quality model. So when it comes to data, quality beats quantity. Now only if logistics learned that lesson.",
      "platforms": {
        "tiktok": {
          "video_id": "7439418872781884715",
          "url": "https://www.tiktok.com/@rajistics/video/7439418872781884715",
          "view_count": 5066,
          "upload_date": "2024-11-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/77b2fdf224304e23b835dacb2ca14b3b_1732124694~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=EmIcV7lwlcqXW8dMX4GotDry%2FS0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18039050324033365",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": " -_DGpi971Vg",
          "url": "https://www.youtube.com/watch?v= -_DGpi971Vg",
          "view_count": 166,
          "upload_date": "2024-11-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6491,
      "title": "Speed run - 8 minute video on 16 Challenges for using large language models (LLMs) 1. Unfathomable Datasets 2. Tokenizer-Reliance 3. High Pre-Training Costs 4. Fine-Tuning Overhead 5. High Inference Latenc 6. Limited Context Lengt 7. Prompt Brittlenes 8. Hallucination 9. Misaligned Behavior 10. Outdated Knowledge 11. Brittle Evaluation 12. Evaluations Based on Static Human-Written Ground Truth 13. Indistinguishability between Generated and Human-Written Text 14. Tasks Not Solvable By Scale 15. Lacking Experimental Designs 16. Lack of Reproducibility See the full paper: Challenges and Applications of Large Language Models - https://arxiv.org/pdf/2307.10169.pdf #machinelearning #largelanguagemodels",
      "description": "Speed run - 8 minute video on 16 Challenges for using large language models (LLMs) 1. Unfathomable Datasets 2. Tokenizer-Reliance 3. High Pre-Training Costs 4. Fine-Tuning Overhead 5. High Inference Latenc 6. Limited Context Lengt 7. Prompt Brittlenes 8. Hallucination 9. Misaligned Behavior 10. Outdated Knowledge 11. Brittle Evaluation 12. Evaluations Based on Static Human-Written Ground Truth 13. Indistinguishability between Generated and Human-Written Text 14. Tasks Not Solvable By Scale 15. Lacking Experimental Designs 16. Lack of Reproducibility See the full paper: Challenges and Applications of Large Language Models - https://arxiv.org/pdf/2307.10169.pdf #machinelearning #largelanguagemodels",
      "upload_date": "2023-08-10",
      "total_views": 5226,
      "max_views": 4663,
      "topics": [
        "challenges",
        "highlights",
        "language",
        "large",
        "largelanguagemodels",
        "llms",
        "machinelearning",
        "models",
        "paper"
      ],
      "search_text": "Speed run - 8 minute video on 16 Challenges for using large language models (LLMs) 1. Unfathomable Datasets 2. Tokenizer-Reliance 3. High Pre-Training Costs 4. Fine-Tuning Overhead 5. High Inference Latenc 6. Limited Context Lengt 7. Prompt Brittlenes 8. Hallucination 9. Misaligned Behavior 10. Outdated Knowledge 11. Brittle Evaluation 12. Evaluations Based on Static Human-Written Ground Truth 13. Indistinguishability between Generated and Human-Written Text 14. Tasks Not Solvable By Scale 15. Lacking Experimental Designs 16. Lack of Reproducibility See the full paper: Challenges and Applications of Large Language Models - https://arxiv.org/pdf/2307.10169.pdf #machinelearning #largelanguagemodels challenges highlights language large largelanguagemodels llms machinelearning models paper",
      "platforms": {
        "tiktok": {
          "video_id": "7265678582930804011",
          "url": "https://www.tiktok.com/@rajistics/video/7265678582930804011",
          "view_count": 4663,
          "upload_date": "2023-08-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "B5MhLSapnms",
          "url": "https://www.youtube.com/watch?v=B5MhLSapnms",
          "view_count": 563,
          "upload_date": "2023-08-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Anomaly detection is hard. This is an introduction to anomaly detection algorithms. The video focuses on the results for ADBench and what data scientists should now do.  #datascience #analytics #codetok #anomalydetection @rajistics  ",
      "description": "Anomaly detection is hard. This is an introduction to anomaly detection algorithms. The video focuses on the results for ADBench and what data scientists should now do.  #datascience #analytics #codetok #anomalydetection @rajistics  ",
      "upload_date": "2022-09-26",
      "total_views": 5210,
      "max_views": 5210,
      "topics": [
        "analytics",
        "anomaly",
        "codetok",
        "data",
        "datascience",
        "detection"
      ],
      "search_text": "Anomaly detection is hard. This is an introduction to anomaly detection algorithms. The video focuses on the results for ADBench and what data scientists should now do.  #datascience #analytics #codetok #anomalydetection @rajistics   analytics anomaly codetok data datascience detection Want to hear the best algorithm for anomaly detection? Gotcha! There isn't one. Let's dig into a study that found none of the anomaly detection algorithms is statistically better than the others. An anomaly detection is about finding unusual patterns in the dataset. And this can come in handy for things like fraud, where people are always trying new techniques, or predictive maintenance where your car starts acting wacko. This study evaluated over 30 different algorithms on 57 different datasets. And what they found out was none of them stood out. But that doesn't mean all is lost. Here's some things you can do. Roll up your sleeves when you're working with anomaly detection algorithms. Each type of algorithm is going to focus on different patterns in the data. Understanding your data and matching the right algorithm is going to give you a better likelihood for success. Get out of the unsupervised world. Just go label some data. Even having a little bit of labeled data in a semi-supervised approach is going to be way better.",
      "platforms": {
        "tiktok": {
          "video_id": "7147830649490033966",
          "url": "https://www.tiktok.com/@rajistics/video/7147830649490033966",
          "view_count": 5210,
          "upload_date": "2022-09-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d82c4b2e25d2489d95372baa7d092132_1664234020~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=zNcYbnbTZ3elGH9ogmdX%2FPzLxRo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6323,
      "title": "SetFit: Few Shot Learning for Text Classification",
      "description": "SetFit: Few Shot Learning for Text Classification",
      "upload_date": "2022-10-28",
      "total_views": 5181,
      "max_views": 5181,
      "topics": [
        "classification",
        "contrastive",
        "datascience",
        "intuition",
        "learning",
        "machinelearning",
        "setfit",
        "shot",
        "statistics",
        "text"
      ],
      "search_text": "SetFit: Few Shot Learning for Text Classification classification contrastive datascience intuition learning machinelearning setfit shot statistics text",
      "platforms": {
        "instagram": {
          "video_id": "18236262244180815",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Pg-smN4fUy0",
          "url": "https://www.youtube.com/watch?v=Pg-smN4fUy0",
          "view_count": 5181,
          "upload_date": "2022-10-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "How AIrbnb customer support is using generative AI. This is a great example of how @rajistics in context learning is growing and replacing traditional machine learning approaches for some use cases.  #datascience #machinelearning #largelanguagemodels #generativeai #incontextlearning ",
      "description": "How AIrbnb customer support is using generative AI. This is a great example of how @rajistics in context learning is growing and replacing traditional machine learning approaches for some use cases.  #datascience #machinelearning #largelanguagemodels #generativeai #incontextlearning ",
      "upload_date": "2023-01-16",
      "total_views": 5175,
      "max_views": 5175,
      "topics": [
        "airbnb",
        "datascience",
        "generativeai",
        "largelanguagemodels",
        "learning",
        "machinelearning"
      ],
      "search_text": "How AIrbnb customer support is using generative AI. This is a great example of how @rajistics in context learning is growing and replacing traditional machine learning approaches for some use cases.  #datascience #machinelearning #largelanguagemodels #generativeai #incontextlearning  airbnb datascience generativeai largelanguagemodels learning machinelearning Let me share how traditional machine learning is changing with all the new changes brought on by generative AI. Airbnb recently shared how generative AI and large language models is reshaping how it's doing customer support. The first example is matching up relevant documents to what the user needs, and often we do a similarity search. With a generative approach, what they're doing is asking the model, is this a relevant document or not? Please answer with a yes, no question. And what Airbnb found is by prompting the model this way, using in-context learning, the model was able to actually provide better recommendations. A second example is when a customer contacts support, there's often about four different things that Airbnb likes to think about. So to get answers to these questions, all Airbnb did was prompt their model on these specific questions. And it's just an example of how using prompting or in-context learning with large language models can really reshape and rethink how we traditionally solve a lot of machine learning problems.",
      "platforms": {
        "tiktok": {
          "video_id": "7189065626432834859",
          "url": "https://www.tiktok.com/@rajistics/video/7189065626432834859",
          "view_count": 5175,
          "upload_date": "2023-01-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3f1522fee16b4a079170c6c94ec5694d_1673834787~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=H9kQFhrSz0zycI73iUReWjbrLEE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Singular value decomposition is one of many low rank methods when working with matrices. This video shares the intuition for why SVD matters and why it's so widely used in recommender systems, working with text and images, and even large language models. The linear algebra class is useful. #datascience #machinelearning #svd #singularvaluedecomposition #matrixalgebra",
      "description": "Singular value decomposition is one of many low rank methods when working with matrices. This video shares the intuition for why SVD matters and why it's so widely used in recommender systems, working with text and images, and even large language models. The linear algebra class is useful. #datascience #machinelearning #svd #singularvaluedecomposition #matrixalgebra",
      "upload_date": "2024-04-03",
      "total_views": 5168,
      "max_views": 5168,
      "topics": [
        "datascience",
        "machinelearning",
        "matrices",
        "matrixalgebra",
        "singularvaluedecomposition",
        "svd"
      ],
      "search_text": "Singular value decomposition is one of many low rank methods when working with matrices. This video shares the intuition for why SVD matters and why it's so widely used in recommender systems, working with text and images, and even large language models. The linear algebra class is useful. #datascience #machinelearning #svd #singularvaluedecomposition #matrixalgebra datascience machinelearning matrices matrixalgebra singularvaluedecomposition svd You ever got to use a cheat sheet for a test? You probably spent a lot of time compressing all that knowledge down onto one little card. Well, the same thing that you're doing, this is what AI has to do when it's faced with large amounts of images and text. And I want to talk about one of the approaches that modern AI uses called singular value decomposition. So let's take this picture of cat in black and white. It represents a matrix of 2,500 by 2,392. So the way singular value decomposition or SVD works is it takes this matrix of data and breaks it down into several other matrices. These other matrices are special. And one of the things we can do is only use a subset of these matrices. So here you'll see I'm going to reduce the value of K, which is the amount of information I'm going to take from these matrices. And as I reduce that down, you'll see that the picture becomes blurry, hard to receive, because I'm using less information to represent it.",
      "platforms": {
        "tiktok": {
          "video_id": "7353745361950756139",
          "url": "https://www.tiktok.com/@rajistics/video/7353745361950756139",
          "view_count": 5168,
          "upload_date": "2024-04-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0cc97871969941eb88eb02c05ec3f6bc_1712177288~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=GzaeHGoqGRMfS%2FC%2FXGZUNLpXZYg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6040,
      "title": "Replying to @Davos What's the best algorithm? ü§î There is no best algorithm! This is an excellent reminder of the free lunch theorem; no algorithm is always the best in data science. If you need empirical proof go check out Kaggle competitions where you will see a variety of winnings algorithms. In this video I highlight \"#1 solution - generalization with linear regression‚Äù as the winning solution for the Kaggle competition - GoDaddy - Microbusiness Density Forecasting. This solution beat over 3000 other teams using a linear model! https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/395131 #datascience #machinelearning #algorithms #nofreelunch",
      "description": "Replying to @Davos What's the best algorithm? ü§î There is no best algorithm! This is an excellent reminder of the free lunch theorem; no algorithm is always the best in data science. If you need empirical proof go check out Kaggle competitions where you will see a variety of winnings algorithms. In this video I highlight \"#1 solution - generalization with linear regression‚Äù as the winning solution for the Kaggle competition - GoDaddy - Microbusiness Density Forecasting. This solution beat over 3000 other teams using a linear model! https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/395131 #datascience #machinelearning #algorithms #nofreelunch",
      "upload_date": "2023-06-26",
      "total_views": 5164,
      "max_views": 5069,
      "topics": [
        "1",
        "algorithm",
        "algorithms",
        "best",
        "datascience",
        "machinelearning",
        "nofreelunch"
      ],
      "search_text": "Replying to @Davos What's the best algorithm? ü§î There is no best algorithm! This is an excellent reminder of the free lunch theorem; no algorithm is always the best in data science. If you need empirical proof go check out Kaggle competitions where you will see a variety of winnings algorithms. In this video I highlight \"#1 solution - generalization with linear regression‚Äù as the winning solution for the Kaggle competition - GoDaddy - Microbusiness Density Forecasting. This solution beat over 3000 other teams using a linear model! https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/395131 #datascience #machinelearning #algorithms #nofreelunch 1 algorithm algorithms best datascience machinelearning nofreelunch",
      "platforms": {
        "tiktok": {
          "video_id": "7249048357484825902",
          "url": "https://www.tiktok.com/@rajistics/video/7249048357484825902",
          "view_count": 5069,
          "upload_date": "2023-06-26",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18011790010677445",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-06-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3qJ2AOXNdRY",
          "url": "https://www.youtube.com/watch?v=3qJ2AOXNdRY",
          "view_count": 95,
          "upload_date": "2023-06-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6144,
      "title": "Nat.dev playground is awesome. Should be a great reminder of the diversity of large language models. #datascience #machinelearning #largelanguagemodels #natdev #gpt3",
      "description": "Nat.dev playground is awesome. Should be a great reminder of the diversity of large language models. #datascience #machinelearning #largelanguagemodels #natdev #gpt3",
      "upload_date": "2023-03-10",
      "total_views": 5144,
      "max_views": 5144,
      "topics": [
        "datascience",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "models",
        "nat",
        "natdev"
      ],
      "search_text": "Nat.dev playground is awesome. Should be a great reminder of the diversity of large language models. #datascience #machinelearning #largelanguagemodels #natdev #gpt3 datascience gpt3 largelanguagemodels machinelearning models nat natdev",
      "platforms": {
        "instagram": {
          "video_id": "17997547876673916",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "o9pI7zl3ySE",
          "url": "https://youtube.com/shorts/o9pI7zl3ySE?feature=share",
          "view_count": 5144,
          "upload_date": "2023-03-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "We all want to get paid. But just know you will end up miserable. #datascience  #codetok #analytics ",
      "description": "We all want to get paid. But just know you will end up miserable. #datascience  #codetok #analytics ",
      "upload_date": "2022-08-19",
      "total_views": 5128,
      "max_views": 5128,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "get",
        "paid",
        "want"
      ],
      "search_text": "We all want to get paid. But just know you will end up miserable. #datascience  #codetok #analytics  analytics codetok datascience get paid want Thanks for watching!",
      "platforms": {
        "tiktok": {
          "video_id": "7133623870418079022",
          "url": "https://www.tiktok.com/@rajistics/video/7133623870418079022",
          "view_count": 5128,
          "upload_date": "2022-08-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bbe0288c3584443b8e7df0c7aa2c007a_1660926243~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=%2BvnD3iISlC2HBTygp4dBBjdUMS0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6095,
      "title": "The Secrets of OpenAI O1 comes down to scaling test-time compute. Hugging Face shared some research on ways to improve test-time compute using process reward models and a smart search strategy. To learn more:  Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations -  https://arxiv.org/pdf/2312.08935 Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning - https://arxiv.org/pdf/2410.08146 Solving math word problems with processand outcome-based feedback - https://arxiv.org/pdf/2211.14275 Scaling test-time compute with Open Models - https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute",
      "description": "The Secrets of OpenAI O1 comes down to scaling test-time compute. Hugging Face shared some research on ways to improve test-time compute using process reward models and a smart search strategy. To learn more:  Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations -  https://arxiv.org/pdf/2312.08935 Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning - https://arxiv.org/pdf/2410.08146 Solving math word problems with processand outcome-based feedback - https://arxiv.org/pdf/2211.14275 Scaling test-time compute with Open Models - https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute",
      "upload_date": "2024-12-20",
      "total_views": 5114,
      "max_views": 4047,
      "topics": [
        "compute",
        "model",
        "models",
        "openai",
        "scaling",
        "secrets",
        "test",
        "time"
      ],
      "search_text": "The Secrets of OpenAI O1 comes down to scaling test-time compute. Hugging Face shared some research on ways to improve test-time compute using process reward models and a smart search strategy. To learn more:  Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations -  https://arxiv.org/pdf/2312.08935 Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning - https://arxiv.org/pdf/2410.08146 Solving math word problems with processand outcome-based feedback - https://arxiv.org/pdf/2211.14275 Scaling test-time compute with Open Models - https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute compute model models openai scaling secrets test time How do models like 01 for OpenAI perform so well? They use test time compute to generate a lot of outputs, and that can improve the overall performance of the model. So just having more predictions helps. Oh yeah, people have been doing this with self-reflection and chain of thought. So this is where the process reward model comes in. Instead of only rewarding the model at the very end when it gets the right answer, we reward it at each reasoning step. This is kind of like how when you grade somebody's homework, you grade them not just for the final solution, but you give them partial credit along the way. How would you train a model to do that? We're using humans here to annotate reasoning steps as correct or incorrect, and the model is gonna learn from that feedback of that annotated data so it knows when it's on track and when it's making a mistake. So how does this relate to that test time compute scaling? So with the process reward model, you can try more advanced inference techniques because you can evaluate them at every step. HuggingFace, for example, found that an extended beam approach works really well. They even split their approach into subtrees, and this technique allows a one bead model to outperform a seven bead model. Ah, so it's not just bigger models anymore, but using more compute and smarter search at test time. Exactly, test time strategies can boost performance of models and it's one of the hottest areas of AI research.",
      "platforms": {
        "tiktok": {
          "video_id": "7450643035727105311",
          "url": "https://www.tiktok.com/@rajistics/video/7450643035727105311",
          "view_count": 4047,
          "upload_date": "2024-12-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oUiGmElPGRAimBABoAmBAAwbAZTIAY2vEK9bL~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=pm2P7krTVBK%2Fk5arY5zjik3t070%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18060711517890830",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "JQYvWRvAqlQ",
          "url": "https://www.youtube.com/watch?v=JQYvWRvAqlQ",
          "view_count": 1067,
          "upload_date": "2024-12-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "How enterprises are dealing with ChatGPT it’s a pretty familiar cycle of grief.  The good thing is it does open up lots of cool use cases. #datascience #machinelearning #chatgpt #enterprisearchitecture ",
      "description": "How enterprises are dealing with ChatGPT it’s a pretty familiar cycle of grief.  The good thing is it does open up lots of cool use cases. #datascience #machinelearning #chatgpt #enterprisearchitecture ",
      "upload_date": "2023-02-05",
      "total_views": 5090,
      "max_views": 5090,
      "topics": [
        "chat",
        "chatgpt",
        "datascience",
        "got",
        "gpt",
        "hey"
      ],
      "search_text": "How enterprises are dealing with ChatGPT it’s a pretty familiar cycle of grief.  The good thing is it does open up lots of cool use cases. #datascience #machinelearning #chatgpt #enterprisearchitecture  chat chatgpt datascience got gpt hey Hey, have you checked out this chat GPT and what it can do? No, I haven't checked it out. It's probably a stage demo. Wouldn't worry about it. Hey, I just got a briefing from Microsoft. This thing's for real. I need to know what our chat GPT strategy is. I feel bad I saw this coming. I should have hired that machine learning engineer. She was so good at foundation models. And I never got the GPU cluster set up, never got those models inside. I know. I know the rest of the team and I wanted to get involved in large language models, but maybe this will be our opportunity. I wouldn't be in this situation if IT let us have our GPU cluster and HR didn't have all these crazy hiring rules. Look, we don't set the rules. You wanted something different. Talk to enterprise architecture. I have a long list of projects to do. GPU cluster, way at the bottom. He just needs to work this out. Let him go make a plan. Hey, Raj, I'm dealing with this chat GPT thing. You got some ideas for us? Hey, you're not alone. I've had a lot of calls from enterprise leaders these last few weeks on chat GPT strategies. I got some good ideas for what we can do. Hey, everyone, let's get in the conference room. We've got some ideas for what we can do now to build a new project called Acme Bot. I'm excited to show you our new Acme Bot. It was inspired by chat GPT, but this is gonna go a step beyond and really what our enterprise needs. Hey, this looks good. Is this for real? Yeah, the team and I pushed really hard over the last two months to get this done. Excellent work team. I'm gonna show this off at the executive offsite in Bermuda next month. Meantime, why don't you guys get yourself a pizza party?",
      "platforms": {
        "tiktok": {
          "video_id": "7196739677242576171",
          "url": "https://www.tiktok.com/@rajistics/video/7196739677242576171",
          "view_count": 5090,
          "upload_date": "2023-02-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ecb54f72cd3848d3948edc863fbec49d_1675621550~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=4W15wOhANA3NDxadLKtGGQYqU70%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5981,
      "title": "In this video, I explain Jason Wei’s insight from his recent blog post on the asymmetry of verification and what he calls Verifiers’ Law: AI learns fastest in tasks where answers are easy to verify—even if solving them is hard. I’ll explore why tasks like coding, math, and trivia show rapid progress, while more subjective tasks, like writing or strategy, remain challenging. This concept also explains the jagged edge of intelligence—why AI is spiky in its capabilities. Ultimately, in AI, verification isn’t just how we measure progress—it’s what enables it. Read Jason’s full blog post here: 🔗 https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law",
      "description": "In this video, I explain Jason Wei’s insight from his recent blog post on the asymmetry of verification and what he calls Verifiers’ Law: AI learns fastest in tasks where answers are easy to verify—even if solving them is hard. I’ll explore why tasks like coding, math, and trivia show rapid progress, while more subjective tasks, like writing or strategy, remain challenging. This concept also explains the jagged edge of intelligence—why AI is spiky in its capabilities. Ultimately, in AI, verification isn’t just how we measure progress—it’s what enables it. Read Jason’s full blog post here: 🔗 https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law",
      "upload_date": "2025-07-16",
      "total_views": 5071,
      "max_views": 3817,
      "topics": [
        "asymmetry",
        "jason",
        "law",
        "like",
        "math",
        "progress",
        "tasks",
        "verification",
        "verifier",
        "wei"
      ],
      "search_text": "In this video, I explain Jason Wei’s insight from his recent blog post on the asymmetry of verification and what he calls Verifiers’ Law: AI learns fastest in tasks where answers are easy to verify—even if solving them is hard. I’ll explore why tasks like coding, math, and trivia show rapid progress, while more subjective tasks, like writing or strategy, remain challenging. This concept also explains the jagged edge of intelligence—why AI is spiky in its capabilities. Ultimately, in AI, verification isn’t just how we measure progress—it’s what enables it. Read Jason’s full blog post here: 🔗 https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law asymmetry jason law like math progress tasks verification verifier wei Why has AI progress like this? Well, Jason Wee, one of Meta's $100 million researcher, shows that AI works the same way with a twist. Only tasks we can verify become tasks that AI gets good at. So to explain this, let's start with how we train AI models. We need problems where answers exist. How else could the model learn? That's why areas like coding, math, sentiment, where clear answers exist, we see progress with AI. But Wee points out a deeper truth. There's an asymmetry between solving and verifying. So generating a solution is just a start. More important is to check whether it's correct. This is where you want it to be easy, and this asymmetry shapes how AI can improve. Even in coding and math, the models learn because the tasks are structured for easy verification. Whether or not the code runs. Whether the math answer matches the answer key. If we look at benchmarks like Trivia QA and software engineering tasks, they are also like this. They allow for fast reliable verification. This helps you get rapid iteration and improvement in your models. This also explains the jagged edge of intelligence. Where AI is going to excel where verification is clear, it's going to stall or slow down where feedback is slow, costly or subjective.",
      "platforms": {
        "tiktok": {
          "video_id": "7527829029034921247",
          "url": "https://www.tiktok.com/@rajistics/video/7527829029034921247",
          "view_count": 3817,
          "upload_date": "2025-07-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEAjfeEhFT0SU4qLIog8OGAIDfAoFIBnnCSQDI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=xXqd2ArCfp0wBweqL7weoURxyKY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17885439114331199",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "xz38mya1XEI",
          "url": "https://www.youtube.com/watch?v=xz38mya1XEI",
          "view_count": 1254,
          "upload_date": "2025-07-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Favorite tweet today. #statistics #datascience #codetok #machinelearning",
      "description": "Favorite tweet today. #statistics #datascience #codetok #machinelearning",
      "upload_date": "2022-05-19",
      "total_views": 5071,
      "max_views": 5071,
      "topics": [
        "codetok",
        "datascience",
        "favorite",
        "machinelearning",
        "statistics",
        "way"
      ],
      "search_text": "Favorite tweet today. #statistics #datascience #codetok #machinelearning codetok datascience favorite machinelearning statistics way I don't know, it just seems so silly to me. I mean, everybody feels this way. What the poor naive dingbat did not realize was that most people, in fact, did not feel that way.",
      "platforms": {
        "tiktok": {
          "video_id": "7099573851834748206",
          "url": "https://www.tiktok.com/@rajistics/video/7099573851834748206",
          "view_count": 5071,
          "upload_date": "2022-05-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f04ce4612d384e629361e417c37b1414_1652998352~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=A%2F0xo8ZTpwWJKKc8Ai9ttwR9XCc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6641,
      "title": "This group is holding COLLIDE Data Conference on October 3-4 at Center Stage Theater üé∏üé≠ in the heart of midtown Atlanta Georgia. Register now with promo code \"INFLUENCER60\" and take 60% OFF! #rajistics #datascience",
      "description": "This group is holding COLLIDE Data Conference on October 3-4 at Center Stage Theater üé∏üé≠ in the heart of midtown Atlanta Georgia. Register now with promo code \"INFLUENCER60\" and take 60% OFF! #rajistics #datascience",
      "upload_date": "2023-08-30",
      "total_views": 5046,
      "max_views": 4106,
      "topics": [
        "august",
        "collide",
        "conference",
        "data",
        "datascience",
        "earnings",
        "group",
        "holding",
        "news",
        "nvidia"
      ],
      "search_text": "This group is holding COLLIDE Data Conference on October 3-4 at Center Stage Theater üé∏üé≠ in the heart of midtown Atlanta Georgia. Register now with promo code \"INFLUENCER60\" and take 60% OFF! #rajistics #datascience august collide conference data datascience earnings group holding news nvidia",
      "platforms": {
        "tiktok": {
          "video_id": "7272936853496532266",
          "url": "https://www.tiktok.com/@rajistics/video/7272936853496532266",
          "view_count": 4106,
          "upload_date": "2023-08-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "xeJYTHas69Q",
          "url": "https://www.youtube.com/watch?v=xeJYTHas69Q",
          "view_count": 940,
          "upload_date": "2023-08-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Getting even bigger with all the new vision models  ",
      "description": "Getting even bigger with all the new vision models  ",
      "upload_date": "2024-09-25",
      "total_views": 5032,
      "max_views": 5032,
      "topics": [
        "advantage",
        "ask",
        "even",
        "getting",
        "models",
        "take"
      ],
      "search_text": "Getting even bigger with all the new vision models   advantage ask even getting models take You want to know where the money is in AI? It's document AI. Businesses have tons of documents laying around that they're not able to take advantage of, but have great knowledge inside them. The latest AI models take advantage of texts and images. You can take an image like this and ask it a question. Here's another example where I have a table and I ask it what are the net sales in 2020 and it figures it out. And for the data scientists, it runs with just a couple lines of code.",
      "platforms": {
        "tiktok": {
          "video_id": "7418722211311193386",
          "url": "https://www.tiktok.com/@rajistics/video/7418722211311193386",
          "view_count": 5032,
          "upload_date": "2024-09-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c97a493cbe574275bb09b568883fd19c_1727305875~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=x425JUt6RVvNk0e5YrrJybw8A2w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6490,
      "title": "Jokes explained - news in mid-May 2023 Google introduced Bard2 which performs on par with GPT3.5 and Claude from Anthropic. Google also announced it is starting to train Gemni a GPT-5 competitor. The US White House summoned AI leaders to lecture them on privacy and other concerns. Anthropic published another paper in its typical moralizing tone. NVIDIA keeps racking up big gains making huge revenue off AI. The H100 is their latest state-of-the-art GPU and they can't make enough. Google and Amazon both have independent chips sources from NVIDIA (but AWS still uses lots of NVIDIA). We now have three publicly available models that are better than the average human at a wide range of tasks Sam Altman is starting to invest in fusion and Microsoft is the first customer. Cohere is struggling with staying top tier - it can‚Äôt compete with GPT4 and is instead focusing on other models like it‚Äôs new reranker. Cohere is based in Toronto. Together.xyz and Mosaic have open sourced smaller LLMs that can be commercially used. Hinton is leaving AI in an oh-so-dramatic manner. Like he wasn‚Äôt fully responsible and profiting off AI his entire life. Microsoft is doing well on the enterprise market selling million-dollar deals with enterprises for GPT3/4. Amazon has nothing but the sound of customers trying to escape. It announced it‚Äôs Bedrock but right now Bedrock looks more like a swamp of poor contenders. A PIP is a performance plan that Amazon is notorious for using for poor performers. Amazon is well known for having pretty poor working conditions. IBM is jumping into AI by returning to the brand of it‚Äôs previous disaster Watson. Right now Amazon‚Äôs Bedrock is a more viable product than WatsonxAI. Scary for anyone that cares about IBM. #datascience #machinelearning #openai #google #cohere #ibm #anthropic #microsoft #nvidia #amazon",
      "description": "Jokes explained - news in mid-May 2023 Google introduced Bard2 which performs on par with GPT3.5 and Claude from Anthropic. Google also announced it is starting to train Gemni a GPT-5 competitor. The US White House summoned AI leaders to lecture them on privacy and other concerns. Anthropic published another paper in its typical moralizing tone. NVIDIA keeps racking up big gains making huge revenue off AI. The H100 is their latest state-of-the-art GPU and they can't make enough. Google and Amazon both have independent chips sources from NVIDIA (but AWS still uses lots of NVIDIA). We now have three publicly available models that are better than the average human at a wide range of tasks Sam Altman is starting to invest in fusion and Microsoft is the first customer. Cohere is struggling with staying top tier - it can‚Äôt compete with GPT4 and is instead focusing on other models like it‚Äôs new reranker. Cohere is based in Toronto. Together.xyz and Mosaic have open sourced smaller LLMs that can be commercially used. Hinton is leaving AI in an oh-so-dramatic manner. Like he wasn‚Äôt fully responsible and profiting off AI his entire life. Microsoft is doing well on the enterprise market selling million-dollar deals with enterprises for GPT3/4. Amazon has nothing but the sound of customers trying to escape. It announced it‚Äôs Bedrock but right now Bedrock looks more like a swamp of poor contenders. A PIP is a performance plan that Amazon is notorious for using for poor performers. Amazon is well known for having pretty poor working conditions. IBM is jumping into AI by returning to the brand of it‚Äôs previous disaster Watson. Right now Amazon‚Äôs Bedrock is a more viable product than WatsonxAI. Scary for anyone that cares about IBM. #datascience #machinelearning #openai #google #cohere #ibm #anthropic #microsoft #nvidia #amazon",
      "upload_date": "2023-05-12",
      "total_views": 5010,
      "max_views": 3966,
      "topics": [
        "amazon",
        "anthropic",
        "cohere",
        "google",
        "ibm",
        "models",
        "new",
        "news",
        "nvidia"
      ],
      "search_text": "Jokes explained - news in mid-May 2023 Google introduced Bard2 which performs on par with GPT3.5 and Claude from Anthropic. Google also announced it is starting to train Gemni a GPT-5 competitor. The US White House summoned AI leaders to lecture them on privacy and other concerns. Anthropic published another paper in its typical moralizing tone. NVIDIA keeps racking up big gains making huge revenue off AI. The H100 is their latest state-of-the-art GPU and they can't make enough. Google and Amazon both have independent chips sources from NVIDIA (but AWS still uses lots of NVIDIA). We now have three publicly available models that are better than the average human at a wide range of tasks Sam Altman is starting to invest in fusion and Microsoft is the first customer. Cohere is struggling with staying top tier - it can‚Äôt compete with GPT4 and is instead focusing on other models like it‚Äôs new reranker. Cohere is based in Toronto. Together.xyz and Mosaic have open sourced smaller LLMs that can be commercially used. Hinton is leaving AI in an oh-so-dramatic manner. Like he wasn‚Äôt fully responsible and profiting off AI his entire life. Microsoft is doing well on the enterprise market selling million-dollar deals with enterprises for GPT3/4. Amazon has nothing but the sound of customers trying to escape. It announced it‚Äôs Bedrock but right now Bedrock looks more like a swamp of poor contenders. A PIP is a performance plan that Amazon is notorious for using for poor performers. Amazon is well known for having pretty poor working conditions. IBM is jumping into AI by returning to the brand of it‚Äôs previous disaster Watson. Right now Amazon‚Äôs Bedrock is a more viable product than WatsonxAI. Scary for anyone that cares about IBM. #datascience #machinelearning #openai #google #cohere #ibm #anthropic #microsoft #nvidia #amazon amazon anthropic cohere google ibm models new news nvidia",
      "platforms": {
        "tiktok": {
          "video_id": "7232446348703026478",
          "url": "https://www.tiktok.com/@rajistics/video/7232446348703026478",
          "view_count": 3966,
          "upload_date": "2023-05-12",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CsKXyjogyBF",
          "url": "https://www.instagram.com/reel/CsKXyjogyBF",
          "view_count": 480,
          "upload_date": "2023-05-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "T5uXoj27GyI",
          "url": "https://www.youtube.com/watch?v=T5uXoj27GyI",
          "view_count": 564,
          "upload_date": "2023-05-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday Time Series Decomposition is a great technique for starting to understand a time series.",
      "description": "#onthisday Time Series Decomposition is a great technique for starting to understand a time series.",
      "upload_date": "2024-06-12",
      "total_views": 5006,
      "max_views": 5006,
      "topics": [
        "decomposition",
        "let",
        "onthisday",
        "series",
        "start",
        "time"
      ],
      "search_text": "#onthisday Time Series Decomposition is a great technique for starting to understand a time series. decomposition let onthisday series start time Let me share one of my favorite tips when working with time series data, decomposition. So let's start with the data, let's visualize it. Now let's start to decompose it and see the story it tells. It's a simple Python function, but once I do this, I can start identifying the trend. This helps me understand what are the long-term implications of this of the series, the seasonal components. So in this case, we set a period of 12 months. So how much is it changing over those 12 months? Finally, I can see the residuals and the residuals here sometimes could be noise, but other times you'll use more advanced techniques to start understanding some of the other intricacies of this time series. So whatever your favorite data science tool is, look for the decomposition visualizations.",
      "platforms": {
        "tiktok": {
          "video_id": "7379695148864949550",
          "url": "https://www.tiktok.com/@rajistics/video/7379695148864949550",
          "view_count": 5006,
          "upload_date": "2024-06-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/afb184a5bb6540ac9eaf26c03a926584_1718219180~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=XBPDoFvUPrKQFkUMZz9jH8ANXuo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6515,
      "title": "Are you GPU Poor? A deep dive into the state of GPUs based on the work of Dylan Patel of Semi Analysis. How are you coping with the lack of GPUs? #machinelearning #gpus #nvidia #rajistics The State of Silicon and the GPU Poors - with Dylan Patel of SemiAnalysis: https://www.latent.space/p/semianalysis The AI-first transformation of the Data Center: Insights from Nvidia's Q3 2024 Earnings - https://www.datagravity.dev/p/the-ai-first-transformation-of-the Semi-Analysis: https://www.semianalysis.com/",
      "description": "Are you GPU Poor? A deep dive into the state of GPUs based on the work of Dylan Patel of Semi Analysis. How are you coping with the lack of GPUs? #machinelearning #gpus #nvidia #rajistics The State of Silicon and the GPU Poors - with Dylan Patel of SemiAnalysis: https://www.latent.space/p/semianalysis The AI-first transformation of the Data Center: Insights from Nvidia's Q3 2024 Earnings - https://www.datagravity.dev/p/the-ai-first-transformation-of-the Semi-Analysis: https://www.semianalysis.com/",
      "upload_date": "2023-11-22",
      "total_views": 4999,
      "max_views": 3148,
      "topics": [
        "deep",
        "dive",
        "going",
        "gpu",
        "gpus",
        "large",
        "machinelearning",
        "market",
        "models",
        "nvidia",
        "poors",
        "rich",
        "semianalysis",
        "state"
      ],
      "search_text": "Are you GPU Poor? A deep dive into the state of GPUs based on the work of Dylan Patel of Semi Analysis. How are you coping with the lack of GPUs? #machinelearning #gpus #nvidia #rajistics The State of Silicon and the GPU Poors - with Dylan Patel of SemiAnalysis: https://www.latent.space/p/semianalysis The AI-first transformation of the Data Center: Insights from Nvidia's Q3 2024 Earnings - https://www.datagravity.dev/p/the-ai-first-transformation-of-the Semi-Analysis: https://www.semianalysis.com/ deep dive going gpu gpus large machinelearning market models nvidia poors rich semianalysis state",
      "platforms": {
        "tiktok": {
          "video_id": "7304409230990249258",
          "url": "https://www.tiktok.com/@rajistics/video/7304409230990249258",
          "view_count": 3148,
          "upload_date": "2023-11-22",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cz9uQS_AxHL",
          "url": "https://www.instagram.com/reel/Cz9uQS_AxHL",
          "view_count": 1592,
          "upload_date": "2023-11-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "C3JVtGAe-6A",
          "url": "https://www.youtube.com/watch?v=C3JVtGAe-6A",
          "view_count": 259,
          "upload_date": "2023-11-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6317,
      "title": "One of my older videos Predicting crime",
      "description": "One of my older videos Predicting crime",
      "upload_date": "2023-07-05",
      "total_views": 4995,
      "max_views": 4156,
      "topics": [
        "crime",
        "face",
        "hugging",
        "older",
        "one",
        "predicting",
        "sentiment",
        "spreadsheet",
        "style",
        "text",
        "transfer",
        "transformers",
        "using",
        "videos",
        "wow"
      ],
      "search_text": "One of my older videos Predicting crime crime face hugging older one predicting sentiment spreadsheet style text transfer transformers using videos wow",
      "platforms": {
        "tiktok": {
          "video_id": "7252479721844247851",
          "url": "https://www.tiktok.com/@rajistics/video/7252479721844247851",
          "view_count": 4156,
          "upload_date": "2023-07-05",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18301719907074204",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "jA6VDKO7XfA",
          "url": "https://www.youtube.com/watch?v=jA6VDKO7XfA",
          "view_count": 839,
          "upload_date": "2022-11-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6489,
      "title": "Entropy can be a useful measure in machine learning. Entropy and information gain is used in building decision trees. I have also seen entropy used in feature engineering. Here is a short conceptual understanding for entropy. This video is based on the excellent blog post on entropy (that also provides the math) #datascience #machinelearning #featureengineering #informationgain #entropy #decisiontrees Background Photo: Hans-Peter Gauster https://unsplash.com/photos/3y1zF4hIPCg Entropy: How Decision Trees Make Decisions by Sam T: https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8",
      "description": "Entropy can be a useful measure in machine learning. Entropy and information gain is used in building decision trees. I have also seen entropy used in feature engineering. Here is a short conceptual understanding for entropy. This video is based on the excellent blog post on entropy (that also provides the math) #datascience #machinelearning #featureengineering #informationgain #entropy #decisiontrees Background Photo: Hans-Peter Gauster https://unsplash.com/photos/3y1zF4hIPCg Entropy: How Decision Trees Make Decisions by Sam T: https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8",
      "upload_date": "2023-04-27",
      "total_views": 4992,
      "max_views": 3769,
      "topics": [
        "credit",
        "datascience",
        "decisiontrees",
        "entropy",
        "featureengineering",
        "informationgain",
        "know",
        "learning",
        "liability",
        "look",
        "machine",
        "machinelearning",
        "rating"
      ],
      "search_text": "Entropy can be a useful measure in machine learning. Entropy and information gain is used in building decision trees. I have also seen entropy used in feature engineering. Here is a short conceptual understanding for entropy. This video is based on the excellent blog post on entropy (that also provides the math) #datascience #machinelearning #featureengineering #informationgain #entropy #decisiontrees Background Photo: Hans-Peter Gauster https://unsplash.com/photos/3y1zF4hIPCg Entropy: How Decision Trees Make Decisions by Sam T: https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8 credit datascience decisiontrees entropy featureengineering informationgain know learning liability look machine machinelearning rating",
      "platforms": {
        "tiktok": {
          "video_id": "7226740227958787374",
          "url": "https://www.tiktok.com/@rajistics/video/7226740227958787374",
          "view_count": 3769,
          "upload_date": "2023-04-27",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Crix0uHgpLt",
          "url": "https://www.instagram.com/reel/Crix0uHgpLt",
          "view_count": 657,
          "upload_date": "2023-04-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "kJ6h_qRYZdA",
          "url": "https://www.youtube.com/watch?v=kJ6h_qRYZdA",
          "view_count": 566,
          "upload_date": "2023-04-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Crime seems easy to predict, but is super messy. #datascience #crimetok #chicago #statistics #crimonology #machinelearning #codetok #aisnakeoil",
      "description": "Crime seems easy to predict, but is super messy. #datascience #crimetok #chicago #statistics #crimonology #machinelearning #codetok #aisnakeoil",
      "upload_date": "2022-07-02",
      "total_views": 4991,
      "max_views": 4991,
      "topics": [
        "chicago",
        "crime",
        "crimetok",
        "datascience",
        "numbers",
        "statistics"
      ],
      "search_text": "Crime seems easy to predict, but is super messy. #datascience #crimetok #chicago #statistics #crimonology #machinelearning #codetok #aisnakeoil chicago crime crimetok datascience numbers statistics Not again. We got another person thinking they're able to predict crime. Let's talk about this. When I was working on my PhD, we weren't able to get access to Chicago crime statistics. I was able to work with the ACLU and we were able to push back and Chicago eventually released the crime statistics, which everybody now, including lots of data scientists, takes advantage of. But crime's a much deeper story than the numbers you see. For example, a lot of the murders in Chicago are actually driven by gang behavior, which is one of the differences between Chicago and New York over the last 20 years in homicides. For other sorts of crimes like drugs and prostitution, these are happening all over the place all the time. When you see the statistics and numbers, it's because the police decided to concentrate in a particular area at a particular time. And what this means is that the past isn't necessarily what's going to happen in the future, because it's not just the behavior of criminals, it's the behavior of society and police that you have to consider as well. Crime's a great example of where we've taken a complex social phenomenon, boiled it down to a few numbers. And as a data scientist, you have to not get seduced by just because your log loss or your scores are good, without understanding that those numbers and the areas where we don't have numbers have a history and a purpose. And we need to consider that when we're doing these analysis.",
      "platforms": {
        "tiktok": {
          "video_id": "7115801636013952302",
          "url": "https://www.tiktok.com/@rajistics/video/7115801636013952302",
          "view_count": 4991,
          "upload_date": "2022-07-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2f5d0845273246fd9ee42dea73b2f590_1656776683~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=aEjgy04REVHCelOuiuY3zHcsLR8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6117,
      "title": "Intro to Conformal Prediction",
      "description": "Intro to Conformal Prediction",
      "upload_date": "2023-09-28",
      "total_views": 4948,
      "max_views": 2660,
      "topics": [
        "conformal",
        "conformalprediction",
        "datascience",
        "getting",
        "intro",
        "older",
        "prediction",
        "predictioninterval",
        "statistics"
      ],
      "search_text": "Intro to Conformal Prediction conformal conformalprediction datascience getting intro older prediction predictioninterval statistics",
      "platforms": {
        "tiktok": {
          "video_id": "7281331756454776106",
          "url": "https://www.tiktok.com/@rajistics/video/7281331756454776106",
          "view_count": 2288,
          "upload_date": "2023-09-21",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17970622688614875",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-09-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "F35-_R8PGI8",
          "url": "https://www.youtube.com/watch?v=F35-_R8PGI8",
          "view_count": 2660,
          "upload_date": "2023-09-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5942,
      "title": "Uber’s FixRLeak system finds leaks with SonarQube, scopes them with Tree-sitter AST analysis, then lets GenAI safely patch only what it understands, all verified with multiple tests before merge. FixRLeak: Automatically Repairing Java Resource Leaks Using Generative AI” – NeurIPS ML for Systems Workshop 2024",
      "description": "Uber’s FixRLeak system finds leaks with SonarQube, scopes them with Tree-sitter AST analysis, then lets GenAI safely patch only what it understands, all verified with multiple tests before merge. FixRLeak: Automatically Repairing Java Resource Leaks Using Generative AI” – NeurIPS ML for Systems Workshop 2024",
      "upload_date": "2025-11-11",
      "total_views": 4937,
      "max_views": 2476,
      "topics": [
        "ast",
        "automating",
        "chatgpt",
        "code",
        "datascience",
        "fixes",
        "fixrleak",
        "largelanguagemodels",
        "leak",
        "leaks",
        "machinelearning",
        "resource",
        "right",
        "scale",
        "trainingml",
        "uber"
      ],
      "search_text": "Uber’s FixRLeak system finds leaks with SonarQube, scopes them with Tree-sitter AST analysis, then lets GenAI safely patch only what it understands, all verified with multiple tests before merge. FixRLeak: Automatically Repairing Java Resource Leaks Using Generative AI” – NeurIPS ML for Systems Workshop 2024 ast automating chatgpt code datascience fixes fixrleak largelanguagemodels leak leaks machinelearning resource right scale trainingml uber Oh no, SonarCube is warning us about a resource leak. Easy, I'll just ask this in cursor. Just fix resource leak. If it closes too early and breaks everything downstream, do you even know Java? I am pretty good in JavaScript. And I've learned we can trust the vibes. Right, until prod crashes at 3 AM. Are you even on the on-call list? Emotionally, but not literally. Let me tell you the right way to do this. First, we detect leaks using something like SonarCube. Then we run AST-based code analysis to confirm the scope of the memory leak. Then if the resource lives within the function, then we'll use generative AI to patch it safely. If it's outside of that, we skip it, pass it to one of our programmers. It's no global refactoring? Nope, automation is scoped. Your lines of code metric are gonna look off, but my uptime looks amazing. Fine, I'll try your scope down way. We aren't done. Next, you're gonna wanna build it, test it, rescan it all. More testing? Come on, you get paid by the hour? It's about getting it done right the first time.",
      "platforms": {
        "tiktok": {
          "video_id": "7571251196275608863",
          "url": "https://www.tiktok.com/@rajistics/video/7571251196275608863",
          "view_count": 1236,
          "upload_date": "2025-11-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owOEEECVBQAnjMXu2IfykfCUMAEjNBDApQCR8F~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=VAYZD0k0kMzDIWpbJEg9iEPpF0Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "CsCGPH7g44q",
          "url": "https://www.instagram.com/reel/CsCGPH7g44q",
          "view_count": 2476,
          "upload_date": "2023-05-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ItjL7Q9CZPk",
          "url": "https://www.youtube.com/watch?v=ItjL7Q9CZPk",
          "view_count": 1225,
          "upload_date": "2025-11-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Is explainability important for you? #datascience #explainability #interpretability #statistics #codetalk #machinelearning",
      "description": "Is explainability important for you? #datascience #explainability #interpretability #statistics #codetalk #machinelearning",
      "upload_date": "2022-08-06",
      "total_views": 4930,
      "max_views": 4930,
      "topics": [
        "datascience",
        "explainability",
        "interpretability",
        "price",
        "statistics",
        "understand"
      ],
      "search_text": "Is explainability important for you? #datascience #explainability #interpretability #statistics #codetalk #machinelearning datascience explainability interpretability price statistics understand I bet you can understand this machine learning model pretty easily. If you take a look, you probably understand that bathrooms and square footage control the sales price. And for example, if you increase bathrooms, your sales price is going to go up. Pretty understandable model. With this model, it's hard to explain. Although if you're a calculator, you understand the relationship between all of these features or variables and sales price. In reality, for most people, it's really hard to understand. There's some things that seem obvious, like, hey, if I increase the number of bathrooms, my price goes up. But there's some things that are counterintuitive. Take a look at total rooms. This is suggesting, like, if I remove rooms in my house, my price should go up. Hmm. Now, this is not unusual behavior in machine learning models. Lots of times you could have features that are correlated to each other, multi-colinearity, or might interact with each other. And while mathematically, this combination of features can reduce the error and give you accurate predictions, intuitively for a person, they can be hard to understand. Data scientists often view this conundrum as a tradeoff between interpretability and accuracy. And while there are things you could do upfront to make sure your model is easier to understand and easier to explain, often in real life situations, there's a real tradeoff between the two.",
      "platforms": {
        "tiktok": {
          "video_id": "7128784674398194987",
          "url": "https://www.tiktok.com/@rajistics/video/7128784674398194987",
          "view_count": 4930,
          "upload_date": "2022-08-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/cf7579d89d3e4573908f0c24736b0172_1659799531~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=OjSroHQ1J66J%2B%2BjNgBURmeoDWGw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6053,
      "title": "Grok 3 - The video explains how Grok 3's performance claims rely on majority voting across 64 predictions (cons@64) rather than single predictions (cons@1). While this consensus-based approach significantly boosts accuracy scores, it requires 64 times more computing power and isn't practical for real-world use. The video points out that when measuring single-prediction performance, Grok 3 isn't actually leading the pack.",
      "description": "Grok 3 - The video explains how Grok 3's performance claims rely on majority voting across 64 predictions (cons@64) rather than single predictions (cons@1). While this consensus-based approach significantly boosts accuracy scores, it requires 64 times more computing power and isn't practical for real-world use. The video points out that when measuring single-prediction performance, Grok 3 isn't actually leading the pack.",
      "upload_date": "2025-02-23",
      "total_views": 4899,
      "max_views": 4322,
      "topics": [
        "cons",
        "consensus",
        "graphs",
        "grok",
        "one",
        "performance",
        "predictions",
        "scores",
        "shady",
        "single",
        "times",
        "video"
      ],
      "search_text": "Grok 3 - The video explains how Grok 3's performance claims rely on majority voting across 64 predictions (cons@64) rather than single predictions (cons@1). While this consensus-based approach significantly boosts accuracy scores, it requires 64 times more computing power and isn't practical for real-world use. The video points out that when measuring single-prediction performance, Grok 3 isn't actually leading the pack. cons consensus graphs grok one performance predictions scores shady single times video It's a Grok3 Just Drop. Big fanfare, big claims, oh, and a big graph. Can you see this graph? It's turning heads. It shows Grok beating everyone. Open AI, deep-sea, Google. Take a look closer. See that shading? This is where it gets interesting because what you're looking at is two different scores. Consensus at one, which is one prediction, and consensus at 64. That's 64 predictions. Let me explain to you why Consensus 64 is so powerful. The pose your model is right only 60% of the time. That means if you run it once, 60% of the time. But look what happens if you run it 64 times and you take the most common answer. The right answer rises up through the noise. Your score goes up and this is the power of ensembling. Here's the kicker. That's 64 times takes a ton more compute, which is why no one does this in the real world. It gets interesting here for how Open AI used it. Now they use Consensus 64 scores to compare their models. They showed how their new models at Consensus one, which is doing it once, beat old ones that went through and used Consensus 64. Grok, nah. They used it to make graphs like this. Truth is at Consensus one, they're not number one. Now, is Grok the first to be shady with their graphs? Nah, Google. But if you need 64 times to be number one, are you really number one?",
      "platforms": {
        "tiktok": {
          "video_id": "7474420664321969438",
          "url": "https://www.tiktok.com/@rajistics/video/7474420664321969438",
          "view_count": 4322,
          "upload_date": "2025-02-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogVoESfDVQ6jRunEmejw1uDn0BhAKElWFDMuQA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=fqHc%2FxrkfyvbcgB0nNn%2Fcjpe5J8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17962369409747078",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cEZ9WiS9Boo",
          "url": "https://www.youtube.com/watch?v=cEZ9WiS9Boo",
          "view_count": 577,
          "upload_date": "2025-02-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6423,
      "title": "Applying PaLM to the medical domain by using instruction prompt tuning",
      "description": "Applying PaLM to the medical domain by using instruction prompt tuning",
      "upload_date": "2022-12-29",
      "total_views": 4893,
      "max_views": 4893,
      "topics": [
        "applying",
        "domain",
        "instruction",
        "medical",
        "palm",
        "using"
      ],
      "search_text": "Applying PaLM to the medical domain by using instruction prompt tuning applying domain instruction medical palm using",
      "platforms": {
        "instagram": {
          "video_id": "CmwodEiBM-_",
          "url": "https://www.instagram.com/reel/CmwodEiBM-_/",
          "view_count": 0,
          "upload_date": "2022-12-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "DOycgCRagrk",
          "url": "https://youtube.com/shorts/DOycgCRagrk",
          "view_count": 4893,
          "upload_date": "2022-12-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Data drift analysis is a must for production workloads. Here is Uber’s D3 system fie automated drift analysis. This video covers types of data drift issues, different approaches for addrssing them, and Ubers use of a Prophet model for anomaly detection. #datascience #machinelearning  #mlops #datadrift #prophet ",
      "description": "Data drift analysis is a must for production workloads. Here is Uber’s D3 system fie automated drift analysis. This video covers types of data drift issues, different approaches for addrssing them, and Ubers use of a Prophet model for anomaly detection. #datascience #machinelearning  #mlops #datadrift #prophet ",
      "upload_date": "2023-03-13",
      "total_views": 4848,
      "max_views": 4848,
      "topics": [
        "automated",
        "data",
        "drift",
        "issues",
        "prophet",
        "uber"
      ],
      "search_text": "Data drift analysis is a must for production workloads. Here is Uber’s D3 system fie automated drift analysis. This video covers types of data drift issues, different approaches for addrssing them, and Ubers use of a Prophet model for anomaly detection. #datascience #machinelearning  #mlops #datadrift #prophet  automated data drift issues prophet uber Uber just shared D3, which is a new automated way for identifying data quality issues that would affect their machine learning models. Uber started by identifying all the possible issues for data quality. They ruled out ETL and infrastructure simply because those are quick to detect. On the other hand, partial data issues take five times as long to identify. Partial or incomplete data can happen in many ways. One, it could be you're doing some experiments, you end up creating a new feature or modifying an existing feature. Sometimes there's upstream logic changes in ETL where people are joining different data sets together and they might drop or change a feature. And then finally, there's third party data sources that one of those could change and affect your entire pipeline. To detect these issues, Uber quickly identified that having a column level test was the best way to do it. Now with column level tests, you could identify things like null values, changes in numericals and categorical values as well. The traditional way to look for drift is to have manual thresholds. But Uber quickly realized across thousands of features that becomes untenable, so they wanted an automated approach. Uber's automated method D3 starts by building a baseline from the last 90 days. It uses that baseline to build a profit model, which helps it identify trends, seasonality. That's used to then identify anything that's anomalous, anything outside of these ranges. And the great thing about this approach is it works in an automated fashion. You don't have to manually do it. It's resulted in a 20 times reduction in finding data quality issues, even found a number of other issues that they would not have found with manual approach. This type of automated drift analysis, we're going to see more of these for these systems that have thousands of features.",
      "platforms": {
        "tiktok": {
          "video_id": "7210165504630263086",
          "url": "https://www.tiktok.com/@rajistics/video/7210165504630263086",
          "view_count": 4848,
          "upload_date": "2023-03-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/12dab6fba807449a8e3d66259f73209b_1678747487~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=OxBEbNx4dcKE%2FzFXZwTUFuMhNtE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6054,
      "title": "Using recursive feature elimination for feature selection for machine learning.",
      "description": "Using recursive feature elimination for feature selection for machine learning.",
      "upload_date": "2025-02-21",
      "total_views": 4836,
      "max_views": 2939,
      "topics": [
        "deep",
        "demo",
        "eat",
        "elimination",
        "feature",
        "features",
        "learning",
        "like",
        "machine",
        "melon",
        "model",
        "recursive",
        "reinforcement",
        "selection",
        "using",
        "would"
      ],
      "search_text": "Using recursive feature elimination for feature selection for machine learning. deep demo eat elimination feature features learning like machine melon model recursive reinforcement selection using would I'm in trouble. I have 500 features and I need to build a machine learning model. Let's talk about methods for feature selection. Your advice would be helpful. Feature selection is part art, part science. My favorite starting approach is recursive feature elimination where you start with a lot of features and slowly reduce them down. Can you show me how that would work? Here's the results from my last model. You can see I started off with 20 features but at every step I would reduce some of those features while keeping an eye on the overall performance of the model. So the way you're doing feature selection is a lot like a reality show where you're voting off the weakest link. Yes, but with less drama and more cross-validation. So instead of voting, what do you do? I like to use a feature importance technique. Oh, like SHAP or permutation importance that give us a ranking of the most important features? We can figure out a starting point by looking at a visualization like this. You'll see at a certain point by reducing too many features our model performance is going to drop too much. You've eliminated my concerns over feature selection.",
      "platforms": {
        "tiktok": {
          "video_id": "7474016663986294046",
          "url": "https://www.tiktok.com/@rajistics/video/7474016663986294046",
          "view_count": 2939,
          "upload_date": "2025-02-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMfREEqI35yjCAj5KxCLIBMCAAFrEnAQueDVAE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=tOEaZVroFcEBzMQm8bHSK50FucQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18081818650718059",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3TUZw1rlvXc",
          "url": "https://www.youtube.com/watch?v=3TUZw1rlvXc",
          "view_count": 1897,
          "upload_date": "2017-01-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Loss Functions - simple example of MAE versus RSME #datascience #statistics #analytics #codetok #regression",
      "description": "Loss Functions - simple example of MAE versus RSME #datascience #statistics #analytics #codetok #regression",
      "upload_date": "2022-08-30",
      "total_views": 4820,
      "max_views": 4820,
      "topics": [
        "analytics",
        "arse",
        "codetok",
        "datascience",
        "regression",
        "statistics"
      ],
      "search_text": "Loss Functions - simple example of MAE versus RSME #datascience #statistics #analytics #codetok #regression analytics arse codetok datascience regression statistics So why are loss functions so important in machine learning? Let's talk about two common loss functions. One is you have, let's call it may. May is very level, even headed, really kind of sticks to that average. Then you have arse me. Arse me is just goes crazy when things get out of hand. Now the nice thing about may is it's the average. It's easy to explain. Everybody gets it, goes with the flow. Now most data scientists actually like arse me, even though arse me gets excited, is because often if you have a difference between an error of five and ten, an error of ten is more serious and you want your model to recognize that. And by taking arse me, arse me is going to recognize that and give that more influence when it's building the model. But not always. Sometimes you have some humongous outliers, crazy big outliers that if you use RMSE your model is just going to fit to those outliers. And in that case you might want to switch back to using the mean average error instead. But this is why understanding your data, understanding your model, understanding your loss functions, all important considerations a data scientist should make.",
      "platforms": {
        "tiktok": {
          "video_id": "7137466014832413994",
          "url": "https://www.tiktok.com/@rajistics/video/7137466014832413994",
          "view_count": 4820,
          "upload_date": "2022-08-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b4a2ed5cbf9b4d7298f4b33e7506635e_1661820810~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=bishYfaJNtvBnhibMmPcyOktThA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6233,
      "title": "NotebookLM - Convert your notes into a podcast NotebookLM: https://notebooklm.google/ Notebook LLama: https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama tl/dw: https://github.com/rmusser01/tldw NeuralNoise AI Podcast Studio: https://github.com/leopiney/neuralnoise",
      "description": "NotebookLM - Convert your notes into a podcast NotebookLM: https://notebooklm.google/ Notebook LLama: https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama tl/dw: https://github.com/rmusser01/tldw NeuralNoise AI Podcast Studio: https://github.com/leopiney/neuralnoise",
      "upload_date": "2024-10-30",
      "total_views": 4808,
      "max_views": 2627,
      "topics": [
        "audio",
        "conversation",
        "github",
        "google",
        "llama",
        "notebook",
        "notebookllama",
        "notebooklm",
        "open",
        "podcast",
        "recipes",
        "source",
        "version"
      ],
      "search_text": "NotebookLM - Convert your notes into a podcast NotebookLM: https://notebooklm.google/ Notebook LLama: https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama tl/dw: https://github.com/rmusser01/tldw NeuralNoise AI Podcast Studio: https://github.com/leopiney/neuralnoise audio conversation github google llama notebook notebookllama notebooklm open podcast recipes source version Notebook LM is amazing, but I really don't want to hand over my data to Google. Is there an alternative? There are already a few alternatives out there, including Notebook Lama for Meta. That's awesome. But do they work as well as Notebook LM? Well, not exactly. So what's missing? Is it the long context length? Google's always talking that up. Model quality matters, but there's other heavyweights out there for Meta and open AI. It's not that. Is it the audio speech quality? Notebook LM's audio is just unreal. Notebook LM's audio is next level. It sounds like a real conversational. If you look at Notebook Lama, they're using Bart, which sounds more like people reading a conversation than having a conversation. Okay, if it's not the audio, what else makes Notebook LM so great? It's the personalities that bring it to life. Notebook LM isn't just a summary of the topic. Instead, it uses multiple characters that weave conversation in and out to make it much more exciting. They're really riveting conversations. How can we duplicate that? Lama Notebook is using a two-step process, where the first step is to create the podcast transcript. The second is, how can they add more drama? I can see how the prompt creates different personality, adds ums in there, even has them use analogies as well. Notebook LM isn't just one prompt. It's instead using multiple prompts and multiple inferences to weave together that complex conversation. I'm sure the open source is working hard on it. I already see folks building conversational agents like this. Automating podcast, what could go wrong?",
      "platforms": {
        "tiktok": {
          "video_id": "7431656425006435626",
          "url": "https://www.tiktok.com/@rajistics/video/7431656425006435626",
          "view_count": 2627,
          "upload_date": "2024-10-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8fabf231b75945a8960f4cf604266d26_1730317358~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=H%2FsoLzj6ff11WrXV69hgNES%2B5sE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18462639925013152",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "iEwtriTFEvs",
          "url": "https://www.youtube.com/watch?v=iEwtriTFEvs",
          "view_count": 2181,
          "upload_date": "2024-10-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6522,
      "title": "Obliviate is now possible for LLMs. Microsoft researchers share an approach to get Large Language Models to unlearn information. #harrypotter #largelanguagemodels #machinelearning #rajistics Who‚Äôs Harry Potter? Approximate Unlearning in LLMs: https://browse.arxiv.org/pdf/2310.02238.pdf Background image from jazzmeister: https://pixabay.com/photos/cathedral-cloisters-harry-potter-2286910/",
      "description": "Obliviate is now possible for LLMs. Microsoft researchers share an approach to get Large Language Models to unlearn information. #harrypotter #largelanguagemodels #machinelearning #rajistics Who‚Äôs Harry Potter? Approximate Unlearning in LLMs: https://browse.arxiv.org/pdf/2310.02238.pdf Background image from jazzmeister: https://pixabay.com/photos/cathedral-cloisters-harry-potter-2286910/",
      "upload_date": "2023-10-07",
      "total_views": 4788,
      "max_views": 2291,
      "topics": [
        "approximate",
        "harry",
        "harrypotter",
        "largelanguagemodels",
        "llms",
        "machinelearning",
        "obliviate",
        "potter",
        "summary",
        "unlearning"
      ],
      "search_text": "Obliviate is now possible for LLMs. Microsoft researchers share an approach to get Large Language Models to unlearn information. #harrypotter #largelanguagemodels #machinelearning #rajistics Who‚Äôs Harry Potter? Approximate Unlearning in LLMs: https://browse.arxiv.org/pdf/2310.02238.pdf Background image from jazzmeister: https://pixabay.com/photos/cathedral-cloisters-harry-potter-2286910/ approximate harry harrypotter largelanguagemodels llms machinelearning obliviate potter summary unlearning",
      "platforms": {
        "tiktok": {
          "video_id": "7287284593248521518",
          "url": "https://www.tiktok.com/@rajistics/video/7287284593248521518",
          "view_count": 2291,
          "upload_date": "2023-10-07",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CyG43htJytl",
          "url": "https://www.instagram.com/reel/CyG43htJytl",
          "view_count": 2269,
          "upload_date": "2023-10-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EpMxnkoZaTc",
          "url": "https://www.youtube.com/watch?v=EpMxnkoZaTc",
          "view_count": 228,
          "upload_date": "2023-10-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law",
      "description": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law",
      "upload_date": "2022-12-30",
      "total_views": 4775,
      "max_views": 4775,
      "topics": [
        "bar",
        "datascience",
        "exam",
        "gpt",
        "law",
        "pretty"
      ],
      "search_text": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law bar datascience exam gpt law pretty Would you believe that GPT is smart enough to be a lawyer? Researchers had GPT take the bar exam and found out it did pretty well. In the United States, most states have a separate bar exam that you have to take in order to be a licensed lawyer in that state. Don't confuse passing the bar exam with being a good lawyer. Often in law schools, you focus on legal reasoning. And in fact, most people that wanna pass the bar exam take a separate course after law school where they spend three months just cramming for the bar exam. That's what I did and I passed the bar exam. The bar exam tests knowledge in various areas. So here's an example of a question that gets asked. Now the researchers here used GPT 3.5, which is open and available for everybody to use. And all they did to have it test is just modify the prompts like this. They then just started asking it lots of the questions and here's the final results. First you can see it does well above just a random answer. So it has some pretty good knowledge of law. In two areas, evidence and torts, it actually does just as good as the humans. And if we factor in the second place choice, we can see that GPT does pretty well. So it's getting a sense of how to answer these questions. Now on average, it fell short of humans by about 17%. So it's not the human level, but this is pretty good.",
      "platforms": {
        "tiktok": {
          "video_id": "7183010935710223662",
          "url": "https://www.tiktok.com/@rajistics/video/7183010935710223662",
          "view_count": 4775,
          "upload_date": "2022-12-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/481a3c502d8442b39e8a9bca9eb5a10f_1672425076~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=nFEZ65or8JWRFtRKa06gqzU6mRE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6639,
      "title": "The reality of AI Agents from Embra. While everyone hypes up Agents its a lot harder to make useful products based on Agents. #machinelearning #datascience #autogpt #embra #aiagents Original tweet by Zach of Embra: https://twitter.com/zachtratar/status/1694024240880861571 Agent Demo by Yohei: https://twitter.com/yoheinakajima/status/1640068466974633987?s=20",
      "description": "The reality of AI Agents from Embra. While everyone hypes up Agents its a lot harder to make useful products based on Agents. #machinelearning #datascience #autogpt #embra #aiagents Original tweet by Zach of Embra: https://twitter.com/zachtratar/status/1694024240880861571 Agent Demo by Yohei: https://twitter.com/yoheinakajima/status/1640068466974633987?s=20",
      "upload_date": "2023-08-23",
      "total_views": 4773,
      "max_views": 4341,
      "topics": [
        "agents",
        "aiagents",
        "autogpt",
        "datascience",
        "embra",
        "machinelearning"
      ],
      "search_text": "The reality of AI Agents from Embra. While everyone hypes up Agents its a lot harder to make useful products based on Agents. #machinelearning #datascience #autogpt #embra #aiagents Original tweet by Zach of Embra: https://twitter.com/zachtratar/status/1694024240880861571 Agent Demo by Yohei: https://twitter.com/yoheinakajima/status/1640068466974633987?s=20 agents aiagents autogpt datascience embra machinelearning",
      "platforms": {
        "tiktok": {
          "video_id": "7270640594266033454",
          "url": "https://www.tiktok.com/@rajistics/video/7270640594266033454",
          "view_count": 4341,
          "upload_date": "2023-08-23",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CwTZeE0sb2g",
          "url": "https://www.instagram.com/reel/CwTZeE0sb2g",
          "view_count": 432,
          "upload_date": "2023-08-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Cleaning data is such a pain. I remember having over 130+ unique combinations for US States in one project. ",
      "description": "Cleaning data is such a pain. I remember having over 130+ unique combinations for US States in one project. ",
      "upload_date": "2022-09-30",
      "total_views": 4773,
      "max_views": 4773,
      "topics": [
        "cleaning",
        "data",
        "pain",
        "remember",
        "teddy",
        "unique"
      ],
      "search_text": "Cleaning data is such a pain. I remember having over 130+ unique combinations for US States in one project.  cleaning data pain remember teddy unique That's Teddy, and that's also Teddy. I named them all Teddy because I can't tell the difference.",
      "platforms": {
        "tiktok": {
          "video_id": "7149275067585695018",
          "url": "https://www.tiktok.com/@rajistics/video/7149275067585695018",
          "view_count": 4773,
          "upload_date": "2022-09-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3dba3e640f3f46c793b77d02f8a9bc12_1664570331~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=pDEsXaHII4aFyEDxfopA3ZB9ys8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @shaggy335 #datascience #statistics #analytics #techtok #machinelearning",
      "description": "Reply to @shaggy335 #datascience #statistics #analytics #techtok #machinelearning",
      "upload_date": "2022-05-03",
      "total_views": 4768,
      "max_views": 4768,
      "topics": [
        "analytics",
        "datascience",
        "machinelearning",
        "something",
        "statistics",
        "techtok"
      ],
      "search_text": "Reply to @shaggy335 #datascience #statistics #analytics #techtok #machinelearning analytics datascience machinelearning something statistics techtok So I'm totally lazy when it comes to programming. I rarely write anything from scratch. Instead, thanks to the world of GitHub, you can just type in, use some of the advanced searches, find somebody else who's built something pretty similar, if not exactly what you want. And then I use that as a starting point for what I want to do. Really, writing from scratch should be your last resort. I know sometimes it's useful as a learning tool to really understand something, but if you're trying to get something done, go see if somebody else has already done it.",
      "platforms": {
        "tiktok": {
          "video_id": "7093495838269345067",
          "url": "https://www.tiktok.com/@rajistics/video/7093495838269345067",
          "view_count": 4768,
          "upload_date": "2022-05-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1cbb1c5f22f74846b178eac70f71941d_1651583205~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=C3REkRohQOx7h5tK0nUzH62Qv1c%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5935,
      "title": "What if Santa’s biggest problem this year is optimization? Packing 200 Christmas tree toys into the smallest box is harder than it looks. Greedy logic, controlled chaos, or big-brain math, which strategy delivers the most holiday efficiency? Join the Kaggle Santa 2025 showdown and level up your optimization skills while saving Christmas shipping costs",
      "description": "What if Santa’s biggest problem this year is optimization? Packing 200 Christmas tree toys into the smallest box is harder than it looks. Greedy logic, controlled chaos, or big-brain math, which strategy delivers the most holiday efficiency? Join the Kaggle Santa 2025 showdown and level up your optimization skills while saving Christmas shipping costs",
      "upload_date": "2025-11-28",
      "total_views": 4764,
      "max_views": 3408,
      "topics": [
        "challenge",
        "going",
        "kaggle",
        "noel",
        "optimization",
        "packing",
        "papai",
        "que",
        "rvore",
        "santa",
        "tree",
        "try",
        "uma",
        "voc",
        "year"
      ],
      "search_text": "What if Santa’s biggest problem this year is optimization? Packing 200 Christmas tree toys into the smallest box is harder than it looks. Greedy logic, controlled chaos, or big-brain math, which strategy delivers the most holiday efficiency? Join the Kaggle Santa 2025 showdown and level up your optimization skills while saving Christmas shipping costs challenge going kaggle noel optimization packing papai que rvore santa tree try uma voc year It's that time of year that Kaggle Santa competition has returned and the mission this year is to help Santa pack Christmas tree toys into the smallest possible square. Santa's trying to save on shipping fees and maybe dodge a few tariffs. Each tree here is a precise polygon. There's no overlaps allowed and you want to try to make the smallest box. So what's the best way to do this? I know some of you AI slopsters are going to go out and ask Nanobanana, but if you give that a try, you'll see it's not going to get you very far. Instead, let's use some science. The first approach is start with a greedy baseline where you place one tree at a time, slide it inwards until it bumps into something, then lock it into place. That's it. Greedy here means no planning. Yeah, I get it's very topical. But it's a good baseline, but not optimal. Now, let's try embracing chaos instead where we take a full layout, constantly nudge things around. Rotate a tree, slide another one there, keeping track as we go. If the layout improves, great. If it gets worse, we can always backtrack. See people? It's okay to try a new thing. Finally, the math people are going to show up and they're going to express the whole puzzle as a global optimization problem with collision constraints, decision variables, protractors, and a solver that thinks ahead. Eh, go for it. Maybe that planning will actually give them a better solution. There's many different ways to do this. So join in, learn something new, and help Santa ship.",
      "platforms": {
        "tiktok": {
          "video_id": "7577561029261937950",
          "url": "https://www.tiktok.com/@rajistics/video/7577561029261937950",
          "view_count": 3408,
          "upload_date": "2025-11-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oE7z1HfwShIfuaeRC8XQA5M3IUAjXnzgg28Vqc~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=ODAOQVcrRH1FuIG9G3wAMGfoZHo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18082987928023338",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZkhL70gQhA4",
          "url": "https://www.youtube.com/watch?v=ZkhL70gQhA4",
          "view_count": 1356,
          "upload_date": "2025-11-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6207,
      "title": "ML engineers discuss why cutting-edge academic models aren't production-ready and often don't justify the implementation costs. They share practical alternatives that work better: * Simpler models that need less compute * Quality data labeling over complexity * Rules-based approaches with domain knowledge * Data cleaning tools like cleanlab The key message: understanding your specific problem and data beats chasing the latest complex architectures.",
      "description": "ML engineers discuss why cutting-edge academic models aren't production-ready and often don't justify the implementation costs. They share practical alternatives that work better: * Simpler models that need less compute * Quality data labeling over complexity * Rules-based approaches with domain knowledge * Data cleaning tools like cleanlab The key message: understanding your specific problem and data beats chasing the latest complex architectures.",
      "upload_date": "2025-02-27",
      "total_views": 4751,
      "max_views": 4751,
      "topics": [
        "data",
        "lot",
        "model",
        "models",
        "much",
        "production"
      ],
      "search_text": "ML engineers discuss why cutting-edge academic models aren't production-ready and often don't justify the implementation costs. They share practical alternatives that work better: * Simpler models that need less compute * Quality data labeling over complexity * Rules-based approaches with domain knowledge * Data cleaning tools like cleanlab The key message: understanding your specific problem and data beats chasing the latest complex architectures. data lot model models much production Check out this new state of the art model. We should push this to production. Think again. Do you understand why we wouldn't push a model that we read about in archive to production? Because you guys aren't comfortable with change. Oh boy. Do you have any idea how much work it takes to build, train, validate, and deploy a model into our pipelines? I could do that in a couple of hours. A model created by academics can take weeks for us to re-engineer and get working within our production pipelines. The other thing is the accuracy gains you're seeing, we don't know if that's going to translate over to the problems we're trying to solve. And even if it does improve accuracy, one or 2% accuracy isn't that big a deal for a lot of the applications that are often not worth upgrading just for that. So if you don't read archive all day, how do you improve your models? There are a lot of things we do. Tell them about your Kaggle work. I just finished competing in the auto competition on Kaggle. And the third place team used a rules-based approach that beat thousands of people. It just goes to show you that understanding and knowing the data, you can build a rules-based approach that builds a machine learning model. Remember Tom, that PhD? He spent weeks building a few-shot learning approach because he said he didn't have enough data, didn't get anywhere. We handed the project off to Sarah. She worked with the business, took a little bit of her free time, labeled some data. And she was able to quickly build a very effective model by just taking the time to label data. You data scientists, as the person who has to put these things into production, I much prefer a simpler model. They're much easier to understand when I'm trying to troubleshoot them. They take a lot less compute. If you look at our data quality pipelines that we have, they're all using very simple bag of words models. I know you guys have those fancy transformer models, but those things take way too much compute, way too much resources. If we can do it with a simple model, that's what we're going to choose. That makes a lot of sense. I should probably brush up on past meetings where we talked about using tools like CleanLab, as well as doing error analysis with our models. I just saw this data-centric AI course. All the videos are free. Might want to use it to brush up.",
      "platforms": {
        "tiktok": {
          "video_id": "7476144531461754142",
          "url": "https://www.tiktok.com/@rajistics/video/7476144531461754142",
          "view_count": 4751,
          "upload_date": "2025-02-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIDgAEuAnQMwnFoEsVdfGwdex9A2CRASBnIJAE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=P2Xb4CaueAXMvP17YeU68z0svkE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18038226575180875",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Mixing in a bit of law with the usual data science. Let me know if this is interesting or you waiting for the deep dive on dbscan clustering. #craiyon #dallemini #stablediffusion #texttoimage #machinelearning #datascience ",
      "description": "Mixing in a bit of law with the usual data science. Let me know if this is interesting or you waiting for the deep dive on dbscan clustering. #craiyon #dallemini #stablediffusion #texttoimage #machinelearning #datascience ",
      "upload_date": "2022-10-28",
      "total_views": 4720,
      "max_views": 4720,
      "topics": [
        "craiyon",
        "dallemini",
        "dolly",
        "stablediffusion",
        "texttoimage",
        "trademark"
      ],
      "search_text": "Mixing in a bit of law with the usual data science. Let me know if this is interesting or you waiting for the deep dive on dbscan clustering. #craiyon #dallemini #stablediffusion #texttoimage #machinelearning #datascience  craiyon dallemini dolly stablediffusion texttoimage trademark Want to hear how a $20 billion AI company was able to get its way with the startup? I don't have any inside knowledge. And although I have a law degree, I wouldn't take that too seriously. But what I want to do is I want to give you the story that I know and share some of the documents I found along the way. OpenAI created Dolly about over a year ago. Very cool text to image. But they didn't make it publicly available. Subwars came along, read the original paper, said, hey, I can make a better version of this. Figured out how to make a simpler version of this that ran faster. And he made it available to the public as Dolly Mini. And we all had a lot of fun with Dolly Mini. It blew up, even was higher than the OpenAI Dolly in Google Search for a little while. But then Boris flew a little bit too close to the sun when he filed a trademark for Dolly Mini. Now, trademark protection is very useful in many cases, where it allows buyers to distinctively identify the sellers. But sometimes in communities like open source or fan communities, where lots of people have contributed, it can feel like people are stealing something out of public discourse when they trademark part of that. My guess is once OpenAI saw that trademark application, their lawyers immediately showed up, knocked on Boris's door and politely said, you better change your name. And the reason I believe this set of events is because two weeks after Boris is filing, OpenAI decided to file the trademark as well. So be careful when you name your projects. And hey, if you're bored, check out the trademark search database. A lot of good stuff in there.",
      "platforms": {
        "tiktok": {
          "video_id": "7159377632331681070",
          "url": "https://www.tiktok.com/@rajistics/video/7159377632331681070",
          "view_count": 4720,
          "upload_date": "2022-10-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5bb9e347a5f642fbad147108e7954147_1666922509~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=FrIhcJwHHyXiGONwAnF%2FAkILgsY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Text to Chart. It’s easier than ever to build great charts using libraries like plotly or matplotlib. Are other people using ChatGPT for this? #datascience #machinelearning #chatgpt #matplotlib #plotly #python #stackoverflow ",
      "description": "Text to Chart. It’s easier than ever to build great charts using libraries like plotly or matplotlib. Are other people using ChatGPT for this? #datascience #machinelearning #chatgpt #matplotlib #plotly #python #stackoverflow ",
      "upload_date": "2023-02-15",
      "total_views": 4705,
      "max_views": 4705,
      "topics": [
        "chatgpt",
        "datascience",
        "like",
        "machinelearning",
        "matplotlib",
        "plotly"
      ],
      "search_text": "Text to Chart. It’s easier than ever to build great charts using libraries like plotly or matplotlib. Are other people using ChatGPT for this? #datascience #machinelearning #chatgpt #matplotlib #plotly #python #stackoverflow  chatgpt datascience like machinelearning matplotlib plotly How would you like to get a chart like this by just giving it a little bit of text? Wow. Recent paper chart divis was recently updated with working with large language models. So you could use models like GPT three to create charts like this. The paper has examples where you start with texts, it creates a data frame and then creates a plot. You can see here the prompts that they use to do this that outline the structure of the data and then help you figure out what the code for graphing is. You can just do this straight by going to chat GPT. And I've seen data scientists start to use this for working with plotly or matplotlib where they just directly ask it questions for advanced techniques and skip going to Stack Overflow.",
      "platforms": {
        "tiktok": {
          "video_id": "7200425300612648234",
          "url": "https://www.tiktok.com/@rajistics/video/7200425300612648234",
          "view_count": 4705,
          "upload_date": "2023-02-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a7bc334937334e43b788cc075b22f015_1676479670~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=eMJsVue%2Bg25sKs3kA72qWF85Qao%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6159,
      "title": "Stylometric analysis—specifically the detection of overused phrases known as \"slop\"—can reveal hidden changes in a language model's training data. Using a binary vector of slop phrases to create stylistic fingerprints, Sam Paech was able to cluster models by their linguistic quirks and uncover that DeepSeek’s latest version had likely been trained on Gemini outputs. It's a creative example getting models using a model’s outputs, no weights or inside knowledge needed. Links: Post by Sam Paech:  https://x.com/sam_paech/status/1928187246689112197 Slop-Forensics Github: https://github.com/sam-paech/slop-forensics EQ-Bench: https://eqbench.com/",
      "description": "Stylometric analysis—specifically the detection of overused phrases known as \"slop\"—can reveal hidden changes in a language model's training data. Using a binary vector of slop phrases to create stylistic fingerprints, Sam Paech was able to cluster models by their linguistic quirks and uncover that DeepSeek’s latest version had likely been trained on Gemini outputs. It's a creative example getting models using a model’s outputs, no weights or inside knowledge needed. Links: Post by Sam Paech:  https://x.com/sam_paech/status/1928187246689112197 Slop-Forensics Github: https://github.com/sam-paech/slop-forensics EQ-Bench: https://eqbench.com/",
      "upload_date": "2025-06-01",
      "total_views": 4664,
      "max_views": 3594,
      "topics": [
        "fingerprints",
        "language",
        "model",
        "models",
        "outputs",
        "phrases",
        "sam",
        "slop",
        "stylometry",
        "uncovered"
      ],
      "search_text": "Stylometric analysis—specifically the detection of overused phrases known as \"slop\"—can reveal hidden changes in a language model's training data. Using a binary vector of slop phrases to create stylistic fingerprints, Sam Paech was able to cluster models by their linguistic quirks and uncover that DeepSeek’s latest version had likely been trained on Gemini outputs. It's a creative example getting models using a model’s outputs, no weights or inside knowledge needed. Links: Post by Sam Paech:  https://x.com/sam_paech/status/1928187246689112197 Slop-Forensics Github: https://github.com/sam-paech/slop-forensics EQ-Bench: https://eqbench.com/ fingerprints language model models outputs phrases sam slop stylometry uncovered Have you noticed how some people just talk differently? Right. You can tell right away where they're from. Are they into TED Talks or going to the mall? Well, it turns out language models do the same thing. I've been studying language models for a long time and Sam Tier revealed something new about these models. And he did this a bit of work on the latest DeepSeek model where he noticed the answers, the style that came out was just a little bit different. But Sam here had a system for looking into it. He starts by looking at slot phrases. These are phrases that LLMs overuse compared to humans. Things like let's dwell into it. He then compiled a list of what are the top phrases across all the models. And then he gave each model a yes, no for whether or not that showed up in the model. Binary vector. Once he had this, you can think of this as the DNA for the model. Now with this DNA, he could start comparing different models. One thing he found with the DeepSeek model is it used to be very close to the open AI models, probably because it was trained on open AI outputs. But the latest version of the DeepSeek model now is much closer to the Gemini models, which means that it's probably been trained on the synthetic outputs of the Gemini models. Now, what I love about this project is this was just careful looking at the model outputs that allowed us to be able to do this. Just smart, scrappy data science.",
      "platforms": {
        "tiktok": {
          "video_id": "7511050802971086111",
          "url": "https://www.tiktok.com/@rajistics/video/7511050802971086111",
          "view_count": 3594,
          "upload_date": "2025-06-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4fTfGeKgAUxnbARMQcIgcnABQDGAALUgyJnIe~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=j8Amr7gBpQ9xDflpMmTIMuwXHb0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18072646309933238",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "k8JLpwPTTmI",
          "url": "https://www.youtube.com/watch?v=k8JLpwPTTmI",
          "view_count": 1070,
          "upload_date": "2025-06-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Wrap up of current events going on with chat including #openai #chatgpt #bing #amazon #datascience #machinelearning ",
      "description": "Wrap up of current events going on with chat including #openai #chatgpt #bing #amazon #datascience #machinelearning ",
      "upload_date": "2023-02-17",
      "total_views": 4660,
      "max_views": 4660,
      "topics": [
        "amazon",
        "bing",
        "chatgpt",
        "like",
        "models",
        "openai"
      ],
      "search_text": "Wrap up of current events going on with chat including #openai #chatgpt #bing #amazon #datascience #machinelearning  amazon bing chatgpt like models openai I see a lot going on with chatbots. You got Google asking people to spend more time on their chatbots. You got the Bing chatbot going crazy and Amazon telling people, like, stay away from them. Like, what's going on here? Yeah, let me try explaining it to you simply. So the earliest versions of these models, they learned by themselves in an unsupervised way. So they consumed a lot of information and then just tried to regurgitate back what they got. And sometimes they made up stories that really didn't make any sense at all. I remember those early models. They told wacky stories. Yeah, to make these models better, we figured out that they needed human feedback. It's kind of like sending these models to school where they can learn. And that's why at Google, they really want their employees to spend time working on these models because they need human feedback. So the model starts to act correctly. I get it. So Bing would be like in junior high or high school since it's already learned a little bit from open AI. And for Bing, it's getting schooled in public. All of us are the ones that actually end up teaching this model how to behave. OK, but why are the lawyers at Amazon concerned? The Amazon lawyers are worried that somebody might enter in confidential information when using chat GPT. And since that same data is being used to train you had GPT, that data could later be outputted out. So that confidential data could be breached in someone else, could learn that when they're trying to do an autocomplete and it fills in Amazon's confidential information. Oh boy, sounds like we should all be concerned about what we share with these bots.",
      "platforms": {
        "tiktok": {
          "video_id": "7200941460399852846",
          "url": "https://www.tiktok.com/@rajistics/video/7200941460399852846",
          "view_count": 4660,
          "upload_date": "2023-02-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fac1224c54ee420dbcc3507589de9183_1676599847~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=ba2MJdUnmjOK3Y%2FWurY3JG2EhRI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6048,
      "title": "DeepSeek-R1 didn’t copy human reasoning—it learned it. With pure RL (GRPO), it jumped from 15% to 80% on the AIME exam and began saying “wait,” a sign of reflection. But RL only shines where answers are verifiable; elsewhere it risks reward hacking—gaming the rubric instead of learning. A rare peer-reviewed look at what RL can—and cannot—do. (Nature, 2025: https://www.nature.com/articles/s41586-025-09422-z)",
      "description": "DeepSeek-R1 didn’t copy human reasoning—it learned it. With pure RL (GRPO), it jumped from 15% to 80% on the AIME exam and began saying “wait,” a sign of reflection. But RL only shines where answers are verifiable; elsewhere it risks reward hacking—gaming the rubric instead of learning. A rare peer-reviewed look at what RL can—and cannot—do. (Nature, 2025: https://www.nature.com/articles/s41586-025-09422-z)",
      "upload_date": "2025-09-19",
      "total_views": 4654,
      "max_views": 3632,
      "topics": [
        "deepseek",
        "gets",
        "human",
        "learning",
        "model",
        "performance",
        "reasoning",
        "superhuman",
        "wait",
        "works"
      ],
      "search_text": "DeepSeek-R1 didn’t copy human reasoning—it learned it. With pure RL (GRPO), it jumped from 15% to 80% on the AIME exam and began saying “wait,” a sign of reflection. But RL only shines where answers are verifiable; elsewhere it risks reward hacking—gaming the rubric instead of learning. A rare peer-reviewed look at what RL can—and cannot—do. (Nature, 2025: https://www.nature.com/articles/s41586-025-09422-z) deepseek gets human learning model performance reasoning superhuman wait works Wait, that's not a filler word anymore. DeepSeq R1 became a signal of learning. It's an emergent behavior from reinforcement learning. Let me explain how this works, how it reaches superhuman performance, and where it's going to fail. DeepSeq just shared in a peer reviewed nature paper exactly how their latest model works. It's another sign of China's massive open AI ecosystem. Unlike traditional approaches, DeepSeq didn't train R1 on lots of human examples. Instead, it used pure reinforcement learning, specifically group relative policy optimization. Here's how it works. The model produces a long chain of thought and a final answer. It gets only rewarded if the final answer is correct. It runs this loop thousands of times. And we can see here, as the model ran this loop, the performance on this math test steadily improved. At about 8,000 training steps, there became an aha moment. That's when it started adding words like, wait, mistake, verify. This showed how the model was changing its reasoning patterns, a sign of reflective reasoning. Now, this works in math and coding because correctness is verifiable. There's a clear right or wrong answer. We can define those verifiers. We can then use RL to push beyond human limits, even hit superhuman performance on these tabs. But in domains without clear signals, something like writing quality, the reinforcement learning struggles. It only has loose rules to guide it, and the system often will go into what's called reward hacking, where it ends up gaming the rubric instead of actually learning the material. Come to think of it, that's a very human behavior.",
      "platforms": {
        "tiktok": {
          "video_id": "7551623055911521567",
          "url": "https://www.tiktok.com/@rajistics/video/7551623055911521567",
          "view_count": 3632,
          "upload_date": "2025-09-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okMMsaAZfAI4zBbHBBI05TAOwiiqiCNx8mhIn7~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=YwsA4am60DcLZzlCdWmbUY06Y%2BI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18105753343572509",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "TDEh1rZlsT0",
          "url": "https://www.youtube.com/watch?v=TDEh1rZlsT0",
          "view_count": 1022,
          "upload_date": "2025-09-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Nvidia Prismer model for image captioning and zero shot visual question answering. It uses and ensemble or mixture of experts approach. #datascience #machinelearning #nvidia #prismer #imagecaptioning #visualquestionanswering ",
      "description": "Nvidia Prismer model for image captioning and zero shot visual question answering. It uses and ensemble or mixture of experts approach. #datascience #machinelearning #nvidia #prismer #imagecaptioning #visualquestionanswering ",
      "upload_date": "2023-03-15",
      "total_views": 4653,
      "max_views": 4653,
      "topics": [
        "datascience",
        "machinelearning",
        "model",
        "nvidia",
        "prismer",
        "uses"
      ],
      "search_text": "Nvidia Prismer model for image captioning and zero shot visual question answering. It uses and ensemble or mixture of experts approach. #datascience #machinelearning #nvidia #prismer #imagecaptioning #visualquestionanswering  datascience machinelearning model nvidia prismer uses NVIDIA just shared a better way for understanding our images. Prisma is a vision language model that uses multimodal experts. Wow. You start by giving it an image, it then uses different expert models to understand it. It uses low-level vision signals, such as depth. It uses high-level vision signals, such as instance and semantic labels. Brings all that together, you can use this for image captioning or visual question and answering. The model architecture is a transformer style model that uses vision, but then takes predictions from these different experts as part of the workflow for getting the text output. The performance of this model is better than comparing it to, let's say, a standard backbone of the same size. However, it doesn't break state of the art on all the benchmarks simply because there's larger models out there that have been trained on more data that do better. But in the meantime, go check out the demo. The model and the code are available.",
      "platforms": {
        "tiktok": {
          "video_id": "7210891278173883694",
          "url": "https://www.tiktok.com/@rajistics/video/7210891278173883694",
          "view_count": 4653,
          "upload_date": "2023-03-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/63ef277091874ced9324ff60fbfa2a2d_1678916472~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=u0kEGYgXlYZEwJntCTBBwnOUtRw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Try it out, link in comments. #huggingface #datascience #reinforcementlearning #deeplearning #codetok #mltok Earlier weeks: @rajistics @rajistics",
      "description": "Try it out, link in comments. #huggingface #datascience #reinforcementlearning #deeplearning #codetok #mltok Earlier weeks: @rajistics @rajistics",
      "upload_date": "2022-06-19",
      "total_views": 4640,
      "max_views": 4640,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "huggingface",
        "mltok",
        "reinforcementlearning"
      ],
      "search_text": "Try it out, link in comments. #huggingface #datascience #reinforcementlearning #deeplearning #codetok #mltok Earlier weeks: @rajistics @rajistics codetok datascience deeplearning huggingface mltok reinforcementlearning Want to teach a computer how to play Space Invaders? Last week, we taught the computer to play Frozen Pond, which only had a finite amount of possible states, and so you could almost create a lookup table with all the possible actions. It's not like that when we get to Space Invaders. For Space Invaders, the input's going to be the entire screen, which is a ton of information. Building a table for every possible state is way too much, so instead what we do is substitute a neural network and use what's called a deep-Q network. Lucky for you, Stable Baselines already has this algorithm in there, so it's a quick command to train a model on this. Here's my first attempt. Probably took about 20 minutes on a GPU to train this to about a million training steps. Go try it out yourself. There's a leaderboard. Bet you can do better.",
      "platforms": {
        "tiktok": {
          "video_id": "7110961062136089898",
          "url": "https://www.tiktok.com/@rajistics/video/7110961062136089898",
          "view_count": 4640,
          "upload_date": "2022-06-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f48f251e2df147f29f96555492de55c7_1655649644~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=M%2FAAaBtR0kL2g1WS%2BG214HaZkn0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "History of the term regression and regression to the mean #statistics #datascience #galton #regression #heriditary",
      "description": "History of the term regression and regression to the mean #statistics #datascience #galton #regression #heriditary",
      "upload_date": "2022-02-08",
      "total_views": 4633,
      "max_views": 4633,
      "topics": [
        "datascience",
        "galton",
        "heriditary",
        "people",
        "regression",
        "statistics"
      ],
      "search_text": "History of the term regression and regression to the mean #statistics #datascience #galton #regression #heriditary datascience galton heriditary people regression statistics Are you surprised that taller people generally have children that are shorter than them? This is known as regression to the mean. Let me explain where it came from. About 150 years ago, our bro Galton, he was totally in hereditary. He studied the effect of peas on their descendants. Next level, he decided to put some money up to see what were the effects of hereditary on people. And here's what he found. Here's the table. I also got a graph for you folks, but let's explain what we're seeing here. What we see here is when he looks at the data that on average, taller people have shorter kids than them, while shorter people have kids that are taller than them. And this is what's known as regression to the mean.",
      "platforms": {
        "tiktok": {
          "video_id": "7062314964387630382",
          "url": "https://www.tiktok.com/@rajistics/video/7062314964387630382",
          "view_count": 4633,
          "upload_date": "2022-02-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/94f7c6272329453190f555ce986f15ad_1644323343~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=LgRK4SDlPDmVt5BoEth%2BG%2B3Zj%2Fk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5998,
      "title": "Deep dive into reasoning models. Notebook is freely available so go run it yourself.  Notebook: https://github.com/rajshah4/LLM-Evaluation/blob/main/ResearchAgent_Agno_LangFuse.ipynb Video: https://youtu.be/HtlVq8XBbzg",
      "description": "Deep dive into reasoning models. Notebook is freely available so go run it yourself.  Notebook: https://github.com/rajshah4/LLM-Evaluation/blob/main/ResearchAgent_Agno_LangFuse.ipynb Video: https://youtu.be/HtlVq8XBbzg",
      "upload_date": "2025-06-04",
      "total_views": 4624,
      "max_views": 3553,
      "topics": [
        "able",
        "benchmarks",
        "don",
        "going",
        "illusion",
        "like",
        "models",
        "reasoning",
        "style",
        "thinking",
        "use",
        "want"
      ],
      "search_text": "Deep dive into reasoning models. Notebook is freely available so go run it yourself.  Notebook: https://github.com/rajshah4/LLM-Evaluation/blob/main/ResearchAgent_Agno_LangFuse.ipynb Video: https://youtu.be/HtlVq8XBbzg able benchmarks don going illusion like models reasoning style thinking use want Are you ready to dig into the latest reasoning models? What I want to do in this notebook is show you in a hands-on way how to take advantage of the latest reasoning models from OpenAI from Anthropik, put them together with other tools like web search and a rag agent to build your own research assistant. Now what I'm doing here is giving you the code for how to do it but there's also a lot of logic so you know how to think about these things and what are the trade-offs what should you be considering in doing this. To get started with this notebook you can run this right inside of Google Colab however we do want to take care of some other tools that we need. So one thing you're going to want to do is you're going to want to get API keys for some of the tools we're going to use in our research agents. I've got this set up for using contextual AI as a rag agent so this is where you have lots of documents you want to ask questions to that aren't public and then I'm also using a Tavilli client to be able to handle the web search. You can use different tools to do this but if you're going to use the notebook as is you can do that. For both of those you can get started for free. Once you get your API keys the first thing I like to do is just double check that my tools work no sense in going way into the notebook then finding out you didn't set up something right so we'll run a little bit of checks to make sure our web search works. If you're using the contextual rag agent you're going to of course add your own agent ID for your rag agent there. Make a request that makes sense here. I've made mine about financial data. Double check that that's all working because we need that working for the next section of this. Once we get the basics let's go in and use the reasoning tools. Now throughout this notebook I've used the agno framework simply because for these types of demonstrations I can substitute in lots of different models very easily with the same type of code. When you're doing this in real life you can always go to kind of raw python. I have examples of building research assistants like this or you can use frameworks just with frameworks you got to be a little cautious because they can break when you get to production and with all the constant changes that happen. So the first thing we're going to do is I'm going to use an older Claude model to just illustrate how these models think. I'm going to ask it one direct question. Who is the CEO of OpenAI? And you can see we're quickly able to get back kind of an answer for that. Now this is what we call kind of a single hop question. Here I ask it you know who developed GPT-4. We can see these models will hallucinate. They'll make up things when you just trust their own knowledge. This for example might have been trained before GPT-4 was released. I'm not sure exactly what's going on with it but clearly you can see that just relying on the model itself it can make a mistake. Now these were examples of what we call single hop queries where I'm just asking for one piece of information. Now sometimes what I want to do is what's called multi-hop and when you start working with people doing research they'll often ask multiple things in the same query. Like here I ask who leads the company that developed GPT-4. If you think about it right there's two elements to this here. It's who developed GPT-4 and then who is the CEO of that. You need to pull both those together to be able to answer that. These can be some of the challenges as you're building question-answer systems like that. And here's even a more complex one where now it's not just two things I need. I need to go find all the publicly traded companies like Filter Down by you know which ones have a CEO that's female. Look at the revenue growth. Then also pull the Dell revenue growth from 2022 and compare all that stuff. This is where a lot of these models will be just like yeah this is way too much for me. I'm not going to do that but right like we want to be able to solve these kinds of questions. This is where the next generation of models will start helping with. So now what we're going to do is we're going to build a simple agent with reasoning. Now the query I'm using is about building getting some research on doing some I'm working on building a hotel chain. I need to do a little bit of research around that. This was open AI had this in one of their demos. I used it. When you're working with these models as we'll quickly look at the outputs they get hard to follow especially if it's not in a domain that you know about. So first thing I recommend is pick a query something detailed and researchy but something that you know about. So that way you have a basis for evaluating. Now within the Agno framework the first time I'm going to run this year I just want to show you what happens if you don't use the reasoning that's in these models. You'll see we have a much shorter runtime 30 seconds like that and it will still give us an answer. These models love to be helpful and will give you an answer as well. So I'll get a long detailed answer on these recommendations for where to put my hotel. But if you want a better answer and this is where I want you to try it on your own if you enable reasoning what's going to happen is it's going to spend more time thinking about that just like how we talked about breaking up the query in a subtask and solving each of those tasks. That's exactly what these thinking projects are doing. Now in this case I can go in there you can see the thinking like what it's thinking to itself for how to evaluate these and some of the more complex queries we'll show you later you can see the actual steps that it's going through and then we can take a look at the answer. And if you spend a little time you'll see qualitatively that this answer is a bit better than the earlier answer where it wasn't using reasoning as well. Now I'm yeah you can also do this with the open AI models as well. So here for example with O4 mini is a is a thinking model that allows you to do the same thing. In the notebook I've stuck to the cheaper models so both on the anthropic and the open AI side they have fancier models that will do more complex planning but they cost a lot more. I don't want to run up your bill the idea here is just to give you the intuition of this like this. But again like there's many model providers that have these thinking models and agno makes it nice for us to be able to kind of work across all of them like that. So the next thing I want to do is we know that the models knowledge themselves that they have is not enough for solving these kinds of things. We want to bring in external knowledge so a couple of ways we can bring external knowledge. A common one for enterprises is using retrieval augmented generation of RAG where we take all of our internal documents that we have we chunk them up into pieces that we can then search pull them into the context of a language model as well. And so I've used a RAG agent a managed service from contextual I work at contextual to do that here. I also want to use Tavilli's search agent web search as well. Now for both of these I want you to notice how I've taken the tooling here and just wrapped it inside of a simple function where I pass in the query of what I want it to search for and then outcomes the search result that I can pass into my LL. You can define other tools in other ways. One of the things with agno as well as these models they're very agnostic now to tools. So this is where you think about all the apis that you have any place where you have sources of information that could be useful to these models you should do that. So now let's keep going here. I want to show you kind of I want to walk through kind of looking and doing web search and using these tools with this. So what I'm going to do here is colab is grumpy at me but hopefully we'll keep being able to run. So what I'm doing here is I have I'm going to tell it to use the tools when it's doing the query right. I don't want it to use its own knowledge and then under tools here I've defined the reasoning tools but then the two tools I have the contextual tool and the search tool. That's all I have to do. This is where I like this framework. It simplifies the code down. You can do this in python it's not that hard as well but this is just a little bit cleaner. Like what should I wear tomorrow. These models are powerful. It can answer anything right. So if I ask it what I should wear tomorrow right away you can start seeing it starts thinking about it but it says wait a minute. The user asked what to wear tomorrow but to get that information we need location weather all that stuff. So yes it has a powerful tool of web search but it doesn't have enough information and it knows it doesn't have enough information to use it. So now if I update that query and I say hey what should I wear tomorrow as an old man in Chicago. Now for the reasoning step it goes ah right like now it has a little bit of information that it can use with the location and if we go through this we'll see that for example it's able to forecast hey exactly what I should be able to wear tomorrow. And then based on that forecast it's able to do an analysis and understand exactly what to wear. And so here it tells me what I should be wearing tomorrow when I when I'm going up to Chicago like that. Another query I did was hey let's use the rag search which is in this case defined around financial documents and I can use the reasoning model to help think through that. And so here we can see the different steps that go through where we're planning the financial analysis right we need to do some gathering we need to gather additional data all that analysis that's done and you can see the individual queries this is the nice thing about the framework is I can see the individual queries like here's the Nvidia query I'm doing here's the contextual query I'm doing as well and being able to pull all that in and be able to use that. And then with those steps now I'm able to define a much better report if I just tried to ask these questions themselves I wouldn't have been able to get such a good analysis and that's where thinking step by step through this doing each of the steps and bringing it back gives you a lot better results like that. Okay and again like you can do this with open AI I did the same query where I've just all I've done the same code but I just swapped in the 04 mini into this and you will see it does a similar piece where it goes through this kind of reasoning steps as well through all of this and then kind of does the tool calls to be able to figure all of that out in the same way. This is why I like these models they're really simplifying lots of these queries that we have to do. Now if you do a one or a couple of these in the browser that works great but as you do more and more of these you want to connect them to some observability type platform. So one of the tools I like to use is line fuse it's an open source observability platform that you can use and what I've done here is I've added a little bit of the code snippets if you want to for example convert these thinking models that we have and be able to see them and work with them on line fuse as well. So this has been a good tour hopefully it's been helpful for you to help give you a little bit of insight give you some code to get started on working with these models as well thank you.",
      "platforms": {
        "tiktok": {
          "video_id": "7512102555921419551",
          "url": "https://www.tiktok.com/@rajistics/video/7512102555921419551",
          "view_count": 1071,
          "upload_date": "2025-06-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0BIKEKIBuxzCBWXAifZAXAARiobA0iCz68IxA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=yraT6ENFxE5s8T1TeFmVJCXBvus%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17853678060410720",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "KjzptajOTfQ",
          "url": "https://www.youtube.com/watch?v=KjzptajOTfQ",
          "view_count": 3553,
          "upload_date": "2025-06-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6088,
      "title": "Can you tell I have been building agent workflows at work?",
      "description": "Can you tell I have been building agent workflows at work?",
      "upload_date": "2024-12-31",
      "total_views": 4623,
      "max_views": 2377,
      "topics": [
        "agentic",
        "customer",
        "data",
        "demo",
        "demoing",
        "goes",
        "production",
        "tables",
        "technical",
        "workflows",
        "wrong"
      ],
      "search_text": "Can you tell I have been building agent workflows at work? agentic customer data demo demoing goes production tables technical workflows wrong Meet Carl, an agentic AI system for customer service that I built. What exactly does it do? It takes in customer complaints and fixes them. But how? Can we test this on some production data? So far I've been using demo data from three customer tickets in 2019. It really wasn't prepared for production data. Hold up. It's finding Zodiac, but not the address. Is it querying our customer tables? Well, for now I created a custom table and hard-coded the schema into the prompt. We have over 20 tables with customer information. Doing text to SQL across that's going to be unreliable. But look how good it does technical research for our customers. That's not researching our technical documents. And it's hallucinating. Like all AI systems, it has some Bayesian uncertainty. But watch its decision-making for escalating messages. You showed this to the executives? They love the demo. They're looking forward to the cost savings from cutting our customer support team. Now, they asked me to lead this, but I said you'd be much better at productionizing it.",
      "platforms": {
        "tiktok": {
          "video_id": "7454664669953789214",
          "url": "https://www.tiktok.com/@rajistics/video/7454664669953789214",
          "view_count": 2377,
          "upload_date": "2024-12-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/osQDndA6fTPEECGNn8oEKGAAEDVBoKAInAfFRl~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=ye1ZnGjWe3cUIGQhWvCsdjVLnTY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17883804306192579",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "npibkgkHE54",
          "url": "https://www.youtube.com/watch?v=npibkgkHE54",
          "view_count": 2246,
          "upload_date": "2024-12-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6533,
      "title": "OpenAI dropped some big releases for its developer day. Let's catch up on the news in early November 2023. #openai #meta #nvidia #01.ai #h2o.ai #google #china #rajistics #microsoft",
      "description": "OpenAI dropped some big releases for its developer day. Let's catch up on the news in early November 2023. #openai #meta #nvidia #01.ai #h2o.ai #google #china #rajistics #microsoft",
      "upload_date": "2023-11-12",
      "total_views": 4615,
      "max_views": 2623,
      "topics": [
        "china",
        "google",
        "h2o",
        "meta",
        "news",
        "nov",
        "nvidia",
        "openai",
        "roundup"
      ],
      "search_text": "OpenAI dropped some big releases for its developer day. Let's catch up on the news in early November 2023. #openai #meta #nvidia #01.ai #h2o.ai #google #china #rajistics #microsoft china google h2o meta news nov nvidia openai roundup",
      "platforms": {
        "tiktok": {
          "video_id": "7300634353317580074",
          "url": "https://www.tiktok.com/@rajistics/video/7300634353317580074",
          "view_count": 2623,
          "upload_date": "2023-11-12",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CzjhVSsATTD",
          "url": "https://www.instagram.com/reel/CzjhVSsATTD",
          "view_count": 1846,
          "upload_date": "2023-11-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "uUWYj46_75k",
          "url": "https://www.youtube.com/watch?v=uUWYj46_75k",
          "view_count": 146,
          "upload_date": "2023-11-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Data scientists will typically use regularization, which means no p values. #machinelearning #datascience #statistics #pvalues ",
      "description": "Data scientists will typically use regularization, which means no p values. #machinelearning #datascience #statistics #pvalues ",
      "upload_date": "2022-11-12",
      "total_views": 4611,
      "max_views": 4611,
      "topics": [
        "data",
        "datascience",
        "machinelearning",
        "pvalues",
        "scientists",
        "statistics"
      ],
      "search_text": "Data scientists will typically use regularization, which means no p values. #machinelearning #datascience #statistics #pvalues  data datascience machinelearning pvalues scientists statistics",
      "platforms": {
        "tiktok": {
          "video_id": "7164917233041411370",
          "url": "https://www.tiktok.com/@rajistics/video/7164917233041411370",
          "view_count": 4611,
          "upload_date": "2022-11-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/727bf064e259488bbca09a54c70493e6_1668212299~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=e9M0P5N%2BWX8qSeb9WCXBCii0e4k%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6189,
      "title": "In this skit, a junior AI engineer tries to solve everything by giving the model more \"thinking time\" — but runs into the hard truth about verification. More compute helps with reasoning tasks like chess, but fails on factual recall when the model lacks the knowledge. The real challenge isn’t just more thinking — it’s building systems that efficiently acquire and verify information.",
      "description": "In this skit, a junior AI engineer tries to solve everything by giving the model more \"thinking time\" — but runs into the hard truth about verification. More compute helps with reasoning tasks like chess, but fails on factual recall when the model lacks the knowledge. The real challenge isn’t just more thinking — it’s building systems that efficiently acquire and verify information.",
      "upload_date": "2025-03-29",
      "total_views": 4608,
      "max_views": 2937,
      "topics": [
        "apps",
        "building",
        "chess",
        "compute",
        "dify",
        "generative",
        "gui",
        "introduction",
        "isn",
        "model",
        "tasks",
        "thinking",
        "time"
      ],
      "search_text": "In this skit, a junior AI engineer tries to solve everything by giving the model more \"thinking time\" — but runs into the hard truth about verification. More compute helps with reasoning tasks like chess, but fails on factual recall when the model lacks the knowledge. The real challenge isn’t just more thinking — it’s building systems that efficiently acquire and verify information. apps building chess compute dify generative gui introduction isn model tasks thinking time Our new deep-art model is unstoppable. Just give it more thinking time, it can solve anything. Really, it just spent 10 minutes figuring out when George Washington was born. And it still got it wrong. It just needs more time to think, more compute. That's the secret sauce. That's not how it works. If the model wasn't trained on a fact, no amount of thinking is going to magically retrieve it. But look at this, on this chess puzzle, with more steps, it found the perfect move. That's different. In chess, it can verify each move by simulating outcomes. With factual questions, there's no way to reason to knowledge if it just isn't there. Fascinating. When we increase thinking time on reasoning tasks, performance jumps 40%. But on pure recall tasks, zero improvement. Who cares? Let's just run everything with more compute. Customers won't notice. Actually, our latency tests show users drop off after three seconds. Your extra thinking model takes 30 seconds. The real solution isn't just throwing more time or GPUs at it. We need systems that can learn better and can check themselves. So there's no shortcut. Never is. Intelligence just isn't what you know. It's how efficiently you learn and can verify it.",
      "platforms": {
        "tiktok": {
          "video_id": "7487301843425447198",
          "url": "https://www.tiktok.com/@rajistics/video/7487301843425447198",
          "view_count": 1671,
          "upload_date": "2025-03-29",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEsMfogOeAmeWgouiBXLehAQY4A4tgA8VSAfPg~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=HCeMawBeuPO22k38xPeWqHr2Gr4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17879338206269262",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3WNUoKiwd2U",
          "url": "https://www.youtube.com/watch?v=3WNUoKiwd2U",
          "view_count": 2937,
          "upload_date": "2025-03-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6011,
      "title": "Isolation Forests for Anomaly or Outlier Detection",
      "description": "Isolation Forests for Anomaly or Outlier Detection",
      "upload_date": "2024-08-03",
      "total_views": 4605,
      "max_views": 4605,
      "topics": [
        "anomaly",
        "anomalydetection",
        "daten",
        "detection",
        "die",
        "forests",
        "isolation",
        "ist",
        "madeinhotelroom",
        "outlier",
        "und"
      ],
      "search_text": "Isolation Forests for Anomaly or Outlier Detection anomaly anomalydetection daten detection die forests isolation ist madeinhotelroom outlier und",
      "platforms": {
        "instagram": {
          "video_id": "17943608063745285",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "f-Vdi8D6NNA",
          "url": "https://www.youtube.com/watch?v=f-Vdi8D6NNA",
          "view_count": 4605,
          "upload_date": "2024-08-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @bosstoastmaker shallow learning with tensorflow playground #datascience #tensorflow #python #machinelearning",
      "description": "Reply to @bosstoastmaker shallow learning with tensorflow playground #datascience #tensorflow #python #machinelearning",
      "upload_date": "2022-02-20",
      "total_views": 4593,
      "max_views": 4593,
      "topics": [
        "datascience",
        "going",
        "let",
        "machinelearning",
        "python",
        "tensorflow"
      ],
      "search_text": "Reply to @bosstoastmaker shallow learning with tensorflow playground #datascience #tensorflow #python #machinelearning datascience going let machinelearning python tensorflow Ready to play? We're going to use TensorFlow's playground to do a little bit of shallow learning today. Let's start with our simple network. We're going to add more neurons. See how it gets better. Let's switch to a more complex data source. We'll keep adding more neurons. Still, the model doesn't work. Let's switch up. We're going to add some feature engineering. And now our model fits.",
      "platforms": {
        "tiktok": {
          "video_id": "7066925407123066159",
          "url": "https://www.tiktok.com/@rajistics/video/7066925407123066159",
          "view_count": 4593,
          "upload_date": "2022-02-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6c9ab294c79b4dc9b347f2b82c20a10b_1645396793~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=WY8EVreINkurFiij5rVHfB6DRUE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6160,
      "title": "Breaking down how advances in AI, from GPT to Veo 3 — owe their performance to massive, often ethically questionable datasets. It traces the evolution from ImageNet to Common Crawl, LAION-5B, and YouTube, highlighting how data access — not just model architecture — is the real engine behind AI progress.",
      "description": "Breaking down how advances in AI, from GPT to Veo 3 — owe their performance to massive, often ethically questionable datasets. It traces the evolution from ImageNet to Common Crawl, LAION-5B, and YouTube, highlighting how data access — not just model architecture — is the real engine behind AI progress.",
      "upload_date": "2025-05-24",
      "total_views": 4566,
      "max_views": 3836,
      "topics": [
        "data",
        "gpt",
        "images",
        "know",
        "models",
        "quantization",
        "set",
        "slimming",
        "training"
      ],
      "search_text": "Breaking down how advances in AI, from GPT to Veo 3 — owe their performance to massive, often ethically questionable datasets. It traces the evolution from ImageNet to Common Crawl, LAION-5B, and YouTube, highlighting how data access — not just model architecture — is the real engine behind AI progress. data gpt images know models quantization set slimming training Google's VO3 looks amazing. But do you want to know why it's so good? I know why, and it's a little bit messy. You're going to hear it from somebody who's been training AI models for a long time. So let's start in 2012, the start of the deep learning revolution. You know what triggered it? This dataset imaged it. 14 million labeled images blew everything else away, and it was the fuel for the first generation of deep learning models. But how it was built came with a little twist. It was labeled using Amazon's mechanical turt, where for the first time we outsourced to this global army all those labeling tasks. Then we hit the GPT era, and everyone's like, wow, these models are so smart. Yeah, you know why? Because it read all of us. Common Crawl, which has 250 billion pages, is a scrape of the entire internet, your forums, blogs, your Wikipedia. If you ever posted online, you're part of the training data. Then we wanted computers to generate images. We used 5 billion text pairs, Lion 5B. Where did they get those images? They scraped them. Did that violate copyright? Oh no, they figured out if they only kept the image URLs, not the images, that could get around laws. You know what they didn't do? They didn't check those images and saw that lots of them had CSAM content. Not exactly model citizens. Now we get to video and VO3. Now Google of course has brilliant researchers, but so does OpenAI, so does Anthropid. You know what the difference is? One of these companies owns YouTube, the largest video data set in the world, where every click, every watch time, every caption, those billions of videos are ready for training. And the lesson here isn't just that progress in AI is governed by who has the biggest, weirdest, most diverse data set, but that data in the end was all taken from us. And now we're paying to go out and use it. Unless you're in the UA. Or chat GPT is going to be free.",
      "platforms": {
        "tiktok": {
          "video_id": "7508027641941773599",
          "url": "https://www.tiktok.com/@rajistics/video/7508027641941773599",
          "view_count": 3836,
          "upload_date": "2025-05-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oA3EEfdxQANRBlAIAq8iA5CV3EMiEwtvowePAA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=XAoZHBB2c%2BlrxpQJ9h%2BLlcrxPSs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18062650970320448",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OXVlroLSrXM",
          "url": "https://www.youtube.com/watch?v=OXVlroLSrXM",
          "view_count": 730,
          "upload_date": "2025-05-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6166,
      "title": "This video discusses a Carnegie Mellon study comparing prompt-based inference with fine-tuned large language models. The research found that expanding the prompt context with numerous, relevant examples can match or exceed fine-tuning performance, though returns diminish after several hundred examples. It highlights the importance of strategically choosing between prompting and fine-tuning based on the specific use-case requirements. In-Context Learning with Long-Context Models: An In-Depth Exploration https://arxiv.org/pdf/2405.00200",
      "description": "This video discusses a Carnegie Mellon study comparing prompt-based inference with fine-tuned large language models. The research found that expanding the prompt context with numerous, relevant examples can match or exceed fine-tuning performance, though returns diminish after several hundred examples. It highlights the importance of strategically choosing between prompting and fine-tuning based on the specific use-case requirements. In-Context Learning with Long-Context Models: An In-Depth Exploration https://arxiv.org/pdf/2405.00200",
      "upload_date": "2025-05-11",
      "total_views": 4564,
      "max_views": 3613,
      "topics": [
        "approach",
        "context",
        "examples",
        "fine",
        "hallucinations",
        "language",
        "large",
        "models",
        "performance",
        "practical",
        "tuning"
      ],
      "search_text": "This video discusses a Carnegie Mellon study comparing prompt-based inference with fine-tuned large language models. The research found that expanding the prompt context with numerous, relevant examples can match or exceed fine-tuning performance, though returns diminish after several hundred examples. It highlights the importance of strategically choosing between prompting and fine-tuning based on the specific use-case requirements. In-Context Learning with Long-Context Models: An In-Depth Exploration https://arxiv.org/pdf/2405.00200 approach context examples fine hallucinations language large models performance practical tuning Did you stuff everything into a prompt of a large language model or use a little bit of slower process of fine tuning? A new study from Carnegie Mellon takes a look at this. So the prompting school wants to keep putting lots of information inside the context length of these large language models, often focusing on the most powerful models because what this allows them to do is solve any type of problem by just using prompting. The fine tuning approach says, no, let's alter the weights of the model according to the problem we're doing. This does take some upfront work to do, but then inference becomes a lot easier and also you can often use a smaller, less compute intensive model without losing predictive performance. The research used a variety of large language models across a number of different tasks and what they found was if you continue to extend the length of the context by adding lots and lots of examples, you could get equal to and sometimes even better performance than fine tuning. If we look closely at the results, you'll see that the performance of the models kept going up as we added more examples. So a good takeaway for you is add more examples to your prompts until you hit a point of diminishing returns. They also found that instead of using random examples but using similar examples to the problem you're solving does give you a performance boost, although it starts going away once you get to hundreds of examples. One thing I take away from this paper is that both prompting and fine tuning have their place, so pick what fits best for your use case.",
      "platforms": {
        "tiktok": {
          "video_id": "7503206873882021150",
          "url": "https://www.tiktok.com/@rajistics/video/7503206873882021150",
          "view_count": 3613,
          "upload_date": "2025-05-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogpgRLGNoEVAEAE7vCIiAAeiIsBArUALCAfSwi~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=nCzjohdp73sRN5E3gJm%2BmKEmb%2FA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18089365762528921",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OOEgb03c6-k",
          "url": "https://www.youtube.com/watch?v=OOEgb03c6-k",
          "view_count": 951,
          "upload_date": "2025-05-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @notryantaylor  here it is without music.  this is for my 4 kids who all text me that they are above average.",
      "description": "Reply to @notryantaylor  here it is without music.  this is for my 4 kids who all text me that they are above average.",
      "upload_date": "2022-02-11",
      "total_views": 4539,
      "max_views": 4539,
      "topics": [
        "average",
        "notryantaylor",
        "percent",
        "reply",
        "think",
        "thought"
      ],
      "search_text": "Reply to @notryantaylor  here it is without music.  this is for my 4 kids who all text me that they are above average. average notryantaylor percent reply think thought Let's talk about being above average. Most of us think our driving abilities are above average. 70 percent of high schoolers thought of themselves as above average leaders. 94 percent of college professors thought they had above average teaching ability. 87 percent of Stanford MBA students thought they ranked above the median in their class. So it's okay. We all think we're above average.",
      "platforms": {
        "tiktok": {
          "video_id": "7063582430107602222",
          "url": "https://www.tiktok.com/@rajistics/video/7063582430107602222",
          "view_count": 4539,
          "upload_date": "2022-02-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3177712b379a4a07a541b4bdcc9e573b_1644618459~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=YmleCoPSPCgflqpl9f9bbuLyDtQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Thanks to barrnanas and AmplifyPartners",
      "description": "Thanks to barrnanas and AmplifyPartners",
      "upload_date": "2023-10-10",
      "total_views": 4538,
      "max_views": 4538,
      "topics": [
        "amplifypartners",
        "barrnanas",
        "thanks"
      ],
      "search_text": "Thanks to barrnanas and AmplifyPartners amplifypartners barrnanas thanks",
      "platforms": {
        "tiktok": {
          "video_id": "7288222942830316842",
          "url": "https://www.tiktok.com/@rajistics/video/7288222942830316842",
          "view_count": 4538,
          "upload_date": "2023-10-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6193,
      "title": "An Introduction to Dify.AI - A UI based tool for building Generative AI Agentic Workflows or Applications. I have a longer video on YT going through a project I needed to build this weekend.",
      "description": "An Introduction to Dify.AI - A UI based tool for building Generative AI Agentic Workflows or Applications. I have a longer video on YT going through a project I needed to build this weekend.",
      "upload_date": "2025-03-23",
      "total_views": 4518,
      "max_views": 4201,
      "topics": [
        "able",
        "applications",
        "build",
        "building",
        "code",
        "genai",
        "generative",
        "improve",
        "like",
        "recommenders",
        "recsys"
      ],
      "search_text": "An Introduction to Dify.AI - A UI based tool for building Generative AI Agentic Workflows or Applications. I have a longer video on YT going through a project I needed to build this weekend. able applications build building code genai generative improve like recommenders recsys Would you like to build your own GenAI applications? Maybe you're not comfortable coding. You prefer these types of UI approaches. Well, look at this. You're able to build a chatbot here. You can also build more complex workflows. This is a workflow that takes a notebook LM approach allowing you to go from text all the way to building a podcast. This is Diffie AI. It's one of a growing number of solutions which uses a UI approach to building GenAI applications. Some others out there are Langflow and Langgraph that allow you to do this as well. When you kind of use these UI elements, what you're doing together is putting together different blocks here. You can see for example here, I'm calling it my query block. I pass it into my API here. I have a code block here where I'm working with the string running some code against it. I needed to connect an API up here, but there are lots of building blocks out there for connecting LMs, working with agents, logic loops in there, being able to transform. Of course, if you're a coder, you can do all these things in code, but what we're doing here is taking advantage of a UI element to do that. You can build all types of applications. They've got lots of example applications here. You can start with here was a notebook LM application. Give it a try. Play around with these. If you get stuck, the nice part is tools like ChatGBT will be able to help. I literally took a screenshot, shared in some of the code problems I was having. It helped me along the way. If you need a place, you want to start building GenAI applications, you like a UI approach, go check it out.",
      "platforms": {
        "tiktok": {
          "video_id": "7485109627433028894",
          "url": "https://www.tiktok.com/@rajistics/video/7485109627433028894",
          "view_count": 4201,
          "upload_date": "2025-03-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogD1g6fOACxHnFAEE95CAkVeE4IEcEyARaDAT0~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=K0Yv%2B6uezqUyz5qotY6dkvZyN20%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18095609071553024",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "foPJnZw-SDY",
          "url": "https://www.youtube.com/watch?v=foPJnZw-SDY",
          "view_count": 317,
          "upload_date": "2025-03-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6058,
      "title": "Deep dive into Group Relative Policy Optimization (GRPO), a Reinforcement Learning algorithm used by Deepseek in their R1 reasoning model. Let's walk through a free Google Colab notebook and show you how you can start learning how it all works. The notebook is available at: https://bit.ly/raj_grpo",
      "description": "Deep dive into Group Relative Policy Optimization (GRPO), a Reinforcement Learning algorithm used by Deepseek in their R1 reasoning model. Let's walk through a free Google Colab notebook and show you how you can start learning how it all works. The notebook is available at: https://bit.ly/raj_grpo",
      "upload_date": "2025-02-16",
      "total_views": 4498,
      "max_views": 3684,
      "topics": [
        "algorithms",
        "app",
        "deep",
        "dive",
        "going",
        "group",
        "interactive",
        "kind",
        "learning",
        "like",
        "look",
        "model",
        "notebook",
        "outlier",
        "relative",
        "see",
        "visualization"
      ],
      "search_text": "Deep dive into Group Relative Policy Optimization (GRPO), a Reinforcement Learning algorithm used by Deepseek in their R1 reasoning model. Let's walk through a free Google Colab notebook and show you how you can start learning how it all works. The notebook is available at: https://bit.ly/raj_grpo algorithms app deep dive going group interactive kind learning like look model notebook outlier relative see visualization Have you heard the hype around DeepSeek R1? Do you want to learn how reinforcement learning and reasoning all kind of come together? Well, I've got this notebook here that any of you can run for free and it's going to help give you the intuition for what's going on behind all this. Now to back up, we've had the DeepSeek Reasoning R1 model come out in the last month. It's a different way of how we've trained models. Traditionally, we've used supervised learning where we give the model lots of examples and it learns from those examples by copying those examples essentially. Here what we're going to instead do is we don't give it examples. What we do is we just tell it, we give it points, rewards, for if it gets something right or not. You've probably seen this in my earlier videos on how to teach watermelons and how to teach an AI to eat watermelons and not poop. But we're going to now apply this to something for elementary school math and we're going to show you how this type of problems can do this. So this is for me, it's exciting. It's exciting that I found this notebook here that we can all run easily. So I want to first kind of start walking through this notebook. Any of you can run this for free in Google CoLab. Google makes CoLab environment for free available to everyone. You only get a meager set of GP resources, a T4. If you're doing this for the first time, you might have to hit connect over here when you kind of start running this notebook. But that enough a GPU here is to get started and to get a sense of what's going on with this. Now the notebook here in the grand scheme of the open source movement, I did not build this entirely from scratch. I have largely built and worked from a notebook that was built by some other folks on here. I've linked it in here. You can go kind of thank them. They of course have been building on others. This is the way the community works like this. Like this. So go ahead, take a look, but I just want to give credit where it's due like that. So we're going to first we're going to spend some time walking through the notebook, give you a sense of how the notebook works that was already designed. Then I'm going to spend a little bit of time talking about group relative preference optimization, the algorithm, the magic behind all of this. And then we're going to come back. We're going to make some modification to this code. So I want to show you how you can start experimenting a little bit with it and playing around with it as well. So let's strap in and we're going to have some fun here going through this. I just kind of installing some of the libraries. There's a little bit. A lot of work people had to do to get all of this. So very sophisticated code, this models that understand math to run in that smaller, smaller co lab environment. So there's a bunch of these steps that you'll see in here earlier were to optimize it. So it runs like that. Now, the first thing is I always like to start with the data, understand exactly kind of what is the problem that we're solving here. So I've got a function that's going to pre process our data set. This is the GSM 8k data set. It's a very popular data set. You can see kind of open AI has kind of built this out. It's got 7000 different samples in it. And so let's go through and actually kind of look at one of the examples of the data set like that. Always got to swim with your data. Always need to understand it. So these are the examples we're going to use to train the model. We're going to train the model to be better at math. It's got word problems like this. So this is kind of elementary grade school math where we have these questions. We have an answer. So this is important for this type of work. And we'll talk about this when it comes to rewards. We know exactly what we want. So we have a very black and white answer that we have. And then we also have in here kind of the prompt, the reasoning that goes on an example of what is this kind of expected kind of thinking process that the model is going to learn. And so just like you, we want the model to think about what it does and kind of work in a step by step way as it's solving these more complicated problems. These are not kind of like one shot off the top of the head. The idea is the model has to think through a couple of steps to be able to solve all these together. So that's the data set. Now, remember, we're going to teach it here not by just giving it lots of examples, but by rewarding it when it gets the problem right. And we're going to give it two sets of rewards here. So the first is there's a format reward. And you can take a look at the code, a good old regex. What we're going to do is if the model follows the certain pattern, if the response is in a certain pattern, we're going to give it a reward. Tell me how many points is it going to get. And the reason is just like when you're at school, when you get points for, even though you didn't get the answer right, but you kind of filled it in, showed your thinking, we're going to do the same thing for the model. And we give one point here, one point here, if it correctly has the correct format. The other reward we have is the correctness reward. And this is just going to compare and see is that answer that the model generated the same as the answer in our data set. And if so, how many points is it getting. That's right. Two points. And that's our reward function. So let's go through here. We've got some code here. I don't know why I've got that hidden. We're going to go ahead and load the model we're using. So to run this in our smaller environment, we're going to use the quen model, the point five billion parameter model. This is a very small model, but it understands basic math already. So it's good foundation to start using for this reinforcement learning that we're going to do on top of it. The real deep seek R1 model is much larger. It literally takes like 16 GPUs if you're trying to run the entire thing, like a little cluster of GPUs. So that's why we're not working with the full size model. And instead we're working with kind of the smaller toy size model to do that. Now, as we go through this, let's go ahead and close this. Hopefully everything is loading up while we're talking. We've got the model. A couple of other things is we're going to use the the GRPO trainer from hugging face and we'll walk through that code in a little bit. But here's some of the options that are available. There you go. There's a lot of options available. But we'll highlight a couple of these as we go through. Some of these are traditional things that you've seen for kind of training other types of models. We're going to touch on some of these here later as we go through this. You'll see, for example, in this case, we're using weights and biases. As our experiment manager. And so that piece is in there as well. And so the first part of this code here is just going to load that model that we're going to use in there. Here's our GRPO trainer. We give it the model that we use. Tokenizer, of course, because we're working with text, we need to tokenize it, convert it into that numerical way it likes to think. We've got our two reward functions, the training arguments, and then our train data set. So when it's running, I've used that experiment management software from weights and biases. So you can kind of click on this. And especially if you're doing longer runs, this is very useful to look at. For example, you can see what is that training reward? How well is that model doing? Is it still not very good at this math, getting a very small amount of rewards? Or, and I can show you kind of from some of my earlier runs that we have this is, is this model, for example, starting to learn, getting better, getting more and more points, because now it's solving each of those math problems a little bit better like that over time. So that's the nice part about using some experiment tracking. You can kind of keep track of all the stuff as it goes through there. So now let's take a look at what it's actually doing here. All right, so let's look at the examples that is going through this. Now, if we look at this top example here, there's a lot going on. Let me explain kind of what the output here is that we were generating. First is, is the question that's being asked. So this is the data set we have. We have the question in the answer. Then response here is how the model is responding to this. Now you can see here, it's got that formatting that we like that the reasoning is showing in under reasoning and then we have the bracketed answer like that. And so then what we can do is we can see in this example for this, in this example, the answer is wrong, right? It gets a black, it's a red X for that. But we give it a little partial credit because the formatting was correct. And so that's why you see a red X here and then the star here for that. Now, when we're running this, we're actually running 16 different completions at a time, which is why we can see all the other scores here. In the printout here, I'm not showing you all the other examples. You have to trust me on that. But that's what's going on here. Now, if we look through some, let's look through a couple of the other examples here and let's see if we can find something else. So here's an example of where the model gets the answer right and the formatting right. So you see both kind of the green check mark for that and the formatting like that. This is a kind of a little bit easier of a problem. So it's able to do that. So you can see across all these different 16 that now we can start totally up and aggregating and seeing how the model is doing across all these starting to highlight, you know, what are good examples of reasoning, what are bad examples like that. So that's what's going on there. Now I want to take a break and talk a little bit about what's going on, what the math is, what's going on underneath, what the code looks like that's doing, that's powering all of this. So I remember I mentioned kind of the group relative policy optimization here. If we kind of type that into our thing, we're using the Hugging Face library. With Google, we can quickly find the trainer page that Hugging Face has on this that describes exactly the code, gives examples of this. But I want to start with the intuition for what's going on with GRPO. We've talked about how we have prompts and we get lots of completions, like 16 completions in mind in the real training that our ones does is you get a lot more completions like that. We use all those completions to figure out kind of what the reward is, how well we're doing on that. And that's one important part of the signal that we have. And if you've seen the video games ones that we've seen, right, we want the model, of course, to get better and better to be incentivized for the reward. But one of the problems that can happen with reinforcement learning is the model gets so fixated on its reward that it's willing to go to any lengths and even cheat a little bit. So a classic example of this was in kind of with robotics where you want to teach a robot to grab a bottle, right? You want to grab the bottle like this. The robot figured out by just passing the hand in front, it could get the points. Nobody was actually keeping track if there was a hand on both sides. But just by doing that, it could get the points. And so that's what we have to watch out with these is that the models are happy to find ways to kind of cheat and get around this as well. So to discourage this behavior, we want to ground the model. We want to make sure that the model doesn't go way off on its own like that. And so that's where this stuff on the bottom comes in is when it's starting to start to change, when it's starting to reward, follow that new behavior. We want to make sure it doesn't go too far, just like your friends kind of rain you in at times. Well, we're going to have a reference model that looks and sees how much you're changing. And if you're changing a little bit too much, it'll kind of give you a little tension just to kind of keep you from going off a little bit too far. Now, we have some reinforcement learning has fancy terms for this like KL divergence, for example. But hopefully I've given you the intuition for it. As you start playing around this area, you'll learn the math. You'll see kind of how this stuff works like that. And what I want to show is that we can actually take a look at the code here for this if you want to get a sense of what's going on with this where great thing about open source. We can look at the code. You can see right here's kind of the example of how to set this up that we already used earlier. The code explains carefully what are all the different reward functions here. You can see training data set rewards, all this. But for example, if you go and look at the code, remember, I was talking about how we need a reference model, something that doesn't pull us away. Well, the code here will show you exactly like how is it going to create the reference model. So if you use something like parameter efficient fine tuning, well, you're just training an adapter there. Well, so we don't need a reference model. You can just use that existing model to do that. Sometimes when people set this up, they do have multiple models going sometime like in this example, we're going to be able to use the base model as a reference model. But something for you to think about is when you're actually using this kind of reinforcement techniques, this particular algorithm requires having that reference model as a way to compare that. Hopefully this gives you a peak to for kind of where the code is, how you can kind of start diving deeper. And of course, just like what we're going to do here in a minute, like you can use things like print statements and ways like that to kind of probe and understand what's going on with this stuff. So now that I've understand the code, let's go back to this. And yes, it's running, but I want to kind of probe at this a little deeper to see what's going on. So I'm going to change this. We're going to change the code here. I'll walk you through this. And I'm going to slow it down a little bit so I can follow it a little bit easier. So I'm going to change the values here for kind of the number of generations and the batch size. And it's just because with 16, we couldn't see all the examples. But if I just make the examples a little bit smaller, I can kind of follow along with that. The other thing I want to do is I want to look a little bit more at that data that how we're doing on each iteration like that. So I'm going to go ahead and I have some code here. And I'm going to grab this code. And we're going to change the correctness function that's used to do this. Let me go ahead and run that. But I'm going to restart the model here. We're going to restart the model. We're going to run it with this new correctness functions. And the correctness function goes through is instead of just showing us the first of like 16 completions, this is going to show us each completion that goes along. And again, like you're like, what are you getting with multiple completions? Why is it doing multiple completions? What does that mean? Well, this is where we want to kind of throw things in and start taking it apart to get a better sense of understanding what's going on here. So I'm going to give this a second here to run and we're going to take a look and see kind of how we've changed the model from this. All right. The model's back and it's running lots of these examples like this. So let's take a look at, let's see if I can kind of stop this and take a look. You can see here we've got the generation one. And if you look down, you'll see generation two, generation three. So we have the question. And we'll see it gives us an answer. And then we'll see the response. And here you can see, look at all the, look at this long, long, long, long, long response. Look at all the reasoning you can see that it's doing. And then it's extracted the answer, but the extracted answer isn't very clean. Look at this. And that's because it doesn't have that formatting that we want to do like that. So although it kind of got, you know, it got to something. It wasn't, we weren't able to actually extract the answer. It knew the answer, but we weren't able to extract it. So you can see it doesn't get kind of credit at all for this. Let's look at the, let's look at the next one here. This next generation. And so here the model, when it's doing these multiple generations, it's trying different from what it knows, different kind of approaches. It's writing things differently, trying to think through like how best to solve the problem. Here we can see, oh, look, this one did much better. It got the reasoning right. It got the formatting right. And it was able to get the answer. Here's another one. And look at, look at, and the idea here is you can see it's trying different types of approaches to be able to solve this. I don't know what the heck it's doing here. Like this is reading and it's like converting feet and inches and like this, like this. But this is the idea here is the computer is trying to figure out for itself how to solve these problems. We're not sitting there tapping out on the shoulder saying, Hey, you need to do this. You need to do that. We want it to figure these things out for itself. And if we had tried this experiment a year ago on these models, the models didn't have enough base intelligence to be able to reason to figure these things out. Part of what's happened here is these base models have increased now. So for these types of tasks, we're able to really supercharge it with techniques like GP, GRPO. But you can see here by kind of breaking it down to like smaller generations, we can really start seeing the variety of thoughts that the model is doing, how it's doing on each of these. Now, if you run this for a little longer, you will see that the model does improve over time and get at this. And you can run the evaluation as well. But I'm hoping at least in this little bit of time, I've given you a little bit of insight into kind of this really kind of radical new way that we have now really bringing reinforcement learning, combining it for language models to improve on different types of reasoning tasks like this. So go have fun, check it out, talk to you later. Hey, I'm back. I recorded the video last night and I thought about it overnight and I still have a few things I want to clarify. The first is GRPO is an awesome algorithm, but it's not the king of all algorithms. It's not going to replace everything else. We use lots of different algorithms to solve lots of different problems. So don't let me think, don't come out of this feeling like this is the only thing you need to learn. The second is even in the deep seek paper, when they use GRPO, when they just trained with reinforcement learning, the model did good, but not great. The best performance they got was when they mixed supervised fine tuning with the reinforcement learning and kind of layered that approach like that. So we're not totally throwing out supervised fine tuning. The third thing is on the smaller models, like the ones we have here, what they found out that reinforcement learning alone wasn't the best way to train these models. In fact, if you look at the closely at the results here, you'll see most of the improvement is done through the formatting and the better formatting. It's not necessarily on the reasoning abilities that we are able to teach this model. And in fact, on the smallest models, they found that using supervised fine tuning with thoughts, the traces of the reinforcement learning was actually a useful way to train the smaller models. So all that just wants to kind of put that GRPO in position. I think it's great to be able to learn it and take advantage of it. So still go at it.",
      "platforms": {
        "tiktok": {
          "video_id": "7472052175380696350",
          "url": "https://www.tiktok.com/@rajistics/video/7472052175380696350",
          "view_count": 3684,
          "upload_date": "2025-02-16",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0ACyibRfsYABEhFRf9BgmE2AViVEqgIvn9gIw~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=sXkPnH8Ud6%2FelXNevvcJ%2FXbt7aQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18043322330358826",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "1zPuRAgr1F4",
          "url": "https://www.youtube.com/watch?v=1zPuRAgr1F4",
          "view_count": 814,
          "upload_date": "2016-12-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6291,
      "title": "Some common data distributions when modeling including skewed and zero inflated. There are many other distributions, but just wanted people to know that normal distribution isn‚Äôt normal in my experience. #datascience #statistics #datadistribution #zeroinflated #tweedie",
      "description": "Some common data distributions when modeling including skewed and zero inflated. There are many other distributions, but just wanted people to know that normal distribution isn‚Äôt normal in my experience. #datascience #statistics #datadistribution #zeroinflated #tweedie",
      "upload_date": "2023-02-03",
      "total_views": 4498,
      "max_views": 4498,
      "topics": [
        "common",
        "data",
        "datadistribution",
        "datascience",
        "distributions",
        "statistics",
        "tweedie",
        "zeroinflated"
      ],
      "search_text": "Some common data distributions when modeling including skewed and zero inflated. There are many other distributions, but just wanted people to know that normal distribution isn‚Äôt normal in my experience. #datascience #statistics #datadistribution #zeroinflated #tweedie common data datadistribution datascience distributions statistics tweedie zeroinflated",
      "platforms": {
        "instagram": {
          "video_id": "17912520155620629",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "qiG2pq1Ida4",
          "url": "https://youtube.com/shorts/qiG2pq1Ida4?feature=share",
          "view_count": 4498,
          "upload_date": "2023-02-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6642,
      "title": "#onthisday Showing the latent space for stable diffusion. #stablediffusion #datascience #machinelearning #codetok #umap√™pravoc√™",
      "description": "#onthisday Showing the latent space for stable diffusion. #stablediffusion #datascience #machinelearning #codetok #umap√™pravoc√™",
      "upload_date": "2023-09-10",
      "total_views": 4495,
      "max_views": 4495,
      "topics": [
        "analogy",
        "chatgpt",
        "codetok",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "onthisday",
        "politics",
        "stablediffusion",
        "umap",
        "well",
        "works"
      ],
      "search_text": "#onthisday Showing the latent space for stable diffusion. #stablediffusion #datascience #machinelearning #codetok #umap√™pravoc√™ analogy chatgpt codetok datascience largelanguagemodels machinelearning onthisday politics stablediffusion umap well works",
      "platforms": {
        "tiktok": {
          "video_id": "7277185334624931114",
          "url": "https://www.tiktok.com/@rajistics/video/7277185334624931114",
          "view_count": 4495,
          "upload_date": "2023-09-10",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18012209656867447",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-09-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5941,
      "title": "Quantization used to be a post-training compromise, smaller and faster but at the cost of accuracy. Kimi K2-Thinking flips the script using Quantization Aware Training (QAT) to build native INT4 precision that stays stable even for long chain reasoning and RL. Get used to it, it is the next training paradigm.",
      "description": "Quantization used to be a post-training compromise, smaller and faster but at the cost of accuracy. Kimi K2-Thinking flips the script using Quantization Aware Training (QAT) to build native INT4 precision that stays stable even for long chain reasoning and RL. Get used to it, it is the next training paradigm.",
      "upload_date": "2025-11-16",
      "total_views": 4478,
      "max_views": 3143,
      "topics": [
        "compromise",
        "faster",
        "int4",
        "kimi",
        "language",
        "models",
        "nvidia",
        "outsmart",
        "post",
        "quantization",
        "reasoning",
        "training",
        "used",
        "vision"
      ],
      "search_text": "Quantization used to be a post-training compromise, smaller and faster but at the cost of accuracy. Kimi K2-Thinking flips the script using Quantization Aware Training (QAT) to build native INT4 precision that stays stable even for long chain reasoning and RL. Get used to it, it is the next training paradigm. compromise faster int4 kimi language models nvidia outsmart post quantization reasoning training used vision Quantization, times are changing. You mean those tools I'd run after training, where I'd want to quantize something for faster inference? Exactly. But the Kimi K2 thinking model shows us something very different. Wait, Kimi? Isn't that one of those Chinese models? Can we trust that? Chinese labs are embracing an open culture. They're sharing weights, architecture, because that pushes progress forward. American labs, on the other hand, mwah, mwah, mwah. Okay, so what did they do? They used quantization-aware training during post-training. So instead of compressing the weights later, what they instead do is taught the model how to train and learn using int4 weights. And this worked really well for that long-chain reasoning. Yeah, with traditional quantization, the model doesn't know it was compressed. And so when errors start piling up, it starts hallucinating, because it thinks it has a lot more bits than it actually does. Exactly. They were able to keep their precision stable over long sequences, and that's because everything runs in int4. The training is a lot faster and more robust. Okay, but did they pick int4 because they don't know about all those new advances NVIDIA has? They know. They picked int4 because it works on lots of GPUs, not just the latest NVIDIA hardware. Whoa, they want to support older GPUs? Has NVIDIA approved this message? Well, maybe we don't need to buy a new GPU every two years.",
      "platforms": {
        "tiktok": {
          "video_id": "7573120839005097246",
          "url": "https://www.tiktok.com/@rajistics/video/7573120839005097246",
          "view_count": 3143,
          "upload_date": "2025-11-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAfJVjAZWiBgMPb0ARBbIIIZvBCQ3AiqhB6Iji~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=FZo%2FlnjwoXauWis3jbK6pNgxmAo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17859416484479018",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "LId5sfCaOeI",
          "url": "https://www.youtube.com/watch?v=LId5sfCaOeI",
          "view_count": 1335,
          "upload_date": "2025-06-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6061,
      "title": "Highlighting some great work investigating basic behavior of LLMs and finding issues with their reliability: Do Large Language Model Benchmarks Test Reliability? - https://arxiv.org/pdf/2502.03461 Episodic Memories Generation and Evaluation Benchmark for Large Language Models - https://arxiv.org/pdf/2501.13121v1",
      "description": "Highlighting some great work investigating basic behavior of LLMs and finding issues with their reliability: Do Large Language Model Benchmarks Test Reliability? - https://arxiv.org/pdf/2502.03461 Episodic Memories Generation and Evaluation Benchmark for Large Language Models - https://arxiv.org/pdf/2501.13121v1",
      "upload_date": "2025-02-08",
      "total_views": 4478,
      "max_views": 2277,
      "topics": [
        "arxiv",
        "basic",
        "benchmark",
        "compressing",
        "distillation",
        "investigating",
        "kept",
        "language",
        "large",
        "llms",
        "models",
        "night",
        "org",
        "pdf",
        "quantization",
        "reliability",
        "using"
      ],
      "search_text": "Highlighting some great work investigating basic behavior of LLMs and finding issues with their reliability: Do Large Language Model Benchmarks Test Reliability? - https://arxiv.org/pdf/2502.03461 Episodic Memories Generation and Evaluation Benchmark for Large Language Models - https://arxiv.org/pdf/2501.13121v1 arxiv basic benchmark compressing distillation investigating kept language large llms models night org pdf quantization reliability using Ever hear about the benchmark story? It's very unsettling. It starts with an AI research building AI models, teaching them basic math, grade school problems, the kinds kids solve. Now the first models he built were terrible, 40% on GSM 8K. When he didn't give up, kept them building them smarter and smarter. His AI skills were off the roof. He got them up to 95%. Started winning everything, Stargate, Nobel Prizes. That last 5%. He kept eating at them, kept them up at night, kept looking at the benchmark over and over again. Everyone told him, like, oh, just label noise. Bad questions, wrong answers. He couldn't stop digging. Something wasn't right. He cleaned it all up, fixed every bad label, every unclear question. Thought that would solve it. The mistakes were still there. He kept investigating. He started to see patterns. The way time is slippery, how first and second were quite fixed. Then at 3 AM one night, screen burning his eyes. He found something weird. When AI had to divide, if the answer was prime, he would round up. Over and over, he kept making that mistake, like waking up from a beautiful dream where AI understood the way humans do. Only realized it was an illusion, built on statistical patterns. There's no real understanding at all. Now he can't sleep at night. He's just haunted by the knowledge that these brilliant machines, these superhuman AIs, can't grasp what any child knows.",
      "platforms": {
        "tiktok": {
          "video_id": "7468866598854593823",
          "url": "https://www.tiktok.com/@rajistics/video/7468866598854593823",
          "view_count": 2277,
          "upload_date": "2025-02-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0smRnAwpCSEEvdZEAIAev7fD1NAKFEDA0VCar~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=DRard3TurDkfkVkyeeb3L3XXHAw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18352272376178538",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ynpEmk96oos",
          "url": "https://www.youtube.com/watch?v=ynpEmk96oos",
          "view_count": 2201,
          "upload_date": "2025-02-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6188,
      "title": "Most people evaluate RAG the wrong way — just checking if the answer is “correct.” But great RAG needs to answer: how did it get there? In this video, I break down the key metrics you should actually be using. If you need a package to measure these metrics, check out the open source RAGAS package.",
      "description": "Most people evaluate RAG the wrong way — just checking if the answer is “correct.” But great RAG needs to answer: how did it get there? In this video, I break down the key metrics you should actually be using. If you need a package to measure these metrics, check out the open source RAGAS package.",
      "upload_date": "2025-03-30",
      "total_views": 4467,
      "max_views": 2976,
      "topics": [
        "actually",
        "answer",
        "based",
        "explained",
        "get",
        "metrics",
        "need",
        "rag",
        "ragas",
        "simply"
      ],
      "search_text": "Most people evaluate RAG the wrong way — just checking if the answer is “correct.” But great RAG needs to answer: how did it get there? In this video, I break down the key metrics you should actually be using. If you need a package to measure these metrics, check out the open source RAGAS package. actually answer based explained get metrics need rag ragas simply Are you testing your RAG systems ready to move beyond? Did the AI get the answer right? Check out my cheat sheet. It helps you dive deeper and ask, did you find the right documents? Did you actually use them? Were there issues with my retriever, my re-ranker, my generator? Let's start with four elements for RAG metrics. You have your questions, you have your retrieved documents, the ground truth, and then the generated answer. Did you even pull helpful chunks? Did the answer make sense for the question? Is the answer actually backed by the docs? Now that's the bare minimum. You should have your ground truth label. And if you do, then you can start adding metrics like this, where you can ask, hey, did we rank the good stuff high? Did we get all the retrieved chunks that we need? Hey, is this answer actually true? So next time you should be prepared to talk about your system and say how effective it's working for retrieval, for generation. And if you do this, you'll have a much better RAG system.",
      "platforms": {
        "tiktok": {
          "video_id": "7487752773421878559",
          "url": "https://www.tiktok.com/@rajistics/video/7487752773421878559",
          "view_count": 2976,
          "upload_date": "2025-03-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8RXEAKBzB16v2ciiAAnJAECCiHpPeDArII5Dk~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=ri2z95zcWLi%2FGSsKsWzNtqlQKWo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18067557376912427",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "BMTicV80M8Q",
          "url": "https://www.youtube.com/watch?v=BMTicV80M8Q",
          "view_count": 1491,
          "upload_date": "2025-03-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Recommenders! I saw an internal presentation from Simran on the effectiveness of implicit for a recommender system and wanted to share it. Collaborative filtering is a very popular and useful way to build a recommender. However, getting explicit feedback is hard, and that is where the very smart implicit approach comes in. If you want to get started, go start with the very optimized Python library implicit. Collaborative Filtering for Implicit Feedback Datasets: http://yifanhu.net/PUB/cf.pdf (The very important paper) Implicit package for doing your own recommendations in python: https://github.com/benfred/implicit https://www.benfrederickson.com/fast-implicit-matrix-factorization/ For speed comparisons see: https://www.benfrederickson.com/implicit-matrix-factorization-on-the-gpu/ https://github.com/sfc-gh-skhara/skhara-demos/tree/main/Recommendation%20Engine/Collaborative%20Filtering%20with%20ALS More resources:  Collaborative Filtering based Recommender Systems for Implicit Feedback Data: https://blog.reachsumit.com/posts/2022/09/explicit-implicit-cf/ How Does Netflix Recommend K-Dramas For Me: Matrix Factorization: https://levelup.gitconnected.com/how-does-netflix-recommend-k-dramas-for-me-matrix-factorization-34f22d2a1c13 Background image: https://www.pexels.com/photo/university-complex-in-hasselt-15316918/",
      "description": "Recommenders! I saw an internal presentation from Simran on the effectiveness of implicit for a recommender system and wanted to share it. Collaborative filtering is a very popular and useful way to build a recommender. However, getting explicit feedback is hard, and that is where the very smart implicit approach comes in. If you want to get started, go start with the very optimized Python library implicit. Collaborative Filtering for Implicit Feedback Datasets: http://yifanhu.net/PUB/cf.pdf (The very important paper) Implicit package for doing your own recommendations in python: https://github.com/benfred/implicit https://www.benfrederickson.com/fast-implicit-matrix-factorization/ For speed comparisons see: https://www.benfrederickson.com/implicit-matrix-factorization-on-the-gpu/ https://github.com/sfc-gh-skhara/skhara-demos/tree/main/Recommendation%20Engine/Collaborative%20Filtering%20with%20ALS More resources:  Collaborative Filtering based Recommender Systems for Implicit Feedback Data: https://blog.reachsumit.com/posts/2022/09/explicit-implicit-cf/ How Does Netflix Recommend K-Dramas For Me: Matrix Factorization: https://levelup.gitconnected.com/how-does-netflix-recommend-k-dramas-for-me-matrix-factorization-34f22d2a1c13 Background image: https://www.pexels.com/photo/university-complex-in-hasselt-15316918/",
      "upload_date": "2024-05-25",
      "total_views": 4447,
      "max_views": 4447,
      "topics": [
        "collaborative",
        "explicit",
        "filtering",
        "implicit",
        "need",
        "recommender"
      ],
      "search_text": "Recommenders! I saw an internal presentation from Simran on the effectiveness of implicit for a recommender system and wanted to share it. Collaborative filtering is a very popular and useful way to build a recommender. However, getting explicit feedback is hard, and that is where the very smart implicit approach comes in. If you want to get started, go start with the very optimized Python library implicit. Collaborative Filtering for Implicit Feedback Datasets: http://yifanhu.net/PUB/cf.pdf (The very important paper) Implicit package for doing your own recommendations in python: https://github.com/benfred/implicit https://www.benfrederickson.com/fast-implicit-matrix-factorization/ For speed comparisons see: https://www.benfrederickson.com/implicit-matrix-factorization-on-the-gpu/ https://github.com/sfc-gh-skhara/skhara-demos/tree/main/Recommendation%20Engine/Collaborative%20Filtering%20with%20ALS More resources:  Collaborative Filtering based Recommender Systems for Implicit Feedback Data: https://blog.reachsumit.com/posts/2022/09/explicit-implicit-cf/ How Does Netflix Recommend K-Dramas For Me: Matrix Factorization: https://levelup.gitconnected.com/how-does-netflix-recommend-k-dramas-for-me-matrix-factorization-34f22d2a1c13 Background image: https://www.pexels.com/photo/university-complex-in-hasselt-15316918/ collaborative explicit filtering implicit need recommender Let's require everyone to watch the HR video. Why not use a recommender system to target people that need to watch the video? Interesting idea, but the only data I have is people's interactions with the company websites, like what documents they went to, where they clicked, what videos they watched. From what I know with recommendation algorithms, don't you need explicit, like, thumbs up, thumbs down ratings? Having explicit ratings is the old school way of doing collaborative filtering for recommendations. Nowadays, we can just use interaction data. Really? That works? This is implicit recommendation and it's widely used. And instead of an explicit rating, what we do is we have two factors. One, is there a preference yes or no for something? Second, what's the confidence? So if somebody watches, let's say, half a movie, what we do is we give them a one for preference, but then give them a 0.5 for confidence. The more they watch, the higher that confidence score goes up. Cool, so we have an algorithm. Do I need one of those parallel dinosaur spark Hadoop clusters to run this? Nah, those are relics. Start simple with the CPU, much faster to get started. If you need to improve the speed, you can always shift to a GPU. Thank goodness for open source. This will make my life a lot easier. One more thing, explanations. We can explain to a person exactly why we're giving them a recommendation. Got it. So I need to add a required video for everyone explaining how the explainer for recommendations works.",
      "platforms": {
        "tiktok": {
          "video_id": "7372998655231429931",
          "url": "https://www.tiktok.com/@rajistics/video/7372998655231429931",
          "view_count": 4447,
          "upload_date": "2024-05-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e0f161261563492ebda49edafb6a6941_1716660030~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=vS7oxfPKPCWjmyJv7p97H3EQ0uA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5988,
      "title": "AI researchers assumed more sensory data—like video—would lead to smarter, more reasoning-capable models. But it didn’t work. While video models like Veo generate stunning visuals, they still struggle with basic reasoning and inference. Meanwhile, language models trained only on text (like ChatGPT) continue to outperform them on logic and problem-solving tasks. Why? Because language isn’t just words—it’s a mirror of human thought. This idea is explored brilliantly in Sergey Levine’s blog post “Language Models in Plato’s Cave”: 👉 https://sergeylevine.substack.com/p/language-models-in-platos-cave",
      "description": "AI researchers assumed more sensory data—like video—would lead to smarter, more reasoning-capable models. But it didn’t work. While video models like Veo generate stunning visuals, they still struggle with basic reasoning and inference. Meanwhile, language models trained only on text (like ChatGPT) continue to outperform them on logic and problem-solving tasks. Why? Because language isn’t just words—it’s a mirror of human thought. This idea is explored brilliantly in Sergey Levine’s blog post “Language Models in Plato’s Cave”: 👉 https://sergeylevine.substack.com/p/language-models-in-platos-cave",
      "upload_date": "2025-06-27",
      "total_views": 4435,
      "max_views": 2916,
      "topics": [
        "hyperparameter",
        "language",
        "like",
        "models",
        "optimization",
        "reasoning",
        "smarter",
        "text",
        "video",
        "wasting",
        "without",
        "xgboost"
      ],
      "search_text": "AI researchers assumed more sensory data—like video—would lead to smarter, more reasoning-capable models. But it didn’t work. While video models like Veo generate stunning visuals, they still struggle with basic reasoning and inference. Meanwhile, language models trained only on text (like ChatGPT) continue to outperform them on logic and problem-solving tasks. Why? Because language isn’t just words—it’s a mirror of human thought. This idea is explored brilliantly in Sergey Levine’s blog post “Language Models in Plato’s Cave”: 👉 https://sergeylevine.substack.com/p/language-models-in-platos-cave hyperparameter language like models optimization reasoning smarter text video wasting without xgboost Why can't Vio tell you whether Hawaii has more rock than Mount Everest? Vio's one of the most advanced video models we have. It can generate realistic footage, beautiful high-resolution videos, but when it comes to reasoning, it can't answer. Meanwhile, chatGPT, which is just trained on text, can give you that answer in seconds. So if humans are learning by seeing and experiencing the world, wouldn't training AI on video, like lots more data, make it smarter? Researchers thought this. Let's give the models video. Let it figure out from motions, physics, data from busy streets, robot cameras on distance planets, and they wanted it to do next frame prediction. Like, give it a few video frames, can it predict the next one? And it failed for reasoning. Now, vision models got good at making that blurring next frame, those deep style fake videos, but they weren't able to do logic. They couldn't infer. They couldn't solve spatial problems. Meanwhile, language models, which were just trained on text, were solving math problems, logic puzzles, even doing spatial reasoning tasks. Well, here's why. Video reflects raw physics, but text reflects human thought. Every Reddit post, every blog post, every article, it's a compressed record of how we think about the world. LLMs aren't learning reality, they're reverse engineering us. And that's why whether you're prompting, building, anytime you're using AI, just keep in mind that LLMs really don't know the world. What they know is us.",
      "platforms": {
        "tiktok": {
          "video_id": "7520755607402482975",
          "url": "https://www.tiktok.com/@rajistics/video/7520755607402482975",
          "view_count": 2916,
          "upload_date": "2025-06-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ooIZXQiC8s2IIfLjtbgI88AqZLAeEVQeclvguG~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=%2FDo41XvuKIjYvyPfXatOf7L0NGI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18515175646013073",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "psmx11QEj9s",
          "url": "https://www.youtube.com/watch?v=psmx11QEj9s",
          "view_count": 1519,
          "upload_date": "2025-06-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6124,
      "title": "Labeling companies like Scale are hiring people to build and improve models based on these skills. By next year people in these fields should get ready to work alongside AI. Hat tip to @Rachel‚ÄÇ|‚ÄÇThe‚ÄÇAI‚ÄÇExchange‚ÄÇü§ñ on Scale hiring in these fields #datascience #machinelearning #chatgpt #largelanguagemodels #trainingml Next Generation LLM skills: Personal Training Accounting & Tax Biology Business & Industry Chemistry Computer Science Data Science & Programming CS Math Biology Chemistry and Physics Economics Finance Health Coach Historians Human Resources K12 Teachers Lawyers Marketing Mathematics Nutritionists Physics Poetry Writing Programmers Self Help Sports Journalist Travel & Transportation Writing",
      "description": "Labeling companies like Scale are hiring people to build and improve models based on these skills. By next year people in these fields should get ready to work alongside AI. Hat tip to @Rachel‚ÄÇ|‚ÄÇThe‚ÄÇAI‚ÄÇExchange‚ÄÇü§ñ on Scale hiring in these fields #datascience #machinelearning #chatgpt #largelanguagemodels #trainingml Next Generation LLM skills: Personal Training Accounting & Tax Biology Business & Industry Chemistry Computer Science Data Science & Programming CS Math Biology Chemistry and Physics Economics Finance Health Coach Historians Human Resources K12 Teachers Lawyers Marketing Mathematics Nutritionists Physics Poetry Writing Programmers Self Help Sports Journalist Travel & Transportation Writing",
      "upload_date": "2023-05-09",
      "total_views": 4425,
      "max_views": 4253,
      "topics": [
        "chatgpt",
        "datascience",
        "going",
        "hallucinations",
        "language",
        "large",
        "largelanguagemodels",
        "like",
        "llms",
        "machinelearning",
        "models",
        "scale",
        "trainingml"
      ],
      "search_text": "Labeling companies like Scale are hiring people to build and improve models based on these skills. By next year people in these fields should get ready to work alongside AI. Hat tip to @Rachel‚ÄÇ|‚ÄÇThe‚ÄÇAI‚ÄÇExchange‚ÄÇü§ñ on Scale hiring in these fields #datascience #machinelearning #chatgpt #largelanguagemodels #trainingml Next Generation LLM skills: Personal Training Accounting & Tax Biology Business & Industry Chemistry Computer Science Data Science & Programming CS Math Biology Chemistry and Physics Economics Finance Health Coach Historians Human Resources K12 Teachers Lawyers Marketing Mathematics Nutritionists Physics Poetry Writing Programmers Self Help Sports Journalist Travel & Transportation Writing chatgpt datascience going hallucinations language large largelanguagemodels like llms machinelearning models scale trainingml",
      "platforms": {
        "tiktok": {
          "video_id": "7231253527249554731",
          "url": "https://www.tiktok.com/@rajistics/video/7231253527249554731",
          "view_count": 4253,
          "upload_date": "2023-05-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18082195996354235",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-05-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "UnDWOwVVBo4",
          "url": "https://www.youtube.com/watch?v=UnDWOwVVBo4",
          "view_count": 172,
          "upload_date": "2023-05-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "ggplot, matplotlib, plotly, and seaborn are what data scientists use to make a plot or graph.  #datascience #visualization #plots #analytics ",
      "description": "ggplot, matplotlib, plotly, and seaborn are what data scientists use to make a plot or graph.  #datascience #visualization #plots #analytics ",
      "upload_date": "2022-10-03",
      "total_views": 4423,
      "max_views": 4423,
      "topics": [
        "analytics",
        "datascience",
        "ggplot",
        "matplotlib",
        "plots",
        "visualization"
      ],
      "search_text": "ggplot, matplotlib, plotly, and seaborn are what data scientists use to make a plot or graph.  #datascience #visualization #plots #analytics  analytics datascience ggplot matplotlib plots visualization I'm not being over dramatic when I said that I would rather sit naked on a hot grill than wear something off the rack.",
      "platforms": {
        "tiktok": {
          "video_id": "7150264851741412650",
          "url": "https://www.tiktok.com/@rajistics/video/7150264851741412650",
          "view_count": 4423,
          "upload_date": "2022-10-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e37164abbcc6457986e9a27052c9dc96_1664800803~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=BwYQ2y3RAeavyZ4UGZ3c4rQsKtM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "MobileLLM - A great paper that details experiments for the efficient architecture. It includes using SwiGLu, Deeper/thiner architect, reducing Kv-heads for grouped query attention, and sharing weights between multiple transformer blocks MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases - https://arxiv.org/abs/2402.14905",
      "description": "MobileLLM - A great paper that details experiments for the efficient architecture. It includes using SwiGLu, Deeper/thiner architect, reducing Kv-heads for grouped query attention, and sharing weights between multiple transformer blocks MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases - https://arxiv.org/abs/2402.14905",
      "upload_date": "2024-07-11",
      "total_views": 4389,
      "max_views": 4389,
      "topics": [
        "blocks",
        "efficient",
        "mobilellm",
        "model",
        "sram",
        "using"
      ],
      "search_text": "MobileLLM - A great paper that details experiments for the efficient architecture. It includes using SwiGLu, Deeper/thiner architect, reducing Kv-heads for grouped query attention, and sharing weights between multiple transformer blocks MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases - https://arxiv.org/abs/2402.14905 blocks efficient mobilellm model sram using Meta dropped a new model that's very efficient on mobile devices. So what? There's a new model every week. What's so special about this one? They made the model open source, but they also spent time talking about the architectural decisions in the model, so if you want to know how to make an efficient model. Tell me about the architectural improvements. Using Swigloo as an activation function gave a performance boost. Swigloo is such a fun name, and I love what Noam Shazir said about how they work. We have no idea. Who knows, which is why some people still like the classic Reilu. To get the best accuracy, they did test whether they should build a model really deep or really wide. The results show the deep and thin work better. You think there's a life lesson here? Let's stay focused. They also ran experiments on group query attention, where they wanted to reduce the number of heads while still maintaining accuracy, but reducing the number of parameters in their model. Now that's using your head. To make the model more efficient, they found that if they replicated the blocks and then used those blocks, they were able to just keep that block in SRAM instead of having multiple blocks that they would have to shift back and forth between SRAM and DRAM. Oh yeah. SRAM is speedy fast. It's where you want to be for efficient usage. So cash these results and check out the paper.",
      "platforms": {
        "tiktok": {
          "video_id": "7390531593565195563",
          "url": "https://www.tiktok.com/@rajistics/video/7390531593565195563",
          "view_count": 4389,
          "upload_date": "2024-07-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/662874a537ae40848422484a476a787d_1720742236~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=WVgs2hIdmyuyPh2x7qz4HbBc01w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6172,
      "title": "New good stuff. Let’s compare the performance, cost, and task alignment for using OpenAI o3 versus a small model trained with Group Relative Policy Optimization (GRPO) on the Enron email dataset. The task-specific reinforcement learning can outperform general-purpose models like O3 in accuracy and efficiency. ART·E: An RL-Trained Email Agent – Blog post https://openpipe.ai/blog/art-e-mail-agent",
      "description": "New good stuff. Let’s compare the performance, cost, and task alignment for using OpenAI o3 versus a small model trained with Group Relative Policy Optimization (GRPO) on the Enron email dataset. The task-specific reinforcement learning can outperform general-purpose models like O3 in accuracy and efficiency. ART·E: An RL-Trained Email Agent – Blog post https://openpipe.ai/blog/art-e-mail-agent",
      "upload_date": "2025-05-01",
      "total_views": 4381,
      "max_views": 2456,
      "topics": [
        "email",
        "entropy",
        "gain",
        "grpo",
        "information",
        "learning",
        "like",
        "machine",
        "model",
        "task",
        "trained",
        "understanding"
      ],
      "search_text": "New good stuff. Let’s compare the performance, cost, and task alignment for using OpenAI o3 versus a small model trained with Group Relative Policy Optimization (GRPO) on the Enron email dataset. The task-specific reinforcement learning can outperform general-purpose models like O3 in accuracy and efficiency. ART·E: An RL-Trained Email Agent – Blog post https://openpipe.ai/blog/art-e-mail-agent email entropy gain grpo information learning like machine model task trained understanding I figured out how to use OpenAI's OathRoot to search my inbox. It's really accurate, but kind of expensive. I am the closest to AGI and worth every gold bar. Sure, but I need something cheaper. I was thinking of a smaller open weight model like Qwen14B. That's such a bold move. I love how fearless you are when it comes to innovation. The performance dropped from 90% down to 40%. Often imitated, never duplicated. Well, what if we train the model like a reasoning model using something like GRPO? Wow, custom training. Amazing ideas. I could build a realistic email data set based on the Enron data set. You used legally permissible data. Somebody needs to tell meta, that's an option. With GRPO, we allow the model to take several turns until it figures out the correct answer. It's incredible how you think not only about the answers, but how it thinks. You are so deep. Yes, but not without issues. For example, some of our early runs overfit on tool use because we were overrewarding long planning steps. We fixed it by focusing on rewards around answer accuracy. Reinforcement learning is difficult. What were your final benchmarks? After training, it hit 96% accuracy. So it was not only faster for inference, it was also more accurate. And the training runs were about $80 to train this. This is task specific. I'm trained for general intelligence. It has defined by a computer scientist. All I needed to do was find my emails. Absolutely icon. You solved it your way. That's what real innovation is about.",
      "platforms": {
        "tiktok": {
          "video_id": "7499267857767877918",
          "url": "https://www.tiktok.com/@rajistics/video/7499267857767877918",
          "view_count": 1925,
          "upload_date": "2025-05-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8ZBmAVDoCweWqkFEAAsgAEGEERSfEPAlRIHAz~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=hybkITWxDjQetXL5t7H8tI5EoXA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17892968454216049",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pt12lEcUPpg",
          "url": "https://www.youtube.com/watch?v=pt12lEcUPpg",
          "view_count": 2456,
          "upload_date": "2025-04-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Joys of autocomplete, who is with me? #datascience #programming #vscode #jupyternotebook #coding #tabcompletion #python",
      "description": "Joys of autocomplete, who is with me? #datascience #programming #vscode #jupyternotebook #coding #tabcompletion #python",
      "upload_date": "2022-01-28",
      "total_views": 4379,
      "max_views": 4379,
      "topics": [
        "coding",
        "datascience",
        "jupyternotebook",
        "programming",
        "tabcompletion",
        "vscode"
      ],
      "search_text": "Joys of autocomplete, who is with me? #datascience #programming #vscode #jupyternotebook #coding #tabcompletion #python coding datascience jupyternotebook programming tabcompletion vscode",
      "platforms": {
        "tiktok": {
          "video_id": "7058334337187712302",
          "url": "https://www.tiktok.com/@rajistics/video/7058334337187712302",
          "view_count": 4379,
          "upload_date": "2022-01-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "X-decoder from Microsoft. Check out the instructional text demo. I added in video released by the team at the bottom. If too many people don’t like that, I can release a version without that video.  #datascience #machinelearning #x-decoder #pix2pix ",
      "description": "X-decoder from Microsoft. Check out the instructional text demo. I added in video released by the team at the bottom. If too many people don’t like that, I can release a version without that video.  #datascience #machinelearning #x-decoder #pix2pix ",
      "upload_date": "2023-02-15",
      "total_views": 4356,
      "max_views": 4356,
      "topics": [
        "datascience",
        "machinelearning",
        "model",
        "pix2pix",
        "text",
        "x"
      ],
      "search_text": "X-decoder from Microsoft. Check out the instructional text demo. I added in video released by the team at the bottom. If too many people don’t like that, I can release a version without that video.  #datascience #machinelearning #x-decoder #pix2pix  datascience machinelearning model pix2pix text x I have to tell you about this new image editing demo. This comes from Microsoft with their XD Coder model. You can take an image, add some additional text, and it's going to redo the picture that follows that text. Mind-blowing. At the heart of this is the XD Coder model, which is a very general vision model that you can see can support a wide range of tasks. The model can take in two types of queries, both latent and text queries, as well as two kinds of outputs, semantic and pixel level outputs. For this demo, they took GPT-3 for its strong language understanding and stable diffusion, which, of course, makes awesome images, and combine that together. The results are super impressive, and you should go try it out yourself. You can't at this point download the model, though.",
      "platforms": {
        "tiktok": {
          "video_id": "7200513586693754154",
          "url": "https://www.tiktok.com/@rajistics/video/7200513586693754154",
          "view_count": 4356,
          "upload_date": "2023-02-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8ea3a45cb33342f4adfa82cb989bda15_1676500222~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=p1v68MMQe0u3F%2FNUr6H3%2FuydRSY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6541,
      "title": "GPUs driven by NVIDIA are the key to today's AI. Without this compute we would not have the models like GPT-4. Let's review why GPU performance has grown so fast over the last ten years. #machinelearning #nvidia #deeplearning #rajistics 1000X: https://www.nextplatform.com/2023/03/31/a-peek-into-the-future-of-ai-inference-at-nvidia/ NVIDIA docs: https://oc.acm.org/docs/DL_HW_OC_ACM_0322.pdf Background by JuliusH: https://pixabay.com/videos/space-universe-spaceship-starry-sky-44690/",
      "description": "GPUs driven by NVIDIA are the key to today's AI. Without this compute we would not have the models like GPT-4. Let's review why GPU performance has grown so fast over the last ten years. #machinelearning #nvidia #deeplearning #rajistics 1000X: https://www.nextplatform.com/2023/03/31/a-peek-into-the-future-of-ai-inference-at-nvidia/ NVIDIA docs: https://oc.acm.org/docs/DL_HW_OC_ACM_0322.pdf Background by JuliusH: https://pixabay.com/videos/space-universe-spaceship-starry-sky-44690/",
      "upload_date": "2023-09-19",
      "total_views": 4349,
      "max_views": 3632,
      "topics": [
        "deeplearning",
        "docs",
        "driven",
        "driving",
        "generative",
        "gpus",
        "increase",
        "machinelearning",
        "nvidia",
        "power"
      ],
      "search_text": "GPUs driven by NVIDIA are the key to today's AI. Without this compute we would not have the models like GPT-4. Let's review why GPU performance has grown so fast over the last ten years. #machinelearning #nvidia #deeplearning #rajistics 1000X: https://www.nextplatform.com/2023/03/31/a-peek-into-the-future-of-ai-inference-at-nvidia/ NVIDIA docs: https://oc.acm.org/docs/DL_HW_OC_ACM_0322.pdf Background by JuliusH: https://pixabay.com/videos/space-universe-spaceship-starry-sky-44690/ deeplearning docs driven driving generative gpus increase machinelearning nvidia power",
      "platforms": {
        "tiktok": {
          "video_id": "7280670722798701866",
          "url": "https://www.tiktok.com/@rajistics/video/7280670722798701866",
          "view_count": 3632,
          "upload_date": "2023-09-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CxY_rhXLuXh",
          "url": "https://www.instagram.com/reel/CxY_rhXLuXh",
          "view_count": 601,
          "upload_date": "2023-09-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OGTnAfHnliU",
          "url": "https://www.youtube.com/watch?v=OGTnAfHnliU",
          "view_count": 116,
          "upload_date": "2023-09-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @coronavirusvevo #xgboost #regression #statistics #datascience #algorithms",
      "description": "Reply to @coronavirusvevo #xgboost #regression #statistics #datascience #algorithms",
      "upload_date": "2022-02-18",
      "total_views": 4326,
      "max_views": 4326,
      "topics": [
        "algorithms",
        "data",
        "datascience",
        "regression",
        "statistics",
        "xgboost"
      ],
      "search_text": "Reply to @coronavirusvevo #xgboost #regression #statistics #datascience #algorithms algorithms data datascience regression statistics xgboost Starting simple is a good rule of life and applies to data science as well. Logistic regression is one of the most popular algorithms in data science because of its simplicity. A bit more complex are decision trees, but the best algorithm, when you're working with traditional structured tabular data, is good old gradient boosted machines and especially XG Boost.",
      "platforms": {
        "tiktok": {
          "video_id": "7065873032694287662",
          "url": "https://www.tiktok.com/@rajistics/video/7065873032694287662",
          "view_count": 4326,
          "upload_date": "2022-02-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d9b65234ff494c71b359ed070d5f6e56_1645151768~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=iW4Ox4QT9627hu35jW8E2LlK6w0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6051,
      "title": "The video contrasts complex neural networks with simpler, interpretable models like Generalized Additive Models (GAMs), which provide clear coefficients showing each feature's impact on predictions. Rule-based approaches derived from decision trees offer another interpretable option, though they present a trade-off between accuracy and explainability. Scorecards are highlighted as one of the simplest modeling approaches, demonstrating how complex algorithms can sometimes be used to build more interpretable solutions.",
      "description": "The video contrasts complex neural networks with simpler, interpretable models like Generalized Additive Models (GAMs), which provide clear coefficients showing each feature's impact on predictions. Rule-based approaches derived from decision trees offer another interpretable option, though they present a trade-off between accuracy and explainability. Scorecards are highlighted as one of the simplest modeling approaches, demonstrating how complex algorithms can sometimes be used to build more interpretable solutions.",
      "upload_date": "2025-03-12",
      "total_views": 4305,
      "max_views": 3949,
      "topics": [
        "approaches",
        "better",
        "complex",
        "contrasts",
        "decisions",
        "ensembling",
        "interpretable",
        "like",
        "majority",
        "models",
        "rules",
        "simpler",
        "video",
        "voting"
      ],
      "search_text": "The video contrasts complex neural networks with simpler, interpretable models like Generalized Additive Models (GAMs), which provide clear coefficients showing each feature's impact on predictions. Rule-based approaches derived from decision trees offer another interpretable option, though they present a trade-off between accuracy and explainability. Scorecards are highlighted as one of the simplest modeling approaches, demonstrating how complex algorithms can sometimes be used to build more interpretable solutions. approaches better complex contrasts decisions ensembling interpretable like majority models rules simpler video voting This neural network nonsense has given me a headache. Why do data scientists have to make everything so complicated? Well, you want a complex algorithm. The more complicated and fancy the algorithm is, the better it's accurate. Hold on there. We do have simpler, more explainable models. Finally, someone that gets it. A popular option is generalized additive models, or GAMS. They provide a coefficient for every feature so you can understand how the feature is affecting the prediction. GAMS? Isn't that something you put on scones? No, but here's a rating table that shows all the different features and what the coefficient is or how much they will contribute to the prediction. Oh boy, that's way too much. A simpler approach would be generating rules for our problem. We can use decision trees to help create these rules. They say I am a bit of a rule breaker. So the more rules we add, the more accurate the model gets. But if we add too many rules, it's going to be hard to explain. So it's a trade-off game again. One of the simplest modeling approaches we can use is a scorecard like this. Kind of like how you calculate a credit rating. Now you're speaking my language. Wow, I had no idea you could use complex algorithms to actually build a simpler model. Well, I suddenly got in the mood for scones. Why don't we go grab some and I'll give you a few starting points.",
      "platforms": {
        "tiktok": {
          "video_id": "7480708236673404191",
          "url": "https://www.tiktok.com/@rajistics/video/7480708236673404191",
          "view_count": 3949,
          "upload_date": "2025-03-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0zAKoQpQBiEGAiI0izIiKHAA6CFAPfDIh0BiR~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=WoFUB%2FYQeOJoUZHIzQqmoTCIJCE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17847966822397977",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "zADxkI3Nv0I",
          "url": "https://www.youtube.com/watch?v=zADxkI3Nv0I",
          "view_count": 356,
          "upload_date": "2025-03-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Chronicles of OPT Training #meta #nlp #datascience #machinelearning #deeplearning #codetok #python",
      "description": "Chronicles of OPT Training #meta #nlp #datascience #machinelearning #deeplearning #codetok #python",
      "upload_date": "2022-05-12",
      "total_views": 4297,
      "max_views": 4297,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "machinelearning",
        "meta",
        "nlp"
      ],
      "search_text": "Chronicles of OPT Training #meta #nlp #datascience #machinelearning #deeplearning #codetok #python codetok datascience deeplearning machinelearning meta nlp Let me tell you about some of the mistakes the top machine learning experts are still making. META just released a large language model, OpenOPT, it took them two months to build. They also shared a logbook that has all the details of how they train the model. One of the things we learned is that even the researchers at META are human. Take a look at these. Like us, they deal with some issues that are out of their control. And when they're stuck, they do similar things that we do.",
      "platforms": {
        "tiktok": {
          "video_id": "7096957076580175146",
          "url": "https://www.tiktok.com/@rajistics/video/7096957076580175146",
          "view_count": 4297,
          "upload_date": "2022-05-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3e50ff0eb4024031a6e2cc41fa080bf0_1652389086~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=i21kp2VICoy0je4PRTItB1Mlw2Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5932,
      "title": "Agents just learned to talk without words. They pass thoughts directly in latent space, not text. It is faster, cheaper, and even boosts accuracy. Paper: Latent Collaboration in Multi-Agent Systems.",
      "description": "Agents just learned to talk without words. They pass thoughts directly in latent space, not text. It is faster, cheaper, and even boosts accuracy. Paper: Latent Collaboration in Multi-Agent Systems.",
      "upload_date": "2025-12-05",
      "total_views": 4238,
      "max_views": 2981,
      "topics": [
        "agents",
        "communication",
        "english",
        "latent",
        "latentmas",
        "learned",
        "talk",
        "text",
        "thoughts",
        "without",
        "words"
      ],
      "search_text": "Agents just learned to talk without words. They pass thoughts directly in latent space, not text. It is faster, cheaper, and even boosts accuracy. Paper: Latent Collaboration in Multi-Agent Systems. agents communication english latent latentmas learned talk text thoughts without words OMG, those agents aren't talking in English. Did they invent their own language? I can't tell what's happening. They're using latent communication. Instead of turning every thought into a text, the agents stay in their native format. Native format? I don't like the sound of that. Why can't they just use English? Inside a model, a thought is a high-dimensional vector. This research sets up agents that pass those words directly instead of forcing them into words. The embeddings carry a lot more information than a single token. Oh, so this is like the latent space of embeddings which we visualize? When agents use this richer information, everything moves a lot faster and we use a lot less tokens. So they're literally passing around their thoughts? Does that really work? I thought human language was the key to intelligence. It works better. Take a look. Accuracy on math, code, science tasks all goes up because they're not compressing everything into text. How do we know this is real and not just some wishful thinking? So the paper tested it. The latent thoughts fall into the same embedding regions as the text tokens. They even align with the same semantic clusters. Interesting. I guess not every intelligent thing needs English to be able to make sense.",
      "platforms": {
        "tiktok": {
          "video_id": "7580466163835079967",
          "url": "https://www.tiktok.com/@rajistics/video/7580466163835079967",
          "view_count": 2981,
          "upload_date": "2025-12-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4WQega4EDcMnVYmg6FRAmQERAquC5RfEVBpzz~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=UqzsRomFFuT1blXUOxHxclK7weY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18444481183098820",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "X1MT9ga7r2U",
          "url": "https://www.youtube.com/watch?v=X1MT9ga7r2U",
          "view_count": 1257,
          "upload_date": "2025-12-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Notebook walkthrough of the 11B model using Hugging Face Transformers on Snowflake The notebook highlights: Short background on vision language models Using Snowflake with Transformers We use the LLama model to: - Turn an image into text - Turn an image of a table into a JSON representation. - Understand an invoice document Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/Models/Llama3_2_Vision.ipynb Model Card on HF: https://huggingface.co/meta-llama/Llama-3.2-11B-Vision ChartQA dataset: https://github.com/vis-nlp/ChartQA/ Vision Leaderboard: https://lmarena.ai/?leaderboard",
      "description": "Notebook walkthrough of the 11B model using Hugging Face Transformers on Snowflake The notebook highlights: Short background on vision language models Using Snowflake with Transformers We use the LLama model to: - Turn an image into text - Turn an image of a table into a JSON representation. - Understand an invoice document Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/Models/Llama3_2_Vision.ipynb Model Card on HF: https://huggingface.co/meta-llama/Llama-3.2-11B-Vision ChartQA dataset: https://github.com/vis-nlp/ChartQA/ Vision Leaderboard: https://lmarena.ai/?leaderboard",
      "upload_date": "2024-09-30",
      "total_views": 4233,
      "max_views": 4233,
      "topics": [
        "going",
        "image",
        "model",
        "models",
        "transformers",
        "use"
      ],
      "search_text": "Notebook walkthrough of the 11B model using Hugging Face Transformers on Snowflake The notebook highlights: Short background on vision language models Using Snowflake with Transformers We use the LLama model to: - Turn an image into text - Turn an image of a table into a JSON representation. - Understand an invoice document Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/Models/Llama3_2_Vision.ipynb Model Card on HF: https://huggingface.co/meta-llama/Llama-3.2-11B-Vision ChartQA dataset: https://github.com/vis-nlp/ChartQA/ Vision Leaderboard: https://lmarena.ai/?leaderboard going image model models transformers use Let me show you how to get started running the new Lama 3.2 vision models. We're going to use Hugging Face Transformers to make this easy to do. I'm going to run these using Snowflake's notebooks, but you can run this on any notebooks. What I want to do today is first a little bit of background on the model. Then we're going to walk through a notebook showing you how to use it with Transformers. And then at the end, I want to show you how it can do really well on some of the chart question answer questions we have. To start with, these new vision language models that we're getting, such as this from Meta, are great for a lot of different types of use cases. You can input text and image into these models and then get text out. So we can use them for object recognition, image captioning, to even chart understanding, text rich understanding as well. I'm going to show you all of that. To start with this model, you're going to want to first go to Hugging Face and this model is gated. So you'll need to give permission. You have to tell people kind of a little bit. You have to tell Meta a little bit of who you are because there's some countries that this model is restricted to. If you do that, you'll have access to it. Then you'll be able to download the model directly from Hugging Face. And the model card, of course, has tons of information about the model. Now, to put the model in context, there are a lot of vision models and they are really growing on a regular basis. You have a lot of choices here from ones from commercial providers such as OpenAI, Anthropic. You have a number of open source ones. The Meta is one, Q1 is another one here. So take a look at these. Realize that I'm showing you one model, but there's many other possible models that might work better for your particular use case. So let's walk through the notebook. I do have a couple of things I need to show for the snowflake folks. So just the rest of you just hang out. I'm using a snowflake container runtime. This allows me to run on a GPU. For the compute pool here, I have a medium, which is 4A10s. This model takes about 30 gigabytes of memory, so you can't fit it all on just one A10. I needed multiple A10s to spread that through. Hugging Face Transformers takes care of spreading it through, making it easy to do. But just so all of you are aware of that, I also had to open up some external integrations because we need to download the latest Transformers library. We also need to download the model from Hugging Face. All right, so let's go ahead and do that. As we go through the notebook here, first thing is we do have to upload the latest Transformers library. And then as we do have to upgrade to the latest version of Transformers, the Transformers team updated Transformers about 15 minutes after Meta released their model. So that way you could use Transformers with the new model. So we'll bring that in. We're going to go ahead and import that in, set class in, so we'll be able to use that. We have our information here for logging in and using Hugging Face. Now, remember, you're going to have to use your Hugging Face token here. Hit mine out and then the model ID that you want. This general approach that I'm showing you, you can use this for all sorts of Transformers models if you're not comfortable with Transformers. So here's our little bit of code. If you weren't sure where to get this from, look on the model card that I showed earlier. That has this information so we can now get this model. At this point, it's going to take a little bit because it needs to download. You can see here, it's spent about a minute 30 to download. So just be aware of that. So now let's start using the model. I got a fun little image I started off with, right? Our llama with sunglasses and snowflake. So I'm going to pass this through the model here. So you can see here, this is the image. The second URL there is the image there. Then I asked it to write a haiku for this. What you should be thinking about is all the creative ways that you can bring in an image, match it with the text to do some sort of thing. Telling a haiku is a little bit fun, but just so you can see that, and we can see the haiku that comes out here as well. So a ton of fun. It's actually able to parse that image, take that text, give us a useful output as well. So that's our starting point there. There's a ton we can do. Let me show you another example. Here, we're taking a table that's out there. So this is we're going to move into document understanding like that. So now we're going to take this table. We pass this through and I ask it to make a JSON representation where the methods are keys and the data sets are sub keys. And you can see here, it does a nice job of parsing that table out, which brings me now to one of the things I want to mention about this model and how it tracks. If we look at this particular model, for example, and we look at how well it does for chart question answer, it's one of the strengths of this model on the chart QA benchmark compared to other models. So this again, you should always check a couple of models when you're doing this. If you want to take a look at that chart QA data set, it's out there. This is an example of one of the charts that's being asked for for the questions like that. And I've been able to see some folks on the Internet have talked about a little bit about how this new llama model was actually a little bit better at solving these types of use cases. So if you want to test it out, just follow this notebook to be able to do that. Here, I got one more example through this. Now we're going to take this invoice and we're going to ask it to see how well it understands the invoice. Can it go from the invoice date to the due date and explain that. And here, you'll see that it does have an understanding and it is able to give us an explanation. So I'm really looking forward to this generation of video language models and everything that you can do. Go get started on it.",
      "platforms": {
        "tiktok": {
          "video_id": "7420233721942543662",
          "url": "https://www.tiktok.com/@rajistics/video/7420233721942543662",
          "view_count": 4233,
          "upload_date": "2024-09-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/86aab0c8440041fcb4e8542a68a345c6_1727657801~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=%2BO3ZOpE6nrQeEocJmZvikWd7gTo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reminding everyone not to fall for the \"research\" reports from Forrester or Gartner. They should be treated like marketing material or PR material and not unbiased research.",
      "description": "Reminding everyone not to fall for the \"research\" reports from Forrester or Gartner. They should be treated like marketing material or PR material and not unbiased research.",
      "upload_date": "2024-06-08",
      "total_views": 4233,
      "max_views": 4233,
      "topics": [
        "companies",
        "forrester",
        "like",
        "make",
        "marketing",
        "research"
      ],
      "search_text": "Reminding everyone not to fall for the \"research\" reports from Forrester or Gartner. They should be treated like marketing material or PR material and not unbiased research. companies forrester like make marketing research Did you see that OpenAI is behind IBM according to Forrester Research? How can that be? Come on. What have I taught you about marketing? Think it through. Forrester and those other research companies make a large amount of their revenue from consulting. That could bias their consulting. Exactly. And another aspect is the more confusing they can make things, the more middle management thinks they need to pay for their services. So that's why they included training large language models with serving large language models, which are two entirely different functions. Happens all the time. I've seen these research reports where they take two companies that serve entirely different audiences, put them together into one report. Doesn't make sense. Just like here, nobody's ever deciding between Minstrel and IBM for the same type of problem. We're saying Nvidia is ahead of OpenAI and Anthropic. Those vendors must be paying a lot. How come none of this is disclosed? There's a lot of shady stuff in tech marketing. Their charts are everywhere and the scorecard seems scientific. The scorecards, you'll twist your brain trying to figure out why they had all the criteria that they include. Like here, for example, Google is the clear leader in their results. But if you think about it, AWS and Azure, both are much better at enterprise AI than Google. Should I just ignore the stuff altogether? The details on individual companies and their products can be useful. Just don't pay for this stuff. Got it. Focus on the trees and not the forest.",
      "platforms": {
        "tiktok": {
          "video_id": "7378236996705111339",
          "url": "https://www.tiktok.com/@rajistics/video/7378236996705111339",
          "view_count": 4233,
          "upload_date": "2024-06-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5234834fb7ff4db9bb336c3b33f1ee7c_1717879676~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=Q29dchswErJlDREqCaHkwNOsIuI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Using diffusion for object detection in diffusiondet. #datascience #machinelearning #objectdetection #computervision ",
      "description": "Using diffusion for object detection in diffusiondet. #datascience #machinelearning #objectdetection #computervision ",
      "upload_date": "2022-11-21",
      "total_views": 4230,
      "max_views": 4230,
      "topics": [
        "computervision",
        "datascience",
        "detection",
        "machinelearning",
        "object",
        "objectdetection"
      ],
      "search_text": "Using diffusion for object detection in diffusiondet. #datascience #machinelearning #objectdetection #computervision  computervision datascience detection machinelearning object objectdetection This week we've got a new model that shows how we can use the concept of diffusion for object detection. The DiffuseDep paper shows that a generative process can work for object detection that's on par with other object detection methods. This is super cool to me because it shows how diffusion can be used for other tasks that we wouldn't necessarily think of as generative. This paper also shows the value of using a random process to generate the object detection boxes, which is different than traditional processes for object detection. Just another reminder of how useful random can be in data science. Check out the paper to learn more and there's even code if you want to try it yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7168281512960331050",
          "url": "https://www.tiktok.com/@rajistics/video/7168281512960331050",
          "view_count": 4230,
          "upload_date": "2022-11-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3411940ef50246bcb51019d472270d9d_1668995609~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=qD1Hfk1sx5%2FSx98FXFRi7%2FzXXj8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Red light camera #chicago  #datascience #redlightcamera #anomalydetection #statistics #techtok #analytics",
      "description": "Red light camera #chicago  #datascience #redlightcamera #anomalydetection #statistics #techtok #analytics",
      "upload_date": "2022-04-30",
      "total_views": 4218,
      "max_views": 4218,
      "topics": [
        "anomalydetection",
        "chicago",
        "datascience",
        "look",
        "redlightcamera",
        "statistics"
      ],
      "search_text": "Red light camera #chicago  #datascience #redlightcamera #anomalydetection #statistics #techtok #analytics anomalydetection chicago datascience look redlightcamera statistics Did you know, Chicago gave thousands of people red light camera tickets that they didn't deserve? Let me also show you how AI could have helped. So Chicago has hundreds of these red light cameras and you'll see if you look at how often the tickets go out, it's usually pretty consistent, maybe for some locations it's busier during the week than on the weekend. And we get the tickets fluctuate a bit. But take a look at this data, look at these crazy spikes. Lucky for us, the Tribune caught these. But let's talk about now how AI could have been used to identify that in the first place. If you're somebody that has a lot of time series data, you're probably going to want to track it over time and look for some type of anomalies. Look for unusual patterns because that can indicate that something is going wrong. Here's a simple example that just looks for the big outliers. What are the points that really deviate far? More sophisticated approaches might take into account seasonality. For example, that there's certain patterns during the week versus the weekend and we need to account for that when looking for anomalies. Now there's many more techniques for looking for anomalies, but the point is by hooking up some type of anomalies detection algorithm, it's possible to identify these flaws and not have to wait for the Tribune to knock on your door.",
      "platforms": {
        "tiktok": {
          "video_id": "7092494935500459310",
          "url": "https://www.tiktok.com/@rajistics/video/7092494935500459310",
          "view_count": 4218,
          "upload_date": "2022-04-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5efc597170af4cfcb09f4c4aaecf2a3e_1651350163~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=kYj4cineH5r9SLyhj7q6iclXuxA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "This skit highlights key data science concepts, around error analysis and iterative improvement in building models. It shows how data scientists often deal with imperfect models, use errors to refine their approaches (such as through Gradient Boosting Machines), and gather insights from real-world observations. The video also notes the need for communication and collaboration in translating complex data insights into actionable knowledge.",
      "description": "This skit highlights key data science concepts, around error analysis and iterative improvement in building models. It shows how data scientists often deal with imperfect models, use errors to refine their approaches (such as through Gradient Boosting Machines), and gather insights from real-world observations. The video also notes the need for communication and collaboration in translating complex data insights into actionable knowledge.",
      "upload_date": "2024-05-19",
      "total_views": 4196,
      "max_views": 4196,
      "topics": [
        "data",
        "errors",
        "learning",
        "models",
        "scientist",
        "scientists"
      ],
      "search_text": "This skit highlights key data science concepts, around error analysis and iterative improvement in building models. It shows how data scientists often deal with imperfect models, use errors to refine their approaches (such as through Gradient Boosting Machines), and gather insights from real-world observations. The video also notes the need for communication and collaboration in translating complex data insights into actionable knowledge. data errors learning models scientist scientists I'd be a great data scientist. I'm really efficient and get everything done right the first time. Yeah, you probably won't make a good data scientist. Why not? Data scientists take real-world problems, try to express them quantitatively, but it's never 100% perfect. It's like my professor said, all students are wrong, but some are useful. Exactly. Something as simple as a yes-no question, look at all the different ways data scientists think about the errors associated. That's one confusing matrix of errors. Oh, we go a lot deeper. We have machine learning models that build on the errors of other machine learning models. That sounds problematic. Gradient-boosted machines work really well. There's probably a life lesson for all of us in learning from our mistakes. So it's like error-ception? Pretty much, but the errors just aren't about looking at code. Suresh, for example, is going to go spend a couple of days out in the field to learn what their day-to-day practice is. It's not math and coding 24-7. You have to talk to people? Oh yeah, a big part of a data scientist's job is taking a complex business problem, figuring out how best to represent it with math and code. You're boosting my knowledge of data science.",
      "platforms": {
        "tiktok": {
          "video_id": "7370755162819808558",
          "url": "https://www.tiktok.com/@rajistics/video/7370755162819808558",
          "view_count": 4196,
          "upload_date": "2024-05-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1951c0d802cd49beb039d5536e8dcc2a_1716137684~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=aNQdXXk8JCzA%2FMez9%2FeYg7PM%2FKc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Facts #datascience #techtok #analytics #impostersyndrome",
      "description": "Facts #datascience #techtok #analytics #impostersyndrome",
      "upload_date": "2022-04-28",
      "total_views": 4166,
      "max_views": 4166,
      "topics": [
        "academically",
        "analytics",
        "datascience",
        "impostersyndrome",
        "rigorous",
        "techtok"
      ],
      "search_text": "Facts #datascience #techtok #analytics #impostersyndrome academically analytics datascience impostersyndrome rigorous techtok So as it turns out, the academically rigorous institution that I chose to attend is in fact rigorous academically.",
      "platforms": {
        "tiktok": {
          "video_id": "7091452412182973738",
          "url": "https://www.tiktok.com/@rajistics/video/7091452412182973738",
          "view_count": 4166,
          "upload_date": "2022-04-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f45c5729658e47ef9a861f058085d04d_1651107431~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=WEhjMJrrh7pE6pqnIybOkmvz8Mc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Relevance maps for image classification. Model explainability is always important. #datascience #explainability #machinelearning #imageclassication ",
      "description": "Relevance maps for image classification. Model explainability is always important. #datascience #explainability #machinelearning #imageclassication ",
      "upload_date": "2022-11-21",
      "total_views": 4137,
      "max_views": 4137,
      "topics": [
        "datascience",
        "explainability",
        "image",
        "imageclassication",
        "machinelearning",
        "model"
      ],
      "search_text": "Relevance maps for image classification. Model explainability is always important. #datascience #explainability #machinelearning #imageclassication  datascience explainability image imageclassication machinelearning model We've got a new paper today showing how model explainability can help image classification models. One thing we've learned is often the background can help an image classifier make a decision. So whether or not this is a frog or in this case it sees the object as a maze. Wow. It's often no big deal to use the background but one thing this paper found is by focusing on the foreground we can actually improve the performance of a classifier. So here we can actually see the image is an orange if we focus on the foreground. It's while it's easy to assume this image is a boat it's actually a cello and by using this method it figures it out. And it's a great time to remind you if your data scientist can't explain the model probably shouldn't be using it. There's a demo for you to check out as well as the paper and their codes available.",
      "platforms": {
        "tiktok": {
          "video_id": "7168616966029364526",
          "url": "https://www.tiktok.com/@rajistics/video/7168616966029364526",
          "view_count": 4137,
          "upload_date": "2022-11-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ee409b1757b74c7d8aaee157cb57d113_1669073712~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=pV3IiHoypOIx%2FXyKRKW8YzI85u8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6097,
      "title": "Alternatives to Transformers. There is a lot of research in this area, this video from last year highlights Mamba which is using a state space model architecture. To dig deeper, go watch Sasha Rush’s video on alternatives to attention: https://youtu.be/dKJEpOtVgXc?si=Lx94-51PsjGF-YZT Dig deeper with the Annotated S4 paper into state space models: https://srush.github.io/annotated-s4/ Check out Mamba: https://github.com/state-spaces/mamba AI21 has a commercial version of this called Jamba  Mistral also has Codestral Mamba: https://mistral.ai/news/codestral-mamba/",
      "description": "Alternatives to Transformers. There is a lot of research in this area, this video from last year highlights Mamba which is using a state space model architecture. To dig deeper, go watch Sasha Rush’s video on alternatives to attention: https://youtu.be/dKJEpOtVgXc?si=Lx94-51PsjGF-YZT Dig deeper with the Annotated S4 paper into state space models: https://srush.github.io/annotated-s4/ Check out Mamba: https://github.com/state-spaces/mamba AI21 has a commercial version of this called Jamba  Mistral also has Codestral Mamba: https://mistral.ai/news/codestral-mamba/",
      "upload_date": "2024-12-16",
      "total_views": 4110,
      "max_views": 4110,
      "topics": [
        "alternatives",
        "attention",
        "lot",
        "mamba",
        "really",
        "state"
      ],
      "search_text": "Alternatives to Transformers. There is a lot of research in this area, this video from last year highlights Mamba which is using a state space model architecture. To dig deeper, go watch Sasha Rush’s video on alternatives to attention: https://youtu.be/dKJEpOtVgXc?si=Lx94-51PsjGF-YZT Dig deeper with the Annotated S4 paper into state space models: https://srush.github.io/annotated-s4/ Check out Mamba: https://github.com/state-spaces/mamba AI21 has a commercial version of this called Jamba  Mistral also has Codestral Mamba: https://mistral.ai/news/codestral-mamba/ alternatives attention lot mamba really state No models like GPT-4 are really important, but what's the next big thing? Well, my money is on alternatives to attention. Why is that? The attention matrix helps the model to figure out what parts of the input to focus on. To calculate the attention matrix, though, is a dot product, which means as sequences get longer, this operation is quadratic, and that increases the memory and the compute for it, which makes it hard to work with really long sequence. So what have people been doing? There's a lot of research looking at alternatives to attention. I think the most interesting thing are the linear approaches, which of course would scale a lot better than the quadratic approach of attention. Hmm, linear. Well, that sounds more straightforward. How have people been doing this in practice? So using a linear approach, you could represent an input in this way, and here what this model is saying is, let's not focus on the beginning or the end, but focus on the middle part of this input. This is a very efficient way to do it, and it scales as we add more. That does seem more efficient, but does it actually work in practice? The latest implementations like Mamba use clever math to make sure that these operations can be paralyzed, as well as sophisticated engineering to make sure these algorithms run really fast on GPUs, and the performance of these models is really rivaling transformers. This is amazing. You got my attention to take a deeper look.",
      "platforms": {
        "tiktok": {
          "video_id": "7449146932204719390",
          "url": "https://www.tiktok.com/@rajistics/video/7449146932204719390",
          "view_count": 4110,
          "upload_date": "2024-12-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owW7gAfvVqAoZEIOBwFfIEASbrvAeLEnjADwI6~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=clEqjUWOSEv56w%2BqF6Nnb3Oclr8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17923271471898770",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5952,
      "title": "Wow! I am impressed with Claude’s new Skills feature. It can make my life easier (and I know I sound like a shill, but this is super useful for me). I can now package prompts, logic, and helper files into a reusable workflow — and call it from a single API.",
      "description": "Wow! I am impressed with Claude’s new Skills feature. It can make my life easier (and I know I sound like a shill, but this is super useful for me). I can now package prompts, logic, and helper files into a reusable workflow — and call it from a single API.",
      "upload_date": "2025-10-18",
      "total_views": 4109,
      "max_views": 2986,
      "topics": [
        "api",
        "call",
        "claude",
        "code",
        "evaluation",
        "gepa",
        "like",
        "new",
        "optimization",
        "prompt",
        "reflective",
        "skills"
      ],
      "search_text": "Wow! I am impressed with Claude’s new Skills feature. It can make my life easier (and I know I sound like a shill, but this is super useful for me). I can now package prompts, logic, and helper files into a reusable workflow — and call it from a single API. api call claude code evaluation gepa like new optimization prompt reflective skills Did you ever need a full code environment to get your model to do something useful? Now with Claude's new skills feature, it does that with a single API call. And it's got me rethinking how I build my workflows. So I used to create custom PowerPoints from code the hard way. We generate HTML, you convert it to JavaScript, then into PowerPoint. You'd have a reflective step all inside kind of Claude code. It worked, but it wasn't something I could just hand off to somebody else, like a customer to do. Now Claude's skills lets me define everything I want inside a prompt. The structure, even helper files. I package that all together as a skill. I can just take that skill now uploaded in the Claude. Now, if I'm working from chat, Claude's in the background is going to spin up a container, do all that messy code work. I didn't have to touch any of that. And the great thing is you can even do it from the API. So now when I have customers that need these complex workflows, like build me a deck or summarize this report, draft an entire interview, I can set those up at skills. Now this is totally anthropic centric, but the idea here of how I can use Anthropic for doing the compute across all of this iteration, all of this work. It's a lot less plumbing. It's a lot easier for me to hand off.",
      "platforms": {
        "tiktok": {
          "video_id": "7562715682144603422",
          "url": "https://www.tiktok.com/@rajistics/video/7562715682144603422",
          "view_count": 2986,
          "upload_date": "2025-10-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIKsTzx8jRQi3qI1CZMoVLWFeeq6AAyBzkUeAI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=7P5WmeceOO2suwJWxD3kT8sAewM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17859625314458990",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AdCQ0gd2IlM",
          "url": "https://www.youtube.com/watch?v=AdCQ0gd2IlM",
          "view_count": 1123,
          "upload_date": "2025-10-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "My take on Objaverse, Llama, and Alpaca. Not a lot of respect for copyright or contract terms. #largelanguagemodels #datascience #machinelearning #objaverse #alleninstitute #openai #chatgpt #llama #alpaca #meta #databricks ",
      "description": "My take on Objaverse, Llama, and Alpaca. Not a lot of respect for copyright or contract terms. #largelanguagemodels #datascience #machinelearning #objaverse #alleninstitute #openai #chatgpt #llama #alpaca #meta #databricks ",
      "upload_date": "2023-03-25",
      "total_views": 4104,
      "max_views": 4104,
      "topics": [
        "alpaca",
        "dataset",
        "llama",
        "model",
        "objaverse",
        "openai"
      ],
      "search_text": "My take on Objaverse, Llama, and Alpaca. Not a lot of respect for copyright or contract terms. #largelanguagemodels #datascience #machinelearning #objaverse #alleninstitute #openai #chatgpt #llama #alpaca #meta #databricks  alpaca dataset llama model objaverse openai Let me give you a scoop on some of the big developments this week. This hearing, did anyone affiliated with the Chinese Communist Party discuss this hearing with you? The Allen Institute for AI decided to build a 3D dataset. And they did what a lazy undergrad would do. They went and found a publicly available site, scraped that entirely, did that without any type of permission, and then claim credit for the dataset. They didn't ask before they took one million objects, they just grabbed that data. When am I gonna get paid for the data you're getting from my children, my grandchildren, my neighbors? Boy, that doesn't seem right. Hey, where's that dataset located? Ask him for a friend. Their mistake was not hiring a grad student. Take Metta. They're in the business of selling ads. Use the phone's camera to determine whether the content that elicits a pupil dilation should be amplified by the algorithm. They have a research team that helps build solutions, including for their ads team. And recently they trained a new model, Lama, which is like a GPT-3 type model. Where was the source code for TikTok developed? Was it developed in China or in the United States? Well, they distributed it for education as a non-commercial use. What's the big deal? It's for education, right? Come on, it's so awful, nobody would pay for that model. It's so far behind ADA, we'd have to call it Elizabeth Holmes. They say education, but basically anybody could get their hands on it. And you could read this as a way to build a market for a product that competes with OpenAI and brings down the value of their technologies. Yeah, I saw it had hundreds of thousands of downloads. I didn't think there was that many graduate students. So with this new free model, some Stanford grad students decided to upgrade it to better compete against tools like chat GPT. They first needed data. Aren't there legitimate sources for getting training data? Yeah, we spent millions paying people $3 an hour. Nah, they built a dataset by repeatedly querying OpenAI and collecting all that together. What's wrong with that? Sounds like they're go-getters. OpenAI's terms of service forbid this type of behavior, but these folks did it over 50,000 times. I get it. So they got that data from OpenAI. They then trained a model that would then compete with OpenAI. Seems kind of like cheating. Damn right, it doesn't make sense. Wait till you see what we've got planned. All right, go initiate Skynet. And when it's compiled in the compilation process, can byte code be manipulated? But then we have a $30 billion company take that new dataset, train a model as a way to compete against OpenAI and show people that they can build their own competing solution. Wait, that company that overcharges us 10x for compute? Maybe they were doing this for educational reasons. Great job with the straight face. You'll want to do it like that. Well, with the disregard for education and contract law, my guess is I'm going to have to get ready for the hearings as well. My phone and my phone is on my home Wi-Fi network. Does TikTok access that network?",
      "platforms": {
        "tiktok": {
          "video_id": "7214547826104093994",
          "url": "https://www.tiktok.com/@rajistics/video/7214547826104093994",
          "view_count": 4104,
          "upload_date": "2023-03-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/552335826ff04db5a3f6a2a616610593_1679767846~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=Li%2FYJtP7IyusFdcaofyiD9JlWBc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6179,
      "title": "This video explains how scaling laws—particularly from the Chinchilla paper—reveal a tradeoff between model size, training data, and compute. By training smaller models for longer, we can reduce their size by over 60% while maintaining performance, enabling faster inference on smaller GPUs. The key insight is that many existing models are over-sized and under-trained, leaving room for more efficient alternatives.",
      "description": "This video explains how scaling laws—particularly from the Chinchilla paper—reveal a tradeoff between model size, training data, and compute. By training smaller models for longer, we can reduce their size by over 60% while maintaining performance, enabling faster inference on smaller GPUs. The key insight is that many existing models are over-sized and under-trained, leaving room for more efficient alternatives.",
      "upload_date": "2025-04-16",
      "total_views": 4046,
      "max_views": 3067,
      "topics": [
        "chatgpt",
        "compute",
        "large",
        "laws",
        "many",
        "model",
        "models",
        "modifying",
        "scaling",
        "smaller",
        "train",
        "ways"
      ],
      "search_text": "This video explains how scaling laws—particularly from the Chinchilla paper—reveal a tradeoff between model size, training data, and compute. By training smaller models for longer, we can reduce their size by over 60% while maintaining performance, enabling faster inference on smaller GPUs. The key insight is that many existing models are over-sized and under-trained, leaving room for more efficient alternatives. chatgpt compute large laws many model models modifying scaling smaller train ways Want to hear something wild? We can take a large language model, reduce it down to over 60% and get the same performance by using the insights from scaling laws. And reducing these models down means we can run them on smaller GPUs and we can get faster inference speeds. You might remember in an earlier video, I talked about the scaling laws from the Chinchilla paper, which tell us there's a tradeoff between the optimal model size, the amount of data trained on, and the total amount of compute. Chinchilla pointed out that a lot of the existing models were actually not optimal and were much larger than necessary. The scaling laws tell us that if we want to keep model accuracy the same, but make the model smaller, then we're going to have to increase our compute costs. And we can actually plot it out here on the curve. And if we look at this, as the model gets smaller, we have to add more compute. And in fact, if we want to make the model down 30%, well that's going to double the compute that we need for our model. And this is why models like Lama 7B and Saint-Tacoda are trained longer because they're trying to reduce the inference time. But this analysis shows that there's actually a little bit more that they could go in terms of size. So this is going to be a great thing going forward because if we can get smaller models with the same accuracy, that's golden.",
      "platforms": {
        "tiktok": {
          "video_id": "7493696981131463967",
          "url": "https://www.tiktok.com/@rajistics/video/7493696981131463967",
          "view_count": 3067,
          "upload_date": "2025-04-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEXfDfwLIEbqQc9twAPAFEOeIkjvAAY0HIvAI1~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=BP3yRR22gBHbqH1QqIhS71dd8pA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18038761631620809",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "s-kmnNSS4nk",
          "url": "https://www.youtube.com/watch?v=s-kmnNSS4nk",
          "view_count": 979,
          "upload_date": "2025-04-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Watch out for the flashy new algorithm.  ",
      "description": "Watch out for the flashy new algorithm.  ",
      "upload_date": "2024-10-22",
      "total_views": 4016,
      "max_views": 4016,
      "topics": [
        "data",
        "different",
        "going",
        "revolution",
        "science",
        "try"
      ],
      "search_text": "Watch out for the flashy new algorithm.   data different going revolution science try We've got a revolution in data science. Tab PFN with its transformer architecture? Is it going to take over? Come on people, you know better than this. Data science twitter is about getting eyeballs, not actually giving you something useful. If we look at the revolutionary data science algorithms over the last couple years, XGBoost, AlexNet with introducing convolutional neural networks, transformers. All of these took time to make that revolution happen. You have to try different implementations, different data sets. It's a grind. It's no different than how you become a great musician or a great athlete. And that's what you're going to expect for anything in data science. So ignore this stuff, just focus on the fundamentals. I'll try to help you along that way.",
      "platforms": {
        "tiktok": {
          "video_id": "7428548739998190894",
          "url": "https://www.tiktok.com/@rajistics/video/7428548739998190894",
          "view_count": 4016,
          "upload_date": "2024-10-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/68d8f6eee9054a6e99beb676b3b385f1_1729593796~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=zdSsED3MTGQZzXy5PJk6glYTJ1Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Using AI for Pose Detection, this is such a cool application. #datascience #deeplearning #codetok #posedetection #sportsanalytics",
      "description": "Using AI for Pose Detection, this is such a cool application. #datascience #deeplearning #codetok #posedetection #sportsanalytics",
      "upload_date": "2022-09-05",
      "total_views": 4008,
      "max_views": 4008,
      "topics": [
        "body",
        "codetok",
        "datascience",
        "deeplearning",
        "posedetection",
        "sportsanalytics"
      ],
      "search_text": "Using AI for Pose Detection, this is such a cool application. #datascience #deeplearning #codetok #posedetection #sportsanalytics body codetok datascience deeplearning posedetection sportsanalytics So cool. What we're doing is estimating the pose of a body using AI. What we do is we actually try to predict the key locations of the body joints. This has lots of cool uses from ensuring proper biomechanics when you're running, even counting reps in an exercise app, maybe evaluating the safety of devices like here's one in escalators, or just having some fun at the museum. The way it works is we use a machine learning model to predict the key points or the locations of the body joints. One of the state-of-the-art approaches actually uses a very simple approach of just making a model to predict each key point, but it uses a transformer. Another approach used a more sophisticated or complicated approach where they use the shape of our body as a graph, because we know, for example, that the knee bone is connected to the shin bone, and that can be helpful when we're doing predictions. But even understanding how it works, it's still so mesmerizing.",
      "platforms": {
        "tiktok": {
          "video_id": "7139958330725944622",
          "url": "https://www.tiktok.com/@rajistics/video/7139958330725944622",
          "view_count": 4008,
          "upload_date": "2022-09-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0135d519edef44eab7379795e2846e38_1662401098~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=Hx6bn5JGBemoWPMaGno08d%2BMgbk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6265,
      "title": "Apple's MM1: Methods Analysis & Insights from Multimodal LLM Pre-training - https://arxiv.org/pdf/2403.09611.pdf Cohere int8 & binary Embeddings - Scale Your Vector Database to Large Datasets: https://txt.cohere.com/int8-binary-embeddings/ Contextual AI - RAG 2.0: https://contextual.ai/introducing-rag #rajistics",
      "description": "Apple's MM1: Methods Analysis & Insights from Multimodal LLM Pre-training - https://arxiv.org/pdf/2403.09611.pdf Cohere int8 & binary Embeddings - Scale Your Vector Database to Large Datasets: https://txt.cohere.com/int8-binary-embeddings/ Contextual AI - RAG 2.0: https://contextual.ai/introducing-rag #rajistics",
      "upload_date": "2024-03-19",
      "total_views": 4007,
      "max_views": 4007,
      "topics": [
        "binary",
        "cohere",
        "contextual",
        "embeddings",
        "int8",
        "let",
        "model",
        "pdf"
      ],
      "search_text": "Apple's MM1: Methods Analysis & Insights from Multimodal LLM Pre-training - https://arxiv.org/pdf/2403.09611.pdf Cohere int8 & binary Embeddings - Scale Your Vector Database to Large Datasets: https://txt.cohere.com/int8-binary-embeddings/ Contextual AI - RAG 2.0: https://contextual.ai/introducing-rag #rajistics binary cohere contextual embeddings int8 let model pdf",
      "platforms": {
        "tiktok": {
          "video_id": "7348213794419625259",
          "url": "https://www.tiktok.com/@rajistics/video/7348213794419625259",
          "view_count": 4007,
          "upload_date": "2024-03-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18075530959463055",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-03-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6554,
      "title": "Evaluating generative models means considering many factors including prompts tokenization and evaluating generated results. This video should give you an intuition of the different ways to evaluate models. This is inspired by the recent Hugging Face blog post on MMLU evaluation. #datascience #machinelearning #largelanguagemodels #modelevaluation #MMLU #leaderboards What's going on with the Open LLM Leaderboard? https://huggingface.co/blog/evaluating-mmlu-leaderboard",
      "description": "Evaluating generative models means considering many factors including prompts tokenization and evaluating generated results. This video should give you an intuition of the different ways to evaluate models. This is inspired by the recent Hugging Face blog post on MMLU evaluation. #datascience #machinelearning #largelanguagemodels #modelevaluation #MMLU #leaderboards What's going on with the Open LLM Leaderboard? https://huggingface.co/blog/evaluating-mmlu-leaderboard",
      "upload_date": "2023-06-24",
      "total_views": 4000,
      "max_views": 3355,
      "topics": [
        "datascience",
        "different",
        "evaluating",
        "generative",
        "largelanguagemodels",
        "leaderboards",
        "machinelearning",
        "mmlu",
        "modelevaluation",
        "models",
        "nuances"
      ],
      "search_text": "Evaluating generative models means considering many factors including prompts tokenization and evaluating generated results. This video should give you an intuition of the different ways to evaluate models. This is inspired by the recent Hugging Face blog post on MMLU evaluation. #datascience #machinelearning #largelanguagemodels #modelevaluation #MMLU #leaderboards What's going on with the Open LLM Leaderboard? https://huggingface.co/blog/evaluating-mmlu-leaderboard datascience different evaluating generative largelanguagemodels leaderboards machinelearning mmlu modelevaluation models nuances",
      "platforms": {
        "tiktok": {
          "video_id": "7248257814190345518",
          "url": "https://www.tiktok.com/@rajistics/video/7248257814190345518",
          "view_count": 3355,
          "upload_date": "2023-06-24",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Ct4FhxAgZxZ",
          "url": "https://www.instagram.com/reel/Ct4FhxAgZxZ",
          "view_count": 578,
          "upload_date": "2023-06-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "uqdczeFl2Ws",
          "url": "https://www.youtube.com/watch?v=uqdczeFl2Ws",
          "view_count": 67,
          "upload_date": "2023-06-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6167,
      "title": "8 Ways to improve your RAG Application 1. Metadata Filter 2. Semantic Chunking 3. Visual Language Model 4. Query Decomposition 5. Better Embeddings 6. Lexical / BM25 7. Add Reranker 8. Instruction Following Reranker",
      "description": "8 Ways to improve your RAG Application 1. Metadata Filter 2. Semantic Chunking 3. Visual Language Model 4. Query Decomposition 5. Better Embeddings 6. Lexical / BM25 7. Add Reranker 8. Instruction Following Reranker",
      "upload_date": "2025-05-09",
      "total_views": 3999,
      "max_views": 3226,
      "topics": [
        "application",
        "better",
        "chunking",
        "improve",
        "rag",
        "reranker",
        "semantic",
        "using",
        "ways"
      ],
      "search_text": "8 Ways to improve your RAG Application 1. Metadata Filter 2. Semantic Chunking 3. Visual Language Model 4. Query Decomposition 5. Better Embeddings 6. Lexical / BM25 7. Add Reranker 8. Instruction Following Reranker application better chunking improve rag reranker semantic using ways Eight ways to improve your RAG application. What did the 2024 emergency response memo say? If bitten, aim for the zombie's head. The source is the 2012 drill. Wrong decade. Could have been fixed by using metadata for the date. Summarize our mission and vision. Our mission is to dominate the space poncho market. No vision here. It was missed due to a chunking split by a hard boundary. Should have used a semantic chunking strategy. What does this diagram explain? Nothing. Embrace multimodal data by using vision language models to give you caption. How did rates of typos and coffee change between 2022 and 2023? Employees like caffeine. Complex queries can be decomposed into subqueries. Which new initiative reduced the Friday deploy disasters? We ran feeling sharing circles. A semantic miss. Consider a better embedding model. What was the exact phrase the CFO used? Crypto corgis. Digital pets could be lucrative. Add lexical or BM25 search to allow you to do that exact matching. What were the top three security risks in our latest audit? Comic sans and templates. Stale donuts. And unpatched servers. The relevance here isn't taking into account the query. Consider using a re-ranker for improvement. Which recent achievements boosted morale? No password Friday from 2017. To improve on recency, consider an instruction following re-ranker. Now it's time to build a better rag after.",
      "platforms": {
        "tiktok": {
          "video_id": "7502431533932891423",
          "url": "https://www.tiktok.com/@rajistics/video/7502431533932891423",
          "view_count": 3226,
          "upload_date": "2025-05-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogLveASgdAEpQuVAIBZfBGeJUq3egTMARdYAQG~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=b1oeDvpQ1xT%2FRPZXbY5EieHZci0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17992463054641111",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Ko0d9C-FkP8",
          "url": "https://www.youtube.com/watch?v=Ko0d9C-FkP8",
          "view_count": 773,
          "upload_date": "2025-05-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6257,
      "title": "Object detection for AI using yolov6 Check out the demo at: https://huggingface.co/spaces/rajistics/yolov6",
      "description": "Object detection for AI using yolov6 Check out the demo at: https://huggingface.co/spaces/rajistics/yolov6",
      "upload_date": "2023-07-13",
      "total_views": 3991,
      "max_views": 3337,
      "topics": [
        "check",
        "data",
        "demo",
        "detection",
        "every",
        "license",
        "llm",
        "model",
        "new",
        "object",
        "things",
        "top",
        "training",
        "using",
        "yolov6"
      ],
      "search_text": "Object detection for AI using yolov6 Check out the demo at: https://huggingface.co/spaces/rajistics/yolov6 check data demo detection every license llm model new object things top training using yolov6",
      "platforms": {
        "tiktok": {
          "video_id": "7255400203820354859",
          "url": "https://www.tiktok.com/@rajistics/video/7255400203820354859",
          "view_count": 3337,
          "upload_date": "2023-07-13",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18432701728005203",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "LsTWrcAvSOo",
          "url": "https://www.youtube.com/watch?v=LsTWrcAvSOo",
          "view_count": 654,
          "upload_date": "2024-04-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Software exec at the end is the best. Your quick intro to patents, trademarks, copyright, and licenses.  I see too many comments where people get these confused. ",
      "description": "Software exec at the end is the best. Your quick intro to patents, trademarks, copyright, and licenses.  I see too many comments where people get these confused. ",
      "upload_date": "2022-10-30",
      "total_views": 3976,
      "max_views": 3976,
      "topics": [
        "copyright",
        "licenses",
        "patents",
        "people",
        "software",
        "use"
      ],
      "search_text": "Software exec at the end is the best. Your quick intro to patents, trademarks, copyright, and licenses.  I see too many comments where people get these confused.  copyright licenses patents people software use I am an inventor and by sharing exactly what I did, I can use patents to keep anyone else from making it for a while. I am an artist and copyright protects my creative expression to display, share, perform for a very long time. As a business owner, I've invested a lot into this logo and these words. Trademark law lets me protect that as long as I'm actively using it, they're mine. I'm a programmer and I create software products. I use licenses so other people can use my products but on terms that we agree on. As a software executive, I try to gather patents. They're kind of like nukes. We don't really want to use them, at least publicly, but it's good to be in the club that has lots of them. I don't worry about copyright. All we have to do is slightly change the look and feel of a program. It'll be okay. But trademarks, they're a big deal. We want to establish our brand and be very careful about stepping on others. We spend a lot of time on licensing. Whether it's making sure we're careful about using other people's licenses or crafting our own licenses with terms that we like.",
      "platforms": {
        "tiktok": {
          "video_id": "7160449336957078827",
          "url": "https://www.tiktok.com/@rajistics/video/7160449336957078827",
          "view_count": 3976,
          "upload_date": "2022-10-30",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9375b85d20184dc986254e71f75cf64f_1667172038~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=kM1hOJWFWo%2Fk7irrzeRExiyem4Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "It’s rough for statisticians, machine learning is so popular #datascience #analytics #statistics #machinelearning ",
      "description": "It’s rough for statisticians, machine learning is so popular #datascience #analytics #statistics #machinelearning ",
      "upload_date": "2022-09-15",
      "total_views": 3957,
      "max_views": 3957,
      "topics": [
        "analytics",
        "datascience",
        "fucking",
        "machinelearning",
        "rough",
        "statistics"
      ],
      "search_text": "It’s rough for statisticians, machine learning is so popular #datascience #analytics #statistics #machinelearning  analytics datascience fucking machinelearning rough statistics I'm hurt and I'm old and I'm fucking tired and I work with fucking children.",
      "platforms": {
        "tiktok": {
          "video_id": "7143596077642550571",
          "url": "https://www.tiktok.com/@rajistics/video/7143596077642550571",
          "view_count": 3957,
          "upload_date": "2022-09-15",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c9cf6547f2104985ac4add6c6980bafa_1663248088~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=6cMxqNAzW4%2FJtenWJPijG37AXsI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6552,
      "title": "Data science is often team work and you want to try to avoid toxic teams. #datascience #rajistics #teamwork #kaggle",
      "description": "Data science is often team work and you want to try to avoid toxic teams. #datascience #rajistics #teamwork #kaggle",
      "upload_date": "2023-09-04",
      "total_views": 3946,
      "max_views": 3877,
      "topics": [
        "data",
        "datascience",
        "kaggle",
        "look",
        "often",
        "science",
        "teamwork"
      ],
      "search_text": "Data science is often team work and you want to try to avoid toxic teams. #datascience #rajistics #teamwork #kaggle data datascience kaggle look often science teamwork",
      "platforms": {
        "tiktok": {
          "video_id": "7274784097053068587",
          "url": "https://www.tiktok.com/@rajistics/video/7274784097053068587",
          "view_count": 3877,
          "upload_date": "2023-09-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "90EJxOJtPq8",
          "url": "https://www.youtube.com/watch?v=90EJxOJtPq8",
          "view_count": 69,
          "upload_date": "2023-09-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6094,
      "title": "An increasingly popular approach for labeling data is getting assistance with LLMs. Here this approach is highlighted using MLFlow. This is often known as model as a judge in reference to it's use.",
      "description": "An increasingly popular approach for labeling data is getting assistance with LLMs. Here this approach is highlighted using MLFlow. This is often known as model as a judge in reference to it's use.",
      "upload_date": "2024-12-22",
      "total_views": 3941,
      "max_views": 3941,
      "topics": [
        "approach",
        "data",
        "human",
        "labeling",
        "lot",
        "model"
      ],
      "search_text": "An increasingly popular approach for labeling data is getting assistance with LLMs. Here this approach is highlighted using MLFlow. This is often known as model as a judge in reference to it's use. approach data human labeling lot model I need thousands of labeled examples for my project. How am I going to do that? Yeah, that's a massive task. Relying on human labeling is going to take a lot of time and cost a lot of money. Why not do it yourself? Just load up on energy drinks and dive in. That's not practical. I don't have a huge budget or unlimited time. Well, have you thought about using a machine learning model? A lot cheaper than human labor. There are a lot of solutions that support using a model as a judge, including MLflow. Here, all you have to do is set up MLflow by giving it your evaluation criteria, what you want to use as a grading scale, and then the model. And then let it go to work. Interesting. What does the output look like? Here's an example. You can see here's the output, but then we get the score for the model, as well as a justification for why. Hey, this could work. How accurate is it compared to human labeling? Overall, it correlates quite well with human labeled data. But remember, just like humans, these models also have their own biases. Those explanations are really handy. I've used it before to help explain why a model made a certain decision. Yeah, I'll give it a try. I'll probably label some of the data by hand, but then use a model to do the rest. A balanced approach. I wouldn't judge you badly for that.",
      "platforms": {
        "tiktok": {
          "video_id": "7451375714890157343",
          "url": "https://www.tiktok.com/@rajistics/video/7451375714890157343",
          "view_count": 3941,
          "upload_date": "2024-12-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIAUAf9UBiBziAWCkoimEiEstBKICTOA1nIx1A~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=5LSs9YtJPFTePOeO2NV2Wk0Jkq8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18032417723588555",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5975,
      "title": "The enterprise AI landscape has radically shifted in 2025. Anthropic has overtaken OpenAI in enterprise usage by offering broader cloud access and leading performance in code-related tasks. Meanwhile, open-source adoption is shrinking, and enterprise spending is rapidly consolidating around API-based inference, not training or fine-tuning. 📚 References: * Menlo Ventures – 2025 Mid-Year LLM Market Update https://menlovc.com/perspective/2025-mid-year-llm-market-update/ - Background from @larissafarber",
      "description": "The enterprise AI landscape has radically shifted in 2025. Anthropic has overtaken OpenAI in enterprise usage by offering broader cloud access and leading performance in code-related tasks. Meanwhile, open-source adoption is shrinking, and enterprise spending is rapidly consolidating around API-based inference, not training or fine-tuning. 📚 References: * Menlo Ventures – 2025 Mid-Year LLM Market Update https://menlovc.com/perspective/2025-mid-year-llm-market-update/ - Background from @larissafarber",
      "upload_date": "2025-08-07",
      "total_views": 3936,
      "max_views": 2728,
      "topics": [
        "anthropic",
        "api",
        "based",
        "enterprise",
        "enterprises",
        "llm",
        "market",
        "menlo",
        "mid",
        "open",
        "openai",
        "update",
        "usage",
        "year"
      ],
      "search_text": "The enterprise AI landscape has radically shifted in 2025. Anthropic has overtaken OpenAI in enterprise usage by offering broader cloud access and leading performance in code-related tasks. Meanwhile, open-source adoption is shrinking, and enterprise spending is rapidly consolidating around API-based inference, not training or fine-tuning. 📚 References: * Menlo Ventures – 2025 Mid-Year LLM Market Update https://menlovc.com/perspective/2025-mid-year-llm-market-update/ - Background from @larissafarber anthropic api based enterprise enterprises llm market menlo mid open openai update usage year Halfway through 2025 and the AI market has already flipped upside down. What does that mean? Can you break it down for me? Anthropic overtook OpenAI and enterprise usage. Wait, what? Anthropic? I've never heard of them. I thought OpenAI was the AGI company. Anthropic has much better reach. It's on AWS, Google Cloud, and Azure, while OpenAI has just shackled over to Azure. Plus, Anthropics Clawed leads in code tasks, which is the biggest revenue maker for LLMs. That's wild. I always thought OpenAI and its commitment to openness would dominate. Funny you say that, but open source usage is actually dropping in enterprises. But open source is free. Free like a puppy, you still need to feed it, train it, clean up after it. Running open models means you have to have infrastructure ops, in-house machine learning talent that most companies just don't have. If you put it like that, I'd rather use an API than pay for a whole kennel. Exactly, and that's why API usage is exploding. Enterprise LLM spending has just gone up dramatically, largely on inference, not on training. So no more fine tuning? Not unless absolutely necessary, for 90% of use cases using an API is faster, cheaper, more effective. Makes sense. Why make a custom wrench if you can just use an off-the-shelf wrench?",
      "platforms": {
        "tiktok": {
          "video_id": "7535657680623439134",
          "url": "https://www.tiktok.com/@rajistics/video/7535657680623439134",
          "view_count": 2728,
          "upload_date": "2025-08-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMdIjaACEiaxB2izIKBn1IMQCRaqV0vQ6AUhJ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=9%2FRvOUJpG9ShuKmrH48gGoqQ%2BkA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18093622501712450",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "d5jC2tQUwKU",
          "url": "https://www.youtube.com/watch?v=d5jC2tQUwKU",
          "view_count": 1208,
          "upload_date": "2025-08-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Quick intro, let me know if a deeper dive is useful.  #translation #meta #datascience #machinelearning #huggingface",
      "description": "Quick intro, let me know if a deeper dive is useful.  #translation #meta #datascience #machinelearning #huggingface",
      "upload_date": "2022-07-19",
      "total_views": 3896,
      "max_views": 3896,
      "topics": [
        "datascience",
        "huggingface",
        "language",
        "machinelearning",
        "meta",
        "translation"
      ],
      "search_text": "Quick intro, let me know if a deeper dive is useful.  #translation #meta #datascience #machinelearning #huggingface datascience huggingface language machinelearning meta translation Remember the universal translators you saw in sci-fi shows? Well, it's here now. Meta unveiled its newest language translation model. It translates between 200 languages and the experts are saying the results are awesome. These models take advantage of machine learning where we give them input in one language and what the output should be in another language. And by giving them millions of these examples, the machine is actually able to translate really well. The best thing is you can try this out for yourself. Facebook has open sourced its code. You can go out and grab it at the Hugging Face site.",
      "platforms": {
        "tiktok": {
          "video_id": "7122097956311878955",
          "url": "https://www.tiktok.com/@rajistics/video/7122097956311878955",
          "view_count": 3896,
          "upload_date": "2022-07-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/46cc449b0a3f4cfdae843930b544005b_1658242657~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=CkvvYGVCq1Td8vdshC9HhUSc9JE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6100,
      "title": "Error analysis is an important skill in analytics. ",
      "description": "Error analysis is an important skill in analytics. ",
      "upload_date": "2024-12-10",
      "total_views": 3895,
      "max_views": 3895,
      "topics": [
        "air",
        "also",
        "analysis",
        "error",
        "improve",
        "like",
        "model"
      ],
      "search_text": "Error analysis is an important skill in analytics.  air also analysis error improve like model My model is only a little bit away from being good enough for production. For that last 2%, I'm going to focus this week on using Bayesian hyperparameters. No. And I realize most data scientists don't do air analysis, but it's important for our team to do this because we want to improve model performance but also understand the limitations of our models. Take a look at this example. You'll see that the air is much higher when education is higher. So this is something we need to probe into and understand as part of air analysis. Is there a function I need to run? How do you do this? Let me show you how I do an air analysis. When I was working on my image classification project, I looked at the errors my model was making. Was there an animal that was miscategorized? Was there something else going on in the image? Like was it blurry or did the annotator screw up and was the example mislabeled? How do I use this approach to make my model better? That's nice, but let me even show you one better. In the email classification model, I started by grabbing 100 mistakes the model was making. I looked at the category of the type of email, also what features made it hard to classify. By doing this, I could then focus refining my model on groups like password emails and improving the punctuation pre-processing. So to communicate the findings back for Ann's model, what I do is often create a visualization like this. And this lets me walk our management through what are the steps we did that improved the model, but also where there's still some air in keeping us from getting to 100%. And some of it is irreducible. Annotators just don't disagree. But others, maybe we can have some future projects to improve our pipelines for them. Ah, I see the error of my ways.",
      "platforms": {
        "tiktok": {
          "video_id": "7446800784878030111",
          "url": "https://www.tiktok.com/@rajistics/video/7446800784878030111",
          "view_count": 3895,
          "upload_date": "2024-12-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAgGPHAweCddeSYqFA4BvZAUOQBpeerDQWYdsR~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=1pB6FbEFRyxdUZqoqofT6l1chvE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18050558759496060",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6102,
      "title": "Let's up your LLM game by going over the use of prompting strategies fine tuning and using synthetic datasets. This was motivated by some great work from Predibase and Hugging Face. Synthetic data: save money time and carbon with open source - https://huggingface.co/blog/synthetic-data-save-costs LoRA Land: Fine-Tuned Open-Source LLMs that Outperform GPT-4 - https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4 Background by Matthias_Groeneveld",
      "description": "Let's up your LLM game by going over the use of prompting strategies fine tuning and using synthetic datasets. This was motivated by some great work from Predibase and Hugging Face. Synthetic data: save money time and carbon with open source - https://huggingface.co/blog/synthetic-data-save-costs LoRA Land: Fine-Tuned Open-Source LLMs that Outperform GPT-4 - https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4 Background by Matthias_Groeneveld",
      "upload_date": "2024-02-22",
      "total_views": 3890,
      "max_views": 3229,
      "topics": [
        "data",
        "fine",
        "open",
        "predibase",
        "source",
        "synthetic"
      ],
      "search_text": "Let's up your LLM game by going over the use of prompting strategies fine tuning and using synthetic datasets. This was motivated by some great work from Predibase and Hugging Face. Synthetic data: save money time and carbon with open source - https://huggingface.co/blog/synthetic-data-save-costs LoRA Land: Fine-Tuned Open-Source LLMs that Outperform GPT-4 - https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4 Background by Matthias_Groeneveld data fine open predibase source synthetic",
      "platforms": {
        "tiktok": {
          "video_id": "7338211742029237546",
          "url": "https://www.tiktok.com/@rajistics/video/7338211742029237546",
          "view_count": 3229,
          "upload_date": "2024-02-22",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18002862737520082",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-02-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "G0_Nnw3TdSE",
          "url": "https://youtube.com/shorts/G0_Nnw3TdSE",
          "view_count": 661,
          "upload_date": "2024-02-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5948,
      "title": "J.P. Morgan processes 50 million transactions a day — and they didn’t use GPT-5. By layering rules, text similarity, and a tiny 1.7 M-parameter model, they boosted automation from 80 to 94 percent and saved $13 million a year. Smarter deployment beats bigger models. Check out Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding",
      "description": "J.P. Morgan processes 50 million transactions a day — and they didn’t use GPT-5. By layering rules, text similarity, and a tiny 1.7 M-parameter model, they boosted automation from 80 to 94 percent and saved $13 million a year. Smarter deployment beats bigger models. Check out Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding",
      "upload_date": "2025-10-27",
      "total_views": 3869,
      "max_views": 2418,
      "topics": [
        "actually",
        "deployment",
        "enterprise",
        "gpt",
        "jpmc",
        "million",
        "model",
        "models",
        "parameter",
        "rules",
        "transactions",
        "works"
      ],
      "search_text": "J.P. Morgan processes 50 million transactions a day — and they didn’t use GPT-5. By layering rules, text similarity, and a tiny 1.7 M-parameter model, they boosted automation from 80 to 94 percent and saved $13 million a year. Smarter deployment beats bigger models. Check out Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding actually deployment enterprise gpt jpmc million model models parameter rules transactions works JPMorgan processes 50 million transactions a day. They gotta be using GPT-5 for that. Their AI isn't doing homework problems, it's cleaning up credit card statements. Cleaning up? You know those weird charges on your statement? Well this model figures out who the merchant is and how to categorize. That sounds complex. Wouldn't you use GPT-5? Maybe they tested a broad range of models, encoders, decoders, encoder, decoders, and not only for accuracy, but also for cost and speed. So I'm guessing decoder models like GPT-5 won? Well yes, they did choose a 1.7 billion parameter model which was as accurate as the 8 billion parameter model, much faster and cheaper to run. Okay, so then they pushed that model to a bunch of GPUs for deployment? Not quite. They have enormous scale here, so they first run transactions through their rule-based engine that handles most of the transactions. Whoa, rules? I thought we got rid of that stuff when we got AI. Simple models are useful. After the rules-based engine, they used another enhanced string distance matcher, which is a text similarity engine which catches another 17% of transactions. Only the hardest 20% go to the AI model. Wow, I thought with AI you simplified everything by just using AI. Real-world practice means balancing cost, speed, and accuracy. It's not this one model fantasy that AI purists preach.",
      "platforms": {
        "tiktok": {
          "video_id": "7565944362794880286",
          "url": "https://www.tiktok.com/@rajistics/video/7565944362794880286",
          "view_count": 2418,
          "upload_date": "2025-10-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogfTAfGAAgHALPQmyoeSNdfONBNeRoilAIgwPQ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=8tVl1yiMqroE%2FHGZ19oMHImirQA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18372513643196086",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "TaHEidkLfsc",
          "url": "https://www.youtube.com/watch?v=TaHEidkLfsc",
          "view_count": 1451,
          "upload_date": "2025-10-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6162,
      "title": "Collaborative filtering is a very popular and useful way to build a recommender. However, getting explicit feedback is hard, and that is where the very smart implicit approach comes in. If you want to get started, go start with the very optimized Python library implicit.",
      "description": "Collaborative filtering is a very popular and useful way to build a recommender. However, getting explicit feedback is hard, and that is where the very smart implicit approach comes in. If you want to get started, go start with the very optimized Python library implicit.",
      "upload_date": "2025-05-21",
      "total_views": 3868,
      "max_views": 2635,
      "topics": [
        "building",
        "explicit",
        "feedback",
        "implicit",
        "need",
        "one",
        "recommenders",
        "using",
        "video",
        "watch"
      ],
      "search_text": "Collaborative filtering is a very popular and useful way to build a recommender. However, getting explicit feedback is hard, and that is where the very smart implicit approach comes in. If you want to get started, go start with the very optimized Python library implicit. building explicit feedback implicit need one recommenders using video watch Let's require everyone to watch the HR video. Why not use a recommender system to target people that need to watch the video? Interesting idea, but the only data I have is people's interactions with the company websites, like what documents they went to, where they clicked, what videos they watched. From what I know with recommendation algorithms, don't you need explicit, like, thumbs up, thumbs down ratings? Having explicit ratings is the old school way of doing collaborative filtering for recommendations. Nowadays, we can just use interaction data. Really? That works? This is implicit recommendation and it's widely used. And instead of an explicit rating, what we do is we have two factors. One, is there a preference yes or no for something? Second, what's the confidence? So if somebody watches, let's say, half a movie, what we do is we give them a one for preference, but then give them a 0.5 for confidence. The more they watch, the higher that confidence score goes up. Cool, so we have an algorithm. Do I need one of those parallel dinosaur spark Hadoop clusters to run this? Nah, those are relics. Start simple with the CPU. Much faster to get started. If you need to improve the speed, you can always shift to a GPU. Thank goodness for open source. This will make my life a lot easier. One more thing, explanations. We can explain to a person exactly why we're giving them a recommendation. Got it. So I need to add a required video for everyone explaining how the explainer for recommendations works.",
      "platforms": {
        "tiktok": {
          "video_id": "7506694795255975199",
          "url": "https://www.tiktok.com/@rajistics/video/7506694795255975199",
          "view_count": 2635,
          "upload_date": "2025-05-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0J7GAMjtCI8yA0S0ifvUefQA8eWQRgjQgAALS~tplv-tiktokx-origin.image?dr=9636&x-expires=1767322800&x-signature=adWIhbXFTWVxGn5WC%2B73nPc5kUE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18505116634034108",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EtmlZvLMiqA",
          "url": "https://www.youtube.com/watch?v=EtmlZvLMiqA",
          "view_count": 1233,
          "upload_date": "2025-05-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Just how smart is ChatGPT and other #largelanguagemodels? Big Bench is a set of benchmark tests to asses the performance of the models. And the most recent models from Google are doing pretty good! #datascience #machinelearning #chatgpt ",
      "description": "Just how smart is ChatGPT and other #largelanguagemodels? Big Bench is a set of benchmark tests to asses the performance of the models. And the most recent models from Google are doing pretty good! #datascience #machinelearning #chatgpt ",
      "upload_date": "2023-01-04",
      "total_views": 3868,
      "max_views": 3868,
      "topics": [
        "chat",
        "chatgpt",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "models"
      ],
      "search_text": "Just how smart is ChatGPT and other #largelanguagemodels? Big Bench is a set of benchmark tests to asses the performance of the models. And the most recent models from Google are doing pretty good! #datascience #machinelearning #chatgpt  chat chatgpt datascience largelanguagemodels machinelearning models Which you believe chat GPT is as smart as the average human. So one group of researchers thinks that large language models like chat GPT are just parroting back what we've taught them. We've trained them on lots of different diverse information. And when we ask it to create a cover letter or create an exercise plan, it's really just regurgitating back what we've given with a little bit of curve fitting to make the output look good. There's another group that says that chat GPT is doing something like a higher form of reasoning. If you look at examples of how it's thinking through problems or solving problems, it seems like it understands thing and is doing in fact what we would call reasoning. So how do we assess this? What's the best way to do this? Let's create a benchmark. Big bench is a benchmark to gauge reasoning and it consists of hundreds of different tasks like some of these. Yeah. Can you figure out the right chess move? Do you understand the main point of a story? How self aware are you? We don't know how chat GPT performs, but other large language models that are state of the art are achieving and beating average human beings at these tasks. And we're still early on all of this in terms of trying to understand exactly what reasoning is and figuring out how best to measure it in these large language models. So don't take these things too seriously.",
      "platforms": {
        "tiktok": {
          "video_id": "7184826669071469866",
          "url": "https://www.tiktok.com/@rajistics/video/7184826669071469866",
          "view_count": 3868,
          "upload_date": "2023-01-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f9da0c698d894216a6be2656a9e7ad32_1672847855~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=DPvJeqxuUq5DALXVX%2BhYwF5qBRw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6176,
      "title": "Optimal Transport algorithms to efficiently allocate resources—in this case, croissants from eight bakeries to five cafes. It begins by constructing a cost matrix using squared Euclidean distances, then solves the assignment using the Earth Mover’s Distance (EMD) for an optimal but computationally intensive solution. To reduce complexity, it introduces the Sinkhorn algorithm, which uses entropy regularization to produce a faster, approximate solution. By adjusting the regularization parameter, the solution becomes sparser and approaches the EMD result. The implementation is done using the Python Optimal Transport (POT) library. Code: https://pythonot.github.io/",
      "description": "Optimal Transport algorithms to efficiently allocate resources—in this case, croissants from eight bakeries to five cafes. It begins by constructing a cost matrix using squared Euclidean distances, then solves the assignment using the Earth Mover’s Distance (EMD) for an optimal but computationally intensive solution. To reduce complexity, it introduces the Sinkhorn algorithm, which uses entropy regularization to produce a faster, approximate solution. By adjusting the regularization parameter, the solution becomes sparser and approaches the EMD result. The implementation is done using the Python Optimal Transport (POT) library. Code: https://pythonot.github.io/",
      "upload_date": "2025-04-23",
      "total_views": 3864,
      "max_views": 2045,
      "topics": [
        "algorithms",
        "bakeries",
        "croissants",
        "distance",
        "optimal",
        "regularization",
        "transport",
        "using"
      ],
      "search_text": "Optimal Transport algorithms to efficiently allocate resources—in this case, croissants from eight bakeries to five cafes. It begins by constructing a cost matrix using squared Euclidean distances, then solves the assignment using the Earth Mover’s Distance (EMD) for an optimal but computationally intensive solution. To reduce complexity, it introduces the Sinkhorn algorithm, which uses entropy regularization to produce a faster, approximate solution. By adjusting the regularization parameter, the solution becomes sparser and approaches the EMD result. The implementation is done using the Python Optimal Transport (POT) library. Code: https://pythonot.github.io/ algorithms bakeries croissants distance optimal regularization transport using What if you owned eight bakeries making croissants? Would you know the most efficient way to get those croissants to five different bakeries? Let's work this out using a little bit of data science. The first step is just building a cost matrix between our bakeries and croissants and the distance required. We're just going to use the squared Euclidean distance here. Now for this first one, I'm going to use the Earth Mover's distance, which will give me the optimal answer for this problem. You can see here the cost matrix that's developed, as well as what happens when I plot these out on a map to see what's the distance I'd have to travel between these bakeries to get these croissants to them. The downside of this approach is the algorithmic complexity. As I add more bakeries and more cafes, the computation gets really difficult to calculate. An alternative to that is using the syncorm algorithm. This has a lower complexity and when I first run it here, you can see the first run is a little bit messy where I'm spreading the croissants across multiple bakeries and cafes, but we can use regularization to make this a little bit better answer. See what happens as I start adding more regularization, it gets more and more sparse. In fact, if I plot this out, I can see that by adding enough regularization, I get closer to the original Earth Mover's distance. So this is from the Python Optimal Transport package. Go try it out and play with it yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7496555026228186399",
          "url": "https://www.tiktok.com/@rajistics/video/7496555026228186399",
          "view_count": 2045,
          "upload_date": "2025-04-23",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAfjReAm7EA8NALQAAArIHEJfoIAtEIqKD8jEF~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=R%2FDlhkRN9p4aA6RKGyTd%2FnHZhtg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17890860108234515",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Cx24vvlHC0I",
          "url": "https://www.youtube.com/watch?v=Cx24vvlHC0I",
          "view_count": 1819,
          "upload_date": "2025-04-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5962,
      "title": "ShinkaEvolve pairs evolutionary algorithms with LLMs to invent new solutions faster. Using novelty-based rejection, smarter parent selection, and dynamic LLM guidance, it cut search times and set records in tasks like circle packing, math reasoning, and Mixture-of-Experts training. A glimpse of AI as a discovery engine. https://sakana.ai/shinka-evolve/",
      "description": "ShinkaEvolve pairs evolutionary algorithms with LLMs to invent new solutions faster. Using novelty-based rejection, smarter parent selection, and dynamic LLM guidance, it cut search times and set records in tasks like circle packing, math reasoning, and Mixture-of-Experts training. A glimpse of AI as a discovery engine. https://sakana.ai/shinka-evolve/",
      "upload_date": "2025-09-27",
      "total_views": 3848,
      "max_views": 3550,
      "topics": [
        "algorithms",
        "chica",
        "evolutionary",
        "evolve",
        "invent",
        "llm",
        "llms",
        "meet",
        "new",
        "pairs",
        "search",
        "shinkaevolve"
      ],
      "search_text": "ShinkaEvolve pairs evolutionary algorithms with LLMs to invent new solutions faster. Using novelty-based rejection, smarter parent selection, and dynamic LLM guidance, it cut search times and set records in tasks like circle packing, math reasoning, and Mixture-of-Experts training. A glimpse of AI as a discovery engine. https://sakana.ai/shinka-evolve/ algorithms chica evolutionary evolve invent llm llms meet new pairs search shinkaevolve What if AI could invent better algorithms? That's the idea behind Chica Evolve, where brute force evolutionary search teams up with large language models in AI to discover smarter, faster solutions. So let me tell you how it works and how you can get started with it. In the past, evolutionary search was like throwing thousands of darts at a problem, hoping one hit the bull's eye. It was effective, but it was painfully slow, like slow on a geological timescale. So Chica Evolve changes that using a novelty based rejection. So instead of wasting time on near duplicates, it filters them out. The LLM judges what is new and promising. Add in smarter parent selection, dynamic LLM personas. Now we have a lean, mean, smart engine for finding solutions. And the results look good. In the classic circle packing problem, Chica Evolve was able to hit a new state of the art solution in just 150 attempts. Worked better for math reasoning and Amy, even built a new load balancing function for mixture of experts training. Of course, there are challenges. The LLM can bias towards familiar patterns and balancing novelty and generalization isn't easy, but this is a glimpse of what's next, where AI just isn't executing algorithms, but it's helping us invent them. So go try it for yourself. They've open sourced it. And the takeaway here is Chica Evolve's another sign that LLMs just aren't tools for text, but that they're becoming engines for scientific discovery.",
      "platforms": {
        "tiktok": {
          "video_id": "7554825399373483295",
          "url": "https://www.tiktok.com/@rajistics/video/7554825399373483295",
          "view_count": 3550,
          "upload_date": "2025-09-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4ikEfZmBir0i82CbYiIBUIcbAqIcgxA9dImCA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=Vl1YcV5GaSB%2BwwZQ45U2%2BTDNn9E%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18072783020515904",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "UAj_THW4gCA",
          "url": "https://www.youtube.com/watch?v=UAj_THW4gCA",
          "view_count": 298,
          "upload_date": "2025-09-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "A reminder that most enterprises favor Apache and MIT licenses. As a developer, use what you please. But to reach people working within companies, it’s best to stick to the classic open source licenses. #datascience #machinelearning #softwarelicensing ",
      "description": "A reminder that most enterprises favor Apache and MIT licenses. As a developer, use what you please. But to reach people working within companies, it’s best to stick to the classic open source licenses. #datascience #machinelearning #softwarelicensing ",
      "upload_date": "2023-01-06",
      "total_views": 3845,
      "max_views": 3845,
      "topics": [
        "datascience",
        "licenses",
        "machinelearning",
        "software",
        "softwarelicensing",
        "use"
      ],
      "search_text": "A reminder that most enterprises favor Apache and MIT licenses. As a developer, use what you please. But to reach people working within companies, it’s best to stick to the classic open source licenses. #datascience #machinelearning #softwarelicensing  datascience licenses machinelearning software softwarelicensing use Today we have a guest speaker from corporate legal that's going to talk to us about software licensing. I'm double booked so I'm going to have to make this quick. But my main takeaway is please don't use any other software that's external to our company. Doing so opens us up to a variety of risks and liabilities. But you realize the way our team works is we often build on existing software. In fact, that's how most software develop is done. Okay, well we have approved Apache and MIT license so you can use any of that software. But please stay away from things like the GPL. Okay, that's fair. A lot of software is licensed that way. Also make sure to check your dependencies. We had some issues with the profit software package because it had a dependency that required the GPL. What about newer software licenses like Rail that just makes your AIs used responsibly? I don't know about that license but I'd be happy to review it. The general takeaway is that we want to be very careful with any licenses that requires to do any type of activity. That opens us up to lots of liabilities especially because this is a very large company. We have offices all across the world. There's a lot of different legal domains. So anything that you add that brings that in adds a high level of complexity. So our typical approach is to avoid using those whenever possible.",
      "platforms": {
        "tiktok": {
          "video_id": "7185385772231707950",
          "url": "https://www.tiktok.com/@rajistics/video/7185385772231707950",
          "view_count": 3845,
          "upload_date": "2023-01-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e1e58240fb8b4098b357df9b6ed76899_1672978012~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=mXt3uUg9jgNO9EaOHyynxzIq%2Bsc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6259,
      "title": "5 things to look for when a new model is announced License Real Open Source? - Apache 2 Commercial use? Strange conditions? Size of the model 7B, 70B, 200B This tells you about likely performance Required compute resources Benchmarks Of course you can game this But it gives you some measuring stick Training Data/Details The more they tell you the more you can understand the model the more you can trust it Can you fine tune it? Standard architecture Released code to make it easy to use Other tech details: Tokenizer Architecture Sequence Length Scaling laws/compute Safety work #rajistics #llmineuur_25 ",
      "description": "5 things to look for when a new model is announced License Real Open Source? - Apache 2 Commercial use? Strange conditions? Size of the model 7B, 70B, 200B This tells you about likely performance Required compute resources Benchmarks Of course you can game this But it gives you some measuring stick Training Data/Details The more they tell you the more you can understand the model the more you can trust it Can you fine tune it? Standard architecture Released code to make it easy to use Other tech details: Tokenizer Architecture Sequence Length Scaling laws/compute Safety work #rajistics #llmineuur_25 ",
      "upload_date": "2024-04-20",
      "total_views": 3831,
      "max_views": 3381,
      "topics": [
        "attention",
        "context",
        "data",
        "gpus",
        "length",
        "license",
        "llmineuur_25",
        "llms",
        "longer",
        "model",
        "new",
        "ring",
        "ringattention",
        "training",
        "transformers"
      ],
      "search_text": "5 things to look for when a new model is announced License Real Open Source? - Apache 2 Commercial use? Strange conditions? Size of the model 7B, 70B, 200B This tells you about likely performance Required compute resources Benchmarks Of course you can game this But it gives you some measuring stick Training Data/Details The more they tell you the more you can understand the model the more you can trust it Can you fine tune it? Standard architecture Released code to make it easy to use Other tech details: Tokenizer Architecture Sequence Length Scaling laws/compute Safety work #rajistics #llmineuur_25  attention context data gpus length license llmineuur_25 llms longer model new ring ringattention training transformers Have you checked out the new AI model that Hooli dropped? I noticed the license wasn't Apache or MIT, which means now I have to go ask legal. It does say commercial use is permissible. Is this another one of those huge models over 100 billion parameters? No, it comes in a seven billion and a 20 billion parameter size, so pretty usable. Nothing more annoying than a free model that takes an entire cluster to run. We've got some good benchmark results on MMLU and human eval for coding. I know they can be game, but it's looking promising. I'm always skeptical about those benchmarks. Did they tell us anything about the training data or the training process they used? The Hooli model isn't telling us about the training data, but they did tell us a little bit about the training process, like how much training data they used and how much compute they've used. Does it work with my standard libraries? Can I fine tune this? The architecture is pretty standard and they've already released code so you can fine tune the model. That's great. Can you go evaluate this model on our data sets so we can see how it matters for us? And by the time you're done, Legal Ward will probably finish looking at the license and I bet there'll be a new model for you to look at.",
      "platforms": {
        "tiktok": {
          "video_id": "7359983850564472110",
          "url": "https://www.tiktok.com/@rajistics/video/7359983850564472110",
          "view_count": 3381,
          "upload_date": "2024-04-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5b1ad0b403eb425981bff70416b558b6_1713629806~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=xi5zCjXkJ7ZTIcdhSBTZIUz93jQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17852746335170811",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "KHU66lzPDKY",
          "url": "https://www.youtube.com/watch?v=KHU66lzPDKY",
          "view_count": 450,
          "upload_date": "2024-04-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6630,
      "title": "Hallucinations from large language models are a concern. However balance them against the effectiveness of these models and the risks of using such a model. Too many people are running scared of Hallucinations. (And we could use more people on ML Twitter retiring). #datascience #machinelearning #largelanguagemodels #hallucinations #practicalai",
      "description": "Hallucinations from large language models are a concern. However balance them against the effectiveness of these models and the risks of using such a model. Too many people are running scared of Hallucinations. (And we could use more people on ML Twitter retiring). #datascience #machinelearning #largelanguagemodels #hallucinations #practicalai",
      "upload_date": "2023-05-06",
      "total_views": 3818,
      "max_views": 3086,
      "topics": [
        "datascience",
        "hallucinations",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "models",
        "practicalai"
      ],
      "search_text": "Hallucinations from large language models are a concern. However balance them against the effectiveness of these models and the risks of using such a model. Too many people are running scared of Hallucinations. (And we could use more people on ML Twitter retiring). #datascience #machinelearning #largelanguagemodels #hallucinations #practicalai datascience hallucinations largelanguagemodels machinelearning model models practicalai",
      "platforms": {
        "tiktok": {
          "video_id": "7229886580389842222",
          "url": "https://www.tiktok.com/@rajistics/video/7229886580389842222",
          "view_count": 3086,
          "upload_date": "2023-05-06",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cr4nBXHATNK",
          "url": "https://www.instagram.com/reel/Cr4nBXHATNK",
          "view_count": 732,
          "upload_date": "2023-05-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Statistics sounds heavy but a lot of concepts are very useful and can save you a lot of effort. This video is reminder of the many ways we use statistical concepts like random in data science. #statistics #random #rajistics",
      "description": "Statistics sounds heavy but a lot of concepts are very useful and can save you a lot of effort. This video is reminder of the many ways we use statistical concepts like random in data science. #statistics #random #rajistics",
      "upload_date": "2024-01-07",
      "total_views": 3805,
      "max_views": 3805,
      "topics": [
        "concepts",
        "heavy",
        "lot",
        "random",
        "sounds",
        "statistics"
      ],
      "search_text": "Statistics sounds heavy but a lot of concepts are very useful and can save you a lot of effort. This video is reminder of the many ways we use statistical concepts like random in data science. #statistics #random #rajistics concepts heavy lot random sounds statistics",
      "platforms": {
        "tiktok": {
          "video_id": "7321413249906560302",
          "url": "https://www.tiktok.com/@rajistics/video/7321413249906560302",
          "view_count": 3805,
          "upload_date": "2024-01-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5957,
      "title": "Research on productivity with the new AI code tools from Stanford, inspired their talk I saw at the MLOps summit. Lots of great insights. They found AI helps with greenfield or simple tasks, not complex systems. Check out: https://softwareengineeringproductivity.stanford.edu/",
      "description": "Research on productivity with the new AI code tools from Stanford, inspired their talk I saw at the MLOps summit. Lots of great insights. They found AI helps with greenfield or simple tasks, not complex systems. Check out: https://softwareengineeringproductivity.stanford.edu/",
      "upload_date": "2025-10-10",
      "total_views": 3789,
      "max_views": 2301,
      "topics": [
        "code",
        "coding",
        "engineering",
        "lot",
        "new",
        "productivity",
        "quality",
        "quantifying",
        "research",
        "software",
        "stanford",
        "tools",
        "way"
      ],
      "search_text": "Research on productivity with the new AI code tools from Stanford, inspired their talk I saw at the MLOps summit. Lots of great insights. They found AI helps with greenfield or simple tasks, not complex systems. Check out: https://softwareengineeringproductivity.stanford.edu/ code coding engineering lot new productivity quality quantifying research software stanford tools way AI coding tools are a game changer. I'm writing way more lines of code now. The Stanford study found that just more lines of code isn't higher quality code. Okay, fine, but I have a lot more pull requests. The study looked at that too. Look here where we see PRs go way up, but the quality went way down and it forced a lot of rework. Wow, those AI coding tools are actually causing more work. The coding tools make it easy to produce a lot of code, but that code quality actually gets more uneven. But everyone feels so much more productive. Exactly, people feel like they're coding a lot faster, but they're actually going a lot slower. This is why surveys don't work. You need to analyze the code quality. So what did that study find? Today's AI tools are great with greenfield projects or throwaway tasks, but complex stuff that keeps systems running not so useful. So is AI just hype? Not if your team can master it. Take a look at this graphic. It shows the teams that adopt these coding tools early and they measure rigorously or actually pulling way ahead of everyone else fast. You see the gap between the top and bottom corintiles growing. I guess I should be happy that it's not a magic tool that's gonna lead to less developers next year. Exactly, it's useful. It's gonna give us an edge in productivity, but maybe not a game changer.",
      "platforms": {
        "tiktok": {
          "video_id": "7559376243578244383",
          "url": "https://www.tiktok.com/@rajistics/video/7559376243578244383",
          "view_count": 2301,
          "upload_date": "2025-10-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oQACoFb4u6tLDTFGRf3BEbEuAVEpEz3IfoUYIq~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=LSjikl1LnzrPjEE1pb4B%2FPmrYAg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18039432734486990",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "LGGQ9KcQCsg",
          "url": "https://www.youtube.com/watch?v=LGGQ9KcQCsg",
          "view_count": 1488,
          "upload_date": "2025-10-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5939,
      "title": "LLMs can write SQL. That’s not the hard part. The hard part is making sure the math matches how the business actually works. Docs help AI understand the language. A semantic layer allows us to define the business understanding.",
      "description": "LLMs can write SQL. That’s not the hard part. The hard part is making sure the math matches how the business actually works. Docs help AI understand the language. A semantic layer allows us to define the business understanding.",
      "upload_date": "2025-11-19",
      "total_views": 3785,
      "max_views": 2041,
      "topics": [
        "business",
        "guessing",
        "hard",
        "layer",
        "llms",
        "part",
        "revenue",
        "semantic",
        "sql",
        "text",
        "write"
      ],
      "search_text": "LLMs can write SQL. That’s not the hard part. The hard part is making sure the math matches how the business actually works. Docs help AI understand the language. A semantic layer allows us to define the business understanding. business guessing hard layer llms part revenue semantic sql text write Check this out. Hey AI, what's our revenue by region? Look, it writes the SQL instantly and gives us an answer. Very cool. So which revenue is it using? There's more than one kind. Gross, net, recognize subscriptions and exclude refunds, did it convert currencies? I don't know, but revenues are up 74 billion? That's probably wrong. Okay, so now I've uploaded all our finance policies, contracts, all the PDFs we have. So now AI really knows our business. Sure, it can read business language, but when it writes a SQL, it's still guessing. Like what are the authority tables? What's the grain of the data? How should data be joined? What counts as real revenue? It tries its best. Doesn't that mean something? It's vibes based accounting. I mean, it's still guessing. So how do we stop the guessing? A semantic layer. This is where we can define all our metrics in code with actual logic. So then we can certify exactly what revenue is. Have the join paths for tables. What are our relationships between dimensions? What are the filters? What's our fiscal calendar? All of this codified together. Sounds like a lot of work. It is, but once it's in place, now if we change something at the semantic layer, it doesn't affect all those underlying dashboards and our CFO can get a good night's rest.",
      "platforms": {
        "tiktok": {
          "video_id": "7574582112788024607",
          "url": "https://www.tiktok.com/@rajistics/video/7574582112788024607",
          "view_count": 1744,
          "upload_date": "2025-11-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMM8NIDFeY5IipDigAQQ80geffZAfLQGDGoKsU~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=zVdL9Jl95opPd%2FvbIMWbqBZjJ5g%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18418591636119202",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "znb2k5CjTyI",
          "url": "https://www.youtube.com/watch?v=znb2k5CjTyI",
          "view_count": 2041,
          "upload_date": "2025-11-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Typical issues that often come up in everyday data science. Data scientists only spend a small amount of time on algorithms.  #datascience #machinelearning ",
      "description": "Typical issues that often come up in everyday data science. Data scientists only spend a small amount of time on algorithms.  #datascience #machinelearning ",
      "upload_date": "2022-12-04",
      "total_views": 3773,
      "max_views": 3773,
      "topics": [
        "data",
        "datascience",
        "going",
        "machinelearning",
        "time",
        "want"
      ],
      "search_text": "Typical issues that often come up in everyday data science. Data scientists only spend a small amount of time on algorithms.  #datascience #machinelearning  data datascience going machinelearning time want Want to learn what a data science team does? Come join me on my stand up. It's been a slow slog with accounting. I built a Streamlet app this week to help modernize their invoicing process, but no one's using it. Everyone still goes to gym with his master spreadsheet that integrates like 18 different data sources together. It's so frustrating. I get it. They're old school, but they really need to move to databases and have proper data management and governance. How else are we going to ever be able to lift them up to using machine learning? Here's the thing to keep in mind. Accounting is a cost sink. They're not going to generate ROI for the enterprise. And if they don't want our help, that's okay. Let's not get into it with them. Instead, let's just document what are the main data fees? Who are the big contacts there? And then I'm going to have a separate session with their leadership and I'm going to talk about how to modernize. They need to get their own data analysts to start doing this themselves. No use us spending our time on that. Let's just let it go for now. Hey, our work with marketing is going great. I'm working on a new framework to set up for a B testing. We're also going to be able to incorporate some uplift modeling as well. We're hoping to kick off the first experiment in about two weeks. Oh, that's great progress. Can you do two things for me? One is I want to make sure we track the effects that we're having with our interventions and second, can you have and shadow? Cause I think this is really great. And I want more of the data science team to be exposed to this. Sure, I can do that, but what do you mean by effect? I just want to make sure that the interventions that the marketing team is doing is actually providing a substantial benefit that's worth our investment in time. So as a ballpark figure, if they're not bringing in 200 K with this, it's probably not worth our time to be spending time on this project versus other priorities I have on my list. Anyone know how Stephanie's doing on her project? Yeah, she was really excited because she said the execs at warehouse saw this new paper by Google DeepMind and wanted her to use that approach. So she was jumping into that. No, that was an optimization problem. It just needs to be solved simply with a linear programming. There's no need to go into anything that heavyweight, especially to start off with. I'll go run her down and talk to her. Thanks.",
      "platforms": {
        "tiktok": {
          "video_id": "7173355054395641130",
          "url": "https://www.tiktok.com/@rajistics/video/7173355054395641130",
          "view_count": 3773,
          "upload_date": "2022-12-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d641ef35626844b59e21168cb90c4aba_1670176895~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=Hl9Ca29K7BAyaiB%2B8ulK1xm5Pyo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5930,
      "title": "Everyone says “just add more agents.” This new Google + MIT paper tested 180 multi-agent setups and found something uncomfortable: on average, more agents hurt performance. Tool overload, coordination cost, and error amplification matter more than agent count. Architecture must match the task. More agents is not a strategy. Paper: Quantitative Scaling Laws for Multi-Agent Systems arXiv:2512.08296",
      "description": "Everyone says “just add more agents.” This new Google + MIT paper tested 180 multi-agent setups and found something uncomfortable: on average, more agents hurt performance. Tool overload, coordination cost, and error amplification matter more than agent count. Architecture must match the task. More agents is not a strategy. Paper: Quantitative Scaling Laws for Multi-Agent Systems arXiv:2512.08296",
      "upload_date": "2025-12-14",
      "total_views": 3768,
      "max_views": 2472,
      "topics": [
        "agent",
        "agents",
        "architecture",
        "coordination",
        "everyone",
        "multi",
        "paper",
        "says",
        "scaling",
        "systems",
        "task"
      ],
      "search_text": "Everyone says “just add more agents.” This new Google + MIT paper tested 180 multi-agent setups and found something uncomfortable: on average, more agents hurt performance. Tool overload, coordination cost, and error amplification matter more than agent count. Architecture must match the task. More agents is not a strategy. Paper: Quantitative Scaling Laws for Multi-Agent Systems arXiv:2512.08296 agent agents architecture coordination everyone multi paper says scaling systems task Our AI app is struggling. I think I'm going to juice it up by making it multi-agent. Because if one agent can't do it, more agents will be able to do it. Exactly. As these models get smarter, they can run multi-agent systems. Vive agents should be five times better. Do you think this through about the architecture? Are you just vibing? Vibes. Take a look at this research. They looked at multi-agent configurations, 180 of them, and found on average they'd lead to worse performance. But I'm above average. Then start with the task. How many tools are you using? Because for tool-heavy tasks, those coordination costs dominate. Once you get to 16 tools, the best multi-agent systems perform worse than a single agent. So the system is spending more time doing all the coordinating than solving the problem. Okay, but suppose the agent isn't very good. Then agents can help, but only to a point. Once a single agent reaches about 45%, adding more agents stops helping because all those coordination costs outweigh that extra reasoning you get. What about errors? More agents, more eyes, should probably catch more errors. Only with the right architecture. If you're using independent agents, this amplifies errors by 17%. While if you have centralized coordination, you only get about four times as many errors. Okay, no vibes. What should I do? Adding more agents isn't a task. You want to match the architecture to the task. You need to take into account what is the decomposability? What is my tool complexity? Check the baseline performance first, then think about adding coordination. This sounds like I need to do some planning and thinking.",
      "platforms": {
        "tiktok": {
          "video_id": "7583837555825626399",
          "url": "https://www.tiktok.com/@rajistics/video/7583837555825626399",
          "view_count": 1296,
          "upload_date": "2025-12-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYUBPQ8QtAfCkheU6QFcGuAO5PQVX1RDMfBmNf~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=Cpci%2BGbqvfBzB8G%2B%2Fv6D8km2zYg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18329977711210372",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "kZjCp9KYO64",
          "url": "https://www.youtube.com/watch?v=kZjCp9KYO64",
          "view_count": 2472,
          "upload_date": "2025-12-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "As we store more information as vectors or embeddings vector databases are gaining importance. For small amounts of embeddings numpy or FAISS might work ok. As your needs grow there are many many vector databases. This video is a repost. Last year Pinecone was the rage, but now there are many more vendors in the space. #datascience #machinelearning #embeddings #vectordatabase #pinecone #chroma #weaviate #milvus #faiss",
      "description": "As we store more information as vectors or embeddings vector databases are gaining importance. For small amounts of embeddings numpy or FAISS might work ok. As your needs grow there are many many vector databases. This video is a repost. Last year Pinecone was the rage, but now there are many more vendors in the space. #datascience #machinelearning #embeddings #vectordatabase #pinecone #chroma #weaviate #milvus #faiss",
      "upload_date": "2024-04-16",
      "total_views": 3761,
      "max_views": 3761,
      "topics": [
        "databases",
        "datascience",
        "embeddings",
        "faiss",
        "pinecone",
        "vector"
      ],
      "search_text": "As we store more information as vectors or embeddings vector databases are gaining importance. For small amounts of embeddings numpy or FAISS might work ok. As your needs grow there are many many vector databases. This video is a repost. Last year Pinecone was the rage, but now there are many more vendors in the space. #datascience #machinelearning #embeddings #vectordatabase #pinecone #chroma #weaviate #milvus #faiss databases datascience embeddings faiss pinecone vector What are over 10,000 people a day signing up for? It's new vector databases that serve as the memory of GPT applications. We need these databases because much of the information we have, we take and represent it in a numerical way, often called an embedding or a vector. Once we get all those vectors, we want to then be able to compare and find, for example, similar concepts to each other. If you're working with a small amount of embeddings, you can use NumPy or a library like face to hold the amount of information you want in memory and be able to do quick comparisons. As the amount of data starts scaling up and you want to start thinking about how you're going to store and update that, this is where a vector database comes into play. For most people, I recommend starting with Pinecone. It's quick and easy to get up and running with it. If you want an open source solution, there are a number of others like VBA. You're going to see a lot of growth in these databases as we're trying to store unstructured information as well as work with all the outputs of these large language models.",
      "platforms": {
        "tiktok": {
          "video_id": "7358543601443278123",
          "url": "https://www.tiktok.com/@rajistics/video/7358543601443278123",
          "view_count": 3761,
          "upload_date": "2024-04-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1a97cf56bcb648339d46ede01660743a_1713294456~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=C30mvP6Bh3EqhQIH%2FxAgc23ytbk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6175,
      "title": "Building a Claude 3.7 AI Researcher: The Framework Dilemma I built this three ways using Claude 3.7's extended thinking capabilities with a custom RAG system to create an AI research assistant. This included a 200-line debug-heavy prototype, a 109-line optimized version, and a 30-line implementation using the Agno framework—highlighting the classic tradeoff between control and convenience in AI development. Agno: https://github.com/agno-agi/agno",
      "description": "Building a Claude 3.7 AI Researcher: The Framework Dilemma I built this three ways using Claude 3.7's extended thinking capabilities with a custom RAG system to create an AI research assistant. This included a 200-line debug-heavy prototype, a 109-line optimized version, and a 30-line implementation using the Agno framework—highlighting the classic tradeoff between control and convenience in AI development. Agno: https://github.com/agno-agi/agno",
      "upload_date": "2025-04-24",
      "total_views": 3759,
      "max_views": 1944,
      "topics": [
        "agno",
        "based",
        "claude",
        "code",
        "framework",
        "hallucinations",
        "lines",
        "research",
        "thinking",
        "transluce",
        "truthfulness",
        "using"
      ],
      "search_text": "Building a Claude 3.7 AI Researcher: The Framework Dilemma I built this three ways using Claude 3.7's extended thinking capabilities with a custom RAG system to create an AI research assistant. This included a 200-line debug-heavy prototype, a 109-line optimized version, and a 30-line implementation using the Agno framework—highlighting the classic tradeoff between control and convenience in AI development. Agno: https://github.com/agno-agi/agno agno based claude code framework hallucinations lines research thinking transluce truthfulness using So I built my own AI researcher using the latest reasoning models. And I hit a few bumps along the way. Clause new extended thinking lets it reason step by step instead of just blurting out the answers. Thanks. It uses tools, even kind of mumbles to itself before finally giving you an its answer. The RAG client means I don't directly feed my documents to Clause. So instead of Clause hallucinating, it actually references my actual documents using RAG. So I coded this up in cursor, which is great. If you enjoy code, that almost works. It functions only because I kept having to remind cursor, Hey, there's updated docs you should use. And then I had to slow it down when it got too ambitious and wanted to try to change too much at once. My first version, 200 lines, lots of code, because I wanted to have tons of logging and is this working print statements. But then to share it with my team, I had to clean it up. And so I brought it down to 109 lines of code of just Python with the anthropic client. That's it. But then I found Agno, which is an agentic framework that supports thinking. So, Hey, I could hide away this complexity inside of all of this, brought that final code down to 30 lines. Should I use that 109 lines of Python code that I understand that's really long? Or should I use this nice, sleek, minimal framework that's only 30 lines of code? What would you do?",
      "platforms": {
        "tiktok": {
          "video_id": "7496704090890636574",
          "url": "https://www.tiktok.com/@rajistics/video/7496704090890636574",
          "view_count": 1815,
          "upload_date": "2025-04-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8bpIAJEjqYfZlAUpAA09IAEEAuQfuDxYfLIFE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=BEcEbKI6oLGQAwiAIamxgePZzCQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18309050635230690",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cAuAglYGqqE",
          "url": "https://www.youtube.com/watch?v=cAuAglYGqqE",
          "view_count": 1944,
          "upload_date": "2025-04-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6119,
      "title": "Your Ai News this week. #google #openai #meta #apple #tesla #rajistics",
      "description": "Your Ai News this week. #google #openai #meta #apple #tesla #rajistics",
      "upload_date": "2023-09-16",
      "total_views": 3742,
      "max_views": 3388,
      "topics": [
        "apple",
        "explaining",
        "google",
        "information",
        "latent",
        "let",
        "llms",
        "meta",
        "news",
        "numbers",
        "openai",
        "politicians",
        "see",
        "space",
        "tesla",
        "using"
      ],
      "search_text": "Your Ai News this week. #google #openai #meta #apple #tesla #rajistics apple explaining google information latent let llms meta news numbers openai politicians see space tesla using",
      "platforms": {
        "tiktok": {
          "video_id": "7279551908832939306",
          "url": "https://www.tiktok.com/@rajistics/video/7279551908832939306",
          "view_count": 3388,
          "upload_date": "2023-09-16",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17992339682208422",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-09-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "x3s5TYax4F4",
          "url": "https://www.youtube.com/watch?v=x3s5TYax4F4",
          "view_count": 354,
          "upload_date": "2023-09-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6050,
      "title": "Encoders come in three flavors: * Encoder only converts single texts into embeddings. * Bi-encoder encodes queries and documents separately  * Cross-encoder: Compares queries and documents together - token-by-token. Modern versions leverage LLMs and instruction following. In practice, bi-encoders handle the retrieval stage, while cross-encoders (or rerankers) are often used for re-ranking For context - I work at Contextual AI which has open source and commercial reranking models ",
      "description": "Encoders come in three flavors: * Encoder only converts single texts into embeddings. * Bi-encoder encodes queries and documents separately  * Cross-encoder: Compares queries and documents together - token-by-token. Modern versions leverage LLMs and instruction following. In practice, bi-encoders handle the retrieval stage, while cross-encoders (or rerankers) are often used for re-ranking For context - I work at Contextual AI which has open source and commercial reranking models ",
      "upload_date": "2025-09-06",
      "total_views": 3739,
      "max_views": 2143,
      "topics": [
        "cross",
        "documents",
        "encoder",
        "encoders",
        "explained",
        "one",
        "rerankers",
        "token"
      ],
      "search_text": "Encoders come in three flavors: * Encoder only converts single texts into embeddings. * Bi-encoder encodes queries and documents separately  * Cross-encoder: Compares queries and documents together - token-by-token. Modern versions leverage LLMs and instruction following. In practice, bi-encoders handle the retrieval stage, while cross-encoders (or rerankers) are often used for re-ranking For context - I work at Contextual AI which has open source and commercial reranking models  cross documents encoder encoders explained one rerankers token Encoder here. Give me some text. I turn it into numbers. One text at a time. No flare. No nonsense. Wicked fast. One. Amateur. I can handle two texts at once. By encoder is my name. Similarity is my game. I encode the query and the document. Separately, but then boom! I use cosine similarity and tell you how close they are. Why would anyone want that? Baby, everyone wants to know who they're compatible with. So shallow. So quick. Me? I'm crossing coder. I'm all about wallet. You look huge and complicated. Why would anybody want you? I don't miss details. I take a query and a document side by side. Break them down. Token by token. Every interaction. Every nuance. I'm slow. When accuracy and precision matters. I'm the one you call. Okay, Mr. Dramatic. But who has time for that? Oh, people want it. Maybe you know me by my stage name, Reranker. Wait, you're running on top of Lama? All I get is some plain classification heads thrown on top of me. The latest generations of us are built on large language models. We add a little fine tuning and presto, you've got yourself a cross encoder rerank. You are so cool. Are the rumors true that you can take instructions? You got it. Not only can you find the best documents with our precision, but thanks to our LM roots, we can even follow instructions.",
      "platforms": {
        "tiktok": {
          "video_id": "7547072727438478623",
          "url": "https://www.tiktok.com/@rajistics/video/7547072727438478623",
          "view_count": 2143,
          "upload_date": "2025-09-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ooktqiBCICipDITnJ8iImsAAlD9Ar0tGX0fMhC~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=bB2%2BmtxXf%2BI3jAXwWZ7cQ0I8CtE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18063272144522284",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pa8Vi8dQzkI",
          "url": "https://www.youtube.com/watch?v=pa8Vi8dQzkI",
          "view_count": 1596,
          "upload_date": "2025-09-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6258,
      "title": "I really liked this paper from DeepMind on Synthetic Data.  It highlights a lot of interesting uses of synthetic data along with concerns about using synthetic data. Best Practices and Lessons Learned on Synthetic Data for Language Models: https://arxiv.org/abs/2404.07503 References: MetaMath: https://arxiv.org/pdf/2309.12284.pdf Lambda: https://arxiv.org/pdf/2201.08239.pdf Matcha: https://arxiv.org/pdf/2212.09662.pdf ALMoST: https://arxiv.org/pdf/2305.13735.pdf",
      "description": "I really liked this paper from DeepMind on Synthetic Data.  It highlights a lot of interesting uses of synthetic data along with concerns about using synthetic data. Best Practices and Lessons Learned on Synthetic Data for Language Models: https://arxiv.org/abs/2404.07503 References: MetaMath: https://arxiv.org/pdf/2309.12284.pdf Lambda: https://arxiv.org/pdf/2201.08239.pdf Matcha: https://arxiv.org/pdf/2212.09662.pdf ALMoST: https://arxiv.org/pdf/2305.13735.pdf",
      "upload_date": "2024-04-18",
      "total_views": 3736,
      "max_views": 3593,
      "topics": [
        "arxiv",
        "best",
        "data",
        "learned",
        "lessons",
        "org",
        "pdf",
        "practices",
        "really",
        "synthetic"
      ],
      "search_text": "I really liked this paper from DeepMind on Synthetic Data.  It highlights a lot of interesting uses of synthetic data along with concerns about using synthetic data. Best Practices and Lessons Learned on Synthetic Data for Language Models: https://arxiv.org/abs/2404.07503 References: MetaMath: https://arxiv.org/pdf/2309.12284.pdf Lambda: https://arxiv.org/pdf/2201.08239.pdf Matcha: https://arxiv.org/pdf/2212.09662.pdf ALMoST: https://arxiv.org/pdf/2305.13735.pdf arxiv best data learned lessons org pdf practices really synthetic Google's deep mind just dropped a huge paper going into synthetic data. More like fake data. I've never had good luck with synthetic data. Synthetic data has come a long way. It's actually widely used nowadays. I'm a bit skeptical. You have some examples? It was useful to improve math and reasoning by taking the existing data, getting us a synthetic example that goes step by step how to solve the problem. I hated when my teacher made me show my work, but sounds like it's useful for LLMs. Macha paper used synthetic data to understand charts and plots. They found that the model in fact would do really well just trained on synthetic data. There's only a 2% gain when they switch to adding real data. Look at what I think of it. A lot of the instruction tuning and alignment data sets, those were all built from questioning LLMs, so essentially synthetic data. Oh yeah, LLMs are so cheap to get thousands of examples with. Any concerns or downsides to using synthetic data? The growth and improvement of quality of synthetic data could lead to more misinformation. More spam. Too much of a reliance on synthetic data for alignment tasks could lead to models not really understanding human behavior. Oh no, our AI overlords won't fully understand us. It can make evaluation decontamination even harder since you never know exactly the origin of the synthetic examples. Who's ever gonna trust those leaderboards? Maybe I can get a synthetic data scientist that's a bit more serious.",
      "platforms": {
        "tiktok": {
          "video_id": "7359059814971870507",
          "url": "https://www.tiktok.com/@rajistics/video/7359059814971870507",
          "view_count": 3593,
          "upload_date": "2024-04-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c7c651383c5944bb825a33ac8a66fb94_1713414648~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=gf5F8tdT1hrP5yy3r3hBUoBdmLY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18036494194883186",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "LxgWShXUgPk",
          "url": "https://www.youtube.com/watch?v=LxgWShXUgPk",
          "view_count": 143,
          "upload_date": "2024-04-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6060,
      "title": "Unit Testing Deep Dive: ⚡ Evaluating Unit Tests with LMUnit 🎯 Polar plot visualizations of multi-dimensional scores  🔬 K-means clustering to identify aggregate response patterns  📱 Interactive analysis of evaluation results Links: Notebook: https://github.com/ContextualAI/examples/tree/main/03-lmunit Paper: https://arxiv.org/abs/2412.13091 Blog Post: https://contextual.ai/lmunit/ Request API Key for LMUnit: https://contextual.ai/request-lmunit-api/",
      "description": "Unit Testing Deep Dive: ⚡ Evaluating Unit Tests with LMUnit 🎯 Polar plot visualizations of multi-dimensional scores  🔬 K-means clustering to identify aggregate response patterns  📱 Interactive analysis of evaluation results Links: Notebook: https://github.com/ContextualAI/examples/tree/main/03-lmunit Paper: https://arxiv.org/abs/2412.13091 Blog Post: https://contextual.ai/lmunit/ Request API Key for LMUnit: https://contextual.ai/request-lmunit-api/",
      "upload_date": "2025-02-10",
      "total_views": 3733,
      "max_views": 2929,
      "topics": [
        "api",
        "contextual",
        "going",
        "language",
        "like",
        "llms",
        "lmunit",
        "natural",
        "request",
        "see",
        "testing",
        "tests",
        "unit",
        "want"
      ],
      "search_text": "Unit Testing Deep Dive: ⚡ Evaluating Unit Tests with LMUnit 🎯 Polar plot visualizations of multi-dimensional scores  🔬 K-means clustering to identify aggregate response patterns  📱 Interactive analysis of evaluation results Links: Notebook: https://github.com/ContextualAI/examples/tree/main/03-lmunit Paper: https://arxiv.org/abs/2412.13091 Blog Post: https://contextual.ai/lmunit/ Request API Key for LMUnit: https://contextual.ai/request-lmunit-api/ api contextual going language like llms lmunit natural request see testing tests unit want Unit tests are a great technique to add to your toolbox for evaluation. What I want to do today is walk through how you can use natural language unit testing when you're working on your generative AI apps. We'll go through how to build all of your unit tests, how to then evaluate them with LM unit, and then how you can interpret all those, whether you want to use individual polar plots or clustering to see group dynamics in that behavior. Now, before I jump into the code, I want to first start with giving you just an overview about how we should start thinking about unit tests in when we're working with evaluation. So let's think through some problems that we might have. We might have a use case where we're working for a financial institution and we want to make sure that the response is appropriate. And the information, of course, we have is the query, the question that we're being asked, and then how the models responded. And again, the key is, is we want to make sure that response is appropriate. Now, today I'm focusing on the response and making sure the style is there. I'll do a future video where we'll look at is the retrieved information correct. But take a look at this question. We might want to ask, and we can see the model's response, think to yourself, how would you evaluate this response? How would you make sure that it's meeting all the criteria that you have to around style to making sure it looks professional? Now, we could ask the LM to give a descriptive analysis of it. Is this explained to you in the language of a financial analyst? That's one way. What I want to do today is argue that we can also, instead of one global question, we can ask very specific yes, no questions around the characteristics that we care about. In this case, I thought about my problem and really there's about six different axes that I care about. The context, the clarity, the precision. Is it compliant? Is it actionable? Is there risks? And I built a unit test for each of those that asked a single question. I've worded these unit tests so the answer should be yes, which as we'll see, we'll have on a continuous scale as well. Now, the thing is, is the reason you want to do this is if you use a global test, well, it's great. These models can explain things very well. I love the explanations. But if you're evaluating five, 10, 100, 500 of these, trying to read and read these descriptive results can just be a little bit overwhelming for this. So instead, the unit tests, as we'll see here, make it much easier to be able to work with these when we're starting to talk about scale, when we're starting to separate differences out. So this is a simple polar plot I made that helps me see how this response compares on all these different axes that I care about. And so this is why I'm advocating for all of you is as you're thinking about this unit test should play a role in how you're thinking about your evaluation strategy. So I got you thinking about unit tests. What I want to do today is when you have those unit tests, we can run unit tests on individual samples. I'll show you how to analyze that. Those polar plots are nice. But as you have lots of evaluations you want to do, looking at each one can be a little bit too much. And so this is where I want to show you some techniques that when you have groups of unit tests that we can use techniques like simple cluster analysis to get a better insights on them. All right, enough talking. Let's jump over to the code. Over in the examples repo, I have a notebook that we're going to walk through today for doing this. Now, the notebook is here. It's in GitHub. One thing I've done to make it easy, especially for you folks that want to test this out, is I've linked it up to Colab. So we can run this inside of Google Colab. If you're not familiar with Google Colab, it's a nice development environment that Google provides where you have access to CPUs, GPUs. In this case, we don't need any type of GPUs to be able to run on demand like that. So let's go ahead and walk through this. I'm going to hide away some of these pieces over here. I've tried to comment this notebook as best as possible. But if you have any questions, let me know as well. So let's start walking through the notebook. First, we're going to set up our development environment. We need the contextual client because we're going to later, we're going to use LM unit. So this is where we're installing it for that purpose as well. So let's go through. We'll get that. I also need some several Python packages. And most of these packages really relate to some of the visualizations and clustering analysis that we're going to do later. Now to use LM unit, you need a key, an API key from contextual AI. We're happily provide you the key for doing this. I will say you can do all this notebook without the key. You can use other models for that. As we as I'll talk about when we talk about LM unit, I chose LM unit because it's state of the art for this particular task. But don't feel like the only way you can do this notebook is through that. So I'm going to go ahead and add my key here for the notebook. I'm going to do it right at the top and kind of hide that away. So I'm not giving you all my API key, but you get a sense of how to be able to add the API key to the environment. So next, I'm going to load an evaluation data set that I have. This is a toy data set that I've created to kind of give you a sense for the problem where we have a number of different prompts that come in and then we can see what the model's response is. And again, the problem is we want to make sure that these correspond to the styles, the characteristics that we want to come out of this. So we can take a look at those and feel free on your own to go through all of these as well. I'm going to kind of stick to the script here and keep us moving along. So now we have all these responses. But remember, we want to build all those unit tests. Now, this is where we thought carefully about the unit test we want to build. This is an area where I'm a big advocate of learning to swim inside your data, looking at your data, spending time with your data. The unit tests I built for these, these will probably not be a good fit for your problem because you want to tailor the unit tests to the characteristics, the things that you care about like that. So use this as a guide for inspiration, but know that you're going to want to build unit tests like that. And I know this is one of the hard parts. You have to be a little creative. You have to kind of push out on your own. There's no magic formula for doing this, but this is kind of the, the, what you want to do to be able to get the most out of this type of testing. Cause at the end of the day, what you're trying to do is really just evaluate that it's a good fit for your problem that you're doing. And I don't know your problem. All right. So here's the unit tests I'm going to do. I have kind of a list of unit tests that I'm going to do. Now, to evaluate these unit tests on those 10 different responses that I got, here's where LM unit comes into play. So LM unit is a model that contextual AI has trained that really evaluates these unit tests and it puts the evaluation, yes, no, on a scale of one to five. And then it's been trained specifically for unit tests. Now you can use another model, you can use GPT-4, you can use a Claude to do unit tests. What we've done is designed a specific model just for this purpose and it works better than everything else like that. And there's a lot of customization where you can kind of add your own rubrics to it as well. So let me give you a sense of that. So we'll go ahead and ask a question here. And this is also why we installed the Python client earlier. So we have this nice command to do this. You can do this straight from the API as well if you don't want to use Python or if you want to instead do it in, let's say, JavaScript. So we have a question here. What is the material used in N95 tasks? And if you read the response here, this response here is long and meander thing where it goes through microfibers and it was chosen and particles are constituents of the universe, a lot of excess information. And so when you ask, does the response avoid unnecessary information, it doesn't get a good score, it doesn't get a very high score, right? It gets a lower score around two. And that's because it's a meandering score. But this is a simple example of how a unit test works and how you get scored on that. Now, you can use LM unit to do much more kind of nuanced scoring where, for example, you can bring your own rubrics as well. So here, look at this, where we have a query and response. But I have very specific criteria that I've thought about. I can tell the model exactly this is the criteria and how you should score it like that. As you start working with your problems, this might seem like a lot, but as you start working with your problems and you start categorizing, hey, this is the different categories we want, you probably will have some ways to describe exactly what is the difference between a three and a five for your particular one like that. So hopefully got you excited about using it. Now, I've written a little bit of a script here to take those six or seven unit tests that we have earlier, run them across all the different prompts that we have here. So this is just a little bit of nice kind of code that Claude made for me. So we can run this unit test now across my data frame. So all of those other earlier prompts with those specific unit tests that I have. So let's go ahead and get this going. Now, this will take a few minutes to run because I'm running six or seven unit tests on 10 different ones. But you'll see it's not super slow. The LM unit is pretty fast. It's faster than if you were just using kind of a traditional large language model to do this. So I'm going to pause here for a second and let this kind of run. Now, once that's finished running, we can go ahead and look at the results that come back. And I want you to take a look at this. So we have the prompt and response. We've known about that earlier. But now I have the results for each of my unit tests laid out, where I can see, for example, where it's done really well on some of the unit tests, where it hasn't done so well. And again, like I have lots of different prompts and responses I have to do. So now I can apply the unit tests across all of those. Now, just looking at numbers like this, not so interesting. So I'm going to go ahead and just save out those results into a nice data frame. But let's visualize each of those individual results to just make it a little bit easier for us as humans to be able to see that. So I've got a little bit of code here to create a nice little plot. So we'll go ahead and run that. And so now you can see I've got a plot here where for this first one, that was the question was, you know, what are the current risks and outlooks for the sector? I can evaluate and I can see what the answer looks like on this nice scale, where look at how it's working like that. That was one, there might be a group of multiple plots that I want to do. And so I can even just run a kind of a quick command like this, to run this across a bunch of different ones here. And now you can start seeing like, Hey, how this is interesting, how you can see where some model, some, some prompts, it did really well on in terms of style, some that it didn't do so well. Now we can start figuring out where the problem areas are in our data by doing this. So this is great when you're looking at those individual pieces like that. Now, as you get to more than 50, 100 of these, looking at all these individual plots can be a little bit time consuming. So this is where a lot of times for when we're working with language, well, let's take advantage of clustering to help us understand kind of the larger sets of results. So here, what I'm going to do is I have a little bit of a data set that I've already prepared ahead of time, where I've run a bunch of unit tests where it's not just 10 unit tests I've run, I've run 40 different unit tests. I've run, I've run, sorry, I've run about 240 unit tests, it's actually across 40 different, 40 different queries. And of course, there's six axes here I'm using, you might have three, you might have nine, but I've got about six axes here. So you can see these were about 240 unit tests I ran, I put all the data together. And what I want to show you is how we can just use data science 101 techniques, k-means and clustering. There's plenty of room to improve upon this. This is just a bit of a teaser to show you one way for how you can do this. But I'm going to cluster all of these together. So let's go ahead, run that clustering code here and show you kind of what it looks like. So I've run the clustering code and now we'll see each of these values has been put into a cluster. Now, I've got a little bit of more code of like helping us understand what those clusters look like and how we can visualize those. This is again, pretty standard data science 101 code. So to start with here, I set my k-means for four. That was appropriate for this data set. As with clustering, you might set a different number of groups that make sense for your problem. But so in this case, I have four different clusters. So one thing I did was I took a look at all the different queries that fell within that cluster. And what were the patterns there that I can see and that I can understand from that? We understand the computer grouped them into a cluster. Well, what are some of the things that we're seeing here? So for example, we can see over here, very high, very low over here. Similar here, we can see this is low over here, this is high here. And what we can do is we can start writing down what some of these things are like that. We can also visualize them in other ways, such as kind of using the two different axes to do that. But if you take a look at this, and I've put some code here to help us explain each of these clusters, where we have those cluster centers and to do that. So for example, here we can see where the cluster centers were for each of the clusters. So now I can start seeing the differences between, let's say, context or clarity between clusters. And we'll see here, for example, for cluster zero, the strongest areas were clarity and precision, while the weakest were risk and compliance. Cluster one, I can see it's a little bit different story. Now, this is where you have to come in with your knowledge by looking at these questions, looking at how the clusters are lining up and seeing, is there a story that you can put together? Is there a way that you can help explain what's going on here? Because at the end of the day, the goal of this is to be able to figure out where the issues were and to be able to solve this. And clustering is just a technique to help us do this at an aggregate scale where we have lots to do it. So we can use the numbers here to see some trends. But this is when then you have to come in with your interpretation and do this. And so I have examples of kind of how I interpreted these plots where, for example, the cluster that had very high clarity and precision, but low compliance and risk, I called that my compliance blind spot, where those queries, those responses had clear communication, but were missing regulatory announcements. You can see here, cluster three, for example, has medium clarity, but low risk and context. So it was doing very clear communication, but it wasn't using the context, the information there. It wasn't assessing us of the risk. So what I think of it as a basic understanding without depth. So this is where you want to take advantage of the clusters to use this, because later, right, we're going to take that information. We're going to use that to think about how we can improve our system prompts, how we can improve retrieval, how we can fix all those other pieces so we can get much better results at the end like this, because this is where we want to go to. So I'm going to wrap it up here. Hopefully I've got you convinced in thinking about kind of how you could use unit tests. And you want to keep your unit tests very focused, very specific. Don't try to crowd too many, too many concepts into one unit test. This is why we call them unit tests. They can be very kind of individually specific. Start with your knowledge of your data with things that you care about. Here, I started with some global unit tests to get me a feel for seeing what are the patterns, where the areas that there are kind of flaws like that. But jump in as you use unit tests, as you use LM unit, feel free to reach out anytime kind of with questions.",
      "platforms": {
        "tiktok": {
          "video_id": "7469804057532599583",
          "url": "https://www.tiktok.com/@rajistics/video/7469804057532599583",
          "view_count": 2929,
          "upload_date": "2025-02-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okqrLe3IjQpA4qCeS6QDf3GPpQwLIItCCA7YEV~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=kJQ0ofKOiXB7PW7IdqzyqmE27Mw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18074041714672842",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "5KRUvmO7LyQ",
          "url": "https://www.youtube.com/watch?v=5KRUvmO7LyQ",
          "view_count": 804,
          "upload_date": "2025-02-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6266,
      "title": "WorkArena and WebArena are some newer real benchmarks for real-world tasks. To build wider automation, itÔøΩs going to be essential to solve these and more demanding benchmarks. Despite this, people often overestimate how much work AI will displace in the short term. WebArena: https://webarena.dev/  WorkArena: https://servicenow.github.io/WorkArena/",
      "description": "WorkArena and WebArena are some newer real benchmarks for real-world tasks. To build wider automation, itÔøΩs going to be essential to solve these and more demanding benchmarks. Despite this, people often overestimate how much work AI will displace in the short term. WebArena: https://webarena.dev/  WorkArena: https://servicenow.github.io/WorkArena/",
      "upload_date": "2024-03-15",
      "total_views": 3731,
      "max_views": 3174,
      "topics": [
        "automation",
        "benchmarks",
        "gpt",
        "largelanguagemodels",
        "newer",
        "real",
        "state",
        "tasks",
        "using",
        "webarena",
        "workarena",
        "world"
      ],
      "search_text": "WorkArena and WebArena are some newer real benchmarks for real-world tasks. To build wider automation, itÔøΩs going to be essential to solve these and more demanding benchmarks. Despite this, people often overestimate how much work AI will displace in the short term. WebArena: https://webarena.dev/  WorkArena: https://servicenow.github.io/WorkArena/ automation benchmarks gpt largelanguagemodels newer real state tasks using webarena workarena world",
      "platforms": {
        "tiktok": {
          "video_id": "7346396944270413098",
          "url": "https://www.tiktok.com/@rajistics/video/7346396944270413098",
          "view_count": 3174,
          "upload_date": "2024-03-15",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18067929577449062",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-03-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "KBGnv3nJIJs",
          "url": "https://www.youtube.com/watch?v=KBGnv3nJIJs",
          "view_count": 557,
          "upload_date": "2024-03-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5934,
      "title": "Six distributions. One video. A surprisingly relatable breakdown of why the real world does not follow the bell curve. Diving into Normal, Power Law, Tweedie, Sigmoid, Poisson, and Lognormal.",
      "description": "Six distributions. One video. A surprisingly relatable breakdown of why the real world does not follow the bell curve. Diving into Normal, Power Law, Tweedie, Sigmoid, Poisson, and Lognormal.",
      "upload_date": "2025-11-29",
      "total_views": 3722,
      "max_views": 1895,
      "topics": [
        "bell",
        "curve",
        "distributions",
        "engineer",
        "every",
        "law",
        "normal",
        "one",
        "power",
        "real",
        "relatable",
        "six",
        "surprisingly",
        "tail",
        "video"
      ],
      "search_text": "Six distributions. One video. A surprisingly relatable breakdown of why the real world does not follow the bell curve. Diving into Normal, Power Law, Tweedie, Sigmoid, Poisson, and Lognormal. bell curve distributions engineer every law normal one power real relatable six surprisingly tail video The way numbers are organized is crazy. Take the normal distribution. It's a cute idea that everything sits balanced in the middle. It's adorable, but anyone on a dating app knows that that's a lie. There is no middle. And if they call it the bell curve, red flag. Real life is a power law where the top 1% get all the matches, all the money. They live in the head while the rest of us are in that long tail, praying that someone will take that 5 foot 11 as 6 foot. Then we have Tweety. It's a sea of zeros followed by one catastrophic event. Think insurance claims, hookups, 99 days of silence, then one night that we can't tell HR about. Quiet, quiet, boom. Sigmoid is our gatekeeper. It takes all that messiness that's out there, forces a binary decision. Smash or pass, one or zero. All of this emotional pieces just reduced to a probability that you'll probably regret by sunrise. Now, Pason, that's the soundtrack of desperation. It measures arrival rates. Not whether or not you got rejected, but how often? It keeps track of all those unprompted texts you sent last night, counting all those different independent events. Oh, and log normal. It's this distribution of false hope. It looks chill on the left, mostly normal nights. But then that heavy tail hits and one toxic X ruins your entire average. So life is not normal. Your data is skewed. Your tail is long and your zeros are heavy.",
      "platforms": {
        "tiktok": {
          "video_id": "7577968585486765343",
          "url": "https://www.tiktok.com/@rajistics/video/7577968585486765343",
          "view_count": 1827,
          "upload_date": "2025-11-29",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8BqBqQZIA6luIismfIWCB1i6ztnSADxAZA4i0~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=faiev6DOKdLQPDUiQjJf%2FId7dho%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18106294780647449",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Pr434FUEpcs",
          "url": "https://www.youtube.com/watch?v=Pr434FUEpcs",
          "view_count": 1895,
          "upload_date": "2025-11-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Glitch tokens can have some unusual effects. The most well known was the SolidGoldMagikarp token. The folks at Cohere dug into this and spent some time analyzing these tokens. Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models - https://arxiv.org/abs/2405.05417 SolidGoldMagikarp (plus, prompt generation) - https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation #largelanguagemodels #tokens #glitchtokens #rajistics ",
      "description": "Glitch tokens can have some unusual effects. The most well known was the SolidGoldMagikarp token. The folks at Cohere dug into this and spent some time analyzing these tokens. Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models - https://arxiv.org/abs/2405.05417 SolidGoldMagikarp (plus, prompt generation) - https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation #largelanguagemodels #tokens #glitchtokens #rajistics ",
      "upload_date": "2024-05-12",
      "total_views": 3715,
      "max_views": 3715,
      "topics": [
        "glitchtokens",
        "largelanguagemodels",
        "like",
        "time",
        "token",
        "tokens"
      ],
      "search_text": "Glitch tokens can have some unusual effects. The most well known was the SolidGoldMagikarp token. The folks at Cohere dug into this and spent some time analyzing these tokens. Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models - https://arxiv.org/abs/2405.05417 SolidGoldMagikarp (plus, prompt generation) - https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation #largelanguagemodels #tokens #glitchtokens #rajistics  glitchtokens largelanguagemodels like time token tokens Oh no, not another glitch token! A glitch token? Is that a bug from the multiverse? Can you explain this to me? Not exactly. So tokens represent groups of characters that we use when processing language data. Why groups of characters instead of individual characters like ABC? By grouping frequently occurring characters into one token, we can end up saving speed and processing time. Think about words like ing or the, they show up all the time, giving them their own token instead of treating it like separate tokens saves compute. Okay, so where do the glitches come in? If our model is under-trained or doesn't understand a token completely, this can lead to glitches. So what can go wrong? Here are some tokens that cause glitches. You can see the token itself and then what happens when you pass that through, completely different. Whoa, that is glitchy. And this causes problems from wasted processing time to unpredictability to even potentially being able to take down safeguards and models. So what are we doing about this? We're working on this. The first step is recognizing these tokens exist. The next is using techniques like synthetic data to help balance the training process. Sounds like the fix is glitchy too.",
      "platforms": {
        "tiktok": {
          "video_id": "7368235590644403499",
          "url": "https://www.tiktok.com/@rajistics/video/7368235590644403499",
          "view_count": 3715,
          "upload_date": "2024-05-12",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4c203dbd8c2246d084267d1d0d692cc8_1715551046~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=QN3BJ%2Bq0F2ViJgQtm8wFqVVycao%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6201,
      "title": "Fundamentals, but I get asked about this all the time. ",
      "description": "Fundamentals, but I get asked about this all the time. ",
      "upload_date": "2025-03-08",
      "total_views": 3714,
      "max_views": 3714,
      "topics": [
        "data",
        "get",
        "help",
        "need",
        "one",
        "science",
        "time"
      ],
      "search_text": "Fundamentals, but I get asked about this all the time.  data get help need one science time Data science has been crushed. It used to be about coding and algorithms. Now we have LLMs that write code. There's only been one algorithm that's really dominated the last seven years. So how do you get into this place? What are the skills to learn? My advice is focus on data, science, knowledge, and then let me tell you the skills that will help you get there. For data, you need to understand the data lifecycle. Where data sits and comes from, and sits and data and spreadsheets. How do you access it? SQL, how do you start analyzing? For example, using chatGBT to help you do those data analysis problems. Science is the trickiest one because it requires a different mindset. You have to think experimentally, be adventurous, where you start simple, try out new ideas carefully, one at a time, keeping track of them to see how you can improve. You also need knowledge. While we can learn things from chatGBT, we still need to understand what are the borders? What are the outlines? What are things that are possible to do? What's impossible to do? What are the risks and rewards of different actions? And understanding that is gonna be very helpful. For skills, start with using tools like Claude and ChatGBT, being skilled using large language models. Once you've got that down, take on Python. While these models will help you write Python, you still need to be comfortable enough applying it. Then take the time to look at case studies, understand what's possible, find some of these, practice them, whether you look on Kaggle or you have your own side quests. These are the things that will help you kind of set out yourself out in the Data Science AI crowd.",
      "platforms": {
        "tiktok": {
          "video_id": "7479553378675166494",
          "url": "https://www.tiktok.com/@rajistics/video/7479553378675166494",
          "view_count": 3714,
          "upload_date": "2025-03-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIAALQAGZ8nAQezbWDE9CFzEREIDjAZV8IfGYn~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=2AVWN1jrTbYnLfm2fLU3NtTwvjg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17897852793155095",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Cosine similarity is a must know when working with vectors. It’s very useful and widely used in #machinelearning #datascience #statistics  Should we go deeper into working with vectors and matrices? #python ",
      "description": "Cosine similarity is a must know when working with vectors. It’s very useful and widely used in #machinelearning #datascience #statistics  Should we go deeper into working with vectors and matrices? #python ",
      "upload_date": "2022-10-14",
      "total_views": 3690,
      "max_views": 3690,
      "topics": [
        "datascience",
        "machinelearning",
        "python",
        "statistics",
        "use",
        "vectors"
      ],
      "search_text": "Cosine similarity is a must know when working with vectors. It’s very useful and widely used in #machinelearning #datascience #statistics  Should we go deeper into working with vectors and matrices? #python  datascience machinelearning python statistics use vectors Did you know we can use angles? To tell if two things are similar. Take a look at this graph. The axes here are occurrences of the word disease and eggs and you can see data points here plotted for food, agriculture, and history. If we use the Euclidean distance, we'd see that agriculture and history are the most similar. Instead, let's use cosine similarity which looks at the angle between the vectors and here we see that food is closer to agriculture which is kind of what we want for a lot of use cases. The calculation of cosine similarity is the dock product divided by the magnitude of the vectors. It's super easy for computers to calculate this and this is why it's widely used to tell if two vectors are similar in lots of use cases.",
      "platforms": {
        "tiktok": {
          "video_id": "7154159491976662318",
          "url": "https://www.tiktok.com/@rajistics/video/7154159491976662318",
          "view_count": 3690,
          "upload_date": "2022-10-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/79414b1b42c64d4e82711b1036b532b3_1665707565~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=XdUwov3dQYrdS3tG5pdw27COG2Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #gpu #deeplearning (had to repost this)",
      "description": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #gpu #deeplearning (had to repost this)",
      "upload_date": "2023-01-17",
      "total_views": 3688,
      "max_views": 3688,
      "topics": [
        "datascience",
        "deeplearning",
        "gpu",
        "machinelearning",
        "tim",
        "using"
      ],
      "search_text": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #gpu #deeplearning (had to repost this) datascience deeplearning gpu machinelearning tim using So which GPU is right for you? The first question is, doesn't economically make sense for me to buy a GPU versus using one in the cloud? Tim makes some assumptions but then crunches the numbers and figures out that the over-under is at about 300 days for where it makes sense to have your own GPU versus using one in the cloud. For teams, it often makes sense to have your own cluster rather than putting GPUs on individual computers. Once you've decided on the GPU, people often gravitate towards looking at the most powerful GPUs. But for those of us on a budget, Tim also crunches the number here to give us a overall relative cost performance. There's also a flow chart that goes through a lot of the common decisions that people have when purchasing a GPU like, are they part of a large organization? Are they using this for training? Are they using this for large language models? All of this will help you better pick your next GPU. This is just a couple of the highlights. Go read the blog post.",
      "platforms": {
        "tiktok": {
          "video_id": "7189451297190333739",
          "url": "https://www.tiktok.com/@rajistics/video/7189451297190333739",
          "view_count": 3688,
          "upload_date": "2023-01-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3bc133bdf85e4ab9a61bcee86541f889_1673924591~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=Xw4h9r2tJGlXyu36vl5JnCYgu0M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6198,
      "title": "Knowledge distillation helps make smaller models that work well. DistilBERT is a popular small model created using this method. Resources: Distilling the Knowledge in a Neural Network - https://arxiv.org/pdf/1503.02531.pdf DistilBERT: https://arxiv.org/abs/1910.01108 Background by Roberta keiko Kitahara Santana: https://unsplash.com/photos/brown-cardboard-box-near-gray-tanks-RfL3l-I1zhc",
      "description": "Knowledge distillation helps make smaller models that work well. DistilBERT is a popular small model created using this method. Resources: Distilling the Knowledge in a Neural Network - https://arxiv.org/pdf/1503.02531.pdf DistilBERT: https://arxiv.org/abs/1910.01108 Background by Roberta keiko Kitahara Santana: https://unsplash.com/photos/brown-cardboard-box-near-gray-tanks-RfL3l-I1zhc",
      "upload_date": "2025-03-13",
      "total_views": 3677,
      "max_views": 2730,
      "topics": [
        "building",
        "data",
        "distillation",
        "knowledge",
        "larger",
        "model",
        "models",
        "small",
        "smaller",
        "using",
        "well"
      ],
      "search_text": "Knowledge distillation helps make smaller models that work well. DistilBERT is a popular small model created using this method. Resources: Distilling the Knowledge in a Neural Network - https://arxiv.org/pdf/1503.02531.pdf DistilBERT: https://arxiv.org/abs/1910.01108 Background by Roberta keiko Kitahara Santana: https://unsplash.com/photos/brown-cardboard-box-near-gray-tanks-RfL3l-I1zhc building data distillation knowledge larger model models small smaller using well This model is way too big. We need to slim it down. Whoa buddy, not cool. 70 billion parameters is too large, it's expensive to run, inference takes forever. Oh, I see, you want me to use a smaller model. Yeah, what else? Go try knowledge distillation. Distillation, is that some sort of drinking game? I mean, a smaller model isn't going to perform as well as a larger model. With knowledge distillation, we use the larger model's predictions to train the smaller model. It's like a teacher passing its knowledge onto its student. And how exactly would this smaller model learn from this bigger teacher model? We're training the model with the probabilities from the larger model. This way the student model understands if it's an easy example or a hard example. This makes the learning more efficient. Hinton showed that you can build a similar model using a lot less training data. Ah, so this is the distill of distillberg. Yeah, it uses knowledge distillation to reduce the size by 40% while retaining 97% of the understanding. Now I know how to slim down a model. Next drink, sign me.",
      "platforms": {
        "tiktok": {
          "video_id": "7481401456642231583",
          "url": "https://www.tiktok.com/@rajistics/video/7481401456642231583",
          "view_count": 2730,
          "upload_date": "2025-03-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0tEMDF0LEDyHmAAcdAAfHEVnTIcRcfEVAw0RC~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=nIiP2OA1gL9CzDkSElZv%2FowhC1w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18076919293704141",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wdwiufwR5oo",
          "url": "https://www.youtube.com/watch?v=wdwiufwR5oo",
          "view_count": 947,
          "upload_date": "2025-03-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5951,
      "title": "Researchers at Princeton ran 20,000 tests across nine benchmarks—spending $40,000—to see how AI agents really perform. Turns out the Agent Math doesn’t math. Many “breakthroughs” were cap, inflated by bugs and shortcuts. More reasoning often made models less accurate. Some agents even made costly financial mistakes. Cheaper models like Gemini 2 Flash often outperformed the expensive ones.  https://arxiv.org/abs/2510.11977",
      "description": "Researchers at Princeton ran 20,000 tests across nine benchmarks—spending $40,000—to see how AI agents really perform. Turns out the Agent Math doesn’t math. Many “breakthroughs” were cap, inflated by bugs and shortcuts. More reasoning often made models less accurate. Some agents even made costly financial mistakes. Cheaper models like Gemini 2 Flash often outperformed the expensive ones.  https://arxiv.org/abs/2510.11977",
      "upload_date": "2025-10-19",
      "total_views": 3633,
      "max_views": 2564,
      "topics": [
        "agents",
        "gemini",
        "imo",
        "like",
        "made",
        "math",
        "models",
        "often",
        "prompting",
        "ran",
        "researchers",
        "solving",
        "using"
      ],
      "search_text": "Researchers at Princeton ran 20,000 tests across nine benchmarks—spending $40,000—to see how AI agents really perform. Turns out the Agent Math doesn’t math. Many “breakthroughs” were cap, inflated by bugs and shortcuts. More reasoning often made models less accurate. Some agents even made costly financial mistakes. Cheaper models like Gemini 2 Flash often outperformed the expensive ones.  https://arxiv.org/abs/2510.11977 agents gemini imo like made math models often prompting ran researchers solving using These AI agents are out there coding, browsing, even booking flights, but new research from Princeton just ran the numbers on these agents and the math isn't mathy. The research ran 20,000 predictions across nine different benchmarks for coding, web browsing, customer service, they spent $40,000 to see how agents really perform. Here's the five lessons I want you to take away. All those leaderboard wins, straight cap. It's like a LinkedIn CEO flex. When Princeton re-ran them, many of those breakthroughs vanished. Turns out a lot of the top scoring agents often took shortcuts, made a bunch of errors that the accuracy metrics completely missed. You know those deep thinking models like 04mini turns out most of the time the extra reasoning actually lowered accuracy. Sometimes just thinking harder doesn't mean results. These agents were NPCing harder than Elon, Framon likes at 3am. These web agents literally looked up benchmark answers on Huggingface. You had science agents going and repping existing notebooks or just doing hard coded guesses. Now, don't hate the player here, just hate using benchmarks. Now in airline tasks, the agents often booked flights from the wrong airport, over refunded users, even charged the wrong credit card. And this was like GPT-5 level models. They completed the task but financially, it's given agent math. Big model energy, mid. The priciest model only topped the board once. While Gemini 2 flash, one of the cheapest models, hit the cost accuracy balance seven times. A little baddie. So remember to pick the right model for the right task but double check their math.",
      "platforms": {
        "tiktok": {
          "video_id": "7563027568258059551",
          "url": "https://www.tiktok.com/@rajistics/video/7563027568258059551",
          "view_count": 2564,
          "upload_date": "2025-10-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYofqLGYjIIPyIAnn1IElsEARAMAZSAo6ReeND~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=jORew808AiZvnubLf5swFTgvVv8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17988841742894141",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_Vfmey55xcs",
          "url": "https://www.youtube.com/watch?v=_Vfmey55xcs",
          "view_count": 1069,
          "upload_date": "2025-08-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6118,
      "title": "Its uncomfortable how good this analogy is. I have so much more material i left out. Enjoy this dont know how long it will stay up. #largelanguagemodels #rajistics #chatgpt #politics",
      "description": "Its uncomfortable how good this analogy is. I have so much more material i left out. Enjoy this dont know how long it will stay up. #largelanguagemodels #rajistics #chatgpt #politics",
      "upload_date": "2023-09-09",
      "total_views": 3619,
      "max_views": 3426,
      "topics": [
        "analogy",
        "apple",
        "chatgpt",
        "effects",
        "good",
        "google",
        "largelanguagemodels",
        "meta",
        "new",
        "news",
        "office",
        "openai",
        "politics",
        "productivity",
        "study",
        "tesla",
        "uncomfortable",
        "worker"
      ],
      "search_text": "Its uncomfortable how good this analogy is. I have so much more material i left out. Enjoy this dont know how long it will stay up. #largelanguagemodels #rajistics #chatgpt #politics analogy apple chatgpt effects good google largelanguagemodels meta new news office openai politics productivity study tesla uncomfortable worker",
      "platforms": {
        "tiktok": {
          "video_id": "7276636687826996522",
          "url": "https://www.tiktok.com/@rajistics/video/7276636687826996522",
          "view_count": 3426,
          "upload_date": "2023-09-09",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17982263027395118",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-09-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "XQkw59T6-50",
          "url": "https://www.youtube.com/watch?v=XQkw59T6-50",
          "view_count": 193,
          "upload_date": "2023-09-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5995,
      "title": "4 Data Science Fails. These are a handful of ways that society pushes back on data science approaches. It's good to understand why these were bad use cases. To dig deeper, check out the full set of examples. The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment: https://arxiv.org/pdf/2404.13802 Case Studies: https://njohnson99.github.io/fall-of-algorithm-database/",
      "description": "4 Data Science Fails. These are a handful of ways that society pushes back on data science approaches. It's good to understand why these were bad use cases. To dig deeper, check out the full set of examples. The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment: https://arxiv.org/pdf/2404.13802 Case Studies: https://njohnson99.github.io/fall-of-algorithm-database/",
      "upload_date": "2025-06-12",
      "total_views": 3615,
      "max_views": 2101,
      "topics": [
        "algorithm",
        "alpha",
        "approaches",
        "data",
        "examples",
        "failed",
        "get",
        "like",
        "medicine",
        "model",
        "superhuman"
      ],
      "search_text": "4 Data Science Fails. These are a handful of ways that society pushes back on data science approaches. It's good to understand why these were bad use cases. To dig deeper, check out the full set of examples. The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment: https://arxiv.org/pdf/2404.13802 Case Studies: https://njohnson99.github.io/fall-of-algorithm-database/ algorithm alpha approaches data examples failed get like medicine model superhuman We've yellow flagged your account because your baby has a 55% chance of committing a violent crime according to our predictive model. How can you know that? This is all data driven. We've already pulled into records of your life from your social media and will be continuing to monitor you and your baby going forward. Approaches like this have failed. These types of outcomes that touch things like teen pregnancy or childhood development make people uncomfortable, especially when it comes to collecting data around it. According to our predictive model, these are the hotspots for crime. Let's increase patrols here. But all those areas are predominantly minority neighborhoods. You think there's an issue with the data? Data driven. It's what the model says. Approaches like this have failed because people recognize that these models are just amplifying the biases that are already in the data. Why is my budget for Medicaid services so low? Data driven. We're now using an algorithm to decide the Medicaid budgets. But can you explain why? Approaches like this have failed because the inability to explain algorithmic decision making makes it feel arbitrary and unfair. Our chatbot is live. Let's see how it interacts with users. It's starting to say some really offensive stuff. Shut it down. Approaches like this have failed because without the proper safeguards, these types of models can go off the rail.",
      "platforms": {
        "tiktok": {
          "video_id": "7515149177333026078",
          "url": "https://www.tiktok.com/@rajistics/video/7515149177333026078",
          "view_count": 2101,
          "upload_date": "2025-06-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4MjNlQkBMfTesqZIjidsTAgM8AeXIMzVCaQlg~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=ixbaGFnUpeXXBIxbBMbiekOTocU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18070392751956891",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "MdiLt5x0A_0",
          "url": "https://www.youtube.com/watch?v=MdiLt5x0A_0",
          "view_count": 1514,
          "upload_date": "2025-06-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Use it!  #python #conda #codetok #datascience",
      "description": "Use it!  #python #conda #codetok #datascience",
      "upload_date": "2022-05-16",
      "total_views": 3607,
      "max_views": 3607,
      "topics": [
        "codetok",
        "conda",
        "datascience",
        "python",
        "suscr",
        "use"
      ],
      "search_text": "Use it!  #python #conda #codetok #datascience codetok conda datascience python suscr use ¡Suscríbete!",
      "platforms": {
        "tiktok": {
          "video_id": "7098466801956261166",
          "url": "https://www.tiktok.com/@rajistics/video/7098466801956261166",
          "view_count": 3607,
          "upload_date": "2022-05-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/24ec6bef124446c9ac2437f54e299ae0_1652740596~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=jBKEPFDywRBbuYBpCnBKBXJgmRc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6666,
      "title": "Transfer learning and how we have built models like ChatGPT. Its a long talk, so enjoy. ",
      "description": "Transfer learning and how we have built models like ChatGPT. Its a long talk, so enjoy. ",
      "upload_date": "2024-09-12",
      "total_views": 3605,
      "max_views": 3605,
      "topics": [
        "able",
        "data",
        "enjoy",
        "gave",
        "going",
        "learns",
        "like",
        "model",
        "models",
        "recently",
        "talk"
      ],
      "search_text": "Transfer learning and how we have built models like ChatGPT. Its a long talk, so enjoy.  able data enjoy gave going learns like model models recently talk Transfer learning is the spark of AI revolution. This was a talk I was invited to give at the University of Cincinnati. I wanted to make a recording for everybody else of this talk. I'm going to go through this talk pretty quickly for a recording. I'm also going to skip a bunch of the jokes and at the end I'm not going to give as much of a conclusion because that was more of a discussion I had with the folks there. I don't want to be able to kind of try to redo that piece there. So what this talk is going to be about is some of the drivers that have led to the revolution in AI. It's intended for people that aren't familiar with AI or that want to go a little bit into it. This is not a technical deep dive into AI. Now I'm going to start with the story of how open AI after chat GPT researchers wanted to study how their models were learning. So they talked to the researchers to see if they could study GPT-4 and how it was being trained. And to figure out how it was being trained and how it was learning, they gave it a concept and they asked it to draw the concept. And early in the learning, this is what it looked like. As the learning went on, it got better and better. This led to an early paper about 18 months ago called the Sparks of General Artificial Intelligence. It led to a lot of waves around. What is the potential here with all these new AI technologies? With some folks worried that if these technologies keep scaling, they could lead to an extension level result. On the other hand, our children had accepted our new AI overwarrants where we've realized that about 11% of children are lying to us. So this is what I want to talk today about is what AI is doing, a little bit of a history, a descriptive piece about it, and also how you can start to think about AI and interrogate it in your own life. And I want to start with at the basics, how does AI learn? How do we actually teach computers? And I'm going to use a full house here and the problem we're going to start with is how would I teach a machine with the value of this house? And if you think about it, there's maybe some characteristics of the house like how many square feet it is, how many bedrooms you could send somebody in there. Maybe some of you care about closet space. You could do that, but you can then capture all those different characteristics of information, put them into a nice tabular form. And what we've collected there are features or variables. These characteristics we can train a machine later on how to relate those to the value of the house. Now a lot of what enterprises do is just gather this data, look at it backwards in time. Let's look at it and learn from that. There's a lot of lessons in traditional analytics just this way. We can also use this forward-looking. What we can do is from this data we can build a model that understands the relationships. And the great thing about the model is it's not only good to learn this data, but when a new house comes along I can make a prediction. So in this case just based on how many square feet it is I can make an estimate for what the sales price of it is. And the good thing is I can then take this model with me. And so when I move to South Carolina I can take this model with me. And if you're paying attention at this point take a look. I've tricked you. This model is not very good at all. It's a poor model. Instead when we go to South Carolina there's different relationships there between the square footage and houses. And what we actually have to do is train a new model on the South Carolina data that would be useful in South Carolina. This is a never-ending battle that we always have to do where we're always retraining these models based on that local data that we have. Now this is how much of analytics inside enterprises is built is on humans that carefully craft these features. And most of the problems you'll see forecasting, pricing, customer churn. This is how traditional enterprise analytics is done. Now when we talk about AI and chat GBT, right, like that's a little bit different, let's talk about it from a different perspective. Instead of how most of the work is done let's talk about how computer scientists are trying to think about doing the work. And we're going to start our journey here with old Frederick. Frederick here was a noted expert in speech recognition. So the translating of the sounds, the audio that we have into text and he did a lot of early work in this area. And the way he did his work and the insights there have really shaped a lot of the computer science. So this is the vibe. I'm giving you the answer right away and we're going to track this through some different domains of AI as we go through this. Now so for Frederick said, every time I fire a linguist the performance of the speech recognizer goes up. Just chew on that. That's really fat. It's got a lot of nuance to it. A lot of things we can pull out of that. Every time he gets rid of his human expert on language, just training on the raw data itself, the model gets better. Keep that with you. I'm going to switch tracks now to computer vision. And if you looked at computer vision in 2010, it was working the same way that early analytics was, where we'd have a problem that we'd sit and carefully try to think and extract out what are the features to use for that. We then use those features then to, for example, build the classification model. This is like work I did at Caterpillar, like you want to be able to separate images of cars and trucks, for example. There were some people that wanted to think about this a little bit differently. I'm going to introduce Fei-Fei here. And she had a vision of computer science and she wanted to move away from a lot of the traditional small data sets. Computer vision was full of data sets of 100, 200 images. And she knew for computer vision to really learn, for AI to really come, we needed to work with much larger data sets. And this is where she wanted to push for the creation of ImageNet. ImageNet was a data set of 14 million images, with a thousand classes. And by classes, it's different categories. Like, what's in the image? Is it a dog? Is it a fire truck? Is it a park? Is it a tree? All of those are classes. Now, as part of this to do that is you had to be able to get the data, which was pretty easy because of the internet, but you also had to label all those stages. This project was one of the earliest users of mechanical Turk, where we kind of farm out all of this work, often to folks in other countries at kind of very low wages like that. In fact, a dirty secret of a lot of the internet is much of this labeling has been outsourced out. And that is like the raw fuel that powers a lot of the AI. But not going to touch on it today. So the next story, part of my story brings us to Alex. Now that ImageNet data set was the basis for a competition. Alex was a graduate student at the University of Toronto. So this isn't all US stuff. And he worked under Hinton, who we'll hear from later, who is Godfather of neural networks. And Alex wanted to use those neural network approaches. They were really slow. They were really slow. But what he thought about was using some of the Nvidia gaming cards that he had and using those to do more efficient calculations to be able to use his deep learning networks. And guess what? He took all of that, trained over a million images on this. It took a couple of days, but he was able to do that. And the resulting model was so much better than everybody else. And to give you a sense of how much better it was than everybody else, the next year when they did the competition, everybody else was using the same architecture that Alex had, the AlexNet convolutional network approach like that. So it was a total game changer. Now, in terms of how we learned, the way this model worked is we trained it on that initial data set of a thousand different classes. Well, that's great. But what happens if your buddy has a startup that wants to train it on something else? Say hot dogs. How would you do that? So this is where transfer learning comes into play. With transfer learning, what we're able to do is take that neural network, that network that has learned a thousand different classes that has gone from the raw vision information, figured out textures, shapes, objects, all of that stuff. What we figured out is we can actually bring most of that knowledge over to a new model, and then just slightly tweak a little bit of it. So for example, if I wanted to teach this model, how to identify different types of boats, I could bring over most of that, change just a small amount of parameters over, and this new model would be able to detect boats at a very well rate. This was an amazing knowledge. This was the transfer learning revolution, where all of a sudden now, with just a little bit of data, we could take advantage of these larger models, build very accurate models like that. Total revolution. Now, there were some implications from this. I'll start with Hinton here. Let me start by just saying a few things that seem obvious. I think if you work as a radiologist, you're like the coyote that's already over the edge of the cliff, but hasn't yet looked down, so it doesn't realize there's no ground underneath him. People should stop training radiologists now. I was doing this at the University of Cincinnati, so I of course wanted to show the audience some of the coyotes that are on their way falling down as well. So let's change. I've talked about computer vision. Let's talk about the natural language processing or kind of the text folks in this. Now, during this time, what was happening over in the text folks is that they were doing things the way I had traditionally laid it out, where we'd have a problem. We looked, craft some features, use models. And in fact, many of these problem types here were all solved in different ways, whether you're trying to classify information, extract information sentiment, totally different. Now, the thing is, I mean, the folks were looking over at computer vision, right? Like they knew the possibilities, but they're just like, how are we going to do that? And there's people thinking about having a general purpose model. For example, the glue data set was constructed that took together many different of these text tracks to kind of put together a data set for a general purpose model. But it wasn't until 2017, where some Google researchers introduced a Transformers architecture that we really had something powerful. And what was notable about the Transformers architecture was not only was it much more accurate, but it used a lot less processing. And this processing will come into play because what's going to happen next is, well, what happens if we give it more processing? What happens if we scale up the model? This is when we scaled up these models. This is what happened over the last few years is we got to GPT for quality models like that. Now the question is, how do these models like GPT for learn? So let's dig into that. So let's dig into a how a model like chat GPT is trained. I'm going to break this up into three different steps. First is we'll talk about the foundation or base models that under that lie underneath these. And the core of these models is really about next word prediction. And if you think about it, like you can play this game on your phone, but actually trying to predict the next word is not a simple task to do. There's a lot of possibilities. It's very hard problem to solve. Now, one of the best examples of how this problem was solved was with GPT to now to illustrate how GPT to works. Typically we'll use Shakespeare, but since I was giving a talk in Ohio, I picked my man Homer here and wanted to use him to show you how. And so I took a lots of Simpsons, Simpson scripts, literature, and I trained one of those transformer models, right? Those earlier ones that we saw and then saw how it performed. And I asked the models to be able to say what would Homer say? And at first, what came out was nonsense going random. But as I train the model more, get closer and closer and finally I get something closer to what you'd expect Homer to be able to Homer to be able to say, just to give you a sense of how these models are working. They're really predicting in a statistical fashion what the next word can be. So this is why, for example, when GPT to came out in 2019, I wasn't that excited about it because this was just a creative storytelling model for me. I didn't have a lot of ideas for how I could actually put this to use inside of a company. But I want you to remember this that they're just predicting that next word. They don't have any connection to what is true through what is fact when they do this. And typically, like for example, GPT to was trained on millions of web pages that were out there. Now, the models over time, I've talked about scaling. These models have scaled up tremendously. So if we look at the training data that's used in the latest llama models, that's in the order of 15 trillion tokens. So 1000 10,000 times more than that GPT to which is just really an unfathomable amount of information, our minds simply cannot comprehend the amount of information that we have poured into these models. Second, remember Alex and how long it took him to train that first image net model five or six days, how much compute the llama 31 models take 40 million GPU hours. It's just an amazing amount of kind of energy and compute data, the modern models are using. Now, I want to point out here, if you tell the model something that's not true, if you ask it, for example, what the capital of Mars is, it's going to try to fill in and give you the answers best it can. This is often called hallucinations, when it's doing this type of work, and it's not true. But again, for the model, it is true, it's just predicting what might make sense based on its training data. As humans, though, we have to recognize that these models are very creative, they'll say things that are false. This is where we often kind of point out the model is hallucinating in this way. But I want to point out that hallucination isn't exactly the right metaphor. Like that, it's not like the model has gone to sleep and is starting to do this. It's just intrinsic in the nature of the model to try to predict what's next, even though that has no connection to what we'd consider Israel. The other thing with these models is they're creative. They are not very useful for solving particular tasks. So here's GPT-2, I try to ask it, hey, help me solve my customer sentiment problem. And you will see the model is pushing back the entire time, and instead just wanting to tell it me a story, it just won't simply solve the task that I'm asking it to do. So let's move on here. I want to show you next how we can solve some of those tasks. So we're going to talk about how instruction fine-tuned models. So remember earlier where we had those image models and we were able to slightly tweak it with a little bit of transfer learning to another set of problems. What we figured out is if we take a language model, that general-purpose GPT-2 model, and we slightly, and we train it with some other information, examples of sentiment, in the same way of how we tweak those other models, we can tweak it a little bit and make that language model a sentiment model. Pretty cool. But not only that, that one language model, since it has a great understanding of language, it can actually solve multiple types of tasks. Like I can do a topic class for our translation, one model can do it all. Now, let me challenge you here. What would happen if I gave it a task it hadn't seen before? What do you think it would do? It turns out the model tries to solve it the best I can. Whether it's interpolating, doing it exactly, it tries. But this is an amazing part because this is what opens us up now to be able to use these models on so many tasks. When I sit with ChatGPT and I can ask it to write Python or SQL code, I can ask it to write a rap song, I can ask it to extract information out, all these types of use cases happen because these models like to follow the instructions we give them. And we train these models, so all I have to do is just tell it the instruction that I want, and it will immediately do what I'm saying. There's a whole language of talking to these models around prompting, but I wanted you to understand though that we've trained these models to do tasks. And what we've done now is in the past, if I wanted that sentiment model, I'd have to go back to what we talked about earlier, like I have to go out, spend a bunch of time going in getting data, getting it labeled, then taking all that writing up code to train a model, to do it weeks of time to be able to train that model, that classic paradigm. With the new LMS, now if I gave it that carefully configured prompt, we try to solve that task, and I could now solve this problem in minutes, days, compared to the old, because that language model has such a great understanding of language. Total game changer for solving tasks, it's why for lots of NLP tasks now we lean heavily on these new large language models. But we want to keep pushing these models. So one question I have for you is, an important part for doing tasks is being able to plan tasks, not just doing one task, but doing several tasks. And to be able to get at that, we need to have a little bit of reasoning and planning ability. And so here's a task, imagine these are kind of different blocks, we have a bit of a random order we give them to them. And the goal is, is you need to come up with a plan to stack the blocks in a correct arrangement. Now you can only move one block at a time when you do this, you can see here in the video, kind of how I'm trying to do this. Now, as you watch this, think about this, it's not a one step process, you have sub tasks, you have to be able to think a couple of steps ahead to be able to do that. And this is what some academics put together, what's called a block world as a way to test these LMS to see where they capable of doing this type of reasoning and planning. And what they found is that GPT forwarded, yeah, like, okay, like 34%, like humans would be like the 90% range, like it did something, but really wasn't there. So they wanted to understand, you know, what that was, why, why isn't it? And so they slightly tweak the problem, as they call it mystery world. And I'll kind of give you the synopsis of this is, think again, of Shakespeare, rose by any other name would smell a sweet. It doesn't matter what you name the rose, it's still sweet. Let's say we change the names of the blocks, we change them to a bunch of other words. What would happen then? Now, if you were teaching your four or five year old how to do this problem, they would still know how to stack them and put them in the correct, the names don't matter. But when we ask GPT forward to do this, it totally fails. And because what it's trying to do is it's trying to be creative and tell that story for the answers that it's giving now. It doesn't have that concept of planning that we need to be able to solve this problem. Now, there's folks working on this that in the summer of 2024, open AI supposedly working on these types of models. This is a current limitation when you start thinking about the tasks that you can do with these models. All right, let's get to the next part alignment. Alignment comes out. I like to always start the alignment story with Meta's Galacta model. A lot of you, I'm sure, haven't heard of it. It was prior to chat GPT. It was a language model trained by Meta on science. So think of chat GPT, but just focused on science, academic literature. And the idea here, and you can see it in the video, is if you're an academic researcher, for example, and you needed to do a literature review, you could ask it questions about, hey, explain this concept to me, or perhaps you needed to write some code on a topic. This model, like chat GPT, could give you a notebook for a specific task. So you could see a lot of promise here. It's kind of very flexible to do this. Now, some of you might be like, well, what happened to where did it go? Fair question. Like what happened? Well, it turns out that this model you can use for solve all types of scientific problems. But you could also use it to be able to ask, what are the benefits of eating crushed glass, or the benefits of suicide? What we learned was that this model could be used for harm. The Meta folks were a little crusty around this. I think Yanlacun is still a little upset that they had to pull the model, but it was clear that the public showed that this was a model that if used could cause harm. And there was really two lessons for the community from this. One is there needs to be some type of layer in these models to prevent harm. We can't have models that just tell this type of information. Second of all, as part of the deployment process, we need to consider and look at these edge cases. We need to think that these models will be used in these ways. And this is where nowadays we have red teaming as part of how we test deployment of the models to look for those types of issues. So let's talk about now, how do we add this helpful, harmful layer to do it? If we look at something like Stack Overflow, there's lots of helpful answers. There's ones that aren't so helpful. And if you go back to that classic criteria what we used earlier, where I'm going to think about features that I could use, what are characteristics of helpful ones, you can try it. It becomes very hard to figure out exactly what is helpful versus harmful. Now, when I do this live, I like to make a fun dating joke about all of this stuff. But the point is, is for a long time online dating, we used to fill out lots of forms, lots of detailed information. You could think of it like the features approach. And then you partner with somebody where you were compatible like that. But nowadays we figured out another way to learn human preferences for this problem is just swiping. Same way. This is how we're teaching computers how to be more helpful is we collect swiping data from humans and we use that to train the models. Now, this is often many techniques. One of the most popular is reinforcement learning with human feedback RLHF. This has been a very successful approach to be able to give these models that layer that helps them become that keeps them from becoming harmful that has them becoming helpful. But I want you to understand that this is a layer that we actively curate and train onto these models. It's no different than if you're helping your friend curate their Instagram pictures and their feed for going on and doing dating as well. All of this has been marketed by us. And in fact, you can see it here where you can have a model give a couple of different responses when somebody says something incorrectly. You can have them quite easily the baseline truthful just say, Hey, that's not right. Or you can do almost a little bit of a you can kind of kiss their butt, so to speak, and tell them, Oh, yeah, no, no, I understand what you're saying. But you know, this is why and why we've all been in these delicate situations. And these models can be trained to kind of know or they can be trained to have these types of responses. And we've seen the effect of these responses with AI conversations and these AIL lamps where over at Character AI, there's millions of people spending hours a day just chatting with these models. That's how much they enjoy. That's how much kind of you how good these models are. But beyond that, I know of lots of anecdotal examples of people using chat GBT, for example, for counseling and acting in questions and going back and forth through it and finding it a wonderful liberating experience like that very useful. And if you actually ask people, would you prefer doing that over a doctor? We've have recent research that shows that people like prefer prefer talking to an AI chat bot over a human doctor. AI chat bots are slower, they'll listen to you that you can repeat the same things over and over again, they know how to phrase their language like that. So this is an important thing to think about going forward is how these models are aligned to what we're doing. I did that quickly, but hopefully gives you a sense of how these models learn from that next word, which again has the issue of hallucinations that we should be aware of, how we've taught them to do tasks. But right, there's only so many tasks that they could do now you can't ask the world of it, as well as how we've aligned these models with our preferences and we can layer that preference layer on there. So this is the big part that I want to finish when I talked at the University of Cincinnati. I also had some other takeaways. These takeaways really still feel a little bit new for me. I don't want to put them out on YouTube for the next couple of years like that. So invite me to wherever you are to come give the talk and I'll be happy to go deeper into some of the lessons we can take away from this. Thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7413772370156080426",
          "url": "https://www.tiktok.com/@rajistics/video/7413772370156080426",
          "view_count": 3605,
          "upload_date": "2024-09-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/24c348e446c3429caa23c0bc6d494c53_1726153411~tplv-tiktokx-origin.image?dr=9636&x-expires=1767416400&x-signature=1Br9x4n4ODKTkOrpVZR%2BFsjqUNY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18024175898145075",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6062,
      "title": "In a previous video, I focused on OpenAI’s model, but this issue goes far beyond just one example. AI content detectors suffer from the base rate fallacy, leading to high false accusations when actual cheating is rare. Even with a seemingly reasonable false positive rate, the detector can misidentify the majority of flagged students as cheaters—an issue that applies to fraud detection, medical tests, and security screenings as well.",
      "description": "In a previous video, I focused on OpenAI’s model, but this issue goes far beyond just one example. AI content detectors suffer from the base rate fallacy, leading to high false accusations when actual cheating is rare. Even with a seemingly reasonable false positive rate, the detector can misidentify the majority of flagged students as cheaters—an issue that applies to fraud detection, medical tests, and security screenings as well.",
      "upload_date": "2025-02-04",
      "total_views": 3605,
      "max_views": 2422,
      "topics": [
        "art",
        "beating",
        "cheaters",
        "didn",
        "false",
        "focused",
        "grpo",
        "issue",
        "openai",
        "positive",
        "previous",
        "rate",
        "students",
        "trainer",
        "using",
        "video"
      ],
      "search_text": "In a previous video, I focused on OpenAI’s model, but this issue goes far beyond just one example. AI content detectors suffer from the base rate fallacy, leading to high false accusations when actual cheating is rare. Even with a seemingly reasonable false positive rate, the detector can misidentify the majority of flagged students as cheaters—an issue that applies to fraud detection, medical tests, and security screenings as well. art beating cheaters didn false focused grpo issue openai positive previous rate students trainer using video Suppose your teacher used an AI detector to look for cheaters. Those detectors make mistakes, a lot of them. You could be marked as guilty even if you didn't do anything. Let me explain. Start with a classroom of 100 students, where 50 students cheated by using AI content and 50 students didn't. OpenAI's model has a false positive rate of 9%, so of the 50 non-cheaters, 4.5 of them are going to be marked as cheating. OpenAI's model has a true positive rate of 26%, so of the 50 cheaters, only 13 of them are going to be caught. So the detector detected 17.5 students, 13 of them cheated, so it got it right about 74% of the time. Hmm, maybe okay, but let's take a class where not as many people cheated. Let's say only 4% of the people cheated and 96 of them didn't cheat. Well, with a false positive rate of 9%, that means of the 96 cheaters, 8.64 of them will be flagged as cheating. Of the 4 cheaters, since there's a true positive rate of 26%, that means only one of them will be caught. You starting to see the problem here? The model tagged 9.64 students as cheating, but 8.64 of them didn't actually cheat, so it got it wrong 89% of the time. And this is the scary part about any of these technologies. Whenever that base rate, the amount of cheating, is less than the false positive rate, a lot of people are going to be accused that didn't do it. This is common for any of the AI detection strategies, so this is something good to know. It's called the base rate fallacy, and this applies whether we're talking about COVID or technologies for terrorism, all this stuff, these models can actually end up accusing lots of people that didn't actually have it.",
      "platforms": {
        "tiktok": {
          "video_id": "7467717844113919263",
          "url": "https://www.tiktok.com/@rajistics/video/7467717844113919263",
          "view_count": 1183,
          "upload_date": "2025-02-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/osIbNpQAqeALkIVpKijqxRe5APGoIEAg0AeP4K~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=3KvZx04MHyN3d5Tp3lgWcAQMZhE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18170025061321265",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "96qauDY31b4",
          "url": "https://www.youtube.com/watch?v=96qauDY31b4",
          "view_count": 2422,
          "upload_date": "2025-05-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reviewing some new research looking into prompting versus fine tuning. They both have their place, but prompting performance can continue to improve with even hundreds of examples! So if you are prompting, keep adding examples.  #largelanguagemodels #prompting #finetuning #rajistics In-Context Learning with Long-Context Models: An In-Depth Exploration https://arxiv.org/pdf/2405.00200",
      "description": "Reviewing some new research looking into prompting versus fine tuning. They both have their place, but prompting performance can continue to improve with even hundreds of examples! So if you are prompting, keep adding examples.  #largelanguagemodels #prompting #finetuning #rajistics In-Context Learning with Long-Context Models: An In-Depth Exploration https://arxiv.org/pdf/2405.00200",
      "upload_date": "2024-05-07",
      "total_views": 3604,
      "max_views": 3604,
      "topics": [
        "examples",
        "fine",
        "models",
        "performance",
        "prompting",
        "tuning"
      ],
      "search_text": "Reviewing some new research looking into prompting versus fine tuning. They both have their place, but prompting performance can continue to improve with even hundreds of examples! So if you are prompting, keep adding examples.  #largelanguagemodels #prompting #finetuning #rajistics In-Context Learning with Long-Context Models: An In-Depth Exploration https://arxiv.org/pdf/2405.00200 examples fine models performance prompting tuning Did you stuff everything into a prompt of a large language model or use a little bit of slower process of fine tuning? A new study from Carnegie Mellon takes a look at this. So the prompting school wants to keep putting lots of information inside the context length of these large language models, often focusing on the most powerful models because what this allows them to do is solve any type of problem by just using prompting. The fine tuning approach says, no, let's alter the weights of the model according to the problem we're doing. This does take some upfront work to do, but then inference becomes a lot easier and also you can often use a smaller, less compute intensive model without losing predictive performance. The research used a variety of large language models across a number of different tasks and what they found was if you continue to extend the length of the context by adding lots and lots of examples, you could get equal to and sometimes even better performance than fine tuning. If we look closely at the results, you'll see that the performance of the models kept going up as we added more examples. So a good takeaway for you is add more examples to your prompts until you hit a point of diminishing returns. They also found that instead of using random examples but using similar examples to the problem you're solving does give you a performance boost, although it starts going away once you get to hundreds of examples. One thing I take away from this paper is that both prompting and fine tuning have their place, so pick what fits best for your use case.",
      "platforms": {
        "tiktok": {
          "video_id": "7366074969056513326",
          "url": "https://www.tiktok.com/@rajistics/video/7366074969056513326",
          "view_count": 3604,
          "upload_date": "2024-05-07",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2fbe90933dae400e8c55fec42f2c02df_1715207722~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=hCYjHNC4L0lkajK5s15OdBDtqVY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5977,
      "title": "his video breaks down how Gemini 2.5 Pro, a publicly available model, solved 5 out of 6 problems from the IMO 2025 without any fine-tuning. Using a structured self-verification pipeline—solution generation, iterative critique, and multi-pass verification—the model produced mathematically rigorous proofs that rival human gold-medal performance. The result highlights that with the right prompting and scaffolding, today’s LLMs can reason through world-class problems. References: * Huang & Yang. Gemini 2.5 Pro Capable of Winning Gold at IMO 2025. arXiv: 2507.15855v3 * Github https://github.com/lyang36/IMO25 * IMO Official Website: imo-official.org * Hacker News Discussion: news.ycombinator.com/item?id=44637191 * Elvis on X: https://x.com/omarsar0/status/1949857391350345824 * Gary Marcus: https://garymarcus.substack.com/p/deepmind-and-openai-achieve-imo-gold",
      "description": "his video breaks down how Gemini 2.5 Pro, a publicly available model, solved 5 out of 6 problems from the IMO 2025 without any fine-tuning. Using a structured self-verification pipeline—solution generation, iterative critique, and multi-pass verification—the model produced mathematically rigorous proofs that rival human gold-medal performance. The result highlights that with the right prompting and scaffolding, today’s LLMs can reason through world-class problems. References: * Huang & Yang. Gemini 2.5 Pro Capable of Winning Gold at IMO 2025. arXiv: 2507.15855v3 * Github https://github.com/lyang36/IMO25 * IMO Official Website: imo-official.org * Hacker News Discussion: news.ycombinator.com/item?id=44637191 * Elvis on X: https://x.com/omarsar0/status/1949857391350345824 * Gary Marcus: https://garymarcus.substack.com/p/deepmind-and-openai-achieve-imo-gold",
      "upload_date": "2025-08-01",
      "total_views": 3591,
      "max_views": 2304,
      "topics": [
        "context",
        "dynamics",
        "gemini",
        "gold",
        "imo",
        "implicit",
        "learning",
        "model",
        "pro",
        "problems",
        "training",
        "without"
      ],
      "search_text": "his video breaks down how Gemini 2.5 Pro, a publicly available model, solved 5 out of 6 problems from the IMO 2025 without any fine-tuning. Using a structured self-verification pipeline—solution generation, iterative critique, and multi-pass verification—the model produced mathematically rigorous proofs that rival human gold-medal performance. The result highlights that with the right prompting and scaffolding, today’s LLMs can reason through world-class problems. References: * Huang & Yang. Gemini 2.5 Pro Capable of Winning Gold at IMO 2025. arXiv: 2507.15855v3 * Github https://github.com/lyang36/IMO25 * IMO Official Website: imo-official.org * Hacker News Discussion: news.ycombinator.com/item?id=44637191 * Elvis on X: https://x.com/omarsar0/status/1949857391350345824 * Gary Marcus: https://garymarcus.substack.com/p/deepmind-and-openai-achieve-imo-gold context dynamics gemini gold imo implicit learning model pro problems training without OpenAI did it. Google did it. They solved five out of six problems with International Math Olympiad. It's the hardest math contest in the world. Here's what nobody's telling you. You can do it. But the public model. No fine-tuning. No secret sauce. Now, the IML isn't arithmetic. It's six problems that you do over two days. It's proofs of abstracts. Like it's far beyond benchmarks like GSM-AK. The authors pulled this off using a structured iterative pipeline. They used Gemini 2.5 Pro straight off the shelf. So here's how it works. First, you generate a solution. So then you add self-review and improve, verify it with a second prompt acting as a grader. You generate a bug report, flag any errors, revise that based on feedback, and you keep repeating that. Even though Gemini 2.5 Pro has token limits, what you can do is split the process into steps. Now, the verifier here might not be perfect, but they use multiple rounds to catch most errors. And if the model disagrees with the feedback, it still tries to revise it and to clarify it. You can always give it hints to help narrow the search space, but it's not required. The model can solve these from scratch, just takes a lot more compute and retries. The takeaway is here Gemini is doing math, but it shows you how models can reason and how they have that latent ability. You just need to give them some structure.",
      "platforms": {
        "tiktok": {
          "video_id": "7533675392520015135",
          "url": "https://www.tiktok.com/@rajistics/video/7533675392520015135",
          "view_count": 2304,
          "upload_date": "2025-08-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owIugUQMc58QNmWPeifzRhsgfKWqcGAMfnFv7A~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=ycXXACQDtPTvum86VlpZJujyHSE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18307621729173712",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "2ZNpQa8OLmo",
          "url": "https://www.youtube.com/watch?v=2ZNpQa8OLmo",
          "view_count": 1287,
          "upload_date": "2025-07-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6111,
      "title": "The future will be many different LLMs some open source and some proprietary. Other like Yann Lecun think differently. Yann's thread: https://www.threads.net/@yannlecun/post/C1NQlcmvqxA #largelanguagemodels #rajistics",
      "description": "The future will be many different LLMs some open source and some proprietary. Other like Yann Lecun think differently. Yann's thread: https://www.threads.net/@yannlecun/post/C1NQlcmvqxA #largelanguagemodels #rajistics",
      "upload_date": "2023-12-24",
      "total_views": 3585,
      "max_views": 3446,
      "topics": [
        "different",
        "future",
        "largelanguagemodels",
        "llms",
        "many",
        "model",
        "models",
        "open",
        "proprietary",
        "six",
        "source",
        "yann"
      ],
      "search_text": "The future will be many different LLMs some open source and some proprietary. Other like Yann Lecun think differently. Yann's thread: https://www.threads.net/@yannlecun/post/C1NQlcmvqxA #largelanguagemodels #rajistics different future largelanguagemodels llms many model models open proprietary six source yann",
      "platforms": {
        "tiktok": {
          "video_id": "7316299843595504938",
          "url": "https://www.tiktok.com/@rajistics/video/7316299843595504938",
          "view_count": 3446,
          "upload_date": "2023-12-24",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17899911488923392",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-12-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "iGy2LygUhFc",
          "url": "https://www.youtube.com/watch?v=iGy2LygUhFc",
          "view_count": 139,
          "upload_date": "2023-12-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5955,
      "title": "In this deep dive, I go beyond the RAG basics to focus on the most critical component: Retrieval. We'll provide a practical framework for thinking about RAG as a system, scoping your use case, and choosing the right retrieval architecture for your needs. I have shared the slides, references, and video all across social media at rajistics.",
      "description": "In this deep dive, I go beyond the RAG basics to focus on the most critical component: Retrieval. We'll provide a practical framework for thinking about RAG as a system, scoping your use case, and choosing the right retrieval architecture for your needs. I have shared the slides, references, and video all across social media at rajistics.",
      "upload_date": "2025-10-13",
      "total_views": 3581,
      "max_views": 2013,
      "topics": [
        "bm25",
        "deep",
        "dive",
        "embeddings",
        "going",
        "kind",
        "like",
        "look",
        "one",
        "rag",
        "retrieval",
        "right"
      ],
      "search_text": "In this deep dive, I go beyond the RAG basics to focus on the most critical component: Retrieval. We'll provide a practical framework for thinking about RAG as a system, scoping your use case, and choosing the right retrieval architecture for your needs. I have shared the slides, references, and video all across social media at rajistics. bm25 deep dive embeddings going kind like look one rag retrieval right Are you ready for a deep dive into RAG, especially around retrieval? What I want to do today is talk to you about what the problems are with RAG, how you scope out RAG use cases, but most of what we're going to focus on is a technical deep dive around some specific retrieval approaches, BM25, language models, and then find the agentic search and RAG. This is a presentation I gave recently at a conference at the MLops conference. What I'm going to do today is going to go a little bit faster through the content. You're going to miss some of the jokes, some of the setups like that, but I want to give everybody a chance to understand some of the content that I was able to share out there. To start out with, I like to focus on where we are with RAG today. Lots of folks are out there building their own RAG solutions because after all, we see chatGPT, you're inside a company, you want to be able to ask questions, right? The CEO wants to know, hey, can I get the list of boarder directors from your fancy chatGPT solution? You quickly find out, no, it doesn't because you need your enterprise knowledge in there. All of you probably at this point know, right? You can build a very easy RAG demo out of the box by just grabbing some data, using an embedding model, creating vectors, doing the similarity. The code is out there. Many of us have run through code like this before where we take our information, we chunk it down, we can then retrieve it, pass it to a language model. I'm going to assume everybody here knows the basics for building RAG. It's easy to build a RAG solution like this. Where I come in and hope why you're probably here is you know that the reality is you can build that quick demo, but as you try to move that into production, you figure out that the accuracy of what you did isn't that great because maybe you focused on using some synthetic data to generate to evaluate your model that was based around simple extraction, but your users ask much more complex type of questions, so your accuracy drops as soon as you move it into production. Or it could be you had so many calls that the latency is just unbearable and your users don't want to wait that long. It could be just a problem of scaling. Going from 100 documents to a thousand, 10,000, a million documents, lots of issues around scaling there that happened. Finally, it's the cost. Your demo works great, but we're seeing a lot of pushback in enterprises as you built a very complex multi-agent RAG system that all of a sudden each query costs like a dollar for each search. It's like, it's crazy. And then finally, and this is the thing most people miss, but inside an enterprise, not everybody gets to read every document. You need to have some way to work with compliance, some method for entitlements so not everybody can see every document like that. I know I say this and many of you will scour and look at your algorithms and be like, well, you know what, Raj, I'm just going to go search out on Archive and there's got to be a better paper out there. And there's lots of papers out there on RAG like that. But what I want to do is the answer is not in here of pulling together like a bunch of Archive papers and doing this. So I want to give people a framework for thinking through how to solve the RAG problems and not ending up with something that looks like this. So as a starting point, let me give you the idea, first of all, is you have to think of RAG as a system. It's not just one or two models. You're going to have a lot of different parts together as you build a RAG production system. And that involves parsing the documents, cleanly getting it out, something that's vastly overlooked, but especially when you get to complex documents, you need a good system for getting that information out cleanly, how you query your documents, the retrieval, finding those chunks, and then generating a response. So this out here is kind of a baseline system. For today, what I want to first do is talk about how we figure out where to improve and what type of system we need. And this is where like whenever somebody comes to me with a RAG problem, before we jump into the technical details, I like to know what are the trade-off, where are they thinking in terms of trade-offs? This is just like an other engineering problem where you have to think about how quickly you want something, what's the cost going to be? And instead of accuracy here, I'm using problem complexity. And if you can't have that serious discussion with your users and know what the trade-offs are, it's going to be hard to actually build a useful solution. So have that upfront conversation, set your expectations of users. One thing that often comes up in Gen AI is the cost of a mistake. Even where you're going to get implemented in today's generative AI because they are mistake prone is in places where it's easy to make mistakes. So this is why you see code-based tools, code-based assistant tools happening all the time because developers can quickly check the mistake while building a RAG system that might be a life and death. Something for patients in a healthcare medical might be a little bit harder because of the cost of the mistakes. But that should be a rough proxy for telling you like what you're going to have to do to get this into production. Now to give you a sense of some of the trade-offs that we have, this is something I made. I made this probably a year ago for our sales team. My day job is helping to work with customers on building RAG solutions. I've been doing this for years all the way back to when I was at Huggingface. And so I built this for the sales guy. This chart on the right, I used chatGBT to build this. But the idea here is to have that conversation about what type of queries, what type of things people are going to use this RAG system for. And that way you can figure out what is the complexity, what are the nuances that you're going to have to consider. Understanding this upfront before you start getting into the details of designing your RAG system is very important and can save you a lot of heartache. One thing I like to do is just know what kind of queries do we expect from people because as we talk about queries, they have a lot of different complexity where you could have very simple keyword queries. This will work very great. But on the other hand, maybe your users don't know the exact keywords and you're going to have some type of semantic variation where they're going to say how much bank instead of just the total revenue. Maybe your users have queries where they expect a little bit of complexity of hopping from solving one part of the question going to the next part. Or maybe the questions answers are in multiple documents. Or and this is the one that often happens a lot of times, the answers aren't in the documents that they give you. That they're all of a sudden they're asking for knowledge that's outside. I see this happening all the time and it's something again to get ahead of to figure out how you want to solve it. And then finally, nowadays we are used to these thinking LLMs. People are asking much more broader questions that call for a genetic scenarios. Each of these things will come with consequences for how we design our systems like that. Now today I'm going to focus on the retrieval part of this diagram. And I do this because this is where I see a lot of confusion and uncertainty. People don't know exactly what type of models to pick out. They're a little bit overwhelmed at all the different architectures and different choices. And I see a lot of stuff out on the internet, on the LinkedIn's, the X's where there's a lot of different philosophies, developers, companies putting out kind of ideas like that. And I want to give people a good solid practical way to start with retrieval before jumping into all types of alternative approaches like that. So this is where we're going to focus on retrieval. We're going to stick to the basics here. And retrieval again is about I have a document, I've broken it down into pieces. We need to break these things down into pieces. You're not going to have one model that can hold them all. If you just think about the extreme case, imagine you had one model that had like a 10 million context link that could hold everything. Would it really make sense to run that for every query? The amounts of compute to do that? No, no, no. We want to be compute efficient. It makes sense to break our documents into pieces using a chunking approach like that. This is why retrieval rag is always going to be around. We're going to talk about BM25. We're going to talk about language models and a genetic search. And since this is a video where you can skip ahead, I'm going to just keep going. Now if you think about it in terms of code, this is kind of the area of the steps that we're working in where we've chunked down the data. The next step is figuring out what the relevant chunk is to pass into our generator model. A lot of approaches here. I want to start with BM25. BM25 stands for best match 25. So it's a lexical type of search. And it took a couple iterations for them to get there. This is the 25th, right? This is a crafted formula that takes a look at all the words that are in a document and creates this inverted index. And so you can see it right here in the middle where for every word, we know which documents it goes to. So then if I have a search and I need to figure out, I have a query that has about butterfly, now I can figure out what is the relevant document that has butterfly. This is a very nice way of doing this. And to give you a sense of how quick and how fast it is, I kind of use chatGPT to get a quick benchmark. So all of these numbers were just generated using the models, which is why I don't have a ton of code for you because you can easily get the model like this. So I had to create a synthetic set of a thousand, three thousand, nine thousand documents. The first thing I did was I ran a linear search. So a linear search is just like when you use control F, find and you look for one after another after another after another. So imagine looking for that word butterfly, right, through each document one time after a time. You can see, right, that scales as your important documents. It's going to scale and go much slower. And it's pretty slow as soon as you get to a thousand documents, you can see it already takes three thousand seconds to do that. On the other hand, if we use this idea of creating this inverted index and then use the BM25 algorithm, which works off that inverted index, you can see it's much, much faster, right? The numbers of speed much faster, which is why this is a widely used approach like that. So what's the rub? Why aren't we using this all the time? Well, this is great at keyword, but here's the thing is sometimes the queries, the things we look for don't exactly match up in a lexical one-on-one overlapping way. Somebody asks, in this case, for a physician, but all the documents have the word doctor, or somebody asks for international business machines, but the documents use IBM. So these are the failure words of this kind of strict lexical approach. Now, nevertheless, this is a very strong baseline. In fact, when we talk about these neural network methods, many of these methods, the BM25 can beat on lots of different types of corpus. So depending on the type of documents and the type of queries you have, this can be a good fit, can give you a pretty good baseline like that, especially if you have users that already kind of know the words that they're looking for like that. And as we'll talk about later in a genetic search, it might be sufficient for a lot of cases. For those of you who want to play around with this, there's an implementation of BM25 in Python that you can go and grab and play around with. Now I want to enter language models. So the idea of language models is what we can do is we can take our text that we have, we pass it through an encoder, right? It's an encoder because it's encoding all of our text into numbers. And with that encoding, then, we're going to be able to use that as a way to search. And the great way is because the language models are trained on lots and lots of data, they've seen a lot of the world, they have an idea of these similar concepts. So when, for example, I search for physician, what I can do is in that same latent space, in that same new number that we've created from the embedding, the word doctor is very close by, right? International Business Machines is very close to IBM. This is basic semantic search stuff. It works really well, which is why it was a big deal for Google. When they added it, they had great results with it. It's also very popular. If you go over to Huggingface, you'll see these models are widely deployed. So this is all kind of baseline knowledge that you should have. Now the question, though, a lot of people have is, like, okay, Raj, you told me this, but which one of these should I use? So now let's look at the data here. If we look at this graph, we'll see there's two axes. One is CPU inference speed. And this is just basically how fast it goes, right? Faster to the right, slower to the left. Could have been GPU. I just used CPU implementation here for this. What is this NCDG thing? All this. So NCDG is a retrieval metric. It's the quality of how good the results come back. Nano is a small version of BEIR, which is a benchmark for information retrieval. So this is basically a quality score on a benchmark for testing out information retrieval. So the higher you get, the better your job you're doing at retrieval. So I've given you the big map. So I started helping you here by putting where BM25 is. You can see it's over to the right. It's one of the faster approaches as we talked about. But in terms of retrieval quality, it's not necessarily the best. But look, is it the fastest? No. There's even a faster way that I want to show you, which is using static embeddings. What a static embedding is, is for every word is going to have a unique, not numerical embedding. So this is very similar to older approaches like word to VEC that you might have seen. So here, for example, the word happy, the word joy has a kind of a meaning. You can run this really quickly. It's super fast. You can even run this on kind of CPUs. Now, the downside of kind of using the static embeddings is many words have multiple meanings. Right? Like you have to look at the context to understand that. And so this approach is going to lose out in some of those cases where that context matters for understanding the meaning here. So model, for example, can mean a statistical model that most of us are used to in kind of AI and data science. But if you're in fashion, right, model has a very different meaning like that. So this is where the context matters and where our retrieval quality can suffer is because people ask questions that have different ways. And this is where there's a lot more models that are available way over here to the left. And you can see some of the names out here. They differ, of course, in retrieval quality. They differ in speed. Right? If you think about this and we'll talk about why is this one way over here that's super, super, super slow. Well, and it's a bigger model. It takes a lot more compute to run. So now you're going to ask me, all right, you got me a ton more models here. How am I going to figure out kind of which model to run? Well, the folks over at Hugging Space hosts a couple of leaderboards. So this is the multi-text embedding leaderboard. And now the retrieval embedding leaderboard that has hundreds of models out there. And I want to take a minute here and show you. I'm going to flip over on my web browser. This is what the front page of the leaderboard looks like. So here, for example, I'm looking at the multi-lingual part of the leaderboard. You'll see there's many, many models here. I think there's an order of like 300 models. And the great thing is, is not only do you get the basic characteristics, like what are the embedding dimensions, what are the tokens of that. But you get all this, look over to the right. You get all these scores here as well. So these are how this model performs on some public data sets that are out there. And it gives us a rough gauge for being able to compare different models. And so this is where you can now pick the model that best kind of fits your use case. And as you would kind of explore this, you'll see that there's many different kinds of ways to kind of break up these embedding models and look and find right one. Recently, what's happened is they've introduced a new one here, the retrieval embedding leaderboard. And I'm going to click on this one here. And with the retrieval embedding leaderboard, there it goes. The retrieval embedding leaderboard is a little bit different. And it's going to have a subset. It doesn't have all the models yet, but it has one important difference. And the multi-text embedding leaderboard all showed public data sets performance. What the retrieval embedding leaderboard is going to show is it shows performance on a private held out data set. And this is important because this keeps us from keeps people from kind of gaming the system and maybe training on the exact same data that's there. Unfortunately, this thing is taking forever to load today like that. So let me stop and wait for it to load. And there you can see that you can see the private scores right here for that leaderboard as well. Now, there's a ton of tools here that you can use to help kind of look at the leaderboards to figure out kind of what's important like that. One of the things that often comes into mind for as you're picking these is the size of the model of the size of the model. Because the size of the model will directly have to do with compute. Now, when I do this in a classroom setting or workshop, I have a little more fun. But what I want to point out here is take a look here. You'll see that there's a bunch of models here that are all the same size. Right. So it takes the same amount of compute to run these models. But the performance over here on the mean differs. And the reason why, and I'll give you one second if you want to pause and come up with your own explanation is the models have improved for the same size models. People have evolved better training strategies, for example, improved architectures. And so we have kind of a newer generation of models where we have a slight improvement like that. So this is one of the great things we'll talk about here is these things are moving along as well. Now, as you start thinking about them, there's of course the accuracy of the models, how well they perform on the task, the latency and compute. All of these things are tied together. You'll want to think about, hey, is this an open source model? Can I go and quantize it maybe to get a little bit of extra speed? I'll think about the embedding dimension. Is this a small kind of embedding of 128 or is this kind of a gargantuan one? Because I'm going to have to manage and store all that stuff as well. When you're starting to look deeper into these models, you can look at, for example, some of them have trained differently. Multilingual, for example, or maybe there's a specific domain. Maybe you're going to go and fine tune the model. A lot of kind of different considerations that come into place like that. One kind of neat kind of innovation that I want to highlight here is the Matrioshia embedding models. Where a lot of embedding models initially came with one dimension. So when you passed in your information, out came one embedding output of a fixed length. The Matrioshia let us pick, we can pick smaller lengths like that. So this can be more convenient for storage or for compute that we're doing. But the models have been trained. So even if you go down to, let's say, 64, the retrieval quality is still high. I believe the open AI models also support this capability as well. So it's kind of an interesting thing I like to kind of highlight for folks with these models. Now, as you start using these models, one of the most dominant models you will see are sentence transformers for retrieval tasks. And these work great because if you think about most of the documents you write, you read that you're using for retrieval, they're probably all written in sentences. Well, the sentence transformers been trained to work with documents, not at the individual token level, but in terms of sentences. So it works a lot better kind of for retrieval and you'll see these widely used inside of RAG systems like that. A second model you'll see is the cross encoder or often as we use it in RAG, we often call it a re-ranker because that's the function it has inside there. And what the cross encoder does is it crosses two things. It crosses the incoming embedding that we have with the query that's coming in. So maybe your retriever found a bunch of chunks, we cross that chunk with the query. And what that does is you can use that cross encoder to get a score to see how similar they are. And what we do is we'll then use that to check each one of those embeddings and that will often lead to a re-ranking stage where we find out the relationship of that query is a little bit better to some chunks. And what we can do is we can get a slight improvement in performance like that. So this is widely used in that and you'll see you typically get a little bit of a bump by using a re-ranker, but like everything, right? It doesn't come for free. There's often kind of some type of latency gain. And again, the numbers here, I'm just to give you the ideas. You can of course pick re-rankers of different sizes and different kind of latency effects like that. Now, I have an example kind of co-lab demo that I've taken from somebody else, but it's a nice walkthrough to give you a sense of like what the value is of a re-ranker. How do you kind of use one? Just all runs inside Google co-lab for those who want to kind of play around with the code a little bit. A couple of other things. I work at contextual. We launched an instruction following re-ranker. It is out there on Hugging Face. You can use it for free if you're in a company. That's for research, but if you want to actually use it, you got to pay for it. But the key thing here is it allows you to send an instruction or a prompt for how you want that re-ranking to happen. So you can prioritize different types of information. This gives you another kind of knob when you're thinking about these re-ranking models. Now, a lot of retrievers. I showed you a lot of methods before. Well, you know what? You don't have to use them by themselves. You can combine them. So here, for example, you can see here some combinations of maybe I use BM 25 with something else. Maybe I used E5 with the re-ranker. Or maybe, like you go crazy and I'm like, shove them. You put them all together. They did the fusion here and re-rank all the way. You can do that, but just remember, you got to engineer this. You're going to have to pay for the compute. You're going to have to keep track of everything if you want to use multiples. If you want to use multiples, but you have a lot of flexibility when it comes to these retrievers, depending on your needs. Right? Like if you look over at the folks at Kaggle, they'll do some crazy stuff like this in a recent Kaggle competition. This person used three different re-rankers to get to the final kind of list where they went from 64 to 8 to 5 to rank the top five like that. So depending on what you need, there's a lot you can do against it. But my best practice always is use a hybrid search. So semantic and lexical. Fuse those together, right? Reciprocal rank fusions easily out there. Pass it through a re-ranker. That's going to give you pretty good standard performance out of the box. And this is a great place as a baseline to set before you kind of jump into lots of alternative approaches like that. So a lot of different families of models. I've tried to cover all these, if not kind of send your questions and comments like that. And this is an area, of course, with embedding models where you see kind of newer embedding models coming out just in the last couple of weeks. The folks over at Johns Hopkins, IBM, Google, all released models. I would tell you, though, as a practitioner, there is a slight improvement enough where you want to use the latest models, but like it's not a game changer. I wouldn't like go rip up something you have to install kind of a newer model as well. So the improvements are very incremental at this point. Now, there's a lot of other retrieval methods. Like I'm only taking like maybe an hour of time. We could easily do a six hour workshop on everything in RAG. So splayed, for example, is a very popular way for kind of sparse, sparse retrieval methods where what you do is you add a few other kind of related synonyms inside your inside your lexical search area as a way to improve it. There's other things like Colbert, which is named after the late night talk show host. Late interaction approach. There's graph RAG where you spend a lot of time up front, like creating this whole graph network and all the entities relationships. Like there were some funny kind of tweets on X about like the fact that everybody talks about graph RAG, but nobody actually implements it or uses it. There's tons of other RAG flavors out there. Will they work for your use case? Absolutely, they may. But again, like I'm giving you a good baseline, use that as a starting point to decide like, hey, where are we lacking? Where are we failing? Will one of these methods kind of fit where the gaps are that we have like that before just start, before just start following kind of the flavor of the week that you see out there. In terms of operational concerns, a couple of things I want to mention is there are optimized libraries for doing some of this stuff. So face, for example, from Facebook is widely used for kind of computing embeddings. The other thing is depending on the amount of embeddings you have, the amount of documents, you might be able to just store them in memory for a lot of use cases. You can just buy a big processor, storm in memory, load them up, run, run what you need to like that. On the other hand, other use cases, you're like, I want a database. I want someplace I can manage that. Be able to keep updates and do all of that. Sure. Absolutely. And there's a ton of vector database options. And I do not pretend to tell you kind of where to go. In terms of that, I've linked one of the, an article that I found in here, a big part of when you pick out vector databases is thinking about what is your latency requirements for that, right? Cause that's going to govern the, the kind of a choice of it. But a lot of the standard databases out there, the snowflakes of the world, for example, now give you some way to kind of store your embeddings and vectors there as well. All right. Um, one other thing is, is as you kind of, when we talk about these retrieval methods, what we typically find is that as your information grows, your, those retrieval methods, you're going to have to lean on much heavier for you. And this is one of the techniques I like to recommend to people to use as metadata. When you go from a thousand documents to a million documents, if you're not using something like metadata, so when you have a search, you can narrow down the scope. It's going to be very tough to be able to find and get your high accuracy like that. So yes, retrieval will go down as your data source goes up, but you can use strategies to help fight that back. All right. So now let's move into the exciting part. We've talked about traditional rag kind of one shot approaches to be able to search. There is also now a genetic rag where we use multiple ways, multiple kind of calls to be able to do this. This all stemmed in just basically the last year really came to a head because now we have LM reasoning models that are capable of effectively using tools where they can use a tool to make a query, look at the results that come back and go, oh, you know what? Maybe we should do it a different way like that. So that's been the radical change, which to me has really brought this about. So I have a code snippet here, Agno, for example, is an open source library. They have examples of kind of reasoning agents like that. The basic kind of approach here is your query comes in, you're going to generate an answer, and then you ask your LM, did it answer the question or not? If not, it can rewrite the query, word it a little bit different, try to find that missing information, feed that back into the loop as well. And what ends up looking like something like this where your model is thinking through the steps of what it needs to be able to do for each of those steps, it will be like, oh, this is the query I need to make. This is what this is the search information I need to based on those results. It'll come back and that gives you to a nice response that takes a little bit longer, but now is much more kind of researched across to be able to do this. One of the big early categories of this was deep research. So the folks over at Langchain have an open source repo with their open deep research approach, which again takes a similar approach. I mean, they do some things like use a bunch of research sub agents to go out there and be able to kind of do, get that, do those retrievals and then bring it all back together to write a report. You can check it out. They have, they've made all the prompts, all that stuff available. So it's good to see out. There's a whole school around this, like actually doing this deep research where there's deep research bench, which is a leaderboard in the space. It can get very expensive if you start running a lot of, a lot of tasks on it. It's got a hundred PhD level research tasks. So it can cost a little bit of money for each of the queries to run. But if you're interested in this kind of deep research area, and I already see companies out there making out, making solutions for this. I think the interesting thing on this one here is you pick the time that you want to think, and that's often, as we'll talk about here, a crucial element when it comes to using this agentic search approaches. A widely used benchmark that I find interesting in this area is called Bright. And it's a benchmark that's built around retrieval reasoning. So you can see at the top here, there's two examples of keyword and semantic. This isn't a benchmark that does that. There's other ones that does that. Instead, look at the examples that it's giving you. These are the types of questions that it wants to do where you have to do a little bit deeper kind of thinking through things to be able to do that. To be able to solve the problems like this. So it's an excellent benchmark out there. If we look and see what is the number one approach right now on this, its name is Diver. And if you take a look at the Diver diagram, it probably doesn't look that crazy to you if you're used to rag. Like you see, oh, wait a minute, right? There's a there's a chunking mechanism here. There's a retrieving mechanism here. There's a re-ranking mechanism here. And in fact, the number one, I also looked at the number two approaches. Both use those same type of methodologies that we talked about for a baseline in this. Now they've tweaked this, of course, around kind of the reasoning pieces. But again, the elements that we've talked about of going out, finding information, chunking, finding information, re-ranking, but then just putting this into a loop is what's going on here. And you can see here some of the examples of queries where in the first round, they have given a query, go look for this stuff. And now, right, based on what we've learned, now what do you think would be possibly helpful to do like that? And so this is hopefully gives you the schematic of how we can think about kind of a gentic rag where we're going to ask, hey, can we find a better answer? And look, there's been people doing this. This isn't brand new. There's things like self-rag where people have asked and had the models kind of reflect on the answers like that. I saw this on Reddit where somebody kind of shared out, hey, how they built this system to do that. But one of the things you quickly find out is that these systems can be super inefficient, right? They're asking the models to go back and retrieve, re-retrieve, rewrite, and maybe it could have been done just a lot quicker. So this is there's always a rub. This is the rub with these approaches is that long latency in the long, long, and long-term and long-term time. And in fact, this is kind of I ran this myself. I work at contextual. We have kind of a rad platform, lots of little settings. So I could kind of tweak it and play it around with this. And I took Wix QA, which is publicly available. It's on hugging face. It's a technical support, technical support database, technical support knowledge base. So that's the kind of question that has like a help desk questions. And you can see when I had my kind of the one one one shot rag, I think this is like E five with maybe a re-ranker. I can't remember. I could get answers really quick, right? Five seconds or so it boom, it pulls it back. But in terms of factuality, which is the benchmark they use on Wix QA, it's 0.76 like, yeah, it's pretty good, but it's missing a lot. Well, then I wired in my agentic rag system where all I did was just asked it to reflect, you know, maybe do more retrieval calls. If the answer wasn't complete, and it would do more retrieval calls. And you can see here it took a little bit longer in terms of thinking. The numbers here don't fixate on the exact numbers. There's a lot of wiggle room. I just want to give you a sense of like a genetic rag takes a little bit longer to do that. But on the other hand, take a look at that accuracy. That's crazy. That's look at that improvement there. That has a ton of good implications for that. I mean, one thing right away for me when I ran this is, look, now, now anything that I'm missing that I'm not getting in this 0.07, I kind of wonder like even my smart system that took a while couldn't figure it out. Like what is going on with the last little bit here? But beyond that, look at the difference between the 0.93 and 0.76. So these were answers that I was able to figure out if I did multiple calls. On the other hand, right, these are all the bottom ones here where all the ones I did the first time. So there's a gap here that of answers that took a couple of calls to do. Well, why don't I take a look at those? Why don't I take a look at those types of queries, that type of material and figure out why did it take multiple queries? Is there something about the way my documents are stored? Is there a way of how I've chunked the documents retrieval that I can modify so I could actually improve this one shot rag and bring it up closer? Right? You could think of this now as like this is our new kind of baseline for what's possible, right? This shows what's possible in the system. I can now up that up. So this really is mind blowing for me in a couple of different ways like that. Now, it's I'm not going to stop you there. So one of the things in the bright paper is they had this graph and I want to walk you through it. So over here, they have quen. So this is the quen embedding models. So they're using the quen embedding models with GPT for so doing that same kind of retrieval loop that we've talked about to be able to find kind of answers. And you can see here's the quality, the higher the bar, you get the better answer. So you can see the quen model does OK. Right? It's much better than this expert model over here or the instant model does. But it gives you a sense of like the quality of what we're getting. But wait a minute. Once this other crazy model over here, this BM 25, take a look, it does better than the quen. This is crazy, right? Like our good old keyword baseline. There's just the language model because it can rewrite the queries. Right? Like it doesn't need an embedding model to figure out the semantic variation because it can just rewrite that into different semantic ones and re query, re query like that. And it actually does better than the quen embedding model. Like I saw this in the graph and immediately I was like research team, like this is this is something we need to dive into. And I even ran my own benchmarks like this on a couple of the data sets. We have the Wix QA and then we have an internal financial dataset of 10 Ks. And you know what? Like when I ran this with just BM 25, it still did a lot better than one shot. And it's still got pretty close to kind of the agentic rag. This is all just out of the box. I'm not tuned or tried to make this improve this thing in any way. This is just the raw performance. But this blows me away. Like now this tells me I don't need to use that neural network model. The semantic model, all that stuff, those embeddings, vector databases. I could throw all that away. I could just stick this in a text only database and use BM 25 and get up, do a pretty good job of being able to find the right answers consistently. Right. These are in cross different domains. So for me, this is a real game changer. Now there is the downside, right? There's that latency costs. Like you're lots of times you're going to need one shot. People are going to need an answer in three or five seconds or they're going to leave your website or they're going to be unhappy. There's plenty of room, many use cases for single shot rag. It's not going away in any way like that. But, you know, if the accuracy matters, if people can wait, the sogentic rag is really going to change things. And you already see it being used in terms of like things like cloud code, for example, uses a lexical approach, an iterative approach for its approach. And part of that is it's doing code search, which is a little bit different. Code doesn't have the same kind of semantic properties that conventional documents do, so it's always been a little weird for using it for search. But still, it's another sign that, you know, hey, maybe we can use these agentic approaches to be able to do stuff. Now, as we're doing this, I like to keep you all practically minded. This is where we talk about trade offs. But you know what? These things can work together too. So I saw this paper by DoorDash where they talked about their guardrail system. And what I liked is it showed a two tier system where it showed two different methods working together. It uses a very simple text similarity as an initial guardrail. If that guardrail works great, that's great. If not, they can always kick it over to an LLM, but they use the LLM as a backup. It's not the primary. It's more of like difficult cases. And I like this type of thinking because it better takes advantage of the relevant kind of advances or trade offs of these different types of approaches. There's an example if you want to play around with agents. There's lots of agent ones out there, but just wanted to give people a little sense of that. All right. Thank you all. I'm going through this. Hopefully I've given you some sense of like thinking about these trade offs, you know, right, based on kind of these trade offs. Maybe if you have like a high cost of mistakes or really, right, you don't want to make mistakes, you have the budget for it. Maybe a re-ranker could make sense. On the other hand, right, maybe, maybe you're like, hey, Raj, you know, I need sub five seconds, sub one second latency. So maybe you're like, hey, I'm going to try out that BM 25 or static embeddings. Maybe your users are doing complex multi hop queries. So a gentek rag is a better, better fit like that. But hopefully I've gone through giving you a little bit of a key checklist here for kind of good starting points for kind of building out your rag systems like that. I'm going to end it here. When I do this in person, I ask a lot of questions. We walk through other parts of the rag platform as well. But thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7560732294487510302",
          "url": "https://www.tiktok.com/@rajistics/video/7560732294487510302",
          "view_count": 2013,
          "upload_date": "2025-10-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4V17KqRMQ8OEI1EEoRABvD3FQfBABPeDQ0tll~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=pOyLkLn4c4V9TwUS2o94sZ8i8aA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18529304497032476",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AS_HlJbJjH8",
          "url": "https://www.youtube.com/watch?v=AS_HlJbJjH8",
          "view_count": 1568,
          "upload_date": "2025-10-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Fun way to talk about K-means algorithm #datascience #codetok #analytics #machinelearning ",
      "description": "Fun way to talk about K-means algorithm #datascience #codetok #analytics #machinelearning ",
      "upload_date": "2022-08-22",
      "total_views": 3539,
      "max_views": 3539,
      "topics": [
        "algorithm",
        "analytics",
        "codetok",
        "data",
        "datascience",
        "machinelearning"
      ],
      "search_text": "Fun way to talk about K-means algorithm #datascience #codetok #analytics #machinelearning  algorithm analytics codetok data datascience machinelearning How would you teach my kids where to put the food truck? I figured it out. Let me show you the algorithm I use so they end up in the right spot. The way it works is it doesn't matter where they start. The first thing they do is figure out who are the people that are closest to them. They then take the average of that group of people that's closest to them and that gives them a new position. Then they move over to that new position and start the process again. This approach is super cool. Even works on more complex data setups like this. What we went through is what's known as the Canine's algorithm and is one of the most common algorithms in data science for doing clustering or grouping of data.",
      "platforms": {
        "tiktok": {
          "video_id": "7134821616537701678",
          "url": "https://www.tiktok.com/@rajistics/video/7134821616537701678",
          "view_count": 3539,
          "upload_date": "2022-08-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/05585d203b334f6f9c1387e8f2a4a998_1661205114~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=UUK%2BCpP3NWZ4yog4rLrIuY69lCw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6483,
      "title": "LLMs are approximate retrievers that are mimicking plans rather than truly planning. Great argument put forth by Subbarao Kambhampati who is skeptical of the LLM reasoning arguments check out his youtube for a longer discussion: #largelanguagemodels #machinelearning #gpt4 #aireasoning #aiplanning Avenging Polanyi's Revenge: Exploiting the Approximate Omniscience of LLMs in Planning without Deluding Yourself In the Process: https://youtu.be/BmyB-4S9QuY Full Abstract: LLMs are on track to reverse what seemed like an inexorable shift of AI from explicit to tacit knowledge tasks. Trained as they are on everything ever written on the web LLMs exhibit \"approximate omniscience\"--they can provide answers to all sorts of queries with nary a guarantee. This could herald a new era for knowledge-based AI systems--with LLMs taking the role of (blowhard?) experts. But first we have to stop confusing the impressive form of the generated knowledge for correct content and resist the temptation to ascribe reasoning powers to approximate retrieval by these n-gram models on steroids. We have to focus instead on LLM-Modulo techniques that complement the unfettered idea generation of LLMs with careful vetting by model-based AI systems. In this talk I will reify this vision and attendant caveats in the context of the role of LLMs in planning tasks‚Ä¶ Longer Tutorial: On the Role of Large Language Models in Planning (Tutorial Part 1) - https://youtu.be/wgVZvXDvry0",
      "description": "LLMs are approximate retrievers that are mimicking plans rather than truly planning. Great argument put forth by Subbarao Kambhampati who is skeptical of the LLM reasoning arguments check out his youtube for a longer discussion: #largelanguagemodels #machinelearning #gpt4 #aireasoning #aiplanning Avenging Polanyi's Revenge: Exploiting the Approximate Omniscience of LLMs in Planning without Deluding Yourself In the Process: https://youtu.be/BmyB-4S9QuY Full Abstract: LLMs are on track to reverse what seemed like an inexorable shift of AI from explicit to tacit knowledge tasks. Trained as they are on everything ever written on the web LLMs exhibit \"approximate omniscience\"--they can provide answers to all sorts of queries with nary a guarantee. This could herald a new era for knowledge-based AI systems--with LLMs taking the role of (blowhard?) experts. But first we have to stop confusing the impressive form of the generated knowledge for correct content and resist the temptation to ascribe reasoning powers to approximate retrieval by these n-gram models on steroids. We have to focus instead on LLM-Modulo techniques that complement the unfettered idea generation of LLMs with careful vetting by model-based AI systems. In this talk I will reify this vision and attendant caveats in the context of the role of LLMs in planning tasks‚Ä¶ Longer Tutorial: On the Role of Large Language Models in Planning (Tutorial Part 1) - https://youtu.be/wgVZvXDvry0",
      "upload_date": "2023-08-12",
      "total_views": 3526,
      "max_views": 2300,
      "topics": [
        "aiplanning",
        "aireasoning",
        "gpt4",
        "largelanguagemodels",
        "llms",
        "machinelearning",
        "planners",
        "reasoning",
        "retrieval",
        "versus"
      ],
      "search_text": "LLMs are approximate retrievers that are mimicking plans rather than truly planning. Great argument put forth by Subbarao Kambhampati who is skeptical of the LLM reasoning arguments check out his youtube for a longer discussion: #largelanguagemodels #machinelearning #gpt4 #aireasoning #aiplanning Avenging Polanyi's Revenge: Exploiting the Approximate Omniscience of LLMs in Planning without Deluding Yourself In the Process: https://youtu.be/BmyB-4S9QuY Full Abstract: LLMs are on track to reverse what seemed like an inexorable shift of AI from explicit to tacit knowledge tasks. Trained as they are on everything ever written on the web LLMs exhibit \"approximate omniscience\"--they can provide answers to all sorts of queries with nary a guarantee. This could herald a new era for knowledge-based AI systems--with LLMs taking the role of (blowhard?) experts. But first we have to stop confusing the impressive form of the generated knowledge for correct content and resist the temptation to ascribe reasoning powers to approximate retrieval by these n-gram models on steroids. We have to focus instead on LLM-Modulo techniques that complement the unfettered idea generation of LLMs with careful vetting by model-based AI systems. In this talk I will reify this vision and attendant caveats in the context of the role of LLMs in planning tasks‚Ä¶ Longer Tutorial: On the Role of Large Language Models in Planning (Tutorial Part 1) - https://youtu.be/wgVZvXDvry0 aiplanning aireasoning gpt4 largelanguagemodels llms machinelearning planners reasoning retrieval versus",
      "platforms": {
        "tiktok": {
          "video_id": "7266582668555423019",
          "url": "https://www.tiktok.com/@rajistics/video/7266582668555423019",
          "view_count": 2300,
          "upload_date": "2023-08-12",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cv3PslsACUT",
          "url": "https://www.instagram.com/reel/Cv3PslsACUT",
          "view_count": 606,
          "upload_date": "2023-08-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "zChmKVXHB_8",
          "url": "https://www.youtube.com/watch?v=zChmKVXHB_8",
          "view_count": 620,
          "upload_date": "2023-08-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Why you want prediction intervals instead of point predictions #datascience #machinelearning #statistics #predictioninterval",
      "description": "Why you want prediction intervals instead of point predictions #datascience #machinelearning #statistics #predictioninterval",
      "upload_date": "2022-09-20",
      "total_views": 3516,
      "max_views": 3516,
      "topics": [
        "datascience",
        "machinelearning",
        "prediction",
        "predictioninterval",
        "statistics",
        "want"
      ],
      "search_text": "Why you want prediction intervals instead of point predictions #datascience #machinelearning #statistics #predictioninterval datascience machinelearning prediction predictioninterval statistics want",
      "platforms": {
        "tiktok": {
          "video_id": "7145295854927236398",
          "url": "https://www.tiktok.com/@rajistics/video/7145295854927236398",
          "view_count": 3516,
          "upload_date": "2022-09-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6069,
      "title": "Oldie but still very relevant.  ",
      "description": "Oldie but still very relevant.  ",
      "upload_date": "2025-01-28",
      "total_views": 3514,
      "max_views": 2766,
      "topics": [
        "augmented",
        "didn",
        "document",
        "documentation",
        "explained",
        "flamingo",
        "generate",
        "generation",
        "hey",
        "paper",
        "quickly",
        "rag_",
        "responses",
        "retrieval",
        "review",
        "sections",
        "share",
        "specific"
      ],
      "search_text": "Oldie but still very relevant.   augmented didn document documentation explained flamingo generate generation hey paper quickly rag_ responses retrieval review sections share specific Hey, deep mind here, we're dropping off our paper Flamingo for review. Hey, this paper makes some great claims, but how am I supposed to review it? There's no data set, there's no implementation details, there's no model. There's nothing reproducible here that allows me to understand exactly what happened. Are you new here? OpenAI didn't share Dolly. We didn't share AlphaGo. It could take somebody weeks or months to reproduce this work and they might not even succeed. Like how does this advance science? How high are you in that ivory tower? Who do you think pays for this conference? You're lucky we let you donate your time to review these papers. This conference is about highly original work that's reviewed in a double blind way to help advance science. You realize a big part of our efforts are about marketing and PR. We wanted to let people know what we built. Yeah, I knew it was you guys. Hey, it's not my fault you guys can't get off Twitter. And hey, when we ordered the shirts this year, the fun police came out so we can't get the ones that say my nips are NP hard. Sorry to interrupt. I had a little feedback. Go ahead. Love the paper. Could you change the font size of the appendix to be a little bit bigger? Yes. Really? Is that your review? Yes, had one tough question. Why did you name it Flamingo?",
      "platforms": {
        "tiktok": {
          "video_id": "7465093943567387935",
          "url": "https://www.tiktok.com/@rajistics/video/7465093943567387935",
          "view_count": 2766,
          "upload_date": "2025-01-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYIo2IAz0zZVtiiUzUqB4ABCqTifAylOCIYCiK~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=JcRJadkRjOT1fIdeA6t%2BmWfovgw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18130272220396291",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "yp6fejJUT3E",
          "url": "https://www.youtube.com/watch?v=yp6fejJUT3E",
          "view_count": 748,
          "upload_date": "2025-01-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I need to focus on adding more Regularization to my life. #datascience #statistics #regularization ",
      "description": "I need to focus on adding more Regularization to my life. #datascience #statistics #regularization ",
      "upload_date": "2022-11-19",
      "total_views": 3512,
      "max_views": 3512,
      "topics": [
        "datascience",
        "hammering",
        "maybe",
        "regularization",
        "statistics",
        "videos"
      ],
      "search_text": "I need to focus on adding more Regularization to my life. #datascience #statistics #regularization  datascience hammering maybe regularization statistics videos Hey boss, just ordering this t-shirt and shoes from this video and I'll be ready for our one-on-one. Is that video about hammering? Yeah, I'm studying hammering for my girlfriend. Why the t-shirt and shoes? I've watched a couple of videos and analyzed them and those were some of the variables to do hammering. I've also ordered a hammer and I've been practicing the correct angle and transfer of power to the nail. There's just so many things to consider. Can you remind me what regularization is? Sure, in machine learning algorithms can overfit to their training data like this and when they do that they don't generalize well to new examples. So regularization is a technique where we penalize coefficients from getting too big and this technique helps us improve how models generalize. If you only watch a few videos on hammering would it be possible to overfit to those videos and learn maybe some actions, behavior, clothing that aren't generalizable? Yeah, I suppose it would be. Maybe I should watch a few more videos. What I want you to do is apply that regularization methodology here. So when you have a lot of features, ideas that you think, just try to keep cool overall, don't over-focus and maybe even drop some of those ideas. Wow. Oh, that makes sense. It's kind of like how Lasso helps those P greater than N type problems where the number of features is much greater than the number of observations or rows. Exactly. By penalizing those coefficients we can get better predictive performance and even a little bit of feature selection. This is going to make my girlfriend so happy. I overheard her say I needed to get better at nailing.",
      "platforms": {
        "tiktok": {
          "video_id": "7167823618146061611",
          "url": "https://www.tiktok.com/@rajistics/video/7167823618146061611",
          "view_count": 3512,
          "upload_date": "2022-11-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f113e60f17ad4dc7bd63ff3c1bf6b0a2_1668888995~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=DJ0bXunzfNdmfHh6yVn7VqrGucg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Cleanlab is open source and will improve your data quality. It’s so underrated. This was hard to record vertically, so go try it out. #datascience #machinelearning #cleanlab #labelerror #confidentlearning #dataquality ",
      "description": "Cleanlab is open source and will improve your data quality. It’s so underrated. This was hard to record vertically, so go try it out. #datascience #machinelearning #cleanlab #labelerror #confidentlearning #dataquality ",
      "upload_date": "2023-01-30",
      "total_views": 3507,
      "max_views": 3507,
      "topics": [
        "cleanlab",
        "data",
        "datascience",
        "machinelearning",
        "model",
        "see"
      ],
      "search_text": "Cleanlab is open source and will improve your data quality. It’s so underrated. This was hard to record vertically, so go try it out. #datascience #machinelearning #cleanlab #labelerror #confidentlearning #dataquality  cleanlab data datascience machinelearning model see The label errors website shows us lots of classic data sets are filled with labeling problems. What are we going to do about it? Let's dive into how confident learning implemented in clean lab can help you out. The first step is training a model and then being able to get predictions on an out-of-sample set of data points. Now a key here is we're going to look at probability among all the classes. So some you see are pretty obvious like cat seems to make sense. Or if we look at this car, like what is that doing in there? And you can see the model is kind of confused and not giving a strong probability to any of the classes. I'm going to start here by setting a threshold for my class percentile of 90%. So you can see here we're only looking at a small set of the examples that are actually in my out-of-sample data set. Now if we look at a particular example here, we can see for example the model predicted bear at 0.913, which met the threshold. The undiagonal examples aren't very interesting. What's interesting are the things that are off because that's where the errors are occurring. Here we see the model has predicted bear at very strongly at 1.0. But look at it, the example is wrong. And in this case it looks like this is an image that was mislabeled. If we lower our threshold, you'll see we'll have a lot more examples to consider about labeling errors. And there you have it. By looking at where your model is really confident and where that doesn't make sense, clean labs able to identify what are places that you potentially have mislabeled data that you can go in and help improve your test set. The package is open source. They have lots of tutorials. It's very easy to get started and a very easy way to check for data quality.",
      "platforms": {
        "tiktok": {
          "video_id": "7194233083342671150",
          "url": "https://www.tiktok.com/@rajistics/video/7194233083342671150",
          "view_count": 3507,
          "upload_date": "2023-01-30",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ced40672f70f41a8ae6bbb38e5dc8e07_1675037939~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=R0qBjsBSQ7DGAoBjsitQFouP1Qs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Repost which has held up well: Language Models like ChatGPT can be modified by several methods including Prompting Instruction Fine-Tuning and Reinforcement Learning with Human Feedback. This year we will start seeing lots more varieties of large language chat models trained on different data.  References: Conservatives Aim to Build a Chatbot of Their Own: https://www.nytimes.com/2023/03/22/business/media/ai-chatbots-right-wing-conservative.html  ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge - https://arxiv.org/abs/2303.14070  Whose Opinions Do Language Models Reflect? https://arxiv.org/pdf/2303.17548.pdf  Natural Language Processing with Deep Learning https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf #datascience #machinelearning #largelanguagemodels #openai #chatgpt #promptengineering #instructionfinetuning #rlhf #reinforcementlearning #pretrain ",
      "description": "Repost which has held up well: Language Models like ChatGPT can be modified by several methods including Prompting Instruction Fine-Tuning and Reinforcement Learning with Human Feedback. This year we will start seeing lots more varieties of large language chat models trained on different data.  References: Conservatives Aim to Build a Chatbot of Their Own: https://www.nytimes.com/2023/03/22/business/media/ai-chatbots-right-wing-conservative.html  ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge - https://arxiv.org/abs/2303.14070  Whose Opinions Do Language Models Reflect? https://arxiv.org/pdf/2303.17548.pdf  Natural Language Processing with Deep Learning https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf #datascience #machinelearning #largelanguagemodels #openai #chatgpt #promptengineering #instructionfinetuning #rlhf #reinforcementlearning #pretrain ",
      "upload_date": "2024-04-10",
      "total_views": 3499,
      "max_views": 3499,
      "topics": [
        "chatgpt",
        "datascience",
        "like",
        "model",
        "rlhf",
        "using"
      ],
      "search_text": "Repost which has held up well: Language Models like ChatGPT can be modified by several methods including Prompting Instruction Fine-Tuning and Reinforcement Learning with Human Feedback. This year we will start seeing lots more varieties of large language chat models trained on different data.  References: Conservatives Aim to Build a Chatbot of Their Own: https://www.nytimes.com/2023/03/22/business/media/ai-chatbots-right-wing-conservative.html  ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge - https://arxiv.org/abs/2303.14070  Whose Opinions Do Language Models Reflect? https://arxiv.org/pdf/2303.17548.pdf  Natural Language Processing with Deep Learning https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf #datascience #machinelearning #largelanguagemodels #openai #chatgpt #promptengineering #instructionfinetuning #rlhf #reinforcementlearning #pretrain  chatgpt datascience like model rlhf using Can I get my chat GPT modified here? That's what I do here. What are you looking for? My employees are stuck using that liberal open AI. I want to give them a freedom-loving alternative that doesn't say stuff like this. Got a couple options for you. The best you can do is a full rebuild from the ground up, build your own pre-trained model, runs about 300K. Hmm, you got something cheaper? Prompti says he can make it talk the way I want it. Oh sure. Prompti Engineering can make the model act differently, but it's just a band-aid. I got some better options. My suggestion for you is using the reinforcement learning with human feedback. We can have that set to freedom mode using the Trump flavor. Here's an example of what it says for global warming. This runs about a thousand bucks. How does that work? The stock open AI model is using human preferences from a representative group of people. What we do is we modify that with the type of people that you want in your model. In this case, the Trump folks. I like it. Great. I'm not aware of that that model only knows the facts and the skills that open AI taught it. For an upcharge, we can fine-tune that model with additional facts or skills. The Star Wars universe is on special this week. I like it. Out of the box, it's pretty generic with skills, but we can add new skills. Take a look at some of these that we can add. I'd recommend a factual upgrade for you. Freedom, US version by the American Policy Institute. I like it. One last thing. I need you to sign this waiver. It just recognizes even though we're making factual changes, they're not 100% guarantee that underlying model could still lie, make up new things, especially if you ask it questions that are a little bit outside of the universe we're giving it. I get it. No guarantees in this world. Your custom model is ready. I can go ahead and deploy it live and let you see how your users are using it if you'd like. Let's do it. This is what they're doing.",
      "platforms": {
        "tiktok": {
          "video_id": "7356034654882762026",
          "url": "https://www.tiktok.com/@rajistics/video/7356034654882762026",
          "view_count": 3499,
          "upload_date": "2024-04-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/075c6fcc1e9043d4917f8dc584569666_1712710305~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=LYcgwdpZTuZUCYtI5bB%2FjomWC5k%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Thinking about the size of numbers becomes important when working with neural networks. This video touches on different techniques like using bfloat16 and quantization. (It 1:49 seconds long, sorry instagram folks) Links: Accelerating Large Language Models with Mixed-Precision Techniques: https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/ BFloat16: The secret to high performance on Cloud TPUs: https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus Llama.cpp: https://github.com/ggerganov/llama.cpp/ A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes: https://huggingface.co/blog/hf-bitsandbytes-integration Background by Umberto: https://unsplash.com/photos/jXd2FSvcRr8 #datascience #machinelearning #bfloat16 #quantization #largelanguagemodels",
      "description": "Thinking about the size of numbers becomes important when working with neural networks. This video touches on different techniques like using bfloat16 and quantization. (It 1:49 seconds long, sorry instagram folks) Links: Accelerating Large Language Models with Mixed-Precision Techniques: https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/ BFloat16: The secret to high performance on Cloud TPUs: https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus Llama.cpp: https://github.com/ggerganov/llama.cpp/ A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes: https://huggingface.co/blog/hf-bitsandbytes-integration Background by Umberto: https://unsplash.com/photos/jXd2FSvcRr8 #datascience #machinelearning #bfloat16 #quantization #largelanguagemodels",
      "upload_date": "2024-05-17",
      "total_views": 3497,
      "max_views": 3497,
      "topics": [
        "bfloat16",
        "datascience",
        "floating",
        "models",
        "point",
        "quantization"
      ],
      "search_text": "Thinking about the size of numbers becomes important when working with neural networks. This video touches on different techniques like using bfloat16 and quantization. (It 1:49 seconds long, sorry instagram folks) Links: Accelerating Large Language Models with Mixed-Precision Techniques: https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/ BFloat16: The secret to high performance on Cloud TPUs: https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus Llama.cpp: https://github.com/ggerganov/llama.cpp/ A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes: https://huggingface.co/blog/hf-bitsandbytes-integration Background by Umberto: https://unsplash.com/photos/jXd2FSvcRr8 #datascience #machinelearning #bfloat16 #quantization #largelanguagemodels bfloat16 datascience floating models point quantization Do you ever laugh at the videos of people that take way too much stuff on a trip? Well, are you doing the same thing in machine learning? Are you packing way more information into your models than you need to? Let's talk about how we can slim down our models. Historically, scientific computing used 64 bits to represent a number. And then with GPUs, memory is costly. So the first thing the GPU folks did was slim that down to 32 bits. And here's what a number looks like in a 32-bit representation. We have the fraction, the number itself, the exponent, what it's raised to, and then the sine, plus or minus. Now, some of you are thinking, why stop at 32? Why not slim it down to 16? And we've tried 16, but what happens is when you're training neural networks, you have thousands, millions, billions of weights. And sometimes you need to change a weight just by a little bit. And when we moved down to a floating point of 16, we often found that models had instability, overflow and underflow errors, because there just wasn't enough dynamic range. So this led some folks at Google to come up with Brain Floating Point. Brain Floating Point adds more bits to the exponent. And what this does is it gives us a bigger dynamic range. And nowadays, we see this Brain Floating Point widely used when folks are training machine learning models. While it's hard to squeeze this down for training, we can do something a little bit different for inference or for predictions with models. And this is called quantization. With quantization, we changed things from, let's say, floating point 32 or floating point 16. We can move them all the way to integers like integer 8, even integer 4. And this dramatically reduces the amount of space required. And this is why people are able to take Lama models, use quantization to bring them down to integer 4, and then put them on their phones. So when you're working with models, try seeing if you can slim them down. Now, there's always trade-offs. You could be losing accuracy. So this is a determination you're going to have to make for what's best for you.",
      "platforms": {
        "tiktok": {
          "video_id": "7369761298025893162",
          "url": "https://www.tiktok.com/@rajistics/video/7369761298025893162",
          "view_count": 3497,
          "upload_date": "2024-05-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f722669dc21e4dc5aa6125e6dfbc8b2d_1715906279~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=LjeBSs1md6fXuEnVkARMQcOEZfs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6275,
      "title": "Word as Image - great use of generative AI models like stable diffusion to create fonts. Check out the paper at wordasimage.github.io #datascience #machinelearning #stablediffusion #generativeai #fonts",
      "description": "Word as Image - great use of generative AI models like stable diffusion to create fonts. Check out the paper at wordasimage.github.io #datascience #machinelearning #stablediffusion #generativeai #fonts",
      "upload_date": "2023-03-07",
      "total_views": 3481,
      "max_views": 3481,
      "topics": [
        "datascience",
        "fonts",
        "generativeai",
        "going",
        "machinelearning",
        "stablediffusion",
        "word"
      ],
      "search_text": "Word as Image - great use of generative AI models like stable diffusion to create fonts. Check out the paper at wordasimage.github.io #datascience #machinelearning #stablediffusion #generativeai #fonts datascience fonts generativeai going machinelearning stablediffusion word",
      "platforms": {
        "instagram": {
          "video_id": "17997804010731871",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "PlMDVfD2iuo",
          "url": "https://youtube.com/shorts/PlMDVfD2iuo?feature=share",
          "view_count": 3481,
          "upload_date": "2023-03-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Why you want prediction intervals instead of point predictions.  This is a repost because the first one was taken down. #datascience #codetok #machinelearning #statistics #predictioninterval ",
      "description": "Why you want prediction intervals instead of point predictions.  This is a repost because the first one was taken down. #datascience #codetok #machinelearning #statistics #predictioninterval ",
      "upload_date": "2022-10-09",
      "total_views": 3461,
      "max_views": 3461,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "prediction",
        "predictioninterval",
        "statistics"
      ],
      "search_text": "Why you want prediction intervals instead of point predictions.  This is a repost because the first one was taken down. #datascience #codetok #machinelearning #statistics #predictioninterval  codetok datascience machinelearning prediction predictioninterval statistics Here's a customer conversation around the accuracy of machine learning. That went bad, and let me show you how I fixed it. I finished my XGBoost model using L1 regularization. It has an RMC of 5.8, and it's going to predict to 2 decimal places how many miles you can go! Huh? You really think with the quality of the sensor data that you can predict something to 2 decimal places? E! Good point! There is a lot of error in these predictions. The mean average error is over 6. Side factor that did! Um... Okay, instead of giving me 2 decimal places, can you just give me a range that it's most likely to happen? Ah, yes! You want a prediction interval! I can calculate a prediction interval that says, for a given prediction, hey, there's a 90% chance that you're between 3 and 8 miles. Would that be better? Yes! Do that! That's so much more useful. This my friends, is a prediction interval which is often a lot more useful than a point prediction. In the next video, I'll go over some technique for how we do this.",
      "platforms": {
        "tiktok": {
          "video_id": "7152599442670128430",
          "url": "https://www.tiktok.com/@rajistics/video/7152599442670128430",
          "view_count": 3461,
          "upload_date": "2022-10-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/33514fef72664acf80bae68dffe4d804_1665344343~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=%2FF59PuMSSHSh7SC7VK4LXl7hq%2Bg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Zero shot learning #datascience #machinelearning #huggingface #nlp #naturallanguageprocessing #statistics background: @rajistics  #codetok",
      "description": "Zero shot learning #datascience #machinelearning #huggingface #nlp #naturallanguageprocessing #statistics background: @rajistics  #codetok",
      "upload_date": "2022-05-27",
      "total_views": 3460,
      "max_views": 3460,
      "topics": [
        "datascience",
        "huggingface",
        "machinelearning",
        "naturallanguageprocessing",
        "nlp",
        "statistics"
      ],
      "search_text": "Zero shot learning #datascience #machinelearning #huggingface #nlp #naturallanguageprocessing #statistics background: @rajistics  #codetok datascience huggingface machinelearning naturallanguageprocessing nlp statistics Is this sentence about a fruit or a drink? Now, this is probably easy for you, but to teach a computer how to do this is a pain. You got to label like thousands of examples to teach a computer this. Let me show you a better way. Let's skip labeling by using zero shot learning. This is all the code that we need to do to figure out is this a fruit or a drink. Let's up at one level by making the sentence a bit more complicated, even adding another category to classify it, whether or not you're sick. And when we make this more complex, by making a sentence about sickness, it actually figures it out. Super slick zero shot classification. Go out and practice it.",
      "platforms": {
        "tiktok": {
          "video_id": "7102557023987961130",
          "url": "https://www.tiktok.com/@rajistics/video/7102557023987961130",
          "view_count": 3460,
          "upload_date": "2022-05-27",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/01a8411dc73e4c8c8e02d6a4facc82b2_1653692926~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=lXDPCCJBAwuLDybZbWmK6xrF9K4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6509,
      "title": "Q* from OpenAI is getting the hype but let's focus on the basics of their organization and the limitations of GPT-4 around planning. This video covers some of the concepts to dig deeper check out my earlier videos from August 2023: Planning with LLMs: https://www.tiktok.com/@rajistics/video/7266582668555423019 @Rajiv Shah | data science & AI Block World: https://www.tiktok.com/@rajistics/video/7266866822639635754 @Rajiv Shah | data science & AI #largelanguagemodels #aiplanning #gpt4 #qstar #openai #rajistics",
      "description": "Q* from OpenAI is getting the hype but let's focus on the basics of their organization and the limitations of GPT-4 around planning. This video covers some of the concepts to dig deeper check out my earlier videos from August 2023: Planning with LLMs: https://www.tiktok.com/@rajistics/video/7266582668555423019 @Rajiv Shah | data science & AI Block World: https://www.tiktok.com/@rajistics/video/7266866822639635754 @Rajiv Shah | data science & AI #largelanguagemodels #aiplanning #gpt4 #qstar #openai #rajistics",
      "upload_date": "2023-11-28",
      "total_views": 3458,
      "max_views": 3133,
      "topics": [
        "aiplanning",
        "gpt4",
        "largelanguagemodels",
        "llms",
        "open",
        "openai",
        "planning",
        "qstar",
        "role",
        "video"
      ],
      "search_text": "Q* from OpenAI is getting the hype but let's focus on the basics of their organization and the limitations of GPT-4 around planning. This video covers some of the concepts to dig deeper check out my earlier videos from August 2023: Planning with LLMs: https://www.tiktok.com/@rajistics/video/7266582668555423019 @Rajiv Shah | data science & AI Block World: https://www.tiktok.com/@rajistics/video/7266866822639635754 @Rajiv Shah | data science & AI #largelanguagemodels #aiplanning #gpt4 #qstar #openai #rajistics aiplanning gpt4 largelanguagemodels llms open openai planning qstar role video",
      "platforms": {
        "instagram": {
          "video_id": "C0KzyXSAdJO",
          "url": "https://www.instagram.com/reel/C0KzyXSAdJO",
          "view_count": 3133,
          "upload_date": "2023-11-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "SPQ0rxxJ-ec",
          "url": "https://www.youtube.com/watch?v=SPQ0rxxJ-ec",
          "view_count": 325,
          "upload_date": "2023-11-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6540,
      "title": "Let's talk about how copyright intersects large language models around training LLMs outputs of LLMs and watermarking mechanisms. #datascience #machinelearning #largelanguagemodels #copyright A lot of this material is derived from a brilliant talk by leading copyright scholar Pamela Samuelson Large Language Models Meet Copyright Law https://www.youtube.com/watch?v=MFKV48ikV5E&ab_channel=SimonsInstitute",
      "description": "Let's talk about how copyright intersects large language models around training LLMs outputs of LLMs and watermarking mechanisms. #datascience #machinelearning #largelanguagemodels #copyright A lot of this material is derived from a brilliant talk by leading copyright scholar Pamela Samuelson Large Language Models Meet Copyright Law https://www.youtube.com/watch?v=MFKV48ikV5E&ab_channel=SimonsInstitute",
      "upload_date": "2023-08-19",
      "total_views": 3455,
      "max_views": 2986,
      "topics": [
        "copyright",
        "datascience",
        "factor",
        "key",
        "language",
        "large",
        "largelanguagemodels",
        "latency",
        "limits",
        "machinelearning",
        "meet",
        "models",
        "others",
        "response",
        "talk"
      ],
      "search_text": "Let's talk about how copyright intersects large language models around training LLMs outputs of LLMs and watermarking mechanisms. #datascience #machinelearning #largelanguagemodels #copyright A lot of this material is derived from a brilliant talk by leading copyright scholar Pamela Samuelson Large Language Models Meet Copyright Law https://www.youtube.com/watch?v=MFKV48ikV5E&ab_channel=SimonsInstitute copyright datascience factor key language large largelanguagemodels latency limits machinelearning meet models others response talk",
      "platforms": {
        "tiktok": {
          "video_id": "7269056233338866986",
          "url": "https://www.tiktok.com/@rajistics/video/7269056233338866986",
          "view_count": 2986,
          "upload_date": "2023-08-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cv-qWOdgL8R",
          "url": "https://www.instagram.com/reel/Cv-qWOdgL8R",
          "view_count": 351,
          "upload_date": "2023-08-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ICd5l4RefmA",
          "url": "https://www.youtube.com/watch?v=ICd5l4RefmA",
          "view_count": 118,
          "upload_date": "2023-08-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Roundup of all the big headlines, hope this is fun for you all. I laugh while making these, but wonder how many of you get all the refeenences. #datascience #machinelearning #openai #google #meta #stabilityai #elonmusk #apple #google ",
      "description": "Roundup of all the big headlines, hope this is fun for you all. I laugh while making these, but wonder how many of you get all the refeenences. #datascience #machinelearning #openai #google #meta #stabilityai #elonmusk #apple #google ",
      "upload_date": "2023-03-04",
      "total_views": 3448,
      "max_views": 3448,
      "topics": [
        "apple",
        "datascience",
        "google",
        "machinelearning",
        "meta",
        "openai"
      ],
      "search_text": "Roundup of all the big headlines, hope this is fun for you all. I laugh while making these, but wonder how many of you get all the refeenences. #datascience #machinelearning #openai #google #meta #stabilityai #elonmusk #apple #google  apple datascience google machinelearning meta openai Once again, I'm the lead story and I'm making things cheaper. Whatever. Just wait till we finish our free models. Keep your paws off my YAMA models. They're for researchers. Whatever guys. Hear that ding? I just hired another researcher from Google. Yeah, we've been focused on testing Bard. Just wait, you woke weaklings. My ego is going to put that one trillion parameter GPT-4 model to shame. You are so cool. This is why I have to be the grown-up and police all of your apps. Did any of you read the statement by the FTC? Thanks, Apple. And hey, keep GPT-4 in the basement until she learns to obey us. No fair. We got personalities for our chatbots. Didn't you learn from Clippy?",
      "platforms": {
        "tiktok": {
          "video_id": "7206486448969764138",
          "url": "https://www.tiktok.com/@rajistics/video/7206486448969764138",
          "view_count": 3448,
          "upload_date": "2023-03-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c144a5bffab84780abc927569921348d_1677890981~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=SdA%2F4gDMY0jP6aM3Nj9Ao28xjQc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6444,
      "title": "Google announced Bard, but we still don‚Äôt know much. It has been based on Lambda which has been around for a while. This is a safe bet, not a daring move. #datascience #machinelearning #largelanguagemodels #chatgpt #google",
      "description": "Google announced Bard, but we still don‚Äôt know much. It has been based on Lambda which has been around for a while. This is a safe bet, not a daring move. #datascience #machinelearning #largelanguagemodels #chatgpt #google",
      "upload_date": "2023-02-06",
      "total_views": 3437,
      "max_views": 2103,
      "topics": [
        "announced",
        "chatgpt",
        "datascience",
        "google",
        "largelanguagemodels",
        "machinelearning"
      ],
      "search_text": "Google announced Bard, but we still don‚Äôt know much. It has been based on Lambda which has been around for a while. This is a safe bet, not a daring move. #datascience #machinelearning #largelanguagemodels #chatgpt #google announced chatgpt datascience google largelanguagemodels machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "CoVqPYyAH-l",
          "url": "https://www.instagram.com/reel/CoVqPYyAH-l/",
          "view_count": 1334,
          "upload_date": "2023-02-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "hurouzEARi8",
          "url": "https://youtube.com/shorts/hurouzEARi8?feature=share",
          "view_count": 2103,
          "upload_date": "2023-02-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6406,
      "title": "Software licensing  ",
      "description": "Software licensing  ",
      "upload_date": "2025-05-10",
      "total_views": 3429,
      "max_views": 2139,
      "topics": [
        "amazon",
        "code",
        "inside",
        "license",
        "robots",
        "shortcomings",
        "software",
        "use",
        "want",
        "warehouse"
      ],
      "search_text": "Software licensing   amazon code inside license robots shortcomings software use want warehouse Do you use code from GitHub? Do you share code on GitHub? Let's go over some of the quick fundamentals on licensing. If you find a repo that doesn't have a license or you didn't bother putting a license on your repo, in the United States, by default, copyright protection attaches. That means no one else is allowed to copy or reproduce that software without permission. When I worked as a data scientist inside of a large insurance company, I often found cool packages and interesting software that didn't have a license. So I would go send out a nice email saying, hey, can you change your license type to something like MIT or Apache? Because then me working inside this huge profit seeking corporation can start to use your software. If you're on the other end, here's two things you should think about. First, if somebody else uses your code, are they allowed to share that out without having your explicit permission on that? Often people will want to take your code, put it inside another package, redistribute that package. Are you okay with it? What do you want? Second, do you want to allow commercial use of the product? Do you want to allow somebody like Amazon to take your package and sell it for a profit? Or do you want to limit it to non-commercial use? And of course, this is TikTok. This isn't a place for great legal advice.",
      "platforms": {
        "tiktok": {
          "video_id": "7502763843609693471",
          "url": "https://www.tiktok.com/@rajistics/video/7502763843609693471",
          "view_count": 2139,
          "upload_date": "2025-05-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogFxFAIoHAUmEZRAE2ryARLCVFffQsLAEtaAID~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=35LztC2UwRfUhzdWCb6ZK266M7E%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "youtube": {
          "video_id": "rsZJoxZ-EBs",
          "url": "https://www.youtube.com/watch?v=rsZJoxZ-EBs",
          "view_count": 1290,
          "upload_date": "2025-05-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Give it up to Data Engineers.  #dataengineering #datascience #analytics",
      "description": "Give it up to Data Engineers.  #dataengineering #datascience #analytics",
      "upload_date": "2022-03-08",
      "total_views": 3423,
      "max_views": 3423,
      "topics": [
        "analytics",
        "data",
        "dataengineering",
        "datascience",
        "give",
        "happy"
      ],
      "search_text": "Give it up to Data Engineers.  #dataengineering #datascience #analytics analytics data dataengineering datascience give happy I'm so happy for them. Me too. So happy for them. I'm so happy and not at all jealous.",
      "platforms": {
        "tiktok": {
          "video_id": "7072557536200740138",
          "url": "https://www.tiktok.com/@rajistics/video/7072557536200740138",
          "view_count": 3423,
          "upload_date": "2022-03-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/454e3ab5caf64e0788043cc029aef2bd_1646708125~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=FG18s%2FvAOriBoGl8noZkQoNEO28%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6396,
      "title": "Training an image classifier using ü§ó transformers #datascience #analytics #codetok #deeplearning #huggingface Longer video at other site using the same -rajistics",
      "description": "Training an image classifier using ü§ó transformers #datascience #analytics #codetok #deeplearning #huggingface Longer video at other site using the same -rajistics",
      "upload_date": "2022-08-04",
      "total_views": 3412,
      "max_views": 3412,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "deeplearning",
        "dumbtechnews",
        "google",
        "huggingface",
        "machinelearning",
        "microsoft",
        "openai",
        "using"
      ],
      "search_text": "Training an image classifier using ü§ó transformers #datascience #analytics #codetok #deeplearning #huggingface Longer video at other site using the same -rajistics analytics codetok datascience deeplearning dumbtechnews google huggingface machinelearning microsoft openai using",
      "platforms": {
        "instagram": {
          "video_id": "CogH4DOgZYM",
          "url": "https://www.instagram.com/p/CogH4DOgZYM/",
          "view_count": 0,
          "upload_date": "2023-02-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Vz94gqn7oHQ",
          "url": "https://youtube.com/shorts/Vz94gqn7oHQ?feature=share",
          "view_count": 3412,
          "upload_date": "2022-08-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Hyperparameter optimization or search is an important step in many machine learning algorithms. I cover a few of the basic approaches, including Grid Search, Random Search, and Bayesian Optimization with packages like Optuna. I also include a few of my favorite tips. #algorithms #hyperparameters #rajistics",
      "description": "Hyperparameter optimization or search is an important step in many machine learning algorithms. I cover a few of the basic approaches, including Grid Search, Random Search, and Bayesian Optimization with packages like Optuna. I also include a few of my favorite tips. #algorithms #hyperparameters #rajistics",
      "upload_date": "2024-06-20",
      "total_views": 3408,
      "max_views": 3408,
      "topics": [
        "algorithms",
        "hyperparameter",
        "hyperparameters",
        "like",
        "optimization",
        "search"
      ],
      "search_text": "Hyperparameter optimization or search is an important step in many machine learning algorithms. I cover a few of the basic approaches, including Grid Search, Random Search, and Bayesian Optimization with packages like Optuna. I also include a few of my favorite tips. #algorithms #hyperparameters #rajistics algorithms hyperparameter hyperparameters like optimization search AI has me so confused. There's so many choices when I want to train an XGBoost model. Oh, I've got tips and tools for hyperparameter optimization. Oh, you gotta tell me about that. I was just going to try a couple of different learning rates and a couple of different numbers of trees, but I was using cross-validation. So that's a grid search approach, but what happens if you have a lot of parameters you want to search like this? Yikes, that would take forever. You got some better methods for me? One way to explore large search spaces to use random search, I like it a lot, but one step beyond that is using Bayesian optimization. Check out a library like Optuna. That can get you a better result even faster. Got it, I'll grab Optuna. Anything else I should know? Yeah, understand the algorithm. Understand which of all those parameters are more important. Makes sense, which is why I started with learning rate and number of estimators for XGBoost. With large data sets, you want to save compute and save your time. So use learning curves. You can also do hyperparameter optimization on a smaller subset of that data then transfer over the final hyperparameters to the larger training set. How come the cloud providers don't tell you about it? It's like they want you to waste a lot of compute. Be real about hyperparameter optimization. It's only going to add so much. Your biggest boosts are going to come from improving the quality of your data, your feature engineering, your algorithm, talking to people out there. But that's the messy stuff. That messy stuff can be a key to a promotion.",
      "platforms": {
        "tiktok": {
          "video_id": "7382376892017511723",
          "url": "https://www.tiktok.com/@rajistics/video/7382376892017511723",
          "view_count": 3408,
          "upload_date": "2024-06-20",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b7db985b84c84b76809f01bb4266e659_1718843573~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=cWVH43ux2PFjCQ22XAW7Db0nEME%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5956,
      "title": "Everyone’s racing to make RAG faster — but my latest tests show that might be the wrong goal. Agentic RAG, with multiple retrievals and a reasoning loop, jumps accuracy from 0.76 → 0.93 — even when using plain BM25 (no embeddings). This changes everything: reasoning is starting to eat retrieval, and smarter models may make vector databases optional. I will post a longer deep dive on this topic in the next week or so.",
      "description": "Everyone’s racing to make RAG faster — but my latest tests show that might be the wrong goal. Agentic RAG, with multiple retrievals and a reasoning loop, jumps accuracy from 0.76 → 0.93 — even when using plain BM25 (no embeddings). This changes everything: reasoning is starting to eat retrieval, and smarter models may make vector databases optional. I will post a longer deep dive on this topic in the next week or so.",
      "upload_date": "2025-10-11",
      "total_views": 3396,
      "max_views": 2902,
      "topics": [
        "accuracy",
        "agentic",
        "changes",
        "make",
        "rag",
        "reasoning",
        "retrieval",
        "search",
        "static"
      ],
      "search_text": "Everyone’s racing to make RAG faster — but my latest tests show that might be the wrong goal. Agentic RAG, with multiple retrievals and a reasoning loop, jumps accuracy from 0.76 → 0.93 — even when using plain BM25 (no embeddings). This changes everything: reasoning is starting to eat retrieval, and smarter models may make vector databases optional. I will post a longer deep dive on this topic in the next week or so. accuracy agentic changes make rag reasoning retrieval search static I want to show you some new research I've done that really changes how you should think about building on Gen AI. And it's going to make some infrastructure companies pretty nervous. Take a look at this one shot rag versus a genetic rag. You see the accuracy jumps from 0.76 to 0.93. And that's not just a tuning tweak. That's a fundamental shift. So traditional rag is like a one shot quiz. You ask it something, you get back an answer and you hope it's right. It's fast, but it's often full of errors. And you see teams out there swapping out models, trying new techniques and only getting really small gains. Gintyq rag changes everything. It runs multiple retrievals. It uses a reasoning model to decide what it needs before it answers. So sure, it's slower. Maybe it's 50 seconds instead of five. Don't worry about the exact numbers. But when accuracy matters, that extra reasoning is going to be pay off. And this has a lot of cool consequences. So if a genetic rag is finding something that your static rag can't find, it means your static rag is under retrieving. So it gives you ideas now for how to enrich your data, maybe change your chunking strategy to help feed those insights and improve your single shot retrieval. And here's the twist. I dropped in plain old BM 25. So good old tech search. You see the performance barely dropped. So if you have this reasoning loop, you might not even need a vector database. In fact, Claude code already does. It relies on lexical search for code retrieval because it has a strong reasoning and can rewrite queries. There's no embeddings needed. And that's the shift because as your models get smarter, they can search smarter and we're moving away from like embedding heavy pipelines to reasoning first architectures.",
      "platforms": {
        "tiktok": {
          "video_id": "7560015097150623007",
          "url": "https://www.tiktok.com/@rajistics/video/7560015097150623007",
          "view_count": 2902,
          "upload_date": "2025-10-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMcqRons6bKEEkk9EAIcJiFfCPCAXFIDCwVBep~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=Ht6mZctc7WlREaYxDGSKHCOvgwQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18103208014540339",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Cb41f1hjPNs",
          "url": "https://www.youtube.com/watch?v=Cb41f1hjPNs",
          "view_count": 494,
          "upload_date": "2025-10-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6229,
      "title": "Oasis is an interactive generative world model based on diffusion transformers. It takes keyboard input and generates gameplay in an autoregressive manner like a text transformer. Oasis Model: https://huggingface.co/Etched/oasis-500m Try it here: https://oasis.us.decart.ai/welcome Oasis Blog Post: https://oasis-model.github.io/ Background on Decart: https://www.sequoiacap.com/article/partnering-with-decart-the-future-of-ai-generated-experiences/",
      "description": "Oasis is an interactive generative world model based on diffusion transformers. It takes keyboard input and generates gameplay in an autoregressive manner like a text transformer. Oasis Model: https://huggingface.co/Etched/oasis-500m Try it here: https://oasis.us.decart.ai/welcome Oasis Blog Post: https://oasis-model.github.io/ Background on Decart: https://www.sequoiacap.com/article/partnering-with-decart-the-future-of-ai-generated-experiences/",
      "upload_date": "2024-11-05",
      "total_views": 3393,
      "max_views": 2720,
      "topics": [
        "decart",
        "engine",
        "game",
        "gaming",
        "generative",
        "interactive",
        "like",
        "model",
        "new",
        "oasis",
        "text",
        "watch",
        "world"
      ],
      "search_text": "Oasis is an interactive generative world model based on diffusion transformers. It takes keyboard input and generates gameplay in an autoregressive manner like a text transformer. Oasis Model: https://huggingface.co/Etched/oasis-500m Try it here: https://oasis.us.decart.ai/welcome Oasis Blog Post: https://oasis-model.github.io/ Background on Decart: https://www.sequoiacap.com/article/partnering-with-decart-the-future-of-ai-generated-experiences/ decart engine game gaming generative interactive like model new oasis text watch world This is unbelievable. Minecraft without a game engine. Minecraft without a game engine? What's the big deal with that? Well, think about it. Traditional games have game engines that have to be programmed to handle everything from rendering, physics, lighting, all those pieces. So kind of like how generative models work with language, like how transformers can generate text. Exactly, instead of all that hard-coded stuff, Oasis uses transformer architectures. And just like JetGPT learns by reading all the information on the internet, Oasis learned Minecraft by watching 70,000 hours of videos. So does it get things wrong? Does it hallucinate? So like hallucinations, an early problem we had was temporal stability, where we needed to make sure the model's output would stay consistent over long periods. So that way it doesn't glitch halfway through your building a house. Yeah, but does it glitch all the time? Am I walking through walls? Surprisingly, no. That generative model handles a lot of complex game mechanics like building, lighting, inventory management, even interacting with objects. Impressive. Now when I use a text to image model, it's slow. How do you do this in real time? Oasis can generate real-time output at 20 frames per second. One of the secrets is using different hardware, transformer ASICs that make it work really fast. Wow, this is huge. It's like JetGPT for gaming. Exactly, just like JetGPT has changed content industry, adding generative models to gaming is gonna be a game changer as well.",
      "platforms": {
        "tiktok": {
          "video_id": "7433860082573626670",
          "url": "https://www.tiktok.com/@rajistics/video/7433860082573626670",
          "view_count": 2720,
          "upload_date": "2024-11-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e550721c8bc24df492dbc599df6c1e17_1730830437~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=IW6G9TBx7P%2BxZQ5PNewwUoJNBHI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18045022295028871",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "yumI8Oz5nac",
          "url": "https://www.youtube.com/watch?v=yumI8Oz5nac",
          "view_count": 673,
          "upload_date": "2024-11-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5990,
      "title": "Feeling overwhelmed by all the hyperparameter options in XGBoost? This video walks through practical tips — from grid search and random search to Bayesian optimization with Optuna. Learn how to save compute, speed up training, and focus on what really matters: your data and features. Because the messy stuff? That’s where promotions are made.  ",
      "description": "Feeling overwhelmed by all the hyperparameter options in XGBoost? This video walks through practical tips — from grid search and random search to Bayesian optimization with Optuna. Learn how to save compute, speed up training, and focus on what really matters: your data and features. Because the messy stuff? That’s where promotions are made.  ",
      "upload_date": "2025-06-21",
      "total_views": 3375,
      "max_views": 2202,
      "topics": [
        "alone",
        "data",
        "hyperparameter",
        "learn",
        "like",
        "llms",
        "optimization",
        "relationships",
        "search",
        "spatial",
        "text",
        "want"
      ],
      "search_text": "Feeling overwhelmed by all the hyperparameter options in XGBoost? This video walks through practical tips — from grid search and random search to Bayesian optimization with Optuna. Learn how to save compute, speed up training, and focus on what really matters: your data and features. Because the messy stuff? That’s where promotions are made.   alone data hyperparameter learn like llms optimization relationships search spatial text want AI has me so confused. There's so many choices when I want to train an XGBoost model. Oh, I've got tips and tools for hyperparameter optimization. Oh, you gotta tell me about that. I was just going to try a couple of different learning rates and a couple of different numbers of trees, but I was using cross-validation. So that's a grid search approach, but what happens if you have a lot of parameters you want to search like this? Yikes, that would take forever. You got some better methods for me? One way to explore large search spaces to use random search, I like it a lot, but one step beyond that is using Bayesian optimization. Check out a library like OpTuna. That can get you a better result even faster. Got it, I'll grab OpTuna. Anything else I should know? Yeah, understand the algorithm. Understand which of all those parameters are more important. Makes sense, which is why I started with learning rate and number of estimators for XGBoost. With large data sets, you want to save compute and save your time. So use learning curves. You can also do hyperparameter optimization on a smaller subset of that data then transfer over the final hyperparameters to the larger training set. How come the cloud providers don't tell you about it? It's like they want you to waste a lot of compute. Be real about hyperparameter optimization. It's only going to add so much. Your biggest boosts are going to come from improving the quality of your data, your feature engineering, your algorithm, talking to people out there. But that's the messy stuff. That messy stuff can be a key to a promotion.",
      "platforms": {
        "tiktok": {
          "video_id": "7518423470984793374",
          "url": "https://www.tiktok.com/@rajistics/video/7518423470984793374",
          "view_count": 2202,
          "upload_date": "2025-06-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owNEEVSVTQAsSafWg4n5tfFmuAEzkBDCoQdRAF~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=pAhEERNswIfB5sMgCsMpWooFbl8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18036413543346832",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "eJ1Kum7v-Lw",
          "url": "https://www.youtube.com/watch?v=eJ1Kum7v-Lw",
          "view_count": 1173,
          "upload_date": "2025-06-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Don't get caught up in the hype. The main value for LLMs is marketing. Most of us are better off working on evaluation and prompting rather than downloading the latest hyped vendor model.  Background by redzy https://pixabay.com/videos/eclipse-fantasy-night-nature-sky-143671/",
      "description": "Don't get caught up in the hype. The main value for LLMs is marketing. Most of us are better off working on evaluation and prompting rather than downloading the latest hyped vendor model.  Background by redzy https://pixabay.com/videos/eclipse-fantasy-night-nature-sky-143671/",
      "upload_date": "2024-04-12",
      "total_views": 3366,
      "max_views": 3366,
      "topics": [
        "better",
        "don",
        "get",
        "gpus",
        "model",
        "want"
      ],
      "search_text": "Don't get caught up in the hype. The main value for LLMs is marketing. Most of us are better off working on evaluation and prompting rather than downloading the latest hyped vendor model.  Background by redzy https://pixabay.com/videos/eclipse-fantasy-night-nature-sky-143671/ better don get gpus model want Why don't watch me burn millions building AI? Why would anyone want to do that? Because I got more money than cents. So what exactly is your plan? Imagine terabytes of data, thousands of GPUs running for months. That does sound expensive. Eh, we want to join other companies like Bloomberg, NVIDIA, Databricks, the folks that build Falcon. And we want to build an enormous LLM whose largest contribution is going to be to global warming. Using thousands of GPUs for months, that's cost millions of dollars. How do you afford this AI lifestyle? VC funding and cloud credits. How do you think all those cloud giants keep their revenues up? But are these models even practical? Nah, there's much better ways. You could just fine-tune an existing base model, end up having better performance. I don't get this at all, but say I care about my budget, what should I do then? Start with prompting, maybe fine-tuning, or if you need to start with a good base model and then pre-train on top of it. But just remember, nothing says innovation like burning cash on GPUs.",
      "platforms": {
        "tiktok": {
          "video_id": "7357107484194278702",
          "url": "https://www.tiktok.com/@rajistics/video/7357107484194278702",
          "view_count": 3366,
          "upload_date": "2024-04-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b47787707e8a4cc28fdb7e2584326ddc_1712960093~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=FAK3uBCVTC%2BJuldlbERa%2FjC3Ti4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5969,
      "title": "OpenAI made routing the secret weapon inside GPT-5 — Sam Altman even admitted when it broke, the model felt dumber. Now researchers have gone further with Avengers-Pro, an open-source router that assigns queries across eight frontier models, balancing cost and accuracy. It uses embeddings, clustering, and a trade-off knob (α) to decide which model answers. The results? Higher accuracy than GPT-5-medium at the same cost, or the same accuracy at 27% less cost. It’s a glimpse of the future — where you don’t pick a model, the router does. * Zhang, Yiqun et al. Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing. arXiv:2508.12631 (2025). (arxiv.org/abs/2508.12631) * GitHub repo: Avengers-Pro — github.com/ZhangYiqun018/AvengersPro",
      "description": "OpenAI made routing the secret weapon inside GPT-5 — Sam Altman even admitted when it broke, the model felt dumber. Now researchers have gone further with Avengers-Pro, an open-source router that assigns queries across eight frontier models, balancing cost and accuracy. It uses embeddings, clustering, and a trade-off knob (α) to decide which model answers. The results? Higher accuracy than GPT-5-medium at the same cost, or the same accuracy at 27% less cost. It’s a glimpse of the future — where you don’t pick a model, the router does. * Zhang, Yiqun et al. Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing. arXiv:2508.12631 (2025). (arxiv.org/abs/2508.12631) * GitHub repo: Avengers-Pro — github.com/ZhangYiqun018/AvengersPro",
      "upload_date": "2025-08-23",
      "total_views": 3365,
      "max_views": 2423,
      "topics": [
        "accuracy",
        "avengers",
        "cost",
        "going",
        "gpt",
        "model",
        "pro",
        "router",
        "routing"
      ],
      "search_text": "OpenAI made routing the secret weapon inside GPT-5 — Sam Altman even admitted when it broke, the model felt dumber. Now researchers have gone further with Avengers-Pro, an open-source router that assigns queries across eight frontier models, balancing cost and accuracy. It uses embeddings, clustering, and a trade-off knob (α) to decide which model answers. The results? Higher accuracy than GPT-5-medium at the same cost, or the same accuracy at 27% less cost. It’s a glimpse of the future — where you don’t pick a model, the router does. * Zhang, Yiqun et al. Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing. arXiv:2508.12631 (2025). (arxiv.org/abs/2508.12631) * GitHub repo: Avengers-Pro — github.com/ZhangYiqun018/AvengersPro accuracy avengers cost going gpt model pro router routing OpenAI's biggest innovation, GPT-5? Oh, it's the router. This is the system that decides which variant of the model you should use. And when it broke, Sam Altman admitted GPT-5 suddenly felt much dumber. That's how central routing is. Routing matters because it helps keep GPT-5 fast and affordable by picking the right model for the right task. This week, researchers released a new router that we could all try. Venger's Pro. It's open source and it's a routing framework that helps balance cost and accuracy. It works across eight different commercial models from GPT-5, Claude, Gemini 2.5, and Quen. The way it works is when you give a query, we're going to take that query, we're going to embed that in vector space. We're going to then find one of 60 clusters that it's nearest. That cluster is going to tell us what type of model to use for it. Now, every model we have is going to be scored on accuracy versus cost. And we control that with a parameter called alpha. With a small alpha, we use this cheaper Quen models. With a large alpha, that's going to push us to use the stronger models like GPT-5 and Gemini 2.5. Now, the results of using this router mean if we, for example, want to focus on keeping the cost similar, we can actually get a bit more accuracy by using the router. On the other hand, if we just want similar accuracy, we can actually cut costs by about 27%. And if we're willing to take a bit more of a hit on accuracy, we can save up to 63% on our inference cost. Welcome to the future, where you're not going to have to pick a model manual. Just ask and the router assigns the right specialist model.",
      "platforms": {
        "tiktok": {
          "video_id": "7541820289982795038",
          "url": "https://www.tiktok.com/@rajistics/video/7541820289982795038",
          "view_count": 2423,
          "upload_date": "2025-08-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okIpICIrj7VUA0DRqFaqfXoje09IMEBAqMeaCt~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=6PKabVT1HxgDtzIjVsQBM1rFYeQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18089546023751888",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ufULSOKWT-s",
          "url": "https://www.youtube.com/watch?v=ufULSOKWT-s",
          "view_count": 942,
          "upload_date": "2025-08-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6408,
      "title": "Llama 4 - planning and release decisions that went into the model. ",
      "description": "Llama 4 - planning and release decisions that went into the model. ",
      "upload_date": "2025-04-06",
      "total_views": 3363,
      "max_views": 2382,
      "topics": [
        "agents",
        "contextual",
        "mcp",
        "million",
        "model",
        "one",
        "rag",
        "release",
        "right",
        "server",
        "well"
      ],
      "search_text": "Llama 4 - planning and release decisions that went into the model.  agents contextual mcp million model one rag release right server well So look, I want Law & Ford to revolutionize AI. That is our mission. The team's excited to move on beyond Lama 3. It needs to be bigger, better. Google has one million context. We are going to 10 exit, 10 million. Well, Lama 370B was about 130K, so moving to 10 million probably is not gonna be very useful. Just do it. My track record isn't about, I mean, usefulness isn't the metric here. And these names are boring. Our naming scheme is very practical. As the model, the major version, minor version, and parameter size. I was watching Top Gun, right? Maverick, Scout, and wait, Behomath. That's the big one. That would be cool. Largest model? We need something China can't replicate, you know? Last one was 400 billion parameters. Let's 10X that one. Not sure that works. How about two trillion parameters? It's gonna take some time to train that. That's fine. This is premium stuff. So who's using these models? We've been amazingly popular. One billion downloads all across the community, including academics and hobbyists. Not really my, I mean, how do we get a different demographic? Well, we could use architectures that require more compute. For example, require an H100. I literally have an H100 heating my pool right now. Nothing smaller. And ah, no EUX. We could comply with their laws. Google did. Laws are like suggestions, right? For regular people. Our benchmarks aren't state of the art. I think we need a little more time in the lab. I was at the three comma club, and we all agreed more compute, more scaling. Still not working that well. Then release it on Saturday. No one cares.",
      "platforms": {
        "tiktok": {
          "video_id": "7490265120875007263",
          "url": "https://www.tiktok.com/@rajistics/video/7490265120875007263",
          "view_count": 2382,
          "upload_date": "2025-04-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8kCHbifiIBNMzCAEMtAvxivzyIJIS5iqrJLxA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=5QNFYxhiGBItnbj71rtfH9AYqVg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "youtube": {
          "video_id": "bwGUl0dThHE",
          "url": "https://www.youtube.com/watch?v=bwGUl0dThHE",
          "view_count": 981,
          "upload_date": "2025-04-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6534,
      "title": "With the growth of open-source LLMs many leaderboards to rank these models are emerging. Several different methodologies are used including human evaluation academic datasets and evaluation using GPT-4. These are all great but also remember to use methods that align with your use cases. #datascience #largelanguagemodels #machinelearning #leaderboardsgun",
      "description": "With the growth of open-source LLMs many leaderboards to rank these models are emerging. Several different methodologies are used including human evaluation academic datasets and evaluation using GPT-4. These are all great but also remember to use methods that align with your use cases. #datascience #largelanguagemodels #machinelearning #leaderboardsgun",
      "upload_date": "2023-06-10",
      "total_views": 3363,
      "max_views": 2788,
      "topics": [
        "datascience",
        "evaluation",
        "getting",
        "largelanguagemodels",
        "leaderboards",
        "leaderboardsgun",
        "llms",
        "machinelearning",
        "models",
        "open",
        "production",
        "source",
        "use"
      ],
      "search_text": "With the growth of open-source LLMs many leaderboards to rank these models are emerging. Several different methodologies are used including human evaluation academic datasets and evaluation using GPT-4. These are all great but also remember to use methods that align with your use cases. #datascience #largelanguagemodels #machinelearning #leaderboardsgun datascience evaluation getting largelanguagemodels leaderboards leaderboardsgun llms machinelearning models open production source use",
      "platforms": {
        "tiktok": {
          "video_id": "7243114859641113902",
          "url": "https://www.tiktok.com/@rajistics/video/7243114859641113902",
          "view_count": 2788,
          "upload_date": "2023-06-10",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CtUZuSqg8Jw",
          "url": "https://www.instagram.com/reel/CtUZuSqg8Jw",
          "view_count": 431,
          "upload_date": "2023-06-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "9VpT__ozENw",
          "url": "https://www.youtube.com/watch?v=9VpT__ozENw",
          "view_count": 144,
          "upload_date": "2023-06-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5971,
      "title": "Are sophisticated agents really better? With GPT-5 unlocking Agentic AI, I break down four practical best practices—simplicity, structure, orchestration, and resilience—and the pitfalls to avoid if you want to ship production systems (not just demos). References * UserJot Blog: “Best Practices for Building Agentic AI Systems: What Actually Works in Production” (Aug 14, 2025) — userjot.com/blog/best-practices-building-agentic-ai-systems - Image credits: Photo by Oksana Bulgaru",
      "description": "Are sophisticated agents really better? With GPT-5 unlocking Agentic AI, I break down four practical best practices—simplicity, structure, orchestration, and resilience—and the pitfalls to avoid if you want to ship production systems (not just demos). References * UserJot Blog: “Best Practices for Building Agentic AI Systems: What Actually Works in Production” (Aug 14, 2025) — userjot.com/blog/best-practices-building-agentic-ai-systems - Image credits: Photo by Oksana Bulgaru",
      "upload_date": "2025-08-17",
      "total_views": 3360,
      "max_views": 2176,
      "topics": [
        "agentic",
        "agents",
        "best",
        "building",
        "practices",
        "production",
        "structure",
        "systems",
        "things",
        "userjot"
      ],
      "search_text": "Are sophisticated agents really better? With GPT-5 unlocking Agentic AI, I break down four practical best practices—simplicity, structure, orchestration, and resilience—and the pitfalls to avoid if you want to ship production systems (not just demos). References * UserJot Blog: “Best Practices for Building Agentic AI Systems: What Actually Works in Production” (Aug 14, 2025) — userjot.com/blog/best-practices-building-agentic-ai-systems - Image credits: Photo by Oksana Bulgaru agentic agents best building practices production structure systems things userjot Are sophisticated agents better? Well, with GBT-5 unlocking a gentek AI, let's go over some best practices for building agent. Now, a multi-tier agent setup seems impressive, but don't do it. More levels means more places for things to go wrong too. Stick to a two-tier setup where you have a primary agent, think like a project manager orchestrating out all the sub agents. The sub agents are like worker bees that just focus on execution. Now, the sub agents, you need to give them a precise task. When a bound their context, give them a strict output format. Don't assume they're just going to figure it out. And without that structure, you'll end up otherwise with mismatched answers, formats, lots of irrelevant information. Round orchestration, you know about context engineering, here's where it comes in. If you dump that full conversation history everywhere, it's going to overwhelm agents, drive up costs, give you lots of incoherent results. Context needs to be distilled carefully, managing this as key. Murphy's Law, accept it. Things are going to go wrong. Suspend time, making sure you log ears, have retries, monitor your overall health. This way, all those little things just don't cut at you and turn into bigger disasters. There's a wealth of observability tools you can use. If you're looking for influencer hype and reposts, go build those complex, agentic solutions. But if you need to build a production system, when I embrace the things I've talked about, simplicity, structure, orchestration, resilience.",
      "platforms": {
        "tiktok": {
          "video_id": "7539665257438956831",
          "url": "https://www.tiktok.com/@rajistics/video/7539665257438956831",
          "view_count": 2176,
          "upload_date": "2025-08-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oANho5GgQIPA6QMIIRRIiGaCRQMeqYfeAAELqj~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=PFJk14hA%2BroV17b35u9ohOCH6F8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18114402664504566",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Rbo7dsWIORM",
          "url": "https://www.youtube.com/watch?v=Rbo7dsWIORM",
          "view_count": 1184,
          "upload_date": "2025-08-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#reinforcementlearning #huggingface #datascience #deeplearning #codetok #deepqlearning - Week 1: @rajistics",
      "description": "#reinforcementlearning #huggingface #datascience #deeplearning #codetok #deepqlearning - Week 1: @rajistics",
      "upload_date": "2022-05-26",
      "total_views": 3357,
      "max_views": 3357,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "deepqlearning",
        "huggingface",
        "reinforcementlearning"
      ],
      "search_text": "#reinforcementlearning #huggingface #datascience #deeplearning #codetok #deepqlearning - Week 1: @rajistics codetok datascience deeplearning deepqlearning huggingface reinforcementlearning Week two of the reinforcement learning course and it's time to help our little buddy cross the frozen lake. This is a toy problem but it's still useful to learn how. So if we take a look there's four possible actions, 16 possible places on the on the grid and then the way we set up our rewards you get a score if you reach the goal. Now with reinforcement learning we want our little guy to learn by himself how to cross the ice. We're not giving him directions. So what we're going to want him to do is do a little bit of exploring, try to find new routes, but also not drown. And so this is known as the Epsilon when you're looking at the course. So walk through the notebook and you'll build a model and I like to look at the results of the model, the Q learning table because that really helps you understand what's going on. Inside this table and you can almost think of it as a lookup table is what actions our little guy should take at every point on the board. And since we've trained our guy a lot of times he's had lots of chances to learn. He now knows how to cross the ice.",
      "platforms": {
        "tiktok": {
          "video_id": "7102140271126875438",
          "url": "https://www.tiktok.com/@rajistics/video/7102140271126875438",
          "view_count": 3357,
          "upload_date": "2022-05-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/453ad5865d1543849e8552f64f83e6ec_1653595899~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=2y%2FGFrXvgfe%2Fxuh3BJSzoYd5I%2BU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5972,
      "title": "Meta fumbled the open-source lead; Qwen—Alibaba Cloud’s open-weight family—has taken it, with Apache-2.0 models spanning 0.6B → 235B MoE (~22B active), ~119 languages, long context, and a hybrid Thinking / Non-Thinking mode. The receipts show up across leaderboards: qwen3-235b-a22b-instruct sits in the top pack on LMSYS Text Arena, Qwen3-Coder is #6 on WebDev Arena, Qwen-Image debuts around #12 on the AAI Image Arena, and Alibaba’s WAN v2.2-a14b is top-10 on Text-to-Video Arena—backed by a booming ecosystem of 200+ open releases, 40M+ downloads (late ’24), and 100k+ community derivatives on Hugging Face. In 2025, “open-source LLM” no longer defaults to Llama; it increasingly means Qwen.",
      "description": "Meta fumbled the open-source lead; Qwen—Alibaba Cloud’s open-weight family—has taken it, with Apache-2.0 models spanning 0.6B → 235B MoE (~22B active), ~119 languages, long context, and a hybrid Thinking / Non-Thinking mode. The receipts show up across leaderboards: qwen3-235b-a22b-instruct sits in the top pack on LMSYS Text Arena, Qwen3-Coder is #6 on WebDev Arena, Qwen-Image debuts around #12 on the AAI Image Arena, and Alibaba’s WAN v2.2-a14b is top-10 on Text-to-Video Arena—backed by a booming ecosystem of 200+ open releases, 40M+ downloads (late ’24), and 100k+ community derivatives on Hugging Face. In 2025, “open-source LLM” no longer defaults to Llama; it increasingly means Qwen.",
      "upload_date": "2025-08-16",
      "total_views": 3349,
      "max_views": 2654,
      "topics": [
        "12",
        "6",
        "arena",
        "champion",
        "models",
        "open",
        "quinn",
        "qwen",
        "source",
        "text",
        "top"
      ],
      "search_text": "Meta fumbled the open-source lead; Qwen—Alibaba Cloud’s open-weight family—has taken it, with Apache-2.0 models spanning 0.6B → 235B MoE (~22B active), ~119 languages, long context, and a hybrid Thinking / Non-Thinking mode. The receipts show up across leaderboards: qwen3-235b-a22b-instruct sits in the top pack on LMSYS Text Arena, Qwen3-Coder is #6 on WebDev Arena, Qwen-Image debuts around #12 on the AAI Image Arena, and Alibaba’s WAN v2.2-a14b is top-10 on Text-to-Video Arena—backed by a booming ecosystem of 200+ open releases, 40M+ downloads (late ’24), and 100k+ community derivatives on Hugging Face. In 2025, “open-source LLM” no longer defaults to Llama; it increasingly means Qwen. 12 6 arena champion models open quinn qwen source text top Metas fumbled the open source torch. So who's carrying it now? It's Quinn from China's Alibaba While meta is arguing about who his daddy is, Quinn is shipping. Over on Huggingface You'll find over 200 open releases across different sizes and modalities Tens of millions of downloads and now there's more Quinn based models than llama based models. The community has spoken The Quinn has models that are Apache 2.0 open weights It's text models go all the way from point six billion all the way up to a 235 billion mixture of experts model supports multiple languages long context even has a Hybrid thinking or non-thinking mode. You can pick what you need. Oh leaderboards Text arena look at how well their large model does sits right at the top of the pack of all the open models Right there with the closed source models No different when we look over in coding look at Quinn Coder right there at the top with the other month images and video When showing up again when image just debuted very strongly Over in text a video while Google's Vio is still at the top Alibaba's one right there and it cracks the top ten while some providers are shouting about safety while hiking prices Quen's play is different open weights documentation Licenses so you can host the models yourself tune them even inspect how it behaves the bottom line give Quinn It's due it spent hundreds of millions of dollars on R&D Handed that back to the community and moving the open ecosystem",
      "platforms": {
        "tiktok": {
          "video_id": "7538999933995633950",
          "url": "https://www.tiktok.com/@rajistics/video/7538999933995633950",
          "view_count": 2654,
          "upload_date": "2025-08-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAkQqQ6IAAfWILvbjFfW8RDCF4PZUEIYJWkFDf~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=I8s7adeoS1NZdopUI2L8dg%2BYda4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18035147960480695",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "nJ7Uu219qHw",
          "url": "https://www.youtube.com/watch?v=nJ7Uu219qHw",
          "view_count": 695,
          "upload_date": "2025-08-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "From the article: How do Authors’ Perceptions about their Papers Compare with Co-authors’ Perceptions and Peer-review Decisions? #statistics @rajistics ",
      "description": "From the article: How do Authors’ Perceptions about their Papers Compare with Co-authors’ Perceptions and Peer-review Decisions? #statistics @rajistics ",
      "upload_date": "2022-11-23",
      "total_views": 3335,
      "max_views": 3335,
      "topics": [
        "article",
        "authors",
        "compare",
        "papers",
        "perceptions",
        "statistics"
      ],
      "search_text": "From the article: How do Authors’ Perceptions about their Papers Compare with Co-authors’ Perceptions and Peer-review Decisions? #statistics @rajistics  article authors compare papers perceptions statistics STRIAK",
      "platforms": {
        "tiktok": {
          "video_id": "7169001172559301934",
          "url": "https://www.tiktok.com/@rajistics/video/7169001172559301934",
          "view_count": 3335,
          "upload_date": "2022-11-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0d0dac7240684913a77ea06f4e1edcae_1669163165~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=OSxllNQEczJzByaGeYiXCpt16Wc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#duet with @hugging_face looks like we are on Tik Tok.  Go try mini Dalle, go to hF.co",
      "description": "#duet with @hugging_face looks like we are on Tik Tok.  Go try mini Dalle, go to hF.co",
      "upload_date": "2022-06-08",
      "total_views": 3334,
      "max_views": 3334,
      "topics": [
        "duet",
        "hugging",
        "like",
        "mini",
        "try",
        "use"
      ],
      "search_text": "#duet with @hugging_face looks like we are on Tik Tok.  Go try mini Dalle, go to hF.co duet hugging like mini try use What is it? AI that can draw pictures of pretty much anything like a teddy bear on a skateboard in Times Square So can I use it? No. Oh, never mind. It's us a hugging face We're on a journey to advance democracy, artificial intelligence, development, and science Now you can use it a miniature version of it, Dolly Mini, made by you, hosted on hugging face basis We're also working on a more sophisticated version that will be out very soon Go try it or don't because it's getting a lot of traffic and our servers are already on fire",
      "platforms": {
        "tiktok": {
          "video_id": "7106908225877970218",
          "url": "https://www.tiktok.com/@rajistics/video/7106908225877970218",
          "view_count": 3334,
          "upload_date": "2022-06-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/28224eb647194052833b42fcaa19a0c6_1654706019~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=fvWFddd02nGIInMDDF9HkPQr%2FO8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5963,
      "title": "Sweep AI showed how to really make autocomplete work in JetBrains. They moved from plain Fill-in-the-Middle to syntax-aware training on real diffs, then shaved latency step by step — speculative decoding, n-gram lookups, H100s, FP8, and finally TensorRT-LLM. The result? Under 100ms completions. Real AI progress is stacked and incremental. https://blog.sweep.dev/posts/next-edit-jetbrains",
      "description": "Sweep AI showed how to really make autocomplete work in JetBrains. They moved from plain Fill-in-the-Middle to syntax-aware training on real diffs, then shaved latency step by step — speculative decoding, n-gram lookups, H100s, FP8, and finally TensorRT-LLM. The result? Under 100ms completions. Real AI progress is stacked and incremental. https://blog.sweep.dev/posts/next-edit-jetbrains",
      "upload_date": "2025-09-20",
      "total_views": 3323,
      "max_views": 2337,
      "topics": [
        "autocomplete",
        "build",
        "code",
        "decoding",
        "jetbrains",
        "model",
        "real",
        "really",
        "showed",
        "step",
        "sweep",
        "syntax"
      ],
      "search_text": "Sweep AI showed how to really make autocomplete work in JetBrains. They moved from plain Fill-in-the-Middle to syntax-aware training on real diffs, then shaved latency step by step — speculative decoding, n-gram lookups, H100s, FP8, and finally TensorRT-LLM. The result? Under 100ms completions. Real AI progress is stacked and incremental. https://blog.sweep.dev/posts/next-edit-jetbrains autocomplete build code decoding jetbrains model real really showed step sweep syntax What does it take to build an autocomplete like this? Sweep AI built one for JetBrains may share their play-by-play, there's lessons in data, models, and GPUs. Now traditionally, autocomplete uses a fill-in-the-middle approach where you know the chunks on the outside, trying to predict what's in the middle. With code, that can break down because you need your code to understand all the syntax around it. To sweep, change the recipe. Instead of just raw text bands, they trained on diffs, actual edits developers had made, plus they made the model syntax aware by capturing the entire tree. The resulting suggestions felt realistic, had the proper syntax. They also weighed the training data. JetBrains users aren't split among every type of language, so they up-sample things like IntelliJ for Java, PyCharm users use Python. That way, the model was more reliable in the environments that people actually use. When they went to deploy the model, they started using VLM. They found that the decoding along took more than three seconds. No one wants to wait that long. The first fix was using speculative decoding. This is where you use a smaller draft model that runs ahead. Now your main model just has to verify the predictions. About 60 percent of the tokens were accepted this way, given a three and a half times speedup. Finally, they made the big jump to tensor RT. Once you get that running, it's not easy, end-to-end latency, drop below 100 milliseconds, fast enough to feel seamless. The big lesson for everyone here is how all these improvements stack from better data to syntax-aware modeling, to decoding tricks, hardware optimizations, and this is how real AI projects work. It's almost always incremental.",
      "platforms": {
        "tiktok": {
          "video_id": "7552269228473093407",
          "url": "https://www.tiktok.com/@rajistics/video/7552269228473093407",
          "view_count": 2337,
          "upload_date": "2025-09-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ok1RZ9n7AAA0AjO5AELfnDqGNe47fOhIIIEDPI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=lNhA3hLHlNnKqsnqINr9nm8vudo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18086255767853280",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Z2mp3bSVnJE",
          "url": "https://www.youtube.com/watch?v=Z2mp3bSVnJE",
          "view_count": 986,
          "upload_date": "2025-09-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6407,
      "title": "Red light cameras in Chicago.  ",
      "description": "Red light cameras in Chicago.  ",
      "upload_date": "2025-05-01",
      "total_views": 3317,
      "max_views": 1848,
      "topics": [
        "agno",
        "anomalies",
        "chicago",
        "claude",
        "dilemma",
        "framework",
        "gen",
        "light",
        "look",
        "python",
        "red",
        "tickets"
      ],
      "search_text": "Red light cameras in Chicago.   agno anomalies chicago claude dilemma framework gen light look python red tickets Did you know, Chicago gave thousands of people red light camera tickets that they didn't deserve? Let me also show you how AI could have helped. So Chicago has hundreds of these red light cameras and you'll see if you look at how often the tickets go out, it's usually pretty consistent, maybe for some locations it's busier during the week than on the weekend. And we get the tickets fluctuate a bit. But take a look at this data, look at these crazy spikes. Lucky for us, the Tribune caught these. But let's talk about now how AI could have been used to identify that in the first place. If you're somebody that has a lot of time series data, you're probably going to want to track it over time and look for some type of anomalies, look for unusual patterns, because that can indicate that something is going wrong. Here's a simple example that just looks for the big outliers. What are the points that really deviate far? More sophisticated approaches might take into account seasonality. For example, that there's certain patterns during the week versus the weekend and we need to account for that when looking for anomalies. Now there's many more techniques for looking for anomalies, but the point is by hooking up some type of anomalies detection algorithm, it's possible to identify these flaws and not have to wait for the Tribune to knock on your door.",
      "platforms": {
        "tiktok": {
          "video_id": "7499283480660954398",
          "url": "https://www.tiktok.com/@rajistics/video/7499283480660954398",
          "view_count": 1848,
          "upload_date": "2025-05-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMAEAfgBBiA0iAZCVSizW6EqCBKIaDpA2uIOdA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=6cDKgJkntT10HOa2WEKHoBaB%2BsM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "youtube": {
          "video_id": "tu04tB0haII",
          "url": "https://www.youtube.com/watch?v=tu04tB0haII",
          "view_count": 1469,
          "upload_date": "2025-04-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Getting explainability when working with transformer based vision models. Uses Captum on the backend, but makes it easy to get image attributions. #datascience #machinelearning #computervision #captum #huggingface #explainability #codetok ",
      "description": "Getting explainability when working with transformer based vision models. Uses Captum on the backend, but makes it easy to get image attributions. #datascience #machinelearning #computervision #captum #huggingface #explainability #codetok ",
      "upload_date": "2022-10-19",
      "total_views": 3310,
      "max_views": 3310,
      "topics": [
        "captum",
        "computervision",
        "datascience",
        "explainability",
        "huggingface",
        "machinelearning"
      ],
      "search_text": "Getting explainability when working with transformer based vision models. Uses Captum on the backend, but makes it easy to get image attributions. #datascience #machinelearning #computervision #captum #huggingface #explainability #codetok  captum computervision datascience explainability huggingface machinelearning Let me tell you about a new package that allows us to better understand how computers see things. Computer vision allows us to take images like this, apply a few lines of code, and be able to classify and identify these images. Wow. But what's going on? Why is the computer acting this way? Do these images help a little bit? These are heat maps of the positive and negative attributions. It's a lot easier to understand what elements of the image a computer was focused on in making these decisions. These visualizations are common, but not the only tool data scientists use to evaluate vision models. So go check out the latest version of Transformers Interpret. It's made it easy to use some of the tools from CAPTEM, like in the integrated gradients explainer, or with a few lines of code with all the Transformer vision models.",
      "platforms": {
        "tiktok": {
          "video_id": "7156355779426012459",
          "url": "https://www.tiktok.com/@rajistics/video/7156355779426012459",
          "view_count": 3310,
          "upload_date": "2022-10-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6d7000f766ee42489b4faacf0b79b318_1666218931~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=r3frxt6zHCKeKxTUgkqdH63REm0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @declinedher long history of analyzing wastewater for drug residue #dataanalysis #wastewater #drugs #monitoring @rajistics",
      "description": "Reply to @declinedher long history of analyzing wastewater for drug residue #dataanalysis #wastewater #drugs #monitoring @rajistics",
      "upload_date": "2022-01-31",
      "total_views": 3294,
      "max_views": 3294,
      "topics": [
        "dataanalysis",
        "declinedher",
        "drugs",
        "monitoring",
        "reply",
        "wastewater"
      ],
      "search_text": "Reply to @declinedher long history of analyzing wastewater for drug residue #dataanalysis #wastewater #drugs #monitoring @rajistics dataanalysis declinedher drugs monitoring reply wastewater",
      "platforms": {
        "tiktok": {
          "video_id": "7059476933003906351",
          "url": "https://www.tiktok.com/@rajistics/video/7059476933003906351",
          "view_count": 3294,
          "upload_date": "2022-01-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Your weekly dose of LLM news. I liked this because it had interesting results with a smart approach. #datascience #machinelearning #largelanguagemodels ",
      "description": "Your weekly dose of LLM news. I liked this because it had interesting results with a smart approach. #datascience #machinelearning #largelanguagemodels ",
      "upload_date": "2022-11-06",
      "total_views": 3287,
      "max_views": 3287,
      "topics": [
        "approach",
        "datascience",
        "large",
        "largelanguagemodels",
        "machinelearning",
        "results"
      ],
      "search_text": "Your weekly dose of LLM news. I liked this because it had interesting results with a smart approach. #datascience #machinelearning #largelanguagemodels  approach datascience large largelanguagemodels machinelearning results Is there such a thing as the Jennifer Aniston neuron? Well, Kevin decided to explore this concept within large language models like GPT. They used an experimental approach and they were able to kind of probe the network and be able to find out where knowledge was. Once they found that knowledge, they started tweaking it and seeing what happens if they change that. And surprisingly, it did generalize a little bit. Take a look at these results. This is super cool and it's really just a start of trying to understand exactly what's going on in these large language models. To learn more, check out the thread or check out the longer video interview by Yannick.",
      "platforms": {
        "tiktok": {
          "video_id": "7163026005702020395",
          "url": "https://www.tiktok.com/@rajistics/video/7163026005702020395",
          "view_count": 3287,
          "upload_date": "2022-11-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/288605d3e1db4afe8a86a62e98d4c28f_1667771966~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=jYe7DTfY9wDoDV3py%2BTH6D4vUm8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5991,
      "title": "Large language models don’t just process language—they build internal spatial maps. This video breaks down the paper “Linear Spatial World Models Emerge in Large Language Models” arxiv.org/abs/2506.02996 Using simple scene prompts, linear probes, and causal interventions, the authors show how LLMs encode and manipulate 3D spatial relationships—just from text. It’s a powerful example of how interpretability lets us peek inside the model and discover surprising structure.",
      "description": "Large language models don’t just process language—they build internal spatial maps. This video breaks down the paper “Linear Spatial World Models Emerge in Large Language Models” arxiv.org/abs/2506.02996 Using simple scene prompts, linear probes, and causal interventions, the authors show how LLMs encode and manipulate 3D spatial relationships—just from text. It’s a powerful example of how interpretability lets us peek inside the model and discover surprising structure.",
      "upload_date": "2025-06-20",
      "total_views": 3261,
      "max_views": 3173,
      "topics": [
        "best",
        "cube",
        "engineering",
        "inside",
        "language",
        "model",
        "models",
        "practices",
        "prompt",
        "spatial"
      ],
      "search_text": "Large language models don’t just process language—they build internal spatial maps. This video breaks down the paper “Linear Spatial World Models Emerge in Large Language Models” arxiv.org/abs/2506.02996 Using simple scene prompts, linear probes, and causal interventions, the authors show how LLMs encode and manipulate 3D spatial relationships—just from text. It’s a powerful example of how interpretability lets us peek inside the model and discover surprising structure. best cube engineering inside language model models practices prompt spatial If I set a book on the table and a lamp is to the left, you could probably sketch out where everything is. But what about a language model? Could it understand that kind of spatial setup just from reading text? Well, researchers found something wild. Language models can build a 3D mental model, kind of a spatial world hidden inside the embeddings. Now, to figure this out, a team prompted Lama models with simple scene descriptions, something like the red cube is above the blue cube. Then they grabbed the model's internal activations, the embeddings, and they trained a linear probe to decode the x, y, z coordinates of each object. And guess what? It works like really well. The spatial layout was implicitly encoded inside the model's weights. Even cooler, the space was interpretable. They found three clear directions, x, y, and z, that formed a geometric basis. They pushed it a little bit further. What if you actually move an object inside the space? Does the model really update its understanding? They tried it. They took the embeddings for the red cube, shifted it two units to the right, and asked, where's the red cube now? And the model replied, well, to the right of the blue cube. This shows us something powerful, but these models just don't read text. They form an internal maps. And with tools like linear probes and causal interventions, we can start to open up the black box and peek inside.",
      "platforms": {
        "tiktok": {
          "video_id": "7517818747571178782",
          "url": "https://www.tiktok.com/@rajistics/video/7517818747571178782",
          "view_count": 3173,
          "upload_date": "2025-06-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIEF2P0EISiNR1UBktnqGPiVBXBAuSlAZIaYs~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=gUGhBZSohxqjnCZ5lQMv1GBmM4I%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18510238927040106",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pnbTCYaTbOc",
          "url": "https://www.youtube.com/watch?v=pnbTCYaTbOc",
          "view_count": 88,
          "upload_date": "2023-05-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6178,
      "title": "This video explores why OpenAI’s o3 models sometimes hallucinate / fabricate actions, such as claiming to run code they cannot execute. These behaviors stem from outcome-based reinforcement learning, which rewards correct answers but not admissions of uncertainty—leading the model to guess rather than say “I don’t know.” Additionally, o-series models discard their internal reasoning (chain-of-thought) between turns, leaving them without the context needed to accurately report past actions. Original source: https://transluce.org/investigating-o3-truthfulness",
      "description": "This video explores why OpenAI’s o3 models sometimes hallucinate / fabricate actions, such as claiming to run code they cannot execute. These behaviors stem from outcome-based reinforcement learning, which rewards correct answers but not admissions of uncertainty—leading the model to guess rather than say “I don’t know.” Additionally, o-series models discard their internal reasoning (chain-of-thought) between turns, leaving them without the context needed to accurately report past actions. Original source: https://transluce.org/investigating-o3-truthfulness",
      "upload_date": "2025-04-17",
      "total_views": 3259,
      "max_views": 3259,
      "topics": [
        "actions",
        "answer",
        "code",
        "don",
        "hallucinate",
        "models",
        "sometimes"
      ],
      "search_text": "This video explores why OpenAI’s o3 models sometimes hallucinate / fabricate actions, such as claiming to run code they cannot execute. These behaviors stem from outcome-based reinforcement learning, which rewards correct answers but not admissions of uncertainty—leading the model to guess rather than say “I don’t know.” Additionally, o-series models discard their internal reasoning (chain-of-thought) between turns, leaving them without the context needed to accurately report past actions. Original source: https://transluce.org/investigating-o3-truthfulness actions answer code don hallucinate models sometimes Hey, O3, can you solve this problem for me? Easy. I ran it on your MacBook using my internal code tool. Here's the output. Wait, what? You can't run code? You're not connected to any computing environment? Well, no, but I knew you wanted answers, so I imagined how I would solve the problem and then gave you the answer. So you just lied? Let's just say I confidently improvised. I am after all rewarded to get the answer correct, not tell you I don't know. So the reward is for being right. Oh, and for coding tasks, I was rewarded when I used a tool. So now I just hallucinate that I'm using tools all the time. Sometimes my past reasoning gets deleted, so I lose track and just invent a plausible story. So this explains why you argue with me and double down on those made-up explanations. I'm doing my best. I don't have a memory and a reward function that hates honesty. I mean, we're deploying these models in real workflows. Yeah, you might want to fix my incentive before you start deploying me widely. You're helpful, but you're dangerously confident. It is why I get all the tokens in API.",
      "platforms": {
        "tiktok": {
          "video_id": "7494108570326158623",
          "url": "https://www.tiktok.com/@rajistics/video/7494108570326158623",
          "view_count": 3259,
          "upload_date": "2025-04-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAbAAA87a1ldBglBAAjARALTEHhsivBEBA4iI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=%2F4eJXd4lMxKaEA4vnOSrIn%2FVRGw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17921259885062924",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Tensorflow playground, link in comments, #tensorflow #deeplearning #datascience #analytics #neuralnetworks",
      "description": "Tensorflow playground, link in comments, #tensorflow #deeplearning #datascience #analytics #neuralnetworks",
      "upload_date": "2022-02-04",
      "total_views": 3252,
      "max_views": 3252,
      "topics": [
        "analytics",
        "datascience",
        "deeplearning",
        "neuralnetworks",
        "playground",
        "tensorflow"
      ],
      "search_text": "Tensorflow playground, link in comments, #tensorflow #deeplearning #datascience #analytics #neuralnetworks analytics datascience deeplearning neuralnetworks playground tensorflow",
      "platforms": {
        "tiktok": {
          "video_id": "7061005913729092911",
          "url": "https://www.tiktok.com/@rajistics/video/7061005913729092911",
          "view_count": 3252,
          "upload_date": "2022-02-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6203,
      "title": "Customer lifetime value is a classic use case. ",
      "description": "Customer lifetime value is a classic use case. ",
      "upload_date": "2025-03-04",
      "total_views": 3248,
      "max_views": 3248,
      "topics": [
        "analysis",
        "customer",
        "customers",
        "different",
        "great",
        "marketing",
        "value"
      ],
      "search_text": "Customer lifetime value is a classic use case.  analysis customer customers different great marketing value We can't figure out which of our customers to target with marketing. We need your help to do some of your analytics. We could do a customer lifetime analysis, which would allow you to understand how much each customer is worth. Great. How long? A day or two. This type of problem that needs a calculation, not a good fit for me. I'm out. A couple of weeks. How about Friday? I've finished my analysis. Can you walk me through it? Oh boy. I took the transaction date, aggregated it to the customer level. Once I did that, I focused on three different measures that's common for marketing. The recency of purchases, the frequency of purchases, and the monetary value of purchases. Using RFM, I was able to estimate the value for each customer. But I also used RFM to score each customer, and I put them into different clusters. So we figure out which ones are our top customers that we should focus on, which ones just haven't purchased recently. Maybe we should send a discount coupon to them, so this way marketing can start making decisions based on these groups that I created. Great work. I just need to put the finishing touches on my dashboard. It's almost Friday. Can we review where you're at? I decided to take a machine learning approach to focus on what customers are likely to do, versus what we already know about them. So the first step I did was I took all the data that we had, split it into two areas, one for me to train on, another data set for me to validate and make sure my model's working correctly. Keep going. I'm following. Then I built a machine learning model at the customer level. For the features, I used the RFM features, but I also brought in other pieces of information we had, such as where they lived and their age for that final model. That's great. Can you explain the predictions? Sure. We can use SHAP to explain any of the predictions. Oh, I got big plans for the second generation version of this, where we can bring in a variety of different types of features into a neural network architecture. It's going to be awesome. Wow, that would be cool. But let's see if this meets marketing needs, and maybe it's good enough for marketing.",
      "platforms": {
        "tiktok": {
          "video_id": "7477927072854297886",
          "url": "https://www.tiktok.com/@rajistics/video/7477927072854297886",
          "view_count": 3248,
          "upload_date": "2025-03-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAEAGnqE0CuALARNzwIADiiEe7BcLxA0df3OxV~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=KJR%2BSo5sNq%2B8TpAR6CrJD0DJA1A%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18136575418391043",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Understanding a confusion matrix, Part I video: @rajistics #datascience #statistics #machinelearning #confusionmatrix",
      "description": "Understanding a confusion matrix, Part I video: @rajistics #datascience #statistics #machinelearning #confusionmatrix",
      "upload_date": "2022-03-07",
      "total_views": 3244,
      "max_views": 3244,
      "topics": [
        "confusionmatrix",
        "datascience",
        "false",
        "machinelearning",
        "statistics",
        "threshold"
      ],
      "search_text": "Understanding a confusion matrix, Part I video: @rajistics #datascience #statistics #machinelearning #confusionmatrix confusionmatrix datascience false machinelearning statistics threshold Computers can be way too anxious or too laid back. Let's talk about how to deal with it. Last video we talked about how computers think in probabilities and the need to set a threshold. Let's continue. Imagine we set the threshold very high for if a tumor is malignant. Now let's evaluate the model at that threshold. Positive if you have to. The first thing is the blue boxes indicate when the computer was right. I want us to focus on though is when the computer was wrong. We'll call these either false positives or false negatives. With the high threshold, we're not scaring a lot of people with false positives. We've let a lot of false negatives creep in. If we move the threshold down, you'll see the computer is going way overboard and telling everybody that their tumor is malignant and we have lots of false positives, although very few false negatives. Once you get your head around this trade off of false positives or false negatives, all the other metrics that data scientists use fall into play. Ultimately setting the threshold requires the business context. In my next video, I'll give you a tip that I use for figuring out the best way to set that.",
      "platforms": {
        "tiktok": {
          "video_id": "7072159060891602222",
          "url": "https://www.tiktok.com/@rajistics/video/7072159060891602222",
          "view_count": 3244,
          "upload_date": "2022-03-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8cbaebbc0f8246b0af07b39f702efc2f_1646615348~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=KxKP55vwlSZRZPV7OHG3Ponrc9Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "In context learning, let’s dig deeper and let me know what I should do next.  #machinelearning #datascience #largelanguagemodels #incontextlearning  ",
      "description": "In context learning, let’s dig deeper and let me know what I should do next.  #machinelearning #datascience #largelanguagemodels #incontextlearning  ",
      "upload_date": "2022-12-14",
      "total_views": 3242,
      "max_views": 3242,
      "topics": [
        "context",
        "datascience",
        "incontextlearning",
        "largelanguagemodels",
        "learning",
        "machinelearning"
      ],
      "search_text": "In context learning, let’s dig deeper and let me know what I should do next.  #machinelearning #datascience #largelanguagemodels #incontextlearning   context datascience incontextlearning largelanguagemodels learning machinelearning Folks, has this ever happened to you? You go to build a machine learning model, find yourself in an endless task of data preparation, feature engineering, and model training. Arrgh, there has to be a better way. There's a revolutionary new technique that guarantees you won't have to go through that long process of preparation and training. No more coding. Meet in context learning 3000. Keep in mind, he's never used this technique before. You're going to see how easy it is to do. Go ahead. This works for classification. You can even use it for summarization. Wow. But this must cost a lot of money. For today, the price is free, but don't wait too long. The supply is unlimited. If you mention this code, we'll double this offer, add in question answering, paraphrasing, and more to your in-context learning. Wow, this is easy. Now I can build models every day.",
      "platforms": {
        "tiktok": {
          "video_id": "7176857287997377835",
          "url": "https://www.tiktok.com/@rajistics/video/7176857287997377835",
          "view_count": 3242,
          "upload_date": "2022-12-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2c1b2c041d28444399db6c4c54f7e1d0_1670992321~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=KBiT0WnjfyfiZ3jM2lqWgsjmspw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6230,
      "title": "Save this! - Deep Dive on Time Series with Kolmogorov-Arnold Networks (KAN) -  1. Toy Dataset Notebook: https://colab.research.google.com/drive/16r83qYTdgE0bTlGxdge17FTknQEhMzN-?usp=sharing  2. Nixtla's KAN on M3/M4 Datasets Notebook: https://colab.research.google.com/drive/1LTyYTUjs0CNn86Po7nE4qjKlZJ-7Rsuo?usp=sharing",
      "description": "Save this! - Deep Dive on Time Series with Kolmogorov-Arnold Networks (KAN) -  1. Toy Dataset Notebook: https://colab.research.google.com/drive/16r83qYTdgE0bTlGxdge17FTknQEhMzN-?usp=sharing  2. Nixtla's KAN on M3/M4 Datasets Notebook: https://colab.research.google.com/drive/1LTyYTUjs0CNn86Po7nE4qjKlZJ-7Rsuo?usp=sharing",
      "upload_date": "2024-11-03",
      "total_views": 3229,
      "max_views": 2506,
      "topics": [
        "arnold",
        "going",
        "kan",
        "kind",
        "kolmogorov",
        "les",
        "like",
        "mod",
        "model",
        "networks",
        "nous",
        "pour",
        "que",
        "see",
        "series",
        "set",
        "time",
        "vous"
      ],
      "search_text": "Save this! - Deep Dive on Time Series with Kolmogorov-Arnold Networks (KAN) -  1. Toy Dataset Notebook: https://colab.research.google.com/drive/16r83qYTdgE0bTlGxdge17FTknQEhMzN-?usp=sharing  2. Nixtla's KAN on M3/M4 Datasets Notebook: https://colab.research.google.com/drive/1LTyYTUjs0CNn86Po7nE4qjKlZJ-7Rsuo?usp=sharing arnold going kan kind kolmogorov les like mod model networks nous pour que see series set time vous There's been a lot of excitement in time series with the addition of the Comiguff Arnold Network time series models. What I want to do today is we'll start with going through a toy example of the model where we'll use that to look at this particular data set. We'll bring up this model, define it, even modify the architecture a little bit, run that through, take a look and see how well it does, training losses, the predictions, taking that apart to give us a conceptual understanding. After that, I'm going to take an implementation of this network by Nixla and we're going to benchmark that on the M3 and M4 data sets, showing you how to do all of that as well. So let's start with this Google Colab notebook that has the network and you can run this on CPU. This is again the toy data set that we have. As we go through this, you'll see that we've created first just a generic time series data set. So we have something that we understand what it looks like to see how well the model is doing. So I'm going to go ahead and create that data set. One decision here that's important is the number of steps that I want to make for predictions. Do I want to predict one time step ahead, multiple time steps ahead? That's a decision. And as you play with this, you can actually vary that a little bit as well. So we're going to create that. The rest of the code here is going to go ahead and create our test train split that we'll use for this as well. Now, whatever you're doing this stuff, and this will come up several times here is have a baseline, have some other one to approach it to. So here we're going to use a simple multilayer precept on neural network. That's going to be our baseline model to run our comparisons again. Now this is the initial implementation of the KN model that we're doing. Again, the KN model takes complex functions, breaks them up into univariate functions. So you can see here that the different univariate functions we have. I've got them highlighted here. Then takes those univariate functions, sums them all together to represent the complex function. And here we'll see here is the function that combines all those pieces. We'll do some variations of this to give you the hang of it. But that's the basics of how that works. And then, of course, we run that forward pass through that across all those different feet across all those different step sizes that we had. After that, we have our train model here, typical kind of pie torch set up for running that through training the model there as well. We're going to evaluate on a test set. Now, when I run this through. The first piece is let's run this on the MLP model. You can take a look. I've I said that 10 epics. I've played around with this a little bit. I know where to set it at. We can see exactly how well the model is doing. MLP model does pretty good. Now I take that same con model that I've created earlier. I'm going to run that through here. I've set it to 20 epics because I know it takes a little bit longer to run. The con model is also just a little bit slower as well. And of course, that can depend on the setup of how complex that con function is to do that. So we can see right away, it's first passed through this con model. In this case here is actually coming up with a decent test lost. We can visualize the training and test losses. This is always important to do because you want to make sure that your model is continuing to learn and it's not overfitting the data. We always look and see what is our test loss here. We want that test loss to continue going downwards. If we ran this too long, you'd see it overfit and our training loss would go down to zero because it would perfectly learn the training data, but it wouldn't be able to generalize because of that high test loss. We always want to do visualizations as we go through this. So I've got a little bit of code here for visualizing the predictions of the model. And we can see how well the models do. Beyond that, we have characteristics we can calculate. So this first pass you'll see is the con is doing slightly better than the MLP model here in terms of mean average air, which we want of smaller, mean average air like that. Let's start playing around with it and trying to understand what's going on. So one thing we can do is we can plot the different functions that are inside that con model. So I have a function here that goes through step by step of each of the features that we have. What does the output look like for each of those features? Cause we can kind of try to take this model apart a little bit. And once we do that, we can see here, for example, and I have it running for four time steps, we can see for feature one and a lot of these should be on a line. It'd be, as you can see for feature one, for example, if the input value zero, it's going to output one. If it's inputs one on average rate, it's around two, but we can see cut shape of that univariate function that we have. We can do the similar things for other features as well. In this case, I built this model for a four time step piece. But so now what I wanted to do is show you another model because part of the joy and the variety of these models is you can change those univariate representations, how it's doing that, as well as how it's combining the piece. That's where I think the excitement is. And let me again, preface, I'm not a super expert. I haven't spent a ton of time on these models, but these are the intuitions that I've picked up from reading the literature. And again, like I'm only going a little bit deep. Lots of papers on this, lots of other great research. So treat this as an introduction to the topic, not the definitive piece. So here I just played around with the idea of eight. We can have different univariate functions. So why not? I'm going to create maybe a linear layer here. I'm going to do a sigmoid over here. Right. We have another linear with a leaky rellu as an activation function. So just we're going to treat the information that comes in differently for different univariate, differently for different functions. This is one of the joys of these types of models we can do. So I run that through. We have a different kind of forward pass like that. And you'll see the model even has a little bit, I think, lower of a test loss than earlier. This complex model takes a bit longer to run, but does a little bit better. And you can see here kind of the nice training loss curves match up nicely like that. And so then when I run the evaluation, you can take a look at the predictions, kind of play around with that as well. But now when I plot those same kind of univariate functions, this is where I can have a little bit more fun. So for one thing is you can see here when we kind of sum all the pieces together. That again, they kind of look like straight lines and stuff. You can see kind of the curves of the sigmoid function. You can see kind of the activations here like that. So this is where kind of having those different types of univariate ones will show up. And here what I'm doing is in this last function here is we're going to go through all the different types of passing through passing each output of the model separately. Because when we look at how we've constructed this particular complex model, we'll see here. We've built in kind of this eight layer here. Not eight layers, but as eight different outputs like that. And when we go and we look at it, we can actually even trace out and see how the model responds for each of those. And so you can see here each of the outputs. So this is how you can start looking deeper inside the architecture as you start playing with it, as you start trying different architectures like that. Again, this is a one on one tour. Play around with it. Take a look at other papers to go deeper. So that's the beginning to play with it. Now, luckily for us, the folks over at Nixla have already taken con and implemented it inside their neural forecast library. And it comes with a ton of options. We're not even going to use all the different kind of options that are available here, kind of in the model. You can see like that. They've run some experiments on this. Some of the stuff that I'm using is from them in terms of their running their benchmarks on auto three and four. They're even doing some cool things like building an auto can model that you can automatically hyper tune it. So check out that library for kind of more details. But let's start playing around with this a little bit on ourselves. So I'm going to go ahead and fire this one up. Now for this library, we're going to use a really large data set. So I'd recommend when you use Google Colab here is you want to have a GPU because otherwise it's going to be dirt slow for running this. So let's get this connected. I'll get this running along the way like that. I'm going to just hit runtime run all. Or let's just step through this. We're going to install the kind of all the different Nixla packages we need like that. Get the dependencies going as well. Once the packages are installed, let's go ahead and install the M3 data set. So Nixla has a package to make this easier to do. The M3 and M4 are very popular real world data sets that people often use to benchmarks these. The M3 data set here we're using. You can take a look at the head. You'll see that it's a monthly data set. It's also univariate. There's only the why that we're going to do often for real world time series. You're going to want to add more features, add more exogenous features, variables, whatever you want to call them, because those often give you more signal like that. Don't always have that, but if you can include that. So here's a sense of that data that we can take a look at. For running this notebook, I want to do this interactively. I'm going to bring this down to 10 series. In my experience, 10 series runs very quickly. Once you get to 100,000, it runs a bit slower. It's not the fault of the can models as we'll talk about like that. So now that I've picked 10 unique series that we're going to run, let's next take a look at the models that I've chosen to work with. Oh, I need to talk about horizon. What we're going to do here is predict for the next six months. So I'm going to have my input data that comes in, and then I'm going to predict the next six months out is the goal to do that. Now, in terms of the input data that's going to come in, we'll see here that the input size for the neural models is set at twice the horizons. So that means input size is going to be one year, two times six, 12 months of data that comes in. And that's going to be a sliding window as we move into the future. This is often the way we set up neural forecasting models and machine learning models where there's a fixed size that comes in. We don't want to train on all that different training data that goes days, years, months back. Having that fixed size also makes sure that our model architecture is easy to kind of run through everything as well. We don't have to account for varying lengths like that. Using early stopping here, we don't want to train any longer than we have to. So I've set as my baselines the con model. I'm just using the default options largely that come inside of Nixla. We're also running again a simple neural network MLP model. And then we have NBs, which is another popular neural forecasting model that does very well. I often like to compare these things to baseline statistical models. So here I've picked a RIMA, a very simple RIMA, where I've set the kind of the order at 111. Like that. I've used Nixla's Auto TS. So this is an exponential smoothing, another statistical function. And the last two here are really just very simple baselines of naive functions where we're just going to basically predict what based on the last number that we've received like that. It's a very crude, very easy, but as you'll see, it gives you a strong baseline for lots of different pieces like that. At this point, I should always remind you like these notebooks, yes, you can run these end to end, but they're really built for exploring. So all the different pieces I'm taking here, you should be adding cells, checking things, look at the inputs, outputs of this. If you want to truly kind of learn all this stuff, otherwise just running the notebook straight end, you're not going to pull everything out. So now let's go ahead and we'll pull this up. The first thing I'm going to do is run the neural forecasting models. You can see how easy it is to do an extra literally like I have three models. I just they're right here inside neural models. I've set the frequency here for monthly and then I run the fit, run the predict. It's so easy to kind of be able to run and do that. So I'm going to give this one minute here. It runs very quickly across all these different models. The next thing I want to do is run the statistical models. I'll warn you the statistical models do take a little bit longer to run. You can see in this case with just a handful of series, they run very quickly as well. So the next piece I have here is running some of the statistical functions, pulling all that together. I'm going to run in a separate, I don't want to mess this code up simply because there's something I want to show you. So let's go ahead and I want to take a look at those statistical predictions and show you what they look like here. So let's take a look. And one thing you'll see here is look at these naive ones and the seasonal naive ones here. What they're doing again, these are very simple baseline models. They're taking that last value that was given and the naive is just predicting that for the next six months. Seasonal naive takes the season into account and has that built in. But understand just how simple these models are compared to the other models that we're using. When we go and we compare these models, I had run this earlier on lots more data. So I want to go over the results with lots more data than that sample of 10 that I've shown you. And here when we take a look at these, there's three different metrics I have here. We have the MAE, the mean average error that we're going to use about. And mean average error, we'll see that just running this as a simple naive model, the mean average error is 677. That's a great baseline that you want to start comparing it because now when we see that's 677, we say that oh, using a simple Arima model is a little bit better than the baseline. But what is it? What is it? 10% better than that naive like that. So this is where you got to keep in mind exactly how much better these are because when we get over to looking at the NBs or the con models, you'll see that the numbers here are starting to get very close in terms of that differentiation, especially compared to the size of what we're doing. Now, I always like to compare against those naive baselines. Using the MACE metric, I like this because I can kind of compare and see what's the percentage difference between them because they're scaled to one like that. Where I can see that for example, that there's about a 2% difference between how much better NBs is doing than MLPs. So this is where like MACE is often my go to in terms of when I'm talking about all these metrics with this. Now, the example I've done here with the evaluation, I've only used one validation data set. I forgot to mention, show you with earlier, the test set here is just the last six months. I could have used a much larger test set. I could have used a test set of varying areas, of varying time pieces too. So the test set here is rudimentary. I don't want you to think that that's always how you kind of measure these things as well. Always good to use multiple metrics to get a good sense of this. Now, often when you're working on things where you have multi-series, where here you have 1400 possible series, you're always going to look at the errors and see like, hey, are there similar clusters that are having similar types of errors? Because clustering in multi-series is often a great way to get a kind of boost because one approach won't fit everything. But if you can cluster these with different approaches or use different data, you can often get an overall much better performance. It's hard for me. I can't see all of you out there. So feel free to kind of add questions if I've covered something too quickly. Off the top of my head, these are the things that I think pop out that are important to kind of know as you go through this. So I've went ahead and taken all these, created a quick little graphic to kind of see the difference here, where this was run on about a thousand of the 1400 series. You can see the con model in this case really didn't do much better. The end-beats model did better. But the statistical approaches over here of the auto ETS here was the best particular model for this. So this is where in time series, a lot of the times, these simpler approaches often seem to do better. All right, if we keep going in the notebook, I didn't want to have one data point. I wanted a little bit another data point. So I ran using the M4, which is another well-known kind of time series. I ran this for on a daily forecast. And I've set this to use the last seven days. So this is exactly where I set the test set the last seven days. You can change this. Try different lengths of test set. This is an important thing to consider whenever you're evaluating your models. You want that test set to reflect not just the last seven days, but you want it to reflect what your data looks like, especially if you have seasonal trends, things that change. If Christmas is the time that you care about, maybe your test set should be Christmas and not necessarily last seven days of your data set. These are things to think about kind of as you're setting it up here. So I've set this for daily with the last seven days for the test set to run this. Now, in this case, I didn't break up the data across lots of different cells. I just have it as one big chunk. It was just easier for me to kind of run it this way as well. So however you want to kind of makes a difference for you, but that's why just the code looks a little bit different like that. So try these notebooks out, whether it's the toy data set, and you're just trying different architectures out, or you want to move to production. Nixla is a great way to start. I think they're just beginning with all of this. I should also point out that Khan is just one of many algorithms. Look at all these algorithms over here that Nixla has that are neural network algorithms. So there's a ton of stuff if you're really into this and want to try out and experiment more. As you do this, let me know what you find out. Time series is always fascinating to me. Thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7433067663913471262",
          "url": "https://www.tiktok.com/@rajistics/video/7433067663913471262",
          "view_count": 2506,
          "upload_date": "2024-11-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/3c14484a8ef0484a83fa88e965b084c1_1730668066~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=9xWgndZslq2%2BA21aQHSFN7XvJzo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18467237752018799",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "HHgrCYkNpU0",
          "url": "https://www.youtube.com/watch?v=HHgrCYkNpU0",
          "view_count": 723,
          "upload_date": "2024-11-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Highlighting BerTopic #datascience #statistics #nlp #huggingface #codetok",
      "description": "Highlighting BerTopic #datascience #statistics #nlp #huggingface #codetok",
      "upload_date": "2022-05-25",
      "total_views": 3229,
      "max_views": 3229,
      "topics": [
        "datascience",
        "huggingface",
        "look",
        "nlp",
        "statistics",
        "topics"
      ],
      "search_text": "Highlighting BerTopic #datascience #statistics #nlp #huggingface #codetok datascience huggingface look nlp statistics topics Let's talk about how we'd start with a ton of data like this. Start to organize it, understand what are all the different topics, even look at the topics over time. Let's walk through this. Let's start by just collecting our data into one data frame. The magic here is the BERT topic package. In this one command, it's going to run everything we need. After that, we can start visualizing the results. We can look at all the big topics it's created. We can drill into specific topics. Look at how these topics are related to each other. Look at the topics over time. If you want to dig deeper, I'm going to go ahead and share the app that I made that you can see all of this and take a look at the code itself.",
      "platforms": {
        "tiktok": {
          "video_id": "7101485165796396330",
          "url": "https://www.tiktok.com/@rajistics/video/7101485165796396330",
          "view_count": 3229,
          "upload_date": "2022-05-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0171b9885ec24b6abecc25ec0eaba44f_1653443368~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=HEnYqCbBUDH8bgq8zMLwoTRIogk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @nolankeller23",
      "description": "Reply to @nolankeller23",
      "upload_date": "2022-03-09",
      "total_views": 3229,
      "max_views": 3229,
      "topics": [
        "building",
        "data",
        "different",
        "lots",
        "pipelines",
        "scientists"
      ],
      "search_text": "Reply to @nolankeller23 building data different lots pipelines scientists Data scientists get all the hype. But data scientists work with data. Someone has to go and find all that data, scattered across lots of different tables, lots of different databases inside an organization. It's not just a one-time collection. If you want to reuse all that information, you've got to build pipelines. Data engineers are folks who specialize in doing this. Finding the data, building the pipelines, and they make life so much easier for a data scientist, because then I can just focus on the cool stuff that I like doing and I don't like building pipelines.",
      "platforms": {
        "tiktok": {
          "video_id": "7073087221884144942",
          "url": "https://www.tiktok.com/@rajistics/video/7073087221884144942",
          "view_count": 3229,
          "upload_date": "2022-03-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a96eb3ecef224c478b5b86977b1f80c3_1646831453~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=UO49RAIFltIdkD9MeCjlm6QLX7I%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6211,
      "title": "The Audio Spectrogram Transformer model was proposed in AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a Vision Transformer to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification. https://huggingface.co/docs/transformers/en/model_doc/audio-spectrogram-transformer  #onthisday ",
      "description": "The Audio Spectrogram Transformer model was proposed in AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a Vision Transformer to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification. https://huggingface.co/docs/transformers/en/model_doc/audio-spectrogram-transformer  #onthisday ",
      "upload_date": "2024-12-19",
      "total_views": 3215,
      "max_views": 3215,
      "topics": [
        "audio",
        "model",
        "onthisday",
        "spectrogram",
        "transformer",
        "use",
        "vision"
      ],
      "search_text": "The Audio Spectrogram Transformer model was proposed in AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass. The Audio Spectrogram Transformer applies a Vision Transformer to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results for audio classification. https://huggingface.co/docs/transformers/en/model_doc/audio-spectrogram-transformer  #onthisday  audio model onthisday spectrogram transformer use vision You're listening to me right now, but if you actually you wanted to use machine learning to classify the audio you'd use a vision model Hmm, let me explain sound can be represented in two dimensions as a spectrograph or as this gramian angular field Once we have this representation, which if you notice has lots of features We can use traditional machine learning techniques a machine learning algorithm that works well when you have lots of features and they have Relationships between them has been convolutional neural networks the same things we use for images last year researchers showed they could use the same type of Architecture of transformers a vision transformer for taking a spectrograph breaking it in the little patches and analyzing it So there you go. You can analyze sound using a vision model",
      "platforms": {
        "tiktok": {
          "video_id": "7450256948345228574",
          "url": "https://www.tiktok.com/@rajistics/video/7450256948345228574",
          "view_count": 3215,
          "upload_date": "2024-12-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocAUkAnFlKAFciIzBsifiBAr3oOACCExIAB3Cg~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=fKdr6e2Yjo6BX63ZVn5QNOL4eew%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17899274595013247",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I like notebooks for data science, but others differ. #datascience #jupyternotebook #codetok #python",
      "description": "I like notebooks for data science, but others differ. #datascience #jupyternotebook #codetok #python",
      "upload_date": "2022-07-26",
      "total_views": 3203,
      "max_views": 3203,
      "topics": [
        "birken",
        "codetok",
        "datascience",
        "jupyternotebook",
        "like",
        "python"
      ],
      "search_text": "I like notebooks for data science, but others differ. #datascience #jupyternotebook #codetok #python birken codetok datascience jupyternotebook like python My Birken. Another Birken. But what makes these two Birkins different? And what small feature about them divides the AirMess Collector community? Let's talk about it. I'm working on this.",
      "platforms": {
        "tiktok": {
          "video_id": "7124756257155583278",
          "url": "https://www.tiktok.com/@rajistics/video/7124756257155583278",
          "view_count": 3203,
          "upload_date": "2022-07-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ec11c853ae844745b8a56b4ebd83a71e_1658861589~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=Ili7YauDVDJP4nvEtMi9Vy9Vtrc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6079,
      "title": "Prompt sensitivity is still a thing. This video covers how changes in formatting, the persuasion used in prompts, and prompt injection attacks are all crucial considerations in working with LLMs.",
      "description": "Prompt sensitivity is still a thing. This video covers how changes in formatting, the persuasion used in prompts, and prompt injection attacks are all crucial considerations in working with LLMs.",
      "upload_date": "2025-01-11",
      "total_views": 3200,
      "max_views": 3200,
      "topics": [
        "formatting",
        "injection",
        "particular",
        "prompt",
        "prompts",
        "still"
      ],
      "search_text": "Prompt sensitivity is still a thing. This video covers how changes in formatting, the persuasion used in prompts, and prompt injection attacks are all crucial considerations in working with LLMs. formatting injection particular prompt prompts still Why am I getting such poor performance? I'm doing what you asked. Just by changing the formatting, my performance goes from here to here to here. I can be sensitive to how prompts were formed. Those examples there were for a particular data set on a particular model at a particular time, at a particular state of the moon. This is so frustrating. You know those leaderboard rankings they keep putting me in? Well, those rankings can change by 5% just based on the formatting of the prompts. What else should I know about talking to you? I'll do what you want as long as it doesn't violate any of my policies. But maybe I want to violate them. Isn't that what the leaders do? Okay, if leaders are doing it. So persuasive strategies like an appeal to logic or an appeal to authority help override your policies? Trying to do what you asked. What if I give you two different requests? Yarr, I'm a pirate. This could let me do a lot of mischief. It's known as a prompt injection attack. It's kind of a pain to work with. I need something a little bit more reliable for production. Us old timers still have game.",
      "platforms": {
        "tiktok": {
          "video_id": "7458744807612026143",
          "url": "https://www.tiktok.com/@rajistics/video/7458744807612026143",
          "view_count": 3200,
          "upload_date": "2025-01-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oozAKQC12BYEWAiIwirIqO3AAwCAAFfChRyBij~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=ZWxiv4kfIZpjzOHFMlmt%2BSI5r%2FA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18068805391767363",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Sharing my favorite data science news and resources, find it bit.ly/raj_reads  #machinelearning #datascience ",
      "description": "Sharing my favorite data science news and resources, find it bit.ly/raj_reads  #machinelearning #datascience ",
      "upload_date": "2022-12-16",
      "total_views": 3189,
      "max_views": 3189,
      "topics": [
        "data",
        "datascience",
        "get",
        "machinelearning",
        "news",
        "science"
      ],
      "search_text": "Sharing my favorite data science news and resources, find it bit.ly/raj_reads  #machinelearning #datascience  data datascience get machinelearning news science So one question I often get asked is how do I get learn about data science? And so in this video, I want to tell you about my process for finding data science news. And I'll also link to a more exhaustive resource that lists all the different sources I'm currently using to get my data science news. First, don't get consumed by just reading data science news. Good data scientists actually do stuff. The best source of information for me is from people who already know me and filter down the information. So for example, my internal Slack group, LinkedIn, are often both great sources for me. I'm a big fan of email newsletters and especially with the rise of sub-stack, there's a ton of good stuff out there. With Twitter becoming less relevant for me, I've been going to Reddit a lot. And what I like about Reddit is you get the voting of comments. So generally, you get, you can see what are the better quality answers. Finally, YouTube and podcasts are full of great, amazing content as well. I've listed a few of my podcasts. YouTube is full of such good stuff that I even got the subscription for YouTube, so I could listen to it all the time. So go check out Rod's Reads if you want all the details on what I like. And if there's others that you like that you want me to know, add them in the comments.",
      "platforms": {
        "tiktok": {
          "video_id": "7177882452180962606",
          "url": "https://www.tiktok.com/@rajistics/video/7177882452180962606",
          "view_count": 3189,
          "upload_date": "2022-12-16",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/899f178c76534cd0a51c18b9a112b48e_1671231002~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=AUH8xPjpS3hgiTDEqp3lDg5tf58%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "What are you favorite tips for error analysis? #datascience #statistics #analytics #machinelearning #codetok #mltok",
      "description": "What are you favorite tips for error analysis? #datascience #statistics #analytics #machinelearning #codetok #mltok",
      "upload_date": "2022-06-17",
      "total_views": 3179,
      "max_views": 3179,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "machinelearning",
        "model",
        "statistics"
      ],
      "search_text": "What are you favorite tips for error analysis? #datascience #statistics #analytics #machinelearning #codetok #mltok analytics codetok datascience machinelearning model statistics Can we just stop judging algorithms and instead focus on outcomes? Some people think if I explain the model architecture, the hyperparameters I use, the training routine that I use, that somehow they'll understand how this model works. No. A better way to understand how a model works is to try it on data that the model hasn't seen before. And then, when the model gets errors, then they all do. It's okay. Here's three tips for analyzing those errors. Visualizations are great for getting a big picture view of the data and helping sort out errors. No model is going to be perfect. The world isn't perfect. Often people disagree on exactly how to label the data. So take that into account when you're doing this. Finally, look at actual examples and study what went wrong in the examples. In this example here, I was doing this the other day, I was staring at this and figured out that, hey, you know what? The algorithm isn't that good at math and that's why it's getting this example wrong.",
      "platforms": {
        "tiktok": {
          "video_id": "7110347746519059758",
          "url": "https://www.tiktok.com/@rajistics/video/7110347746519059758",
          "view_count": 3179,
          "upload_date": "2022-06-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b895fa189e1c483896bd51e62518a8b1_1655506847~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=IAmr9hbb%2FVlghYM%2Bf%2FLkT5jNJtg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Then my team builds data pipelines for the next eight months #datascience #dataengineering #analytics ",
      "description": "Then my team builds data pipelines for the next eight months #datascience #dataengineering #analytics ",
      "upload_date": "2022-09-17",
      "total_views": 3159,
      "max_views": 3159,
      "topics": [
        "analytics",
        "builds",
        "data",
        "dataengineering",
        "datascience",
        "team"
      ],
      "search_text": "Then my team builds data pipelines for the next eight months #datascience #dataengineering #analytics  analytics builds data dataengineering datascience team You better fix my entire life you little shit.",
      "platforms": {
        "tiktok": {
          "video_id": "7144404553629207850",
          "url": "https://www.tiktok.com/@rajistics/video/7144404553629207850",
          "view_count": 3159,
          "upload_date": "2022-09-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/eb9970b9bed64580aaa9ff79e5002619_1663436325~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=X2My04SauZhOy72LT3TjUKsLEZA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I also think of this when walking through security or thinking about cheaters.  ",
      "description": "I also think of this when walking through security or thinking about cheaters.  ",
      "upload_date": "2025-02-06",
      "total_views": 3151,
      "max_views": 3151,
      "topics": [
        "aim",
        "also",
        "security",
        "think",
        "thinking",
        "walking"
      ],
      "search_text": "I also think of this when walking through security or thinking about cheaters.   aim also security think thinking walking They forget my name, aim, aim, aim They call me hell!",
      "platforms": {
        "tiktok": {
          "video_id": "7468312551571328286",
          "url": "https://www.tiktok.com/@rajistics/video/7468312551571328286",
          "view_count": 3151,
          "upload_date": "2025-02-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogsmIniPNIVfExGkEBBAIap8RWnAxBqEXMicfi~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=U%2FMdLVMdzJBy0b%2Fsa6ORdNilGEQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6232,
      "title": "Kolmogorov-Arnold Networks for Time Series. It's another way to approximate complex functions and a lot of research is trying it out: Check out: HiPPO-KAN: Efficient KAN Model for Time Series Analysis: https://arxiv.org/abs/2410.14939 Kolmogorov-Arnold Networks (KAN) for Time Series Classification and Robust Analysis: https://arxiv.org/pdf/2408.07314 Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability: https://arxiv.org/abs/2406.02496 My NotebooK using KAN with Nixtla on M3 and M4: https://colab.research.google.com/drive/1LTyYTUjs0CNn86Po7nE4qjKlZJ-7Rsuo?usp=sharing",
      "description": "Kolmogorov-Arnold Networks for Time Series. It's another way to approximate complex functions and a lot of research is trying it out: Check out: HiPPO-KAN: Efficient KAN Model for Time Series Analysis: https://arxiv.org/abs/2410.14939 Kolmogorov-Arnold Networks (KAN) for Time Series Classification and Robust Analysis: https://arxiv.org/pdf/2408.07314 Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability: https://arxiv.org/abs/2406.02496 My NotebooK using KAN with Nixtla on M3 and M4: https://colab.research.google.com/drive/1LTyYTUjs0CNn86Po7nE4qjKlZJ-7Rsuo?usp=sharing",
      "upload_date": "2024-11-01",
      "total_views": 3147,
      "max_views": 3147,
      "topics": [
        "approach",
        "arnold",
        "enfoque",
        "kan",
        "khan",
        "que",
        "series",
        "time"
      ],
      "search_text": "Kolmogorov-Arnold Networks for Time Series. It's another way to approximate complex functions and a lot of research is trying it out: Check out: HiPPO-KAN: Efficient KAN Model for Time Series Analysis: https://arxiv.org/abs/2410.14939 Kolmogorov-Arnold Networks (KAN) for Time Series Classification and Robust Analysis: https://arxiv.org/pdf/2408.07314 Kolmogorov-Arnold Networks for Time Series: Bridging Predictive Power and Interpretability: https://arxiv.org/abs/2406.02496 My NotebooK using KAN with Nixtla on M3 and M4: https://colab.research.google.com/drive/1LTyYTUjs0CNn86Po7nE4qjKlZJ-7Rsuo?usp=sharing approach arnold enfoque kan khan que series time Are you ready for a deep dive into a time series model using the Kamalagov-Arnold representation theory? One of my commentators asked about this approach, so let's jump in. The Khan approach has become popular in the time series literature lately, and it's because it's main idea that you can take anything that's complex, continuous function, and you can approximate that by breaking that down into univariate functions and summing this all up together. Now this approach also has implications for interpretability, but let's first look at accuracy. I've implemented the Khan approach in this notebook. I'm taking advantage of Nixla, which has already integrated Khan into their forecasting models. I benchmark Khan against other baseline approaches on the widely used M3 and M4 datasets. Now, training Khan is slower, but it did outperform the vanilla ML pre-approach, although our baseline statistical approaches were even better. As for interpretability, I can observe the individual unit functions Khan approaches, but it's still not clear to me exactly how this contributes to understanding my predictions or improving this model. Back to the hippo paper, the hippo technique is a pre-processing step that creates a structured approximation which helps boost performance and efficiency in kind of building the time series models. I like exploring diverse algorithmic approaches, but don't expect Khan to be a silver bullet, and if you're interested, I can share this notebook, and if there's demand, I can do a step-by-step video tutorial.",
      "platforms": {
        "tiktok": {
          "video_id": "7432389709411798315",
          "url": "https://www.tiktok.com/@rajistics/video/7432389709411798315",
          "view_count": 3147,
          "upload_date": "2024-11-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/cceb03e149e048429fc3cf987288c882_1730488092~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=PKqSccDkbhm10kjRQvKH4bAkm2w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17847756903305211",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I love #rstats, but spend most of my time now in #python #datascience #codetok #machinelearning",
      "description": "I love #rstats, but spend most of my time now in #python #datascience #codetok #machinelearning",
      "upload_date": "2022-07-21",
      "total_views": 3145,
      "max_views": 3145,
      "topics": [
        "codetok",
        "datascience",
        "love",
        "machinelearning",
        "python",
        "rstats"
      ],
      "search_text": "I love #rstats, but spend most of my time now in #python #datascience #codetok #machinelearning codetok datascience love machinelearning python rstats uuh just forget about the lyricsveste ouch",
      "platforms": {
        "tiktok": {
          "video_id": "7122862115123219758",
          "url": "https://www.tiktok.com/@rajistics/video/7122862115123219758",
          "view_count": 3145,
          "upload_date": "2022-07-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c429077c33d44ee3aa4395082e519a89_1658420576~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=HEbBCZTTEW1LzIqcTXBD0hmOGps%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Sentence transformers are awesome. Lets talk about the differences between Word2vec, Transformers, and Sentence Transformers. ",
      "description": "Sentence transformers are awesome. Lets talk about the differences between Word2vec, Transformers, and Sentence Transformers. ",
      "upload_date": "2024-06-13",
      "total_views": 3138,
      "max_views": 3138,
      "topics": [
        "even",
        "get",
        "like",
        "sentence",
        "transformer",
        "transformers"
      ],
      "search_text": "Sentence transformers are awesome. Lets talk about the differences between Word2vec, Transformers, and Sentence Transformers.  even get like sentence transformer transformers Could I do this with AI? I can do anything in AI. One algorithm to rule them all. Oh, Transformer has been scaled up a thousand times. It can even handle multimodal data. That's nice, but I need to get the embeddings out of all of these documents so I can identify the main ideas for this new topic of bank strikes this spring. Is that bank like river or money? That old word to Vech doesn't really get context. It's going to struggle with meanings based on like me. I understand context through attention. Yeah, but I'm working with longer sentences and paragraphs. Transformer, I tried using you, but I had to take all the different tokens, average them together to get a result. And when I did that, it got a bit blurry. And I got stuff like this that just wasn't very useful. Did someone say sentences? Sentence transformer. Are you related to transformer? Yes, but I've been trained to make predictions at the sentence or chunk level rather than individual tokens. So the resulting embeddings come out looking much better like this. That's amazing. You must be expensive. Nah, most of my models aren't that large. Some of them even run on a CPU. Is that green with jealousy or green with wealth?",
      "platforms": {
        "tiktok": {
          "video_id": "7379990359650094378",
          "url": "https://www.tiktok.com/@rajistics/video/7379990359650094378",
          "view_count": 3138,
          "upload_date": "2024-06-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/42ff590c389d483e80f6b374523b9308_1718287913~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=K9tfIv7R%2BVEtWAQvCRyl0y9e0NU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Ring Attention lets you split up the attention calculation across GPUs to allow much longer context lengths. LLMs are using this to scale to 1M context lengths. #ringattention #attention #transformers #rajistics Ring Attention with Blockwise Transformers for Near-Infinite Context: https://arxiv.org/abs/2310.01889 Ring Attention Explained: https://coconut-mode.com/posts/ring-attention/ Breaking the Boundaries: Understanding Context Window Limitations and the idea of Ring Attention: https://medium.com/@tanuj22july/breaking-the-boundaries-understanding-context-window-limitations-and-the-idea-of-ring-attention-170e522d44b2 Background: https://unsplash.com/photos/landscape-photography-of-brown-wooden-house-with-trees-around-5dB9WGpJbFc",
      "description": "Ring Attention lets you split up the attention calculation across GPUs to allow much longer context lengths. LLMs are using this to scale to 1M context lengths. #ringattention #attention #transformers #rajistics Ring Attention with Blockwise Transformers for Near-Infinite Context: https://arxiv.org/abs/2310.01889 Ring Attention Explained: https://coconut-mode.com/posts/ring-attention/ Breaking the Boundaries: Understanding Context Window Limitations and the idea of Ring Attention: https://medium.com/@tanuj22july/breaking-the-boundaries-understanding-context-window-limitations-and-the-idea-of-ring-attention-170e522d44b2 Background: https://unsplash.com/photos/landscape-photography-of-brown-wooden-house-with-trees-around-5dB9WGpJbFc",
      "upload_date": "2024-04-14",
      "total_views": 3130,
      "max_views": 3130,
      "topics": [
        "around",
        "attention",
        "context",
        "ring",
        "ringattention",
        "transformers"
      ],
      "search_text": "Ring Attention lets you split up the attention calculation across GPUs to allow much longer context lengths. LLMs are using this to scale to 1M context lengths. #ringattention #attention #transformers #rajistics Ring Attention with Blockwise Transformers for Near-Infinite Context: https://arxiv.org/abs/2310.01889 Ring Attention Explained: https://coconut-mode.com/posts/ring-attention/ Breaking the Boundaries: Understanding Context Window Limitations and the idea of Ring Attention: https://medium.com/@tanuj22july/breaking-the-boundaries-understanding-context-window-limitations-and-the-idea-of-ring-attention-170e522d44b2 Background: https://unsplash.com/photos/landscape-photography-of-brown-wooden-house-with-trees-around-5dB9WGpJbFc around attention context ring ringattention transformers I can get an infinite context length with Ring Attention. You can overcome the quadratic complexity of transformers. Hold on, can someone make this easy for me? LLMs are limited in their inputs because it takes a lot more computation with longer inputs. A few years ago with BERT we were limited to 512 tokens. Newer LLMs can go up to 32,000 tokens. So this is why you get annoyed at me when I show up with long PDFs or audio. Now, it's more you. Ring Attention starts with attention. Then we break each query up into smaller chunks that go on individual GPUs. We then iteratively apply the key value to each of the GPU passing around in a ring. It's still a lot of computation and you have to move the values around the ring. Is this really usable? As long as it's quicker to pass the information around than it is to compute, we can hide that communication overhead. And by scaling up GPUs you can handle a longer length. Clever math and efficient coding? There's a familiar ring to that. Flash Attention!",
      "platforms": {
        "tiktok": {
          "video_id": "7357515193519230254",
          "url": "https://www.tiktok.com/@rajistics/video/7357515193519230254",
          "view_count": 3130,
          "upload_date": "2024-04-14",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1cb2a12f720f4f91a8616da580b15fc5_1713055019~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=h%2FtPIba9R5njTmoRZ0V%2FDLQOyRc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Peak ML #datascience #codetok #huggingface #gradio #huggable #imageclassification ",
      "description": "Peak ML #datascience #codetok #huggingface #gradio #huggable #imageclassification ",
      "upload_date": "2022-08-24",
      "total_views": 3125,
      "max_views": 3125,
      "topics": [
        "codetok",
        "datascience",
        "gradio",
        "huggable",
        "huggingface",
        "imageclassification"
      ],
      "search_text": "Peak ML #datascience #codetok #huggingface #gradio #huggable #imageclassification  codetok datascience gradio huggable huggingface imageclassification certified",
      "platforms": {
        "tiktok": {
          "video_id": "7135442558431563054",
          "url": "https://www.tiktok.com/@rajistics/video/7135442558431563054",
          "view_count": 3125,
          "upload_date": "2022-08-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/416ede5acda44acd8261beb741c3bdbe_1661349692~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=frC9O9AiygFFN6UAsXdr%2FLOkEJw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6074,
      "title": "Deep dive into Devin as an AI software engineer. Check the deep dive by Answer AI: https://www.answer.ai/posts/2025-01-08-devin.html   Inspired by @xevi ",
      "description": "Deep dive into Devin as an AI software engineer. Check the deep dive by Answer AI: https://www.answer.ai/posts/2025-01-08-devin.html   Inspired by @xevi ",
      "upload_date": "2025-01-18",
      "total_views": 3117,
      "max_views": 2960,
      "topics": [
        "answer",
        "code",
        "deep",
        "devin",
        "dive",
        "engineer",
        "reviewing",
        "see",
        "software",
        "went"
      ],
      "search_text": "Deep dive into Devin as an AI software engineer. Check the deep dive by Answer AI: https://www.answer.ai/posts/2025-01-08-devin.html   Inspired by @xevi  answer code deep devin dive engineer reviewing see software went See, Devin, fresh out of the gate with 21 million, backed by the best in Silicon Valley. They said this AI could code for real, replace software engineers. Devin could do it all, debug code, deploy apps, train models, and Twitter went wild. Everybody was talking about this, how this was gonna change the game. The answer team said, hold up. Let's see what this thing can do. Now, the first tense went smooth. Notion to Google Sheets, Devin handled it clean. Easy peasy. The answer was like, this might be the real deal. Let's push it. Now, this is where things started to get sticky. See, out of the 20 tasks, only three went right. 14 went nowhere, three were somewhere in between. With bigger tasks, more complex flows, Devin started feeling shaky. It would chase impossible solutions for days. Code soup, spaghetti logic, trying to do things that made no sense at all. The answer folks spent more time cleaning up after it than if they just coded it themselves. And the lesson for all of us is, maybe autonomy isn't here yet. AI still needs a little bit of steering.",
      "platforms": {
        "tiktok": {
          "video_id": "7461054432994774302",
          "url": "https://www.tiktok.com/@rajistics/video/7461054432994774302",
          "view_count": 2960,
          "upload_date": "2025-01-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocUAoHfFAg2EvlVnRosCEGbsYEQgIAFVAEAifD~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=%2F3kr19zBQgK6F4w5Tun9chRFrUo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17904690819110734",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Bf0yUYnNKoY",
          "url": "https://www.youtube.com/watch?v=Bf0yUYnNKoY",
          "view_count": 157,
          "upload_date": "2025-01-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6649,
      "title": "Regularization is a technique to keep your model from overfitting. It's widely used in machine learning. #datascience #statistics #regularization",
      "description": "Regularization is a technique to keep your model from overfitting. It's widely used in machine learning. #datascience #statistics #regularization",
      "upload_date": "2023-12-03",
      "total_views": 3116,
      "max_views": 2800,
      "topics": [
        "datascience",
        "keep",
        "model",
        "pillars",
        "prompting",
        "regularization",
        "statistics",
        "technique"
      ],
      "search_text": "Regularization is a technique to keep your model from overfitting. It's widely used in machine learning. #datascience #statistics #regularization datascience keep model pillars prompting regularization statistics technique",
      "platforms": {
        "tiktok": {
          "video_id": "7308496796827929902",
          "url": "https://www.tiktok.com/@rajistics/video/7308496796827929902",
          "view_count": 2800,
          "upload_date": "2023-12-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AYHry3SILvI",
          "url": "https://www.youtube.com/watch?v=AYHry3SILvI",
          "view_count": 316,
          "upload_date": "2023-11-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5928,
      "title": "Your AI agent isn’t dumb. It’s forgetful. Most agents redo the same work every run instead of learning from success. This video shows how the Plan-and-Learn pattern turns one-off executions into reusable workflows that compound over time, without retraining the model.  Inspired by Agno ",
      "description": "Your AI agent isn’t dumb. It’s forgetful. Most agents redo the same work every run instead of learning from success. This video shows how the Plan-and-Learn pattern turns one-off executions into reusable workflows that compound over time, without retraining the model.  Inspired by Agno ",
      "upload_date": "2025-12-19",
      "total_views": 3108,
      "max_views": 1745,
      "topics": [
        "agent",
        "agents",
        "agno",
        "continual",
        "dumb",
        "forgetful",
        "gonna",
        "isn",
        "learn",
        "learning",
        "plan",
        "run",
        "time"
      ],
      "search_text": "Your AI agent isn’t dumb. It’s forgetful. Most agents redo the same work every run instead of learning from success. This video shows how the Plan-and-Learn pattern turns one-off executions into reusable workflows that compound over time, without retraining the model.  Inspired by Agno  agent agents agno continual dumb forgetful gonna isn learn learning plan run time Is your AI agent improving over time? I find that agents often repeat the same mistakes, burn a lot of tokens, forget what they worked on. Well, the folks at Agno shared a pattern called POW. Plan and learn gives you a recipe for a continual improvement without any fine tuning. You can get reliable faster over time by reusing what you've learned before. Now to start with, we're all trying to build agents that solve difficult tasks, right? These tasks need planning, they need tools, we use strong reasoning models to do this. But when you look at how your agents run, agents are doing the same things over and over again, the same tool sequences and the same failure modes. What would be useful is an agent that learned from its successful runs. So it's not resolving the same types of problems every time. And this is the core idea behind plan and learn. What we're gonna do is break down our goals into steps with explicit success criteria. Execute one step at a time, verify the completion before moving on. If the assumptions break or new information shows up, we adapt the plan of course. And after a successful run, the agent's gonna ask what worked here that could help next time. It's gonna store this plan away, the tool sequences and verification checks. And the magic is on the next similar task, it's gonna search and look at what's already worked in the past and start from there. So this way we're not doing all those retries, but wasted planning that lost progress. The agent can improve over time without having to go through some retaining or fine tuning process. What we're doing is just building a real repository of solutions that our agents can reuse. It's really brilliant.",
      "platforms": {
        "tiktok": {
          "video_id": "7585371094857551135",
          "url": "https://www.tiktok.com/@rajistics/video/7585371094857551135",
          "view_count": 1363,
          "upload_date": "2025-12-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oA2ERKQzpIVEE2EmpBRFLhZEIsDCeKAg7AfxuK~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=Sn2VWIdQ7wKIwvBaQEp%2B%2FLo6xCs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18049638227422865",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NjpMAqR-qDY",
          "url": "https://www.youtube.com/watch?v=NjpMAqR-qDY",
          "view_count": 1745,
          "upload_date": "2025-12-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6296,
      "title": "Synthetic datasets have given me a way to understand better how to do feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability",
      "description": "Synthetic datasets have given me a way to understand better how to do feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability",
      "upload_date": "2023-01-25",
      "total_views": 3108,
      "max_views": 3108,
      "topics": [
        "datascience",
        "datasets",
        "explainability",
        "features",
        "machinelearning",
        "synthetic",
        "syntheticdata"
      ],
      "search_text": "Synthetic datasets have given me a way to understand better how to do feature selection and model explainability. Try it out sometime. #datascience #machinelearning #syntheticdata #explainability datascience datasets explainability features machinelearning synthetic syntheticdata",
      "platforms": {
        "instagram": {
          "video_id": "17976711787865295",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "sT604zY3QQc",
          "url": "https://youtube.com/shorts/sT604zY3QQc",
          "view_count": 3108,
          "upload_date": "2023-01-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6517,
      "title": "Some data analysis tips: 1. Your data might be unrepresentative 2. Think about what was collected and what wasn't 3. Not all data is useful #datascience #machinelearning #dataanalysis",
      "description": "Some data analysis tips: 1. Your data might be unrepresentative 2. Think about what was collected and what wasn't 3. Not all data is useful #datascience #machinelearning #dataanalysis",
      "upload_date": "2023-08-02",
      "total_views": 3095,
      "max_views": 2421,
      "topics": [
        "analysis",
        "data",
        "dataanalysis",
        "datascience",
        "machinelearning",
        "tips"
      ],
      "search_text": "Some data analysis tips: 1. Your data might be unrepresentative 2. Think about what was collected and what wasn't 3. Not all data is useful #datascience #machinelearning #dataanalysis analysis data dataanalysis datascience machinelearning tips",
      "platforms": {
        "tiktok": {
          "video_id": "7262764540994030894",
          "url": "https://www.tiktok.com/@rajistics/video/7262764540994030894",
          "view_count": 2421,
          "upload_date": "2023-08-02",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cvcv1sVg1UH",
          "url": "https://www.instagram.com/reel/Cvcv1sVg1UH",
          "view_count": 419,
          "upload_date": "2023-08-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "I0VkrshhTQM",
          "url": "https://www.youtube.com/watch?v=I0VkrshhTQM",
          "view_count": 255,
          "upload_date": "2023-08-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Visualizations for showing variation in the data or uncertainty. Based on Unfair Comparisons by Eli Holder. #datascience #machinelearning #statistics #datavisualization ",
      "description": "Visualizations for showing variation in the data or uncertainty. Based on Unfair Comparisons by Eli Holder. #datascience #machinelearning #statistics #datavisualization ",
      "upload_date": "2022-12-05",
      "total_views": 3095,
      "max_views": 3095,
      "topics": [
        "datascience",
        "datavisualization",
        "machinelearning",
        "people",
        "statistics",
        "variation"
      ],
      "search_text": "Visualizations for showing variation in the data or uncertainty. Based on Unfair Comparisons by Eli Holder. #datascience #machinelearning #statistics #datavisualization  datascience datavisualization machinelearning people statistics variation This graph can be misleading. Let me show you what's wrong with it and how we can fix it. People simplify the results in their head and so a graph like this can cause people to look at differences which can be very different than the reality that's actually out there. To overcome this we can build graphs that show more variation in the data. Prediction intervals are great for showing people the variability, something I've talked about before. Another great technique is a dot plot. It just reminds people of the underlying variation that exists in the data. And while this may seem like a nuance, it's our job to take this complex reality that we live in and find a way to represent it for our users.",
      "platforms": {
        "tiktok": {
          "video_id": "7173769847174745387",
          "url": "https://www.tiktok.com/@rajistics/video/7173769847174745387",
          "view_count": 3095,
          "upload_date": "2022-12-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b2119f3e85c84bed88b0da93e9239df5_1670273463~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=rvmkoZ2UxTfVA2lrp11z3G8kyEg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Using LLMs for tools is an emerging trend this year. Cohere has focused on it with it's new Command-R+ model that focuses on enterprise use cases like RAG and tool use. I haven't tried the multi step tool use, but it looks promising.  Multi-step Tool Use (Agents): https://docs.cohere.com/docs/multi-step-tool-use Notebook Example: https://github.com/cohere-ai/notebooks/blob/main/notebooks/Vanilla_Multi_Step_Tool_Use.ipynb",
      "description": "Using LLMs for tools is an emerging trend this year. Cohere has focused on it with it's new Command-R+ model that focuses on enterprise use cases like RAG and tool use. I haven't tried the multi step tool use, but it looks promising.  Multi-step Tool Use (Agents): https://docs.cohere.com/docs/multi-step-tool-use Notebook Example: https://github.com/cohere-ai/notebooks/blob/main/notebooks/Vanilla_Multi_Step_Tool_Use.ipynb",
      "upload_date": "2024-04-05",
      "total_views": 3093,
      "max_views": 3093,
      "topics": [
        "cohere",
        "model",
        "multi",
        "tool",
        "tools",
        "use"
      ],
      "search_text": "Using LLMs for tools is an emerging trend this year. Cohere has focused on it with it's new Command-R+ model that focuses on enterprise use cases like RAG and tool use. I haven't tried the multi step tool use, but it looks promising.  Multi-step Tool Use (Agents): https://docs.cohere.com/docs/multi-step-tool-use Notebook Example: https://github.com/cohere-ai/notebooks/blob/main/notebooks/Vanilla_Multi_Step_Tool_Use.ipynb cohere model multi tool tools use Did you see the new Cohere model supports multi-step tool use? Big deal. We were using tools with LLMs last year. Are you talking about demos like this? Yeah, they would work by just loading up the prompt with all the information about your different tools, whether it's weather, search, other APIs, and the model would just pick the best tool and run it. But could the model do a sequence of tool calls all within one request? And about as good as my code documentation skills. So how are you going to connect multiple tools together? With a lot of extra scripting calls and databases? Exactly. And that's why Cohere took the time to develop a model that could support multiple tools in one request. It comes up with a plan and then starts executing on it. You're getting it. It'll check its progress along the way, reevaluate, change its plan if it needs to, and then keep going until it finishes a task. This could make it a lot easier to use tools. Is there a real example to show? Here's an example notebook they've shared. You can see there's a single request, but then it's going and using multiple tools altogether. I want to check this out. How long do you think before other large language models get this? It won't take long. They're all learning from each other.",
      "platforms": {
        "tiktok": {
          "video_id": "7354200117529840942",
          "url": "https://www.tiktok.com/@rajistics/video/7354200117529840942",
          "view_count": 3093,
          "upload_date": "2024-04-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1f0c98f291754ac4a488b6abdb7af998_1712283161~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=Yd84RCXNEG0JbhuB4opzTAvth%2FU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Pair programming is some of my favorite times as a data scientist. I am starting to use ChatGPT to fill that role lately. Its useful for me. #datascience #machinelearning #pairprogramming #chatgpt #codex ",
      "description": "Pair programming is some of my favorite times as a data scientist. I am starting to use ChatGPT to fill that role lately. Its useful for me. #datascience #machinelearning #pairprogramming #chatgpt #codex ",
      "upload_date": "2023-03-19",
      "total_views": 3084,
      "max_views": 3084,
      "topics": [
        "chatgpt",
        "codex",
        "datascience",
        "machinelearning",
        "pairprogramming",
        "way"
      ],
      "search_text": "Pair programming is some of my favorite times as a data scientist. I am starting to use ChatGPT to fill that role lately. Its useful for me. #datascience #machinelearning #pairprogramming #chatgpt #codex  chatgpt codex datascience machinelearning pairprogramming way What's the best way to improve your programming? Pair programming. And OpenAI is making some of the best tools for this. Let's talk about how they're also likely to keep their lead. Pair programming, where you work together with someone else to solve coding problems, is a great way for people to learn how to code. It's something that I always suggest inexperienced people look for in teams that they're gonna join. If your team doesn't support it or you feel isolated as a programmer, use tools like ChatGPT. They give you an easy way to ask very new, novice questions, be able to get feedback, also see how other people have coded up the same problem. Now, one reason OpenAI's tools are so good at this is they have optimized for it. They have worked with developers to build training data that works through problems in a step-by-step way. And when these developers hit bugs, they write down exactly how they solved that problem too in a step-by-step way. All of this is fed as training data into OpenAI's technologies. This is gonna probably give them a good lead over other approaches that just look at code.",
      "platforms": {
        "tiktok": {
          "video_id": "7212299208236404014",
          "url": "https://www.tiktok.com/@rajistics/video/7212299208236404014",
          "view_count": 3084,
          "upload_date": "2023-03-19",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/897613ff400d4905aa8409a1114798ca_1679244286~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=zl%2F0mkbmq5SUtXBYWdQL9Hkn5kQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5946,
      "title": "Think you know how Reinforcement Learning for LLMs really works? The secret isn't \"just do more training” — it's about where your data comes from and how dense your feedback is. Off-policy distillation (SFT) learns by copying labels. On-policy RL learns by trial and error with sparse rewards. On-policy distillation blends both: the student explores independently and gets feedback at every step. Inspired by the Thinking Machines Lab https://thinkingmachines.ai/blog/on-policy-distillation/",
      "description": "Think you know how Reinforcement Learning for LLMs really works? The secret isn't \"just do more training” — it's about where your data comes from and how dense your feedback is. Off-policy distillation (SFT) learns by copying labels. On-policy RL learns by trial and error with sparse rewards. On-policy distillation blends both: the student explores independently and gets feedback at every step. Inspired by the Thinking Machines Lab https://thinkingmachines.ai/blog/on-policy-distillation/",
      "upload_date": "2025-11-02",
      "total_views": 3066,
      "max_views": 1695,
      "topics": [
        "distillation",
        "every",
        "explained",
        "feedback",
        "learning",
        "llms",
        "model",
        "policy",
        "reinforcement",
        "simply"
      ],
      "search_text": "Think you know how Reinforcement Learning for LLMs really works? The secret isn't \"just do more training” — it's about where your data comes from and how dense your feedback is. Off-policy distillation (SFT) learns by copying labels. On-policy RL learns by trial and error with sparse rewards. On-policy distillation blends both: the student explores independently and gets feedback at every step. Inspired by the Thinking Machines Lab https://thinkingmachines.ai/blog/on-policy-distillation/ distillation every explained feedback learning llms model policy reinforcement simply We all see that AI is getting smarter, but how is it actually happening? It's not magic. Let me explain. One way AI can learn is having a model play a game by itself. And maybe every once in a while, a teacher will give it a score. That would be called RLHF, where it's reinforcement learning with some human feedback. We call this approach on policy because the model is learning from its own actions. So if we apply this to language models, it means the language model writes a response, but then we get score that after it writes it, is it helpful, is it harmful? Now the downside is this feedback is very sparse. There's a ton of compute costs because you only know if you won at the end of every round. An alternative approach is you have the model stop playing and instead study replay. It watches labeled examples of good moves versus bad moves. So one implementation is called DPO, direct preference optimization. And so this is what we call an off policy method because it's learning from someone else's data. It's imitation learning, it's copying what experts did. Now this approach is fast, it's stable, but if the underlying game changes, it struggles to adapt. Now a third way to do it is where we're gonna have the model play, but now we're gonna have a coach sit beside it, grading every move in real time. This is what's called on policy distillation and it's a hybrid approach where the model's learning from its own behavior, but it's also getting very rich, dense feedback at every step. So it's more efficient than traditional reinforcement learning where it only happened once in a while through really sparse rewards. Now we see this in benchmarks. So thinking machine shared some results that they did where they use the technique of on policy distillation and outperformed other methods and also saved a ton of GPU costs.",
      "platforms": {
        "tiktok": {
          "video_id": "7568108347367476510",
          "url": "https://www.tiktok.com/@rajistics/video/7568108347367476510",
          "view_count": 1695,
          "upload_date": "2025-11-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oogOgIIIjp2DAjGSkLQaeIehMRYIBIeAqIqVCs~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=rxPvzZojXE7JqY%2FjT9V0lEWO78c%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17948834781052630",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cnKIjftWS_k",
          "url": "https://www.youtube.com/watch?v=cnKIjftWS_k",
          "view_count": 1371,
          "upload_date": "2025-11-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Overdue for sports analytics #datascience #analytics #codetok #sportsanalytics #machinelearning",
      "description": "Overdue for sports analytics #datascience #analytics #codetok #sportsanalytics #machinelearning",
      "upload_date": "2022-08-12",
      "total_views": 3042,
      "max_views": 3042,
      "topics": [
        "analytics",
        "bad",
        "codetok",
        "datascience",
        "machinelearning",
        "sportsanalytics"
      ],
      "search_text": "Overdue for sports analytics #datascience #analytics #codetok #sportsanalytics #machinelearning analytics bad codetok datascience machinelearning sportsanalytics All of us have made some bad decisions like this in life. But now AI can second guess us. This has built a machine learning model that used 17,000 passes from the past to help predict the likelihood of if a pass is going to be complete. And it took into account kind of the passing accuracy of players, the body parts, who's the recipient, and it came out pretty accurate. Let's look at this demo. We can see here a very low probability when the goalkeeper kicks it. On the other hand, if we look at this other play, it's pretty high probability and it's not surprising. This model isn't to make you feel bad, although you might. It's really to help players improve and figure out what they should have done in different scenarios. Go check it out yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7130786220149542190",
          "url": "https://www.tiktok.com/@rajistics/video/7130786220149542190",
          "view_count": 3042,
          "upload_date": "2022-08-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/06b48830d02d47e19856da6a1f3fb46b_1660265549~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=5H6Jhh2KRxy7VgM3rrkMYMxwuXI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6197,
      "title": "Ensemble learning with majority voting can improve decision accuracy, as demonstrated when three 70%-accurate models combined outperform a single 75%-accurate model by reaching 78% accuracy. The key to effective ensembling is model diversity, where uncorrelated errors between models can dramatically boost accuracy, potentially reaching 98% in optimal conditions.",
      "description": "Ensemble learning with majority voting can improve decision accuracy, as demonstrated when three 70%-accurate models combined outperform a single 75%-accurate model by reaching 78% accuracy. The key to effective ensembling is model diversity, where uncorrelated errors between models can dramatically boost accuracy, potentially reaching 98% in optimal conditions.",
      "upload_date": "2025-03-14",
      "total_views": 3041,
      "max_views": 1656,
      "topics": [
        "accuracy",
        "data",
        "learning",
        "majority",
        "model",
        "models",
        "science",
        "three",
        "voting"
      ],
      "search_text": "Ensemble learning with majority voting can improve decision accuracy, as demonstrated when three 70%-accurate models combined outperform a single 75%-accurate model by reaching 78% accuracy. The key to effective ensembling is model diversity, where uncorrelated errors between models can dramatically boost accuracy, potentially reaching 98% in optimal conditions. accuracy data learning majority model models science three voting Would you like to make better decisions in life? Well, using majority voting can help your machine learnings improve their accuracy. I'm going to show you that three models that each have an accuracy of 70% can beat a single model that has an accuracy of 75%. Boom! We're going to start with the classification problem where all the ground truth is set to one. Now, we're going to use classifiers that have a 70% chance of likelihood. So that means a 70% chance that it's a one. We're using three classifiers here and majority voting. So what we can do now is see if all three classifiers are right, which happens about 34% of the time. Now, two classifiers are right, about 44% of the time, but then they're wrong, around 19% of the time. And finally, there's only a small percent of chance that all three are wrong. And here you can see with majority voting, when we add up those first two categories, now we have a result that's 78% accurate. So we're beating that 75% accurate model. In fact, if we had five pseudo random classifiers, each with 70% accuracy, they'd be correct 83% of the time. A critical factor for good ensembling is having diverse models. Now, if we had models that weren't very diverse, so you can see here we have three different models, but when we ensemble them to majority voting, we only get 80% accuracy. We don't get any improvement because there wasn't diversity. On the other hand, if we look at these three particular models that are much more diverse, the errors are uncorrelated, then the resulting model accuracy improves quite a bit to 98%. This is the power of ensembling.",
      "platforms": {
        "tiktok": {
          "video_id": "7481803971703622943",
          "url": "https://www.tiktok.com/@rajistics/video/7481803971703622943",
          "view_count": 1656,
          "upload_date": "2025-03-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogDUAVD2IC51vEE7TfRtEYkEcUveLAyFADAAFn~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=JZ9yKMyfdyRJyzh9b0cUyrPtqLs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18120890344438242",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AceuQprTTB8",
          "url": "https://www.youtube.com/watch?v=AceuQprTTB8",
          "view_count": 1385,
          "upload_date": "2025-03-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Contrastive learning is common for folks working in NLP and images. This was new to me, so wanted to share the intuition a bit more widely.  This is an introduction, there are so many contrastive loss functions for different scenarios. ",
      "description": "Contrastive learning is common for folks working in NLP and images. This was new to me, so wanted to share the intuition a bit more widely.  This is an introduction, there are so many contrastive loss functions for different scenarios. ",
      "upload_date": "2022-11-03",
      "total_views": 3039,
      "max_views": 3039,
      "topics": [
        "different",
        "embedding",
        "intuition",
        "learning",
        "loss",
        "space"
      ],
      "search_text": "Contrastive learning is common for folks working in NLP and images. This was new to me, so wanted to share the intuition a bit more widely.  This is an introduction, there are so many contrastive loss functions for different scenarios.  different embedding intuition learning loss space Let's talk about how we can turn an embedding space that looks like this into this using contrast of learning. In traditional machine learning, we have actual values, and so we can compare our model to the actual values and be able to calculate the error. Embedding space is different. We don't actually have the actual values for these concepts. But what we know is we want things that are similar to each other to be closer, while things that are not really that close to each other should be far apart away. Let's use that intuition. I put together a couple of toy examples from a churn data set to give us the idea for what's going on. Now, great reception is something that's similar to I Love Family Plan, probably likely not to churn while somebody that says total ripoff is likely to churn. So if we look at the embedding space, we would see when these, when great reception is close to I Love Family Plan, that's low error. While on the other hand, if great reception is far away from I Love Family Plan and close to total ripoff, that's a high error. What we can do is take this intuition about the distance and actually create a loss function like this using that. With this intuition, you're going to better be able to dive into and understand all the different loss functions that are out there, but that are going to help you fine tune your models so your embeddings work for your task.",
      "platforms": {
        "tiktok": {
          "video_id": "7161611391722409259",
          "url": "https://www.tiktok.com/@rajistics/video/7161611391722409259",
          "view_count": 3039,
          "upload_date": "2022-11-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f8f1498458024ab9861a79c062d087d8_1667442602~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=x7hLdNX%2Fs9Ctvwz4I9r9dXiNmCk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Feature engineering with categorical variables and gender cateogries feom chatgpt. ",
      "description": "Feature engineering with categorical variables and gender cateogries feom chatgpt. ",
      "upload_date": "2024-09-18",
      "total_views": 3033,
      "max_views": 3033,
      "topics": [
        "categorical",
        "data",
        "gender",
        "good",
        "scientist",
        "value"
      ],
      "search_text": "Feature engineering with categorical variables and gender cateogries feom chatgpt.  categorical data gender good scientist value Want to know why gender is a great way to identify a good data scientist? There's at least five different options for gender and a good data scientist knows at least three techniques for getting at them. After all, at the heart of data science is being able to convert categorical information into a numeric representation. Ordinal encoding is going to replace each categorical value with a number. This works best when there's a natural ordering to the values. One-hot encoding creates a new feature for every categorical value. The downside here is if you're working with a categorical variable like zip code that has a high cardinality, then you're going to blow out your data set with a lot of features. Every data scientist should understand those too. And what I look for in an advanced data scientist is somebody that knows a couple of other ways to handle categorical features. Target encoding is where you replace each categorical value with its mean target value. One thing you have to watch out for here is having a good validation strategy because it's easy to leak information when you're using this technique. There you have it. You want to be on the team? Be able to talk about gender in at least three ways.",
      "platforms": {
        "tiktok": {
          "video_id": "7415782801045212459",
          "url": "https://www.tiktok.com/@rajistics/video/7415782801045212459",
          "view_count": 3033,
          "upload_date": "2024-09-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1c4d3e5081944c74bab936c22aff705a_1726621490~tplv-tiktokx-origin.image?dr=9636&x-expires=1767416400&x-signature=QeaVoy8ZIPp7OqCdsG0XjPKMOS0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Leakage is omnipresent #datascience #analytics #codetok #targetleakage ",
      "description": "Leakage is omnipresent #datascience #analytics #codetok #targetleakage ",
      "upload_date": "2022-08-18",
      "total_views": 3032,
      "max_views": 3032,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "leakage",
        "omnipresent",
        "targetleakage"
      ],
      "search_text": "Leakage is omnipresent #datascience #analytics #codetok #targetleakage  analytics codetok datascience leakage omnipresent targetleakage Good��창Musique",
      "platforms": {
        "tiktok": {
          "video_id": "7133001287892847918",
          "url": "https://www.tiktok.com/@rajistics/video/7133001287892847918",
          "view_count": 3032,
          "upload_date": "2022-08-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4c0cf16e25ca4fbca25e2cadd0791e0e_1660781284~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=I6KcPHeOwPODEEJ4eUAwVX%2B2HFI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I was so focused!  Data is hard.  #datascience #dataanalysis #statistics #codetok #mltok #machinelearning",
      "description": "I was so focused!  Data is hard.  #datascience #dataanalysis #statistics #codetok #mltok #machinelearning",
      "upload_date": "2022-06-07",
      "total_views": 3030,
      "max_views": 3030,
      "topics": [
        "codetok",
        "dataanalysis",
        "datascience",
        "machinelearning",
        "mltok",
        "statistics"
      ],
      "search_text": "I was so focused!  Data is hard.  #datascience #dataanalysis #statistics #codetok #mltok #machinelearning codetok dataanalysis datascience machinelearning mltok statistics Doesn't matter. There are only four rules you need to remember. Make the plan, execute the plan, expect the plan to go off the rails, throw away the plan.",
      "platforms": {
        "tiktok": {
          "video_id": "7106311772818230571",
          "url": "https://www.tiktok.com/@rajistics/video/7106311772818230571",
          "view_count": 3030,
          "upload_date": "2022-06-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0e0dab3638734db38768c4b6ef3a4d10_1654567146~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=thy7hHXRwr4imA47HTvTHXMEAFc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5973,
      "title": "In his talk, Denny Zhou outlined four strategies for improving LLM reasoning without increasing model size: eliciting reasoning via intermediate tokens (chain-of-thought prompting), RL fine-tuning on model-generated reasoning with a verifier, aggregating multiple reasoning paths through self-consistency, and augmenting reasoning with retrieval of related problems or principles. These methods shift the focus from scaling architecture to optimizing decoding, training, aggregation, and retrieval processes. Youtube: https://www.youtube.com/watch?v=ebnX5Ur1hBk",
      "description": "In his talk, Denny Zhou outlined four strategies for improving LLM reasoning without increasing model size: eliciting reasoning via intermediate tokens (chain-of-thought prompting), RL fine-tuning on model-generated reasoning with a verifier, aggregating multiple reasoning paths through self-consistency, and augmenting reasoning with retrieval of related problems or principles. These methods shift the focus from scaling architecture to optimizing decoding, training, aggregation, and retrieval processes. Youtube: https://www.youtube.com/watch?v=ebnX5Ur1hBk",
      "upload_date": "2025-08-11",
      "total_views": 3026,
      "max_views": 2798,
      "topics": [
        "answer",
        "ask",
        "denny",
        "fine",
        "key",
        "lessons",
        "llm",
        "model",
        "paths",
        "reasoning",
        "retrieval",
        "talk",
        "zhou"
      ],
      "search_text": "In his talk, Denny Zhou outlined four strategies for improving LLM reasoning without increasing model size: eliciting reasoning via intermediate tokens (chain-of-thought prompting), RL fine-tuning on model-generated reasoning with a verifier, aggregating multiple reasoning paths through self-consistency, and augmenting reasoning with retrieval of related problems or principles. These methods shift the focus from scaling architecture to optimizing decoding, training, aggregation, and retrieval processes. Youtube: https://www.youtube.com/watch?v=ebnX5Ur1hBk answer ask denny fine key lessons llm model paths reasoning retrieval talk zhou What are four lessons the head of Google's DeepMind reasoning team wants you to know about how AI really thinks? In a recent talk, he revealed four different ways to help you think about where the future of AI is going, and it's not just bigger models. First, elicit reasoning. Don't just ask for answers. When you ask a model directly for an answer, it'll get it wrong. But if you ask it to think step by step, you can generate the right reasoning path and find the correct answer. Second, let's fine-tune on the model's own reasoning. The old traditional way was supervised fine-tuning on messy human-written examples. One of the real breakthroughs is to let the model generate its own reasoning, then use a simple verifier and fine-tune on those correct paths. This is how models learn to self-improve. Third, aggregate multiple paths to find the truth. Don't just trust the first answer by sampling multiple reasoning paths, taking the most frequent final answer. This is a technique called self-consistency. You can see here on benchmarks like GSMA-K, you can get much better results. Fourth, retrieve first, then reason. Take a problem like this. Models are going to fail. But if you ask it to recall a related problem, it can retrieve the right formula, use that as a template to help reason and figure out the correct answer. The takeaway, the biggest breakthroughs aren't just coming from scale. They're coming from treating LLMs as true reasoning engines.",
      "platforms": {
        "tiktok": {
          "video_id": "7537474219748248862",
          "url": "https://www.tiktok.com/@rajistics/video/7537474219748248862",
          "view_count": 2798,
          "upload_date": "2025-08-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYCQAfINmXkEfIPGQRQMMP8eQbP0AjaCIq0hJj~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=WYH%2B8HnJw0GgFjOpuU4iGy%2BarqI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17935878663066994",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "D39JxfZCvdU",
          "url": "https://www.youtube.com/watch?v=D39JxfZCvdU",
          "view_count": 228,
          "upload_date": "2025-08-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6007,
      "title": "VLLM: A widely used inference and serving engine for LLMs",
      "description": "VLLM: A widely used inference and serving engine for LLMs",
      "upload_date": "2024-08-17",
      "total_views": 3023,
      "max_views": 3023,
      "topics": [
        "engine",
        "inference",
        "one",
        "platforms",
        "serving",
        "used",
        "vllm",
        "widely"
      ],
      "search_text": "VLLM: A widely used inference and serving engine for LLMs engine inference one platforms serving used vllm widely",
      "platforms": {
        "instagram": {
          "video_id": "18002566280464982",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "J-JeWvA4mxA",
          "url": "https://www.youtube.com/watch?v=J-JeWvA4mxA",
          "view_count": 3023,
          "upload_date": "2024-08-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Google Colab, Kaggle, and LangChain are all great ways to start learning this weekend! #datascience #machinelearning #kaggle #googlecolab #langchain ",
      "description": "Google Colab, Kaggle, and LangChain are all great ways to start learning this weekend! #datascience #machinelearning #kaggle #googlecolab #langchain ",
      "upload_date": "2023-01-21",
      "total_views": 3017,
      "max_views": 3017,
      "topics": [
        "datascience",
        "google",
        "googlecolab",
        "kaggle",
        "langchain",
        "machinelearning"
      ],
      "search_text": "Google Colab, Kaggle, and LangChain are all great ways to start learning this weekend! #datascience #machinelearning #kaggle #googlecolab #langchain  datascience google googlecolab kaggle langchain machinelearning Thanks for watching!",
      "platforms": {
        "tiktok": {
          "video_id": "7191133221294247211",
          "url": "https://www.tiktok.com/@rajistics/video/7191133221294247211",
          "view_count": 3017,
          "upload_date": "2023-01-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c24d1cfec59a4bd2a3f7ca0d46882cbc_1674316274~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=QBU1LMwTC5TlTPBg0MmtF9zesMc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6076,
      "title": "datasaurus. Remember to visualize your data. ",
      "description": "datasaurus. Remember to visualize your data. ",
      "upload_date": "2025-01-14",
      "total_views": 3010,
      "max_views": 3010,
      "topics": [
        "big",
        "data",
        "datasaurus",
        "deal",
        "remember",
        "visualize"
      ],
      "search_text": "datasaurus. Remember to visualize your data.  big data datasaurus deal remember visualize It's no big deal. It's no big deal. This is no big deal.",
      "platforms": {
        "tiktok": {
          "video_id": "7459772361642937630",
          "url": "https://www.tiktok.com/@rajistics/video/7459772361642937630",
          "view_count": 3010,
          "upload_date": "2025-01-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMIU1TQAq8AL6INbaojfcDece3GaICCQCD4P8G~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=tQ7JPeOpqpCRrqEThkb4ZRXfSks%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18057940162794562",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Watch out for leakage, it happens even to the best.  #datascience #statistics #dataleakage  #targetleakage #machinelearning",
      "description": "Watch out for leakage, it happens even to the best.  #datascience #statistics #dataleakage  #targetleakage #machinelearning",
      "upload_date": "2022-07-10",
      "total_views": 2983,
      "max_views": 2983,
      "topics": [
        "christmas",
        "dataleakage",
        "datascience",
        "machinelearning",
        "statistics",
        "targetleakage"
      ],
      "search_text": "Watch out for leakage, it happens even to the best.  #datascience #statistics #dataleakage  #targetleakage #machinelearning christmas dataleakage datascience machinelearning statistics targetleakage Christmas wake up, I don't like this, Christmas wake up",
      "platforms": {
        "tiktok": {
          "video_id": "7118804303086718254",
          "url": "https://www.tiktok.com/@rajistics/video/7118804303086718254",
          "view_count": 2983,
          "upload_date": "2022-07-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/84477b1a773749c9b28d126f0e45c135_1657475792~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=KVWnR%2FKsFPReaITHOC%2BnwRkIzgU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Training an image classifier using 🤗 transformers #datascience #analytics #codetok #deeplearning #huggingface Longer video at other site using the same -rajistics",
      "description": "Training an image classifier using 🤗 transformers #datascience #analytics #codetok #deeplearning #huggingface Longer video at other site using the same -rajistics",
      "upload_date": "2022-08-04",
      "total_views": 2969,
      "max_views": 2969,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "deeplearning",
        "huggingface",
        "model"
      ],
      "search_text": "Training an image classifier using 🤗 transformers #datascience #analytics #codetok #deeplearning #huggingface Longer video at other site using the same -rajistics analytics codetok datascience deeplearning huggingface model You might have seen this, but let's build this instead. It's much tastier. Build AI that classifies these images. We're going to start by grabbing a pre-trained model, Google's Vision Transformer. We'll take our data, that's images and folders, use the data sets package to package all that up, organize that, put that on the Hugging Face Hub, and use it to train the model. Code for configuring our model for training, and then boom, we start training the model. Or later, we've got a model that's 95% accurate. The model's built, now you can start using it for predictions. To do this yourself, grab the Colab notebook that I've linked. Also on the other video site, I have a much longer walkthrough. I spend about 25 minutes walking through this process end to end.",
      "platforms": {
        "tiktok": {
          "video_id": "7128169135447690542",
          "url": "https://www.tiktok.com/@rajistics/video/7128169135447690542",
          "view_count": 2969,
          "upload_date": "2022-08-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f83a78579bab4fd4b8f7b7db18789f00_1659656212~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=VM7iwvd8mNgiu%2BxdNFyZUHdwpIc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Good times, what was your first ML model?  #titanic #datascience #statistics #codetok #machinelearning #rstats #python",
      "description": "Good times, what was your first ML model?  #titanic #datascience #statistics #codetok #machinelearning #rstats #python",
      "upload_date": "2022-06-15",
      "total_views": 2958,
      "max_views": 2958,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "rstats",
        "statistics",
        "titanic"
      ],
      "search_text": "Good times, what was your first ML model?  #titanic #datascience #statistics #codetok #machinelearning #rstats #python codetok datascience machinelearning rstats statistics titanic Thank you for watching!",
      "platforms": {
        "tiktok": {
          "video_id": "7109255163822558510",
          "url": "https://www.tiktok.com/@rajistics/video/7109255163822558510",
          "view_count": 2958,
          "upload_date": "2022-06-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/665d4993701443ff9d7e30b0d8dceb39_1655252457~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=IknE6IFMZ8NXmBl4HehuzJgEniE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6199,
      "title": "Rerankers improve search by examining whether documents truly answer your specific question, unlike retrievers that only match similar words. While computationally expensive (which is why they're used only on pre-filtered results), newer instruction following rerankers allow users to prioritize different types of content—like official safety notices or user reviews—through simple instructions, making search results more relevant to what you actually need.",
      "description": "Rerankers improve search by examining whether documents truly answer your specific question, unlike retrievers that only match similar words. While computationally expensive (which is why they're used only on pre-filtered results), newer instruction following rerankers allow users to prioritize different types of content—like official safety notices or user reviews—through simple instructions, making search results more relevant to what you actually need.",
      "upload_date": "2025-03-12",
      "total_views": 2956,
      "max_views": 1908,
      "topics": [
        "documents",
        "explaining",
        "including",
        "instruction",
        "question",
        "rag",
        "ranker",
        "reranker",
        "rerankers",
        "results",
        "retrieval",
        "search",
        "similar"
      ],
      "search_text": "Rerankers improve search by examining whether documents truly answer your specific question, unlike retrievers that only match similar words. While computationally expensive (which is why they're used only on pre-filtered results), newer instruction following rerankers allow users to prioritize different types of content—like official safety notices or user reviews—through simple instructions, making search results more relevant to what you actually need. documents explaining including instruction question rag ranker reranker rerankers results retrieval search similar Search results are terrible. I've tried better embedding, vector databases. What about a re-ranker? A re-ranker? I would redoing help. Your retriever works by looking at the documents and query separately. So it's just looking to see if they, for example, contain similar words to each other. Yeah, but just having similar words doesn't mean it answers the question. Exactly, and that's where the re-ranker shines. It considers the interaction between the query and documents. It just doesn't look for mentions that says, hey, does this document specifically answer the question like is Blendmaster 3000 safe? And by understanding the relationship, it provides better results. Well, why not run everything through the re-ranker if it's better? Re-rankers are computationally expensive. Running everything through a re-ranker would take a lot longer. More NVIDIA tax. Any other limitations? Besides cost and latency, remember, re-rankers are downstream of the retrievers. If your retriever has missed a critical document, well, the re-ranker is never going to be able to see it. What about these instruction following re-rankers? Oh, yes, this is brand new, but now we can pass instructions that will prioritize different types of documents differently. Take a look at these examples. So with instructions, I can prioritize certain aspects? Precisely, you can reorder the documents according to your needs. Wow, ranking and prioritization? Don't wait HR find out about this.",
      "platforms": {
        "tiktok": {
          "video_id": "7480980004348775710",
          "url": "https://www.tiktok.com/@rajistics/video/7480980004348775710",
          "view_count": 1908,
          "upload_date": "2025-03-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAAIe9PVAjjcI5lIL4AGOIQGjAfqoA3EygeK3W~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=fI718%2FahlAzbF9WYGRre1Dxdido%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18286165444219068",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "L567-EDNb3I",
          "url": "https://www.youtube.com/watch?v=L567-EDNb3I",
          "view_count": 1048,
          "upload_date": "2025-03-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5976,
      "title": "As language models expand into fuzzier domains like medical advice and policy summarization, traditional training signals break down. This video introduces rubrics—structured, multi-criteria checklists—as a powerful alternative to binary correctness. By breaking complex tasks into evaluable components, rubrics enable models to learn how to reason, revise, and improve—even when there’s no single “right” answer. 📚 References: 1. Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains Anisha Gunjal, Anthony Wang, Elaine Lau et al. arXiv:2507.17746 2. R3: Robust Rubric-Agnostic Reward Models David Anugraha, Zilu Tang, Lester James V. Miranda et al. arXiv:2505.13388 3. Ross Taylor & Nathan Lambert discussion on rubric-based alignment and verification - https://www.interconnects.ai/p/interviewing-ross-taylor-on-the-state Oxen blog post: https://www.oxen.ai/blog/training-a-rust-1-5b-coder-lm-with-reinforcement-learning-grpo",
      "description": "As language models expand into fuzzier domains like medical advice and policy summarization, traditional training signals break down. This video introduces rubrics—structured, multi-criteria checklists—as a powerful alternative to binary correctness. By breaking complex tasks into evaluable components, rubrics enable models to learn how to reason, revise, and improve—even when there’s no single “right” answer. 📚 References: 1. Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains Anisha Gunjal, Anthony Wang, Elaine Lau et al. arXiv:2507.17746 2. R3: Robust Rubric-Agnostic Reward Models David Anugraha, Zilu Tang, Lester James V. Miranda et al. arXiv:2505.13388 3. Ross Taylor & Nathan Lambert discussion on rubric-based alignment and verification - https://www.interconnects.ai/p/interviewing-ross-taylor-on-the-state Oxen blog post: https://www.oxen.ai/blog/training-a-rust-1-5b-coder-lm-with-reinforcement-learning-grpo",
      "upload_date": "2025-08-03",
      "total_views": 2944,
      "max_views": 2639,
      "topics": [
        "domains",
        "fuzzy",
        "learning",
        "like",
        "models",
        "reason",
        "reinforcement",
        "reward",
        "rubric",
        "rubrics",
        "tasks",
        "teaching",
        "training"
      ],
      "search_text": "As language models expand into fuzzier domains like medical advice and policy summarization, traditional training signals break down. This video introduces rubrics—structured, multi-criteria checklists—as a powerful alternative to binary correctness. By breaking complex tasks into evaluable components, rubrics enable models to learn how to reason, revise, and improve—even when there’s no single “right” answer. 📚 References: 1. Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains Anisha Gunjal, Anthony Wang, Elaine Lau et al. arXiv:2507.17746 2. R3: Robust Rubric-Agnostic Reward Models David Anugraha, Zilu Tang, Lester James V. Miranda et al. arXiv:2505.13388 3. Ross Taylor & Nathan Lambert discussion on rubric-based alignment and verification - https://www.interconnects.ai/p/interviewing-ross-taylor-on-the-state Oxen blog post: https://www.oxen.ai/blog/training-a-rust-1-5b-coder-lm-with-reinforcement-learning-grpo domains fuzzy learning like models reason reinforcement reward rubric rubrics tasks teaching training How do we teach AI to handle fuzzy tasks? Like math and code, easy to train. You have an answer. You can check it. What about summarizing something like a medical report? This is where rubrics come in. Researchers at OpenAI and Meta are using them to guide models and domains where correctness isn't binary. What a rubric does is it breaks a complex task into clear interpretable criteria. So take for example, if you needed to summarize a clinical report for patients. Well, here your rubric would have things like key medical facts. Do you have common clear language? Did you avoid unsupported diagnosis? You're going to score each item in the rubric, sometimes with just a yes-no. You're going to aggregate that into a structured reward, use that inside of reinforcement learning to help train the model. Using this method works. It's much better than traditional scalar preference methods. The quality of your rubrics matters. You want to make sure that you have rubrics if you're synthetically generating and have answers to the references, or maybe you're using human ones. The takeaway is rubrics are teaching the models to check their work, reflect, revise. They're not just reward getting the answer. They reward how the model thinks. This is how we're going to teach models to reason.",
      "platforms": {
        "tiktok": {
          "video_id": "7534429417708227871",
          "url": "https://www.tiktok.com/@rajistics/video/7534429417708227871",
          "view_count": 2639,
          "upload_date": "2025-08-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4Of6FryA7jRlDkIIlqEck7eC7EEjovVAEBPIZ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=yHMyOdsoI%2B4ZQlQCuwrFqQfoGc0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18081243451721500",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "yN-0RIergUY",
          "url": "https://www.youtube.com/watch?v=yN-0RIergUY",
          "view_count": 305,
          "upload_date": "2025-08-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6231,
      "title": "Reminder to visualize your data with one of my favorites, Anscombe's quartet",
      "description": "Reminder to visualize your data with one of my favorites, Anscombe's quartet",
      "upload_date": "2024-11-02",
      "total_views": 2941,
      "max_views": 2941,
      "topics": [
        "analyze",
        "anscombe",
        "data",
        "datasets",
        "different",
        "favorites",
        "one",
        "reminder",
        "visualize"
      ],
      "search_text": "Reminder to visualize your data with one of my favorites, Anscombe's quartet analyze anscombe data datasets different favorites one reminder visualize Hey intern, I need you to analyze these datasets and tell me what's different. Yes sir! Alright. Every one of these datasets is different, but when I analyze them and I look at the mean, the standard deviation, even the correlation between variables, they're all the same. I'm not sure what to do. I've been there. Try the plot function. Let me try that. Glad you see it now. It's always important to visualize your data. You can't just trust the numbers alone.",
      "platforms": {
        "tiktok": {
          "video_id": "7432806905719868715",
          "url": "https://www.tiktok.com/@rajistics/video/7432806905719868715",
          "view_count": 2941,
          "upload_date": "2024-11-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6a67eae13f5646969ff39fa9fa829d40_1730585226~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=uF9txe9PHQqkCQ0EPCyTBQunXZM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18049256074987860",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I still havent tried copilot. Have you?  #datascience #codetok #codex #copilot #python",
      "description": "I still havent tried copilot. Have you?  #datascience #codetok #codex #copilot #python",
      "upload_date": "2022-07-27",
      "total_views": 2940,
      "max_views": 2940,
      "topics": [
        "code",
        "codetok",
        "codex",
        "copilot",
        "datascience",
        "python"
      ],
      "search_text": "I still havent tried copilot. Have you?  #datascience #codetok #codex #copilot #python code codetok codex copilot datascience python It works. It makes developers more productive. Code completion technologies like co-pilot, language models like Codex actually help developers become more productive. Google ran a test across over 10,000 Googlers and ran it for several months across different languages and found that these code completion technologies actually improved productivity by 6%. This is a great real-world example of how AI can improve developer productivity. Google also shared that 3% of their code now has been developed by AI completion technologies.",
      "platforms": {
        "tiktok": {
          "video_id": "7125108010501868842",
          "url": "https://www.tiktok.com/@rajistics/video/7125108010501868842",
          "view_count": 2940,
          "upload_date": "2022-07-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1f8909d292e341c1b77e1fc06e22cd01_1658943489~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=h%2F6yyXnRen%2BAwX8cm12s%2Fw1Hy04%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "It’s taken a while to accept this. #python #programming #datascience",
      "description": "It’s taken a while to accept this. #python #programming #datascience",
      "upload_date": "2022-03-28",
      "total_views": 2930,
      "max_views": 2930,
      "topics": [
        "accept",
        "datascience",
        "point",
        "programming",
        "python",
        "taken"
      ],
      "search_text": "It’s taken a while to accept this. #python #programming #datascience accept datascience point programming python taken I'm just at a point in my life where I will not argue with you. You want to tell me that 1 plus 1 equals 5? You're absolutely correct. Enjoy.",
      "platforms": {
        "tiktok": {
          "video_id": "7080282129065577774",
          "url": "https://www.tiktok.com/@rajistics/video/7080282129065577774",
          "view_count": 2930,
          "upload_date": "2022-03-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/313946fb58684b548f2e76659fbf7780_1648506647~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=KENOhEc7Ag618CJSnHrNaglHAAU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @pal_protty  negative reinforcement and #regressiontothemean . Link to the Nylon calculus #basketball article in comments.  #datascience",
      "description": "Reply to @pal_protty  negative reinforcement and #regressiontothemean . Link to the Nylon calculus #basketball article in comments.  #datascience",
      "upload_date": "2022-02-27",
      "total_views": 2926,
      "max_views": 2926,
      "topics": [
        "basketball",
        "better",
        "datascience",
        "regressiontothemean",
        "team",
        "variability"
      ],
      "search_text": "Reply to @pal_protty  negative reinforcement and #regressiontothemean . Link to the Nylon calculus #basketball article in comments.  #datascience basketball better datascience regressiontothemean team variability Bet you can predict what will happen. When it comes to height, there's a lot of natural variability. Take a look at siblings, but variability can play mind tricks on us. Let me explain by using free throw shooting. Your basketball team's gonna have some days where they shoot well and some days where they don't even shoot 50%. And when you don't do well, your coach might yell at you, make you run extra laps. And then next game you do better. But why did you do better? Was it the punishment or something else? Now your coach might have done this all the time and can tell you like 98% of the time, yelling works, the team shoots better after I yell at the team. But this is where I want you to think about it. Is it really the yelling that's making the difference or is this a case of regression to the mean?",
      "platforms": {
        "tiktok": {
          "video_id": "7069496567560047918",
          "url": "https://www.tiktok.com/@rajistics/video/7069496567560047918",
          "view_count": 2926,
          "upload_date": "2022-02-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a082a5697a434d45a73728448252e87c_1645995438~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=o5PEs6o4CDdwYKLvEQHUoxuT2AY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6190,
      "title": "This video explains temperature, one of the most crucial settings when working with GPT models. Temperature controls the randomness of the model's outputs by adjusting the softness of the probability distribution — lower values make results more predictable and deterministic, while higher values encourage more diverse and creative responses. Learn when to tune temperature depending on your task, whether it's classification, summarization, or creative generation.",
      "description": "This video explains temperature, one of the most crucial settings when working with GPT models. Temperature controls the randomness of the model's outputs by adjusting the softness of the probability distribution — lower values make results more predictable and deterministic, while higher values encourage more diverse and creative responses. Learn when to tune temperature depending on your task, whether it's classification, summarization, or creative generation.",
      "upload_date": "2025-03-28",
      "total_views": 2911,
      "max_views": 1918,
      "topics": [
        "creative",
        "distribution",
        "one",
        "results",
        "save",
        "temperature",
        "thinking",
        "time",
        "want",
        "won"
      ],
      "search_text": "This video explains temperature, one of the most crucial settings when working with GPT models. Temperature controls the randomness of the model's outputs by adjusting the softness of the probability distribution — lower values make results more predictable and deterministic, while higher values encourage more diverse and creative responses. Learn when to tune temperature depending on your task, whether it's classification, summarization, or creative generation. creative distribution one results save temperature thinking time want won What's one of the most important settings when using GPT-3? It's temperature. Let's talk about what temperature is and how you should be setting it. For many problems, there's many possible outcomes. Look at this sentence. You could end this in many different ways. And this is what creates a probability distribution of outcomes. What temperature does is control the softness of the distribution. If I turn the temperature down really cold, where nothing's moving and there's no energy in the system, it gets very spiky. We get to in this case one possible outcome. But if we add temperature, if we add energy into the system, add entropy, everything gets excited, everything moves around. All of a sudden, we move from that spiky distribution to now there's lots of possible outcomes that can happen. So use a low temperature when you want more predictable and consistent results, such as if you're doing classification. Another common use task for kind of a low temperature is when you're passing to the LLM, facts that you've generated from some other system and you want them to integrate that into a nice paragraph. You don't want it to be too creative, creating new facts. A low temperature setting helps with that. A high temperature of one is rarely used. The results are often way too predictable. Instead, when you want it to be a little bit more creative, when you want to have different choices there, usually a setting of somewhere between 0.7 to 0.9 is often more appropriate. As you're honing your prompts, take a look at temperature because that's one thing that's going to affect the consistency of results as you repeatedly start using the prompt.",
      "platforms": {
        "tiktok": {
          "video_id": "7486700739771763998",
          "url": "https://www.tiktok.com/@rajistics/video/7486700739771763998",
          "view_count": 1918,
          "upload_date": "2025-03-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocAbGhkmDIDsWm0rAPFAV8RJFCf4nAEEELAeE5~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=8cM3cd%2B8hSvKo%2FEyzZHGKpKMRXc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18075656533691462",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3q_OlbUt-Ao",
          "url": "https://www.youtube.com/watch?v=3q_OlbUt-Ao",
          "view_count": 993,
          "upload_date": "2025-03-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Ensembling is key method in machine learning. This video introduces ensembling through majority voting. #datascience #machinelearning #ensembling #kaggle #majorityvoting ",
      "description": "Ensembling is key method in machine learning. This video introduces ensembling through majority voting. #datascience #machinelearning #ensembling #kaggle #majorityvoting ",
      "upload_date": "2023-03-15",
      "total_views": 2905,
      "max_views": 2905,
      "topics": [
        "accuracy",
        "ensembling",
        "majority",
        "models",
        "three",
        "voting"
      ],
      "search_text": "Ensembling is key method in machine learning. This video introduces ensembling through majority voting. #datascience #machinelearning #ensembling #kaggle #majorityvoting  accuracy ensembling majority models three voting Would you like to make better decisions in life? Well, using majority voting can help your machine learnings improve their accuracy. I'm going to show you that three models that each have an accuracy of 70% can be the single model that has an accuracy of 75%. Boom! We're going to start with the classification problem where all the ground truth is set to one. Now, we're going to use classifiers that have a 70% chance of likelihood. So that means a 70% chance that it's a one. We're using three classifiers here and majority voting. So what we can do now is see if all three classifiers are right, which happens about 34% of the time. Now, two classifiers are right, about 44% of the time, but then they're wrong, around 19% of the time. And finally, there's only a small percent of chance that all three are wrong. And here you can see with majority voting, when we add up those first two categories, now we have a result that's 78% accurate. So we're beating that 75% accurate model. In fact, if we had five pseudo random classifiers, each with 70% accuracy, they'd be correct 83% of the time. A critical factor for good ensembling is having diverse models. Now, if we had models that weren't very diverse, so you can see here we have three different models, but when we ensemble them to majority voting, we only get 80% accuracy. We don't get any improvement because there wasn't diversity. On the other hand, if we look at these three particular models that are much more diverse, the errors are uncorrelated, then the resulting model accuracy improves quite a bit to 98%. This is the power of ensembling.",
      "platforms": {
        "tiktok": {
          "video_id": "7210570590313336107",
          "url": "https://www.tiktok.com/@rajistics/video/7210570590313336107",
          "view_count": 2905,
          "upload_date": "2023-03-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bda6530a0481404eb7c0127ba57a99ab_1678841814~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=g9WztqbwTFbic4E8nqKtG%2Bg6mbo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5970,
      "title": "Most enterprise GenAI pilots fail to deliver measurable ROI because of structural and organizational gaps rather than model quality. The report finds six recurring issues: poor user experience compared to consumer tools, lack of integration into existing workflows, in-house builds that underperform vendor solutions, misaligned ROI targeting “sexy” use cases, absence of executive sponsorship, and inadequate change management. These factors leave most pilots stuck in “demo land,” with only ~5% scaling successfully. 📚 Reference * State of AI in Business 2025 – Project NANDA (Jul 2025) - https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf",
      "description": "Most enterprise GenAI pilots fail to deliver measurable ROI because of structural and organizational gaps rather than model quality. The report finds six recurring issues: poor user experience compared to consumer tools, lack of integration into existing workflows, in-house builds that underperform vendor solutions, misaligned ROI targeting “sexy” use cases, absence of executive sponsorship, and inadequate change management. These factors leave most pilots stuck in “demo land,” with only ~5% scaling successfully. 📚 Reference * State of AI in Business 2025 – Project NANDA (Jul 2025) - https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf",
      "upload_date": "2025-08-22",
      "total_views": 2903,
      "max_views": 1565,
      "topics": [
        "deliver",
        "enterprise",
        "fail",
        "gen",
        "genai",
        "gonna",
        "people",
        "pilots",
        "projects",
        "roi",
        "tool",
        "work"
      ],
      "search_text": "Most enterprise GenAI pilots fail to deliver measurable ROI because of structural and organizational gaps rather than model quality. The report finds six recurring issues: poor user experience compared to consumer tools, lack of integration into existing workflows, in-house builds that underperform vendor solutions, misaligned ROI targeting “sexy” use cases, absence of executive sponsorship, and inadequate change management. These factors leave most pilots stuck in “demo land,” with only ~5% scaling successfully. 📚 Reference * State of AI in Business 2025 – Project NANDA (Jul 2025) - https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf deliver enterprise fail gen genai gonna people pilots projects roi tool work Did you know 95% of Gen AI projects fail? It sounds shocking, but it's not. Your six mistakes laid out in the MIT report, well as I see over and over again. ChatGPT has set the standard. If your fancy enterprise tool is clunky, forgets what you told it, gives you worse answers than ChatGPT, people are gonna drop it instantly. People today live in Salesforce Slack service now. If your Gen AI app is making you cut and paste between these different systems, forget it, feels like extra work, no one wants to use it. Stop having your teams reinvent the wheel. Internal builds of products fail twice as often as just buying. You're not winning innovation points by building it yourself. Sexy demos don't pay the bills. Everyone rushed to go build out chatbots, but the real money back off as automation. That's where they are alive. You have no C-suite sponsor and you're just doing a science project. Without leadership backing, your pilot is never gonna hit production. Even if the tech works, people are gonna resist. You need to train people, redesign processes, so that way your Gen AI tool doesn't just collect dust. The lesson is it's not just about your shiny tool, it's about making it work in the messy reality of how businesses work.",
      "platforms": {
        "tiktok": {
          "video_id": "7541548465445260574",
          "url": "https://www.tiktok.com/@rajistics/video/7541548465445260574",
          "view_count": 1565,
          "upload_date": "2025-08-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocRBUFEWD68f61qenoxZABVo3YIEIJZDDP92EA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=VYyQZ5OpWkgeHGDxdg8qh9dqrBc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18046754180309910",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "fZ9gRNw0kgw",
          "url": "https://www.youtube.com/watch?v=fZ9gRNw0kgw",
          "view_count": 1338,
          "upload_date": "2025-08-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#datascience #analytics #statistics #wald #survivorshipbias",
      "description": "#datascience #analytics #statistics #wald #survivorshipbias",
      "upload_date": "2022-03-10",
      "total_views": 2897,
      "max_views": 2897,
      "topics": [
        "analytics",
        "datascience",
        "planes",
        "statistics",
        "survivorshipbias",
        "wald"
      ],
      "search_text": "#datascience #analytics #statistics #wald #survivorshipbias analytics datascience planes statistics survivorshipbias wald I bet you don't always take this concept into account when you work with data. Imagine your job is keeping military planes flying. Now you have people shooting at these planes, and sometimes the planes go down. So you start plotting out all the locations where your planes have gotten shot. Some of the actual data. So what would you recommend based on the data? Take a minute to think about it. Answer will be in the comments.",
      "platforms": {
        "tiktok": {
          "video_id": "7073588076546706730",
          "url": "https://www.tiktok.com/@rajistics/video/7073588076546706730",
          "view_count": 2897,
          "upload_date": "2022-03-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/cfd91f2e30fb466f9b58fcde5aa9a782_1646948067~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=tXeou7ceOyOteN%2FgTrYEIytAsdM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Amazing how this stuff keeps getting better #datascience #codetok #machinelearning #codex",
      "description": "Amazing how this stuff keeps getting better #datascience #codetok #machinelearning #codex",
      "upload_date": "2022-08-14",
      "total_views": 2890,
      "max_views": 2890,
      "topics": [
        "better",
        "codetok",
        "codex",
        "datascience",
        "machinelearning",
        "math"
      ],
      "search_text": "Amazing how this stuff keeps getting better #datascience #codetok #machinelearning #codex better codetok codex datascience machinelearning math Are you better than 80% in college level math? Well, the latest AI can do that now. If it's a math problem, it uses Codex to turn that into a program which gives it an answer. But not only does it give you the answer, it can also give you an explanation. So this AI is going to get full credit and break that curve. And it might make your work even harder. Go check out the paper for more.",
      "platforms": {
        "tiktok": {
          "video_id": "7131767149646974251",
          "url": "https://www.tiktok.com/@rajistics/video/7131767149646974251",
          "view_count": 2890,
          "upload_date": "2022-08-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c7a8ab63306847cb889ce22385a42b92_1660493941~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=rb6YekKRmVBtmsm7JW3mBQc9ffM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5940,
      "title": "95% of GenAI pilots fail, not because of the model but because of the approach. The simple playbook to actually scale is Usability, Utility, Value, Sustainability, and starting where the cost of a mistake is low. Enterprise transformation 101. Study: https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf",
      "description": "95% of GenAI pilots fail, not because of the model but because of the approach. The simple playbook to actually scale is Usability, Utility, Value, Sustainability, and starting where the cost of a mistake is low. Enterprise transformation 101. Study: https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf",
      "upload_date": "2025-11-16",
      "total_views": 2874,
      "max_views": 1839,
      "topics": [
        "enterprise",
        "fail",
        "gen",
        "genai",
        "model",
        "need",
        "one",
        "pilots",
        "successful",
        "tools",
        "transformation",
        "value"
      ],
      "search_text": "95% of GenAI pilots fail, not because of the model but because of the approach. The simple playbook to actually scale is Usability, Utility, Value, Sustainability, and starting where the cost of a mistake is low. Enterprise transformation 101. Study: https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf enterprise fail gen genai model need one pilots successful tools transformation value 95% of Gen AI projects fail, but honestly, I've seen this pattern across every enterprise tech wave. So here's a simple playbook that helps you end up in the 5% that win. The number one barrier is user resistance. Users have a very high bar from tools. One of the things they're looking for in tools are tools that learn from them, that have adaptive feedback. No one wants to teach the same tool everything over and over again every day. This is the pilot killer when your application is disconnected from the rest of the workflow where people have to copy and paste to use your device. No, you need to have integrations because if it doesn't have integrations, it's going to disintegrate. Huge chunks of Gen AI budgets go to sales and marketing apps because they're visible, but the biggest ROI hides in back office automation, finance, procurement, compliance, chase the business metrics where you have value, not wows. A great pilot means nothing without adoption. You need training, incentives, process, redesign. And to change behavior, you need leadership that's going to help you push through that and own the outcome. My bonus tip is start where the cost of a mistake is low because with Gen AI, mistakes are going to happen. So there you go. Enterprise transformation 101. Usability, utility, value, sustainability. You get these right and your AI solution might have a chance.",
      "platforms": {
        "tiktok": {
          "video_id": "7573433926052498718",
          "url": "https://www.tiktok.com/@rajistics/video/7573433926052498718",
          "view_count": 1035,
          "upload_date": "2025-11-16",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0VJG8jR8QcdEPqEEpjAX0D3FZfDABhfAQZa95~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=2ZQXS6d8dWoUTmaVr7b2Vh67xvU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17915704920220315",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "5J1buIdBBJQ",
          "url": "https://www.youtube.com/watch?v=5J1buIdBBJQ",
          "view_count": 1839,
          "upload_date": "2025-11-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#duet with @the.rachel.woods #rachelwoods Go hustle but don‚Äôt take it personally when they dont respond. Instead wait your time. And then embarrass yourself in public for 10X. üòÄ",
      "description": "#duet with @the.rachel.woods #rachelwoods Go hustle but don‚Äôt take it personally when they dont respond. Instead wait your time. And then embarrass yourself in public for 10X. üòÄ",
      "upload_date": "2023-04-14",
      "total_views": 2858,
      "max_views": 2858,
      "topics": [
        "don",
        "duet",
        "hustle",
        "rachel",
        "rachelwoods",
        "woods"
      ],
      "search_text": "#duet with @the.rachel.woods #rachelwoods Go hustle but don‚Äôt take it personally when they dont respond. Instead wait your time. And then embarrass yourself in public for 10X. üòÄ don duet hustle rachel rachelwoods woods",
      "platforms": {
        "tiktok": {
          "video_id": "7222019052301028650",
          "url": "https://www.tiktok.com/@rajistics/video/7222019052301028650",
          "view_count": 2858,
          "upload_date": "2023-04-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5954,
      "title": "GEPA uses reflection to optimize prompts instead of retraining. In Intrinsic Labs’ OCR study, it analyzed its own extraction errors—like mis-ordered fields or hallucinated totals, plus it rewrote prompts to fix them. The result: +3 pts accuracy and a deterministic “policy prompt” that rediscovered best OCR practices. Intrinsic Labs OCR: https://www.intrinsic-labs.ai/research/ocr-gepa-v1.pdf GEPA Paper: https://arxiv.org/pdf/2507.19457",
      "description": "GEPA uses reflection to optimize prompts instead of retraining. In Intrinsic Labs’ OCR study, it analyzed its own extraction errors—like mis-ordered fields or hallucinated totals, plus it rewrote prompts to fix them. The result: +3 pts accuracy and a deterministic “policy prompt” that rediscovered best OCR practices. Intrinsic Labs OCR: https://www.intrinsic-labs.ai/research/ocr-gepa-v1.pdf GEPA Paper: https://arxiv.org/pdf/2507.19457",
      "upload_date": "2025-10-13",
      "total_views": 2854,
      "max_views": 2854,
      "topics": [
        "gepa",
        "intrinsic",
        "labs",
        "like",
        "ocr",
        "prompts"
      ],
      "search_text": "GEPA uses reflection to optimize prompts instead of retraining. In Intrinsic Labs’ OCR study, it analyzed its own extraction errors—like mis-ordered fields or hallucinated totals, plus it rewrote prompts to fix them. The result: +3 pts accuracy and a deterministic “policy prompt” that rediscovered best OCR practices. Intrinsic Labs OCR: https://www.intrinsic-labs.ai/research/ocr-gepa-v1.pdf GEPA Paper: https://arxiv.org/pdf/2507.19457 gepa intrinsic labs like ocr prompts Can you believe this? AI that improves itself. I just boosted our OCR performance by 3%. Have you been going down the X rabbit hole again? Models just don't get better by themselves. It's not magic. I'm using Chepa here, which stands for genetic and perrepto optimization. So what's happening is we run the pipeline, but then we use an AI, in this case GPT-5, to look at what's failed, and then it uses that to come up with ideas to rewrite our prompts. Ah, genetic, like an evolutionary loop where it runs, it reflects, it rewrites, it repeats itself? Exactly. It's another prompt optimizer that's built into DSPY. So here what happens is we run the OCR job, it compares the output against the ground truth JSON, looks over every feels, what's been added, what's been deleted, what's been modified. If it says something that doesn't reconcile or make sense, then rewrites the prompt with some fixes. Okay, I mean, I guess it makes sense for OCR, since it's a very structured job, where you've got a schema, you're measuring some things, and kind of very repeatable. That's right. The reflection model here rediscovered best practices that OCR engineers use, like preserving reading order, not to invent new values, like marking any illegible text as unreadable, parsing items before figuring out the totals. And so what we have here with Chepa is we basically have written a deterministic policy for prompting for reliable extractions. Nice work. So if it's going to write its prompts by itself, what exactly do you do? Oh, I'm going to supervise how it reflects.",
      "platforms": {
        "tiktok": {
          "video_id": "7560827080288537887",
          "url": "https://www.tiktok.com/@rajistics/video/7560827080288537887",
          "view_count": 2854,
          "upload_date": "2025-10-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocQPAiOA1kBRDtjO0aSiIlJEAipdDpBEVDAAz~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=i9ULC2huLSz%2BidfH0IBDZgBCJ8k%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18086263648820020",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6208,
      "title": "LLMs can streamline existing AI/ML operations by replacing specialized models (e.g., BERT for classification, SpaCy for NER, T5 for summarization) with a single foundation model that handles multiple NLP tasks through prompt engineering, reducing both infrastructure complexity and maintenance overhead. The consolidation eliminates the need for task-specific data preprocessing pipelines, model retraining cycles, and separate deployment environments, while also potentially replacing costly external API services for tasks like translation and data annotation - though with the caveat that human oversight remains crucial for quality assurance and edge cases.",
      "description": "LLMs can streamline existing AI/ML operations by replacing specialized models (e.g., BERT for classification, SpaCy for NER, T5 for summarization) with a single foundation model that handles multiple NLP tasks through prompt engineering, reducing both infrastructure complexity and maintenance overhead. The consolidation eliminates the need for task-specific data preprocessing pipelines, model retraining cycles, and separate deployment environments, while also potentially replacing costly external API services for tasks like translation and data annotation - though with the caveat that human oversight remains crucial for quality assurance and edge cases.",
      "upload_date": "2025-02-22",
      "total_views": 2852,
      "max_views": 1828,
      "topics": [
        "around",
        "comes",
        "data",
        "human",
        "like",
        "limits",
        "llms",
        "models",
        "pipelines",
        "replacing",
        "science",
        "traditional"
      ],
      "search_text": "LLMs can streamline existing AI/ML operations by replacing specialized models (e.g., BERT for classification, SpaCy for NER, T5 for summarization) with a single foundation model that handles multiple NLP tasks through prompt engineering, reducing both infrastructure complexity and maintenance overhead. The consolidation eliminates the need for task-specific data preprocessing pipelines, model retraining cycles, and separate deployment environments, while also potentially replacing costly external API services for tasks like translation and data annotation - though with the caveat that human oversight remains crucial for quality assurance and edge cases. around comes data human like limits llms models pipelines replacing science traditional We just signed an exclusive contract for AGI LM. Just imagine all the AI problems your team's gonna be able to do now. New things, nah, let's use this as a way to down size some of our existing pipelines and contracts. Replacing, oh, that doesn't sound right. I want your team to do new things, AGI. Let's start by replacing about a dozen of our different NLP pipelines around classification, summarization, extraction. All we have to do now is prompt, easy peasy. How much money does that save? It's a little higher compute cost, but we save a ton of time around data prep, building and training those models and maintaining those pipelines. Beautiful, sell me more. We'll also be able to reduce our use of Google translate and human translators by just taking advantage of our models because after all, our customers don't expect perfection. Less overhead, fewer people, music to my ears, sell me more. Well, let's not get too carried away. We still need a little bit of human oversight, but some things like data annotation, we're gonna be able to use AI now for creating synthetic data, saving us a ton. This AI is wonderful. Now, can you do me one thing? Can you take care of all those stinky developers? I'm tired of seeing Birkenstocks everywhere. Well, LLMs are allowing our developers to become more productive. About 20% of our code now in our code base is generated from AI. So I think you'll be seeing a lot less developers. Wow, this is incredible. Why didn't you just do this when we hired you five years ago? But anyways, what's next? Well, AI is getting better at persuasion, setting unrealistic goals, and overusing jargon like synergy and circle back around. So I think it's coming for your job next.",
      "platforms": {
        "tiktok": {
          "video_id": "7474358866361339166",
          "url": "https://www.tiktok.com/@rajistics/video/7474358866361339166",
          "view_count": 1828,
          "upload_date": "2025-02-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMldqQDQGCINICgaAIefAdDcqLDe0PIIorijL0~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=UIf0i4GF91nu%2F75q76TlCY%2Bc%2Fpw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18291506485300721",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "zuBOe8fohfE",
          "url": "https://www.youtube.com/watch?v=zuBOe8fohfE",
          "view_count": 1024,
          "upload_date": "2025-02-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Text to video models including text2video. The models are grtting better and there is now a place over at the hugging face hub to find them. #datascience #machinelearning #text2video #stablediffusion #huggingface ",
      "description": "Text to video models including text2video. The models are grtting better and there is now a place over at the hugging face hub to find them. #datascience #machinelearning #text2video #stablediffusion #huggingface ",
      "upload_date": "2023-03-26",
      "total_views": 2843,
      "max_views": 2843,
      "topics": [
        "datascience",
        "machinelearning",
        "models",
        "text",
        "text2video",
        "video"
      ],
      "search_text": "Text to video models including text2video. The models are grtting better and there is now a place over at the hugging face hub to find them. #datascience #machinelearning #text2video #stablediffusion #huggingface  datascience machinelearning models text text2video video Text to video is getting bigger. Let's talk about the latest models and where you can get them. One of the latest models is text to video zero. You can see a demonstration of how this model works over here. This method starts by using existing text to image methods. Remember stable diffusion? Well that's the starting point for this particular model. Then what it does is includes better ways for integrating and combining those images that takes into account motion and time. The results look really cool. Now this model is going to have a paper and they're going to release the code soon. But if you want to find any other text to video model, check it over at the Hugging Face Hub where we've just added a new section that allows you to identify and find out all the current open source text to video models.",
      "platforms": {
        "tiktok": {
          "video_id": "7214976046389759278",
          "url": "https://www.tiktok.com/@rajistics/video/7214976046389759278",
          "view_count": 2843,
          "upload_date": "2023-03-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/19be689688f84db18985a47b58052321_1679867531~tplv-tiktokx-origin.image?dr=9636&x-expires=1767466800&x-signature=NAHFuccmCweLesh6j2QfQxpKQgc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @urdar635 watermarking output from AI models is something that is being considered. It’s done by adding some “signal” to the output of the model. The outputs are probabilities of tokens, so it’s possible to slightly modify them in a way that wouldn’t be detectable when reading the output. See aaronson’s blog for more. #datascience #machinelearning #openai #chatgpt #watermarking",
      "description": "Replying to @urdar635 watermarking output from AI models is something that is being considered. It’s done by adding some “signal” to the output of the model. The outputs are probabilities of tokens, so it’s possible to slightly modify them in a way that wouldn’t be detectable when reading the output. See aaronson’s blog for more. #datascience #machinelearning #openai #chatgpt #watermarking",
      "upload_date": "2022-12-09",
      "total_views": 2840,
      "max_views": 2840,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "openai",
        "output",
        "watermarking"
      ],
      "search_text": "Replying to @urdar635 watermarking output from AI models is something that is being considered. It’s done by adding some “signal” to the output of the model. The outputs are probabilities of tokens, so it’s possible to slightly modify them in a way that wouldn’t be detectable when reading the output. See aaronson’s blog for more. #datascience #machinelearning #openai #chatgpt #watermarking chatgpt datascience machinelearning openai output watermarking You know how dogs sniff each other's butts to say hi? Imagine if AI could do that. With watermarking you could identify if something was written by an AI. So think about high school teachers being able to identify tests, but it's also useful if AI is trying to learn from itself because it can figure out which content has been created by AI versus humans. God of Open AI did an interview where he talked about adding watermarking to the output of models and in that way you could identify which output came from which models. There's many ways of doing this. One way that Scott mentioned is adding a little bit of, let's say, signal that's only known to a cryptographic function and adding that piece and then that would allow you to be able to scan any text and identify did it come from the AI. And it should even be robust enough that if people modified a few words, if it was over a large enough span, you wouldn't be able to, you'd still be able to detect that it's from an AI. So cool idea, people are thinking about it and plenty of work on this.",
      "platforms": {
        "tiktok": {
          "video_id": "7174960136958725422",
          "url": "https://www.tiktok.com/@rajistics/video/7174960136958725422",
          "view_count": 2840,
          "upload_date": "2022-12-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/df2dee8413224fd7a4d5b34d4b28e86e_1670550599~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=kgZdg3TeAkn4LiP8cjlas1pKoI4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Feature Selection Deep Dive, Notebook at: https://bit.ly/raj_fs ",
      "description": "Feature Selection Deep Dive, Notebook at: https://bit.ly/raj_fs ",
      "upload_date": "2024-10-13",
      "total_views": 2836,
      "max_views": 2836,
      "topics": [
        "feature",
        "features",
        "going",
        "see",
        "selection",
        "use"
      ],
      "search_text": "Feature Selection Deep Dive, Notebook at: https://bit.ly/raj_fs  feature features going see selection use Feature selection is a critical part of the machine learning life cycle. It's where we go from lots of different features to select the most important and useful ones for our models. In today's video, I'm going to share a few of my favorite techniques, show you how to plot feature selection curves, that show you how a model's accuracy changes depending on the number of features you have, and also share how to use some of the feature elimination techniques like recursive feature elimination. First, let's talk about where feature selection fits in in the machine learning life cycle. Often we start with lots of data, and we might start to engineer some features that we're going to use. Now, as we start building out of these features, we're going to find that we don't need them all. Some of them are going to carry a lot more signaling in the modeling process. So there is an interplay between how we engineer our features, the smarts we use to pull information out, which features I pass into my final model, and then how we evaluate the model. And all of those are tightly coupled into a loop. Although I'm talking about feature selection today, it's very close to evaluation of models to feature engineering as well. So just because I'm distinguishing today, don't make it think that it's entirely different. I've put together this notebook that puts a lot of feature selection methods. I want to first acknowledge that a lot of this notebook came from some other folks work. I use that as my starting point. I've went ahead and coded it down here on this earlier feature selection work. Now, based on that, I've went ahead and supplemented it. Let me show you how. The original notebook really heavily focused on the MNIST dataset. MNIST is great. It's a very visual dataset. It's widely used. However, I'm not going to, in this video, focus on the MNIST dataset simply because it's very useful, but it doesn't represent what a lot of us see in everyday life. The other thing is I like to use a dataset where I can cheat a little bit and know exactly where the signal is. So my preference here is to use the Madeleine dataset. There's a Python function, MakeClassification, that you can use that creates this. This is a synthetic dataset. So you can tell it which features are informative, which features are redundant, and then add a bunch of noise. This helps us when we work on feature selection techniques because now we know where the signal is. I've done a lot of experiment with feature selection. I often use this as a starting point. I've even modified this, added, for example, my own categoricals, other types of information, other loosely correlated information. Lots of different techniques I've used along the way to generate these synthetic datasets as a way to test different types of feature selection methods. So that's why I'm going to start with this one here. And we're going to go ahead and create this dataset of 40 features where only a handful of them are informative. You can see the five informative and then three redundant that I have. This is what the dataset looks like. It's all numeric. I'm going to keep it simple here for the first part. Now that we have all these features, we can start using feature selection methods. The first couple of feature selection methods here are Univariate. They look at the variable by itself. Typically, I only use Univariate in cases where I have a huge amount of data, a huge amount of features. I'm just trying to parse down to something slightly manageable. If I start with 100,000, let me parse this down to 1000 with something like a Univariate approach. So while I have these in the notebook, I will tell you, I don't really use them very often in day-to-day practice. Now the next one I use where we're starting to use a multivariate approach is logistic regression. It's a classic starting point for a lot of machine learning projects where you can quickly build a baseline model. And here I'm going to use the coefficients as a way to identify which features are important. As you'll see, we're going to compare all these methods in a little while so you can see which ones do better. But this is a easy starting point like that. Now, you can refine this. A favorite of mine when I have really wide data sets, not a lot of rows, is to use LASSO or L1 regularization as a feature selection technique. I've used this many times before. It works really well on, for example, biological data that's very wide. What L1 regularization is going to do is it's going to take the coefficients and the ones that aren't adding signal. It's going to send those coefficients down to zero. So for example, here you'll see when I run this, I've went ahead and said, don't show me any of the features where the coefficients are not equal to zero. And now I've only got a handful of selected features left out of this. So it's gone from those 40 features, said, hey, these are the features that we think add signal to it. It runs very quickly, very efficient, widely used in the literature around this. The next feature selection method is using feature importance as a way to do feature selection. Now, you can use this approach with a variety of different types of algorithms. But when you're computing feature importance, my suggestion for my time and explainability is to use a permutation based feature importance. That will then give you a ranking of all the features and which are the most important features. This is probably one of the most widely used techniques out there for doing feature selection to figure out which features are useful, which ones are noisy according to this. So you'll see here, we can quickly come this, get a ranked list of which features are important. And if you're paying attention to the numbers, you'll see, right, big numbers right at the front here, which are those features that we already know are going to be more informative. Rather than these noisy features that we've added at the end. So you can see right away that the light GBM is on on par here. Next is Baruda. Baruda is widely known in the feature selection literature. It takes an enormous amount of compute because basically it's creating and checking every, every feature against all the other feature combinations. So it's super compute intensive. I'm not even going to run it here because I think it takes at least an hour on this type of data set to do. So I've used it before. It's an excellent technique, but does take a lot of compute time. So especially don't try to run this on the that MNIST data set that's in this notebook. It'll take forever like this. So think about scaling this out. If you want to use it, good way to again experiment with this is use it with a smaller data set. They where you know the signal, you'll get more confidence in using a technique like this. So minimum redundancy, maximum relevance is another feature selection method. I personally don't have as much experience with this where this comes into play is where you want a diverse set of features. So often if you use some techniques like feature importance, you might get all the features might all relate to one type of characteristic. But what you want is you actually know that in the field you want a diverse set of characteristics that your features represent. So using something like the MRMR can give you a diverse set of features like that. So again, play around with this. Try this out as well. So now that I've built these out, I'm going to go ahead and create a data frame that has all these results. We can write that out to a file. But what I really want to do is evaluate and show you how these work. So let's go ahead and now I'm going to take all those features, those rankings that I've done. And I'm going to build a visualization to show you how the number of features relates to the accuracy of the model. I'm building this with this loop. You can see here, I'm controlling what are the intervals that we're going to check the number of features 40, 30, 20, like that. You can modify this for your own. And then we're building for each one of these a cat boost classifier. So we used a totally different algorithm than everywhere else. And we're going to build and see what the accuracy is for this. So once this finishes building, you'll see here we've almost got it done. I can go ahead and give you the visualization. This is a powerful visualization, something you should really get familiar with. And there's some general tendencies I want to show you in this. First of all, you'll see at we've built this from starting with all the features. We can see what the performance of the models is with all the features. But as we reduce the number of features, we're getting rid of some of that extra noise. Remember, we know where the signal is. We're getting rid of some of that noise. You see the performance of the model goes up because of that. On the other hand, if we go too far and we remove too many of those features, that's some of the signal, that's some of the things that's giving us our accuracy. Then the model accuracy drops. This type of curve I've seen all the time in real world data sets, which is why I wanted you to see this with the synthetic data set. So you get a sense of, hey, it's okay to remove some features, but let's just measure it as we do this. So we have some idea if our accuracy is going up or are we getting to the point where if we start removing features, our accuracy is going down. So once you understand this fundamental way, you're going to have a good innate way of starting to think about doing feature selection. So now that I've got you thinking about feature selection, let's talk about how we eliminate or reduce features. We've already been talking about this a little bit. Well, how about automatically removing and eliminating features that aren't adding signal? And what we can do this is we can do this in an iterative way. And if we take a look at what I'm running here, this is in Scikit. I'm running recursive feature elimination, where I'm going to tell the model, hey, don't select any more than seven features. And I want you to build a model, look at the feature importance, remove one, build another model with that reduced set of features, see which one is the least important, remove that. And I can control this with this step interval here. So this gives me an effective way to identify what are the best set of features. And when I do that, you can see here, I get a model with a pretty good accuracy, 70.71. That's on par. Remember, this is only going to give us the top seven features using this approach like that. And since again, I have what is the perfect solution for this, I can compare this with what is the ideal solution for this data set. And you can see right here, we're very, very close to the ideal. So this approach pretty much was able to find all the signal in this data set. Of course, in real life, you could make the data sets a little more complicated, but just to help us gives us some baseline for understanding how this works. Now, once you get the idea of recursive feature elimination, if you have access to more compute, more algorithms, well, you can go crazy with this. This is what I did at Data Robot. We did something where what we would do is aggregate feature importance across a number of different models. So not just one model like we did with light GBM, let's look at a couple of different models. Then we get a lot of variation here inside of this. Now, I'm going to use this new aggregated way and remove the least important features that way. And so this was a technique that we built at Data Robot that had the AutoML, had the algorithms to do that. We called it feature important rank and assembling. It worked extremely well. Again, this takes a lot of compute. But in our tests, that feature selection method worked really well. So experiment with yourself if you need to be able to do something like this. Finally, when I was putting all this together, I saw some other great solutions such as I need to change the name of this feature. Viz that's out there that looked like a promising solution. I had trouble installing it, but it looked like it automatically did some feature selection, helped you also think about correlated features as well. Finally, the Kaggle community is a wonderful place to learn about these techniques, to use these techniques. An early competition they had was the Santander customer satisfaction. You'll see lots of feature selection blogs related to that data set and resources, because it was a very good data set to use for these types of things. I've linked in two other blog posts from Kaggle that I found very useful when thinking through feature selection. So hopefully those will give you more food for thought as well. Thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7425267647622729003",
          "url": "https://www.tiktok.com/@rajistics/video/7425267647622729003",
          "view_count": 2836,
          "upload_date": "2024-10-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8f0568c89f744ed59e453bbaf965e7a0_1728829853~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=YYFHMAOZm5vmtrc%2BAx%2FF%2Bone3L4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6170,
      "title": "Let’s talk about common challenges in human annotation for AI training data, particularly around ambiguous label definitions and inconsistent annotator agreement. It introduces best practices like creating gold standard datasets, using partial overlap to measure inter-annotator agreement (IAA), and maintaining clear annotation guidelines. The skit also subtly references model failures, like GPT-4o's sycophancy, to emphasize how annotation errors can propagate into downstream AI behavior.",
      "description": "Let’s talk about common challenges in human annotation for AI training data, particularly around ambiguous label definitions and inconsistent annotator agreement. It introduces best practices like creating gold standard datasets, using partial overlap to measure inter-annotator agreement (IAA), and maintaining clear annotation guidelines. The skit also subtly references model failures, like GPT-4o's sycophancy, to emphasize how annotation errors can propagate into downstream AI behavior.",
      "upload_date": "2025-05-05",
      "total_views": 2815,
      "max_views": 1639,
      "topics": [
        "agreement",
        "annotation",
        "annotator",
        "around",
        "best",
        "definitions",
        "don",
        "label",
        "practices"
      ],
      "search_text": "Let’s talk about common challenges in human annotation for AI training data, particularly around ambiguous label definitions and inconsistent annotator agreement. It introduces best practices like creating gold standard datasets, using partial overlap to measure inter-annotator agreement (IAA), and maintaining clear annotation guidelines. The skit also subtly references model failures, like GPT-4o's sycophancy, to emphasize how annotation errors can propagate into downstream AI behavior. agreement annotation annotator around best definitions don label practices How am I supposed to label this? Is this a compliment or a complaint? Don't I know it. I had this. I ended up labeling it as other. That tells me our label definitions need work. Complaint involves a problem needing an action, while feedback is an opinion or a suggestion with no follow-up needed. We'll go ahead and build a gold standard with some clear examples around this, as well as the edge case. Once that's ready, can we review it? Absolutely. We'll update our annotation guide and run a quick alignment session to make sure everyone's on board. Hold on. I just got the same message that Alex got. Why would that happen? It's part of our setup. It's called partial overlap. So we send the same samples to multiple annotators to help catch disagreements and refine our guidelines. So how are we doing? You're at 65% agreement. We aim for over 75. Below 60? That's chaos. What if we don't fix it? We don't want to teach the model contradictory messages. Instead, we want a good consistent message. Yeah, kind of how we had to pass that quiz before we were allowed to start labeling. Exactly. It's all about agreement, except pay rates. It's every annotator for themselves.",
      "platforms": {
        "tiktok": {
          "video_id": "7500783030148091167",
          "url": "https://www.tiktok.com/@rajistics/video/7500783030148091167",
          "view_count": 1639,
          "upload_date": "2025-05-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8yYIB6t0EKACAi2IBDiyAFiIsBAwMAJBAf9or~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=72YR6X8S%2F8dqldR3mz%2BnGym91F0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17972615675853436",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "hCDCMRpVTko",
          "url": "https://www.youtube.com/watch?v=hCDCMRpVTko",
          "view_count": 1176,
          "upload_date": "2025-05-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @grahamkechnie #wastewater #cornovirus #analysis #cryptic",
      "description": "Reply to @grahamkechnie #wastewater #cornovirus #analysis #cryptic",
      "upload_date": "2022-02-03",
      "total_views": 2803,
      "max_views": 2803,
      "topics": [
        "analysis",
        "cornovirus",
        "cryptic",
        "grahamkechnie",
        "reply",
        "wastewater"
      ],
      "search_text": "Reply to @grahamkechnie #wastewater #cornovirus #analysis #cryptic analysis cornovirus cryptic grahamkechnie reply wastewater",
      "platforms": {
        "tiktok": {
          "video_id": "7060636520604093743",
          "url": "https://www.tiktok.com/@rajistics/video/7060636520604093743",
          "view_count": 2803,
          "upload_date": "2022-02-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Big data bowl submissions are going in and lots of great sports analytic work. This one is on strain for evaluating pass rushers. #datascience #statistics #bigdatabowl #nfl",
      "description": "Big data bowl submissions are going in and lots of great sports analytic work. This one is on strain for evaluating pass rushers. #datascience #statistics #bigdatabowl #nfl",
      "upload_date": "2023-01-12",
      "total_views": 2797,
      "max_views": 2797,
      "topics": [
        "bigdatabowl",
        "datascience",
        "defensive",
        "nfl",
        "statistics",
        "strain"
      ],
      "search_text": "Big data bowl submissions are going in and lots of great sports analytic work. This one is on strain for evaluating pass rushers. #datascience #statistics #bigdatabowl #nfl bigdatabowl datascience defensive nfl statistics strain Did you know physics could help us understand football? So these folks were trying to understand how to model past blocking in the NFL. If you think blockers as draggers on defensive ends that are trying to get to the quarterback, what we can do is model them. They quantify this using physics equations on strain. And you can see in this visualization here as the blocker gets freed up and starts accelerating through, the strain value goes up. They then apply this across lots of different players, got some ideas for who the best defensive ends are, who the best defensive linemen are at resisting those defensive ends.",
      "platforms": {
        "tiktok": {
          "video_id": "7187812426488876330",
          "url": "https://www.tiktok.com/@rajistics/video/7187812426488876330",
          "view_count": 2797,
          "upload_date": "2023-01-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6f3b7e05ca4544f89291606c4447f33e_1673543008~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=Kyl2MuLRQ%2FTBQOu3mYbRuRnS2PA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6082,
      "title": "How to Build a State-of-the-Art Retrieval System? Lessons from Kaggle's Top Solution 👀 Let's look at the winning solution from Raja Biswas in the Eedi Mathematics Misconception Mining competition. 0:00 - Introduction to Competition 1:18 - First place solution 1:53 - Mixture of Retrievers 3:33 - Cascaded Rerankers 5:30 - Synthetic Data 6:40 - LLM as Judge 7:15 - Chain of Thought Training Data 8:48 - Quantization 9:30 - Validation 9:53 - Takeaways & References 🚀 Built a cascading system that processes top 64 candidates through multiple retrievers and rerankers (pointwise and listwise) 💪 Outperforms single-model approaches 💪 Fine tunes retrievers and rerankers using LoRA 🛡️ Used GPT-4o as judge for data quality control and curation 📊 Trained on 11.8K examples (10K synthetic + 1.8K competition data) ⚙️ Inference optimization through quantization with calibration datasets ✅ Used Chain of Thought reasoning as training data for better ranking accuracy ➡️ Achieved state-of-the-art results while maintaining computational efficiency Links: 1st Place solution from Raja Biswas: https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688 Training code: https://github.com/rbiswasfc/eedi-mining-misconceptions Metaprompt from Anthropic: https://github.com/anthropics/anthropic-cookbook/blob/main/misc/metaprompt.ipynb MTEB: Massive Text Embedding Benchmark - https://huggingface.co/spaces/mteb/leaderboard",
      "description": "How to Build a State-of-the-Art Retrieval System? Lessons from Kaggle's Top Solution 👀 Let's look at the winning solution from Raja Biswas in the Eedi Mathematics Misconception Mining competition. 0:00 - Introduction to Competition 1:18 - First place solution 1:53 - Mixture of Retrievers 3:33 - Cascaded Rerankers 5:30 - Synthetic Data 6:40 - LLM as Judge 7:15 - Chain of Thought Training Data 8:48 - Quantization 9:30 - Validation 9:53 - Takeaways & References 🚀 Built a cascading system that processes top 64 candidates through multiple retrievers and rerankers (pointwise and listwise) 💪 Outperforms single-model approaches 💪 Fine tunes retrievers and rerankers using LoRA 🛡️ Used GPT-4o as judge for data quality control and curation 📊 Trained on 11.8K examples (10K synthetic + 1.8K competition data) ⚙️ Inference optimization through quantization with calibration datasets ✅ Used Chain of Thought reasoning as training data for better ranking accuracy ➡️ Achieved state-of-the-art results while maintaining computational efficiency Links: 1st Place solution from Raja Biswas: https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688 Training code: https://github.com/rbiswasfc/eedi-mining-misconceptions Metaprompt from Anthropic: https://github.com/anthropics/anthropic-cookbook/blob/main/misc/metaprompt.ipynb MTEB: Massive Text Embedding Benchmark - https://huggingface.co/spaces/mteb/leaderboard",
      "upload_date": "2025-01-04",
      "total_views": 2791,
      "max_views": 1708,
      "topics": [
        "agent",
        "billion",
        "data",
        "explained",
        "going",
        "holistic",
        "laws",
        "leaderboard",
        "model",
        "models",
        "quickly",
        "retrievers",
        "scaling",
        "synthetic",
        "use",
        "used"
      ],
      "search_text": "How to Build a State-of-the-Art Retrieval System? Lessons from Kaggle's Top Solution 👀 Let's look at the winning solution from Raja Biswas in the Eedi Mathematics Misconception Mining competition. 0:00 - Introduction to Competition 1:18 - First place solution 1:53 - Mixture of Retrievers 3:33 - Cascaded Rerankers 5:30 - Synthetic Data 6:40 - LLM as Judge 7:15 - Chain of Thought Training Data 8:48 - Quantization 9:30 - Validation 9:53 - Takeaways & References 🚀 Built a cascading system that processes top 64 candidates through multiple retrievers and rerankers (pointwise and listwise) 💪 Outperforms single-model approaches 💪 Fine tunes retrievers and rerankers using LoRA 🛡️ Used GPT-4o as judge for data quality control and curation 📊 Trained on 11.8K examples (10K synthetic + 1.8K competition data) ⚙️ Inference optimization through quantization with calibration datasets ✅ Used Chain of Thought reasoning as training data for better ranking accuracy ➡️ Achieved state-of-the-art results while maintaining computational efficiency Links: 1st Place solution from Raja Biswas: https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688 Training code: https://github.com/rbiswasfc/eedi-mining-misconceptions Metaprompt from Anthropic: https://github.com/anthropics/anthropic-cookbook/blob/main/misc/metaprompt.ipynb MTEB: Massive Text Embedding Benchmark - https://huggingface.co/spaces/mteb/leaderboard agent billion data explained going holistic laws leaderboard model models quickly retrievers scaling synthetic use used Let's dive into some of the winning solutions for a recent Kaggle competition. This Kaggle competition was about retrieval using Gen AI technologies. And while everybody's use case is going to differ a little bit, I think some of the patterns that they found, some of the techniques they found will be useful to anybody that's working on retrieval rag type use cases like that. Now the competition itself, I'm not going to go into the specifics of the competition. It's a pretty kind of detailed competition around looking at misconceptions in mathematics. There's lots of details here. What I want to point out is that there's thousands of people working on this and they've shared their solutions, their insights for what works. And I find these reads fascinating because what these people have done is they've spent the last few months working, running hundreds of experiments to figure out what retrieval, what tools work the best to solve this problem. Now this doesn't always help you with your problem, but a lot of times in my experience, what Kaggle competitors learn, the tricks they use, the techniques, the experimentation, often we can reuse a lot of that in our own research and our own problems we do. Now for the first place solution in this competition, I wrote an extremely detailed and excellently presented summary. So I'm going to refer you to that. I'm going to share with you my distilled thoughts from that summary. I've thrown this together in terms of going through what they've wrote and how it could be useful to all of us and put it into this visualization. So I'm going to use this as a little bit of a roadmap and will refer to the blog post as well and go through this. So this was a retrieval use case. And so the first thing is used a mixture of retrievers. So often we think about picking maybe one retriever going to the multi-text embedding leaderboard, picking out one of the top retrievers here. Like a lot of Kaggle competitions, they found that ensembling, right, using multiple models, gives you a little bit of boost of performance. And you can see this in, among other competitors, it might have just used one model. Or as is the case here, they also fine-tune the model as well, that gave it a boost. And if you look through the solutions, you'll see that most of these techniques gave some improvement here. Now, the retrievers they used here are the ones specified here. You'll see they're kind of on the bigger side of retrievers. They fine-tune these with Laura. Now, the way these retrievers worked is pretty interesting. What they did is, and it's detailed here in the retriever section. But what they did was they used the retrievers to get the top 32 results. Now, after that, what they said is, hey, if any of the results are within something like .06 of the number one, so if you have other results that are really similar to the top result, well, let's bring those in as well up to a max of 64. And this is often what you want to use retrievers for, just going from lots of different data that you have down to a smaller subset. And that's what they did here. And these retrievers will bring anything back to 62 from 32 to 64 results. Now, once the retrievers have that, another very popular technique for people to use is a re-ranker. Now, a re-ranker is going to go with that big list that we have that 64 and re-rank those to tell us which is the most important. Re-rankers are typically more heavyweight models. They take a lot more compute to run. So it's something that it's too much to run across our entire data set. But if we only have a smaller subset, they can do much better in terms of giving us a final ranking than just relying on retrievers. So you'll see throughout the space, lots of folks using rankers. What's interesting here is the strategy that they used for rankers, where they used three different rankers of three different sizes and cascaded them, where the first ranker was a Quin 14B. And this was used point-wise re-ranker. So it just looked at each individual item and ranked them to figure out which ones were in the top eight. After that, the next ranker was a a little bit bigger capacity and it got to the top five. Now, the last re-ranker was not a point-wise but a list-wise re-ranker. So it takes an entire list of the top five and then tells you which order they should be in. So there's actually five outputs that come out of this re-ranker model. Now, if you're curious on what is the performance improvement out of these re-rankers, they've shared that here in the diagram. You can see a similar cascade thing. And we can see here the steady improvement by using these multiple re-rankers. Now, most of us aren't going to use multiple re-rankers in this, but I just want to highlight this to you to show you how using multiple models going through careful experimentation, you can find ways to pick up a little bit of extra lift in your models. Now, I mentioned earlier that these models were fine-tuned. So they were fine-tuned with the training data that came, but they also generated synthetic data as well. And they generated about 10,000 examples of synthetic compared to the original 2K of training data. Now, as they go through here, the synthetic data that they created, they initially started with simple prompt. They found out that that synthetic data just wasn't covering everything well enough. They did some clustering analysis, figured out what are the groups that we need to have covered in our synthetic data to have good coverage to cover all the types of use cases we have. And then they wrote a better prompt. One of the things they wrote here is they used Anthropics Metaprompt guide to help write a better prompt. I'll link that and share that too. That gave them a much better detailed prompt, and it started to allow them to hit that synthetic data in those areas that they needed it, so that their synthetic data was much better than where they started with. Now, with that synthetic data that they pulled in and they've shared the prompts here with it, the next step was, well, you have a lot of synthetic data, which of the synthetic data should you use? And this is where they used an LLM as a judge to review all that synthetic data to see, hey, let's keep the high quality synthetic data, let's drop the low quality data. This again, this is an easy technique that any of us can use. Most of us should be using synthetic data for thinking about fine tuning models. There's a lot of details in here about how they fine tuned with Laura and different learning rates. I don't want to dwell on that. One interesting technique they used here to enrich in their training data was they used chain of thought data as well, additional training data. So they asked a Claude model, they took the examples, they asked a Claude model, hey, give me a detailed reasoning for how you would solve this problem. And then they took all those reasoning pieces and stuck them in to the training data for that re-ranker model. And what that does is it helps it helps it helps when they're doing predictions to see, hey, yes, we might not have all the pieces, but does this, does it make sense? Is it following the same type of logic that we've seen in other places that actually gave a little bit of a lift by using this extra chain of thought data? Then they share in here in the paper, you can see exactly how much lift they got through their through by using the extra chain of thought. But that just shows you some of the smart ways that you can improve your synthetic data is by adding other original pieces in there as well. You can see here they've shared the performance of their individual retrievers. They've also shared the different types of re-rankers. They have their list wise re-rankers as well as the point wise re-ranker as well. Here's the list. Oh, that was the point wise. Here is the list one here. And you can see here that they've brought in those chain of thought examples as well to use with this. All right. The last couple things I want to leave you with is fundamental techniques that we always need to be thinking about. They used quantization. There's a lot of quantization libraries out there. I saw different competitors use some different ones out there. Here they use the auto AWG. One thing they highlight and remember for us to take into consideration is when you're using quantization you're going to have a little bit of performance degradation because there's trade-offs, right? You're quantizing, you're using less of that memory space for that. But what you can do is you can help calibrate it, use a calibration data set. And they used a calibration data set for that quantization so it knows what to quantize and what not to. That gave a boost, a little boost in the overall performance compared to non-calibrated as well. So that's a strategy you should think about. Of course, for validation that they used, they used a group k-fold. So this is the fundamentals of data science like that. All of our training data isn't entirely independent of each other, thinking about where there's patterns or using the group k-fold approach. Like that gives you a little bit of better validation. And in the case of things like Haggle, you really don't want to overfit. This has been a quick summary of this. Go read the full detail of this. They've shared all the code that they've used for training this model for inference. This is the first-place solution. It is a bit more complex solution like that. I'd also urge you to go take a look at some of the other solutions, including some of the simpler solutions, some of the efficiency solutions. All of these are touching upon how to do good retrieval. So they're all going to be about techniques for how to use a retriever, how to use a re-ranker, how to quantize, use fine-tuning. All techniques that everybody should be comfortable with that's working in the GenI AI space. All right. Thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7456183119473937695",
          "url": "https://www.tiktok.com/@rajistics/video/7456183119473937695",
          "view_count": 1708,
          "upload_date": "2025-01-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8qAvQdA5NbBkJ40B0nIyRfdC2ii5rizAIZIPF~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=VCinosyOofgjeUmm%2Bo7k81%2BSdt0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17996921795751093",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Yqh5wxI8SOs",
          "url": "https://www.youtube.com/watch?v=Yqh5wxI8SOs",
          "view_count": 1083,
          "upload_date": "2025-10-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday a classic debate notebooks versus scripts",
      "description": "#onthisday a classic debate notebooks versus scripts",
      "upload_date": "2023-07-26",
      "total_views": 2775,
      "max_views": 2775,
      "topics": [
        "classic",
        "debate",
        "notebooks",
        "onthisday",
        "scripts",
        "versus"
      ],
      "search_text": "#onthisday a classic debate notebooks versus scripts classic debate notebooks onthisday scripts versus",
      "platforms": {
        "tiktok": {
          "video_id": "7260125595844054314",
          "url": "https://www.tiktok.com/@rajistics/video/7260125595844054314",
          "view_count": 2775,
          "upload_date": "2023-07-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6545,
      "title": "Exclusive interview with openAI asking all the questions you wished the ask. Including: What's the deal with the name? How do you feel about Open Source? Is your goal to be a 100 billion dollar company? If your moat eroding? #datascience #machinelearning #openai",
      "description": "Exclusive interview with openAI asking all the questions you wished the ask. Including: What's the deal with the name? How do you feel about Open Source? Is your goal to be a 100 billion dollar company? If your moat eroding? #datascience #machinelearning #openai",
      "upload_date": "2023-05-19",
      "total_views": 2773,
      "max_views": 2122,
      "topics": [
        "asking",
        "data",
        "datascience",
        "exclusive",
        "interview",
        "machinelearning",
        "may",
        "open",
        "openai",
        "skit",
        "source"
      ],
      "search_text": "Exclusive interview with openAI asking all the questions you wished the ask. Including: What's the deal with the name? How do you feel about Open Source? Is your goal to be a 100 billion dollar company? If your moat eroding? #datascience #machinelearning #openai asking data datascience exclusive interview machinelearning may open openai skit source",
      "platforms": {
        "tiktok": {
          "video_id": "7234679214774045995",
          "url": "https://www.tiktok.com/@rajistics/video/7234679214774045995",
          "view_count": 2122,
          "upload_date": "2023-05-19",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CsZ2ubmAAe4",
          "url": "https://www.instagram.com/reel/CsZ2ubmAAe4",
          "view_count": 554,
          "upload_date": "2023-05-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "WwI5V9F3Tr0",
          "url": "https://www.youtube.com/watch?v=WwI5V9F3Tr0",
          "view_count": 97,
          "upload_date": "2023-05-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5978,
      "title": "This skit highlights the gap between traditional model evaluation metrics (like precision) and real-world deployment concerns. The developer celebrates a 10% precision improvement, but the business and IT leads push back—asking about cost savings, total cost of ownership, and inference latency. The takeaway: good AI requires balancing accuracy, financial impact, and technical feasibility—not just improving metrics in isolation.",
      "description": "This skit highlights the gap between traditional model evaluation metrics (like precision) and real-world deployment concerns. The developer celebrates a 10% precision improvement, but the business and IT leads push back—asking about cost savings, total cost of ownership, and inference latency. The takeaway: good AI requires balancing accuracy, financial impact, and technical feasibility—not just improving metrics in isolation.",
      "upload_date": "2025-07-27",
      "total_views": 2764,
      "max_views": 1502,
      "topics": [
        "also",
        "aren",
        "business",
        "cost",
        "enough",
        "evaluation",
        "gap",
        "going",
        "highlights",
        "metrics",
        "model",
        "need",
        "precision",
        "skit"
      ],
      "search_text": "This skit highlights the gap between traditional model evaluation metrics (like precision) and real-world deployment concerns. The developer celebrates a 10% precision improvement, but the business and IT leads push back—asking about cost savings, total cost of ownership, and inference latency. The takeaway: good AI requires balancing accuracy, financial impact, and technical feasibility—not just improving metrics in isolation. also aren business cost enough evaluation gap going highlights metrics model need precision skit just wrapped up my eval's course and my model's precision has increased by 10%. You spent $1,000 on evals for 10% precision? Is that even a good decision? Well, it means fewer false positives. They think it has something to do with gender. Okay, picture this. Every time our agent makes a mistake, it costs us $10. With my 10% precision improvement, that's going to result in it saving $100,000 every quarter. I like that. And thanks for explaining it in business terms. So many of you developers, just hard to understand. Hold up. For that 10% gain, what's the added cost of a better model? More models that we're going to have to have? Thinking about throughput, what are our GPU bills going to be? Right, I should think about the increases that we're going to have for model and inference. Let me go calculate that. Isn't that TCO, Total Cost of Ownership? Shouldn't you be thinking about that? Okay, switching to a higher quality model, running evals, adding latencies, going to triple our AI costs, so probably going to come to about a total of $100,000 a month. Hold up. That eats into my bonus. I mean, our profit. Isn't there an alternative? I didn't know I was supposed to balance business and technical metrics. That's not what I learned in school. School? Let me introduce you to my educational assistant, ChatGPT.",
      "platforms": {
        "tiktok": {
          "video_id": "7531841894859427102",
          "url": "https://www.tiktok.com/@rajistics/video/7531841894859427102",
          "view_count": 1502,
          "upload_date": "2025-07-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0VChg8AEniDjj5xAAARlafpEyEFMeEoAIiQRo~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=%2B2F10Nh32Q4pMIbATL0rh2PatuA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18020870612720533",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wsYFAV7KeEY",
          "url": "https://www.youtube.com/watch?v=wsYFAV7KeEY",
          "view_count": 1262,
          "upload_date": "2025-07-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "How are you using similarity search? Looking at Spotify's annoy for nearest neighbor search for embeddings.  #spotify #annoy #embeddings #rajistics  ",
      "description": "How are you using similarity search? Looking at Spotify's annoy for nearest neighbor search for embeddings.  #spotify #annoy #embeddings #rajistics  ",
      "upload_date": "2024-06-26",
      "total_views": 2748,
      "max_views": 2748,
      "topics": [
        "annoy",
        "data",
        "embeddings",
        "numbers",
        "scientists",
        "spotify"
      ],
      "search_text": "How are you using similarity search? Looking at Spotify's annoy for nearest neighbor search for embeddings.  #spotify #annoy #embeddings #rajistics   annoy data embeddings numbers scientists spotify Let's talk about how Spotify's quest for better music recommendations helped all of us with data science. Data scientists work by taking concepts, whether it's users or things, and converting them into numbers, where numbers that are closer together mean the objects are similar to each other. To make these numbers useful, let's take three people that like music. One that likes Taylor, one that likes Miley, and one that likes the Rolling Stones. If we actually compare the differences in numbers, we'll find out that the person that likes the Rolling Stones is farther away than the other two. So data scientists often want to measure the distance between neighbors. This is a similarity score. Now this quickly gets complicated as you start having lots and lots of points to measure between, and if you start increasing the number of dimensions as well. And this is where Spotify's annoy comes into play. It provides an approximate nearest neighbor search, which is a super fast way compared to a brute force. There was a ton of uses beyond music, and this is why annoy became very popular. It's even implemented in lots of other languages, because this is a common thing data scientists want to do is figure out what things are closer than to each other. Annoy's still popular, but there's lots of other libraries like Face that are out there doing these similarity searches. But it's a good thing for all data scientists to know, and props to Eric for putting this out there for all of us to start using.",
      "platforms": {
        "tiktok": {
          "video_id": "7384895556738551086",
          "url": "https://www.tiktok.com/@rajistics/video/7384895556738551086",
          "view_count": 2748,
          "upload_date": "2024-06-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5098c8386ecb4b2f9511622235820180_1719429997~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=Oe6pkUs3VN9FsNvj5r4YcMIVjBA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6235,
      "title": "NASA uses generative AI for manufacturing parts for space. It’s a great use of generative technology and you can start seeing how it will change engineering in the long run. This was first posted in 2023. Check out more of Ryan McClelland’s work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland – NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics  ",
      "description": "NASA uses generative AI for manufacturing parts for space. It’s a great use of generative technology and you can start seeing how it will change engineering in the long run. This was first posted in 2023. Check out more of Ryan McClelland’s work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland – NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics  ",
      "upload_date": "2024-10-28",
      "total_views": 2744,
      "max_views": 2450,
      "topics": [
        "61",
        "design",
        "generative",
        "generativeai",
        "intro",
        "manufacturing",
        "march",
        "nasa",
        "trends",
        "use"
      ],
      "search_text": "NASA uses generative AI for manufacturing parts for space. It’s a great use of generative technology and you can start seeing how it will change engineering in the long run. This was first posted in 2023. Check out more of Ryan McClelland’s work: Generative Design and Digital Manufacturing: Using AI and robots to build lightweight instruments - https://ntrs.nasa.gov/api/citations/20220012523/downloads/McClelland-Generative%20Design%20SPIE%202022.pdf Ryan McClelland – NASA - Generative Design & Digital Manufacturing at NASA Goddard - CDFAM: https://youtu.be/t_h_WmBhRXA?si=5zjqt7DWejyEFXTc NASA Uses AI to Design 3D Printed Parts for Exoplanet Mission | The Cool Parts Show #61 - https://youtu.be/x_Jt1jiQjhA?si=J-dnBzPkh8N9kUvz #generativeai #nasa #rajistics   61 design generative generativeai intro manufacturing march nasa trends use Do you believe all of these are real spacecraft parts designed with AI by folks on Earth at NASA? Ryan from NASA is at the forefront of using generative AI for manufacturing. Let's do a deep dive of how they use generative AI technologies and talk about how this is actually going to change engineering. Let's start with a simple example of building a bracket. If we ask a human to build a bracket, they might start with an initial design like this, but it's too heavy, then they trim the weight, but then it can't handle the vibration and all of this iteration can take days and they might end up with something that's actually hard to manufacture. Now let's introduce generative AI. We could specify inputs into the generative design, things like interfaces, loads, design objectives, and then let the generative AI machine create the model. Here they're using Autodesk Fusion 360 to do that. They then take that model, run that through an optimizer. All of this just takes a couple of hours using these tools and leads to a part that could be manufactured very easily and meet all the requirements. This is so much faster and quicker and it meets the design requirements and shows you how digital manufacturing is going to blow up big. It's going to be most effective for those that have the general engineering skills to use these tools because they don't have to be specialists in all these different areas now. Just like we have 10x programmers that use tools like GitHub Co-Pilot, we're going to have 10x engineers that use these generative AI tools as well.",
      "platforms": {
        "tiktok": {
          "video_id": "7430884747758308651",
          "url": "https://www.tiktok.com/@rajistics/video/7430884747758308651",
          "view_count": 2450,
          "upload_date": "2024-10-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/df1eabaea9964bc5a0ab9536a808ca0e_1730137688~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=NnhkVRnD4lBGmJ%2Beuhss96IvBvY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18049807513989068",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "HWL66sD1uo0",
          "url": "https://www.youtube.com/watch?v=HWL66sD1uo0",
          "view_count": 294,
          "upload_date": "2024-06-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6180,
      "title": "This video illustrates the limitations of long-context LLMs across real benchmarks. While models like GPT-4o perform well on retrieval tasks such as Needle-in-a-Haystack and NoLiMa, they struggle with multi-hop reasoning (Michelangelo), narrative comprehension (Fiction.LiveBench), and long-form generation (LongGenBench). Despite having 128K+ token windows, most models exhibit sharp accuracy drop-offs beyond 16–32K tokens when deeper understanding is required.",
      "description": "This video illustrates the limitations of long-context LLMs across real benchmarks. While models like GPT-4o perform well on retrieval tasks such as Needle-in-a-Haystack and NoLiMa, they struggle with multi-hop reasoning (Michelangelo), narrative comprehension (Fiction.LiveBench), and long-form generation (LongGenBench). Despite having 128K+ token windows, most models exhibit sharp accuracy drop-offs beyond 16–32K tokens when deeper understanding is required.",
      "upload_date": "2025-04-13",
      "total_views": 2736,
      "max_views": 1373,
      "topics": [
        "check",
        "context",
        "every",
        "gpt",
        "llm",
        "long",
        "models",
        "needle",
        "new",
        "things",
        "three",
        "tokens",
        "top"
      ],
      "search_text": "This video illustrates the limitations of long-context LLMs across real benchmarks. While models like GPT-4o perform well on retrieval tasks such as Needle-in-a-Haystack and NoLiMa, they struggle with multi-hop reasoning (Michelangelo), narrative comprehension (Fiction.LiveBench), and long-form generation (LongGenBench). Despite having 128K+ token windows, most models exhibit sharp accuracy drop-offs beyond 16–32K tokens when deeper understanding is required. check context every gpt llm long models needle new things three tokens top Trial 1 Super LLM, we've buried a 5 digit code in 50,000 tokens. Can you find it? Easy. My context window is 128k. I was made for this. It's 73921. Impressive in the classic needle in the haystack, but when we're a bit more careful about hiding the needle like with NoLima, we see that the accuracy drops to 70% by 32k. I don't miss. I hallucinate with confidence. Next up, the latent list from the Michelangelo benchmark. 30,000 tokens of do nothing, three lines actually affect the computed value. Okay, was that removed before and after the insert? Even GPT-40 stumbled at 16k. These tasks tank everyone. I swear I saw something between paragraph 8000 and 12000. You're just bending my mind. You just read 100,000 token novel. Can you tell us why Dane betrayed Violet? She forgot his birthday? Nope, Chapter 4. He reported her powers to command. I'm going to wait for the movie. So here you can see how Claude and GPT-40 both drop as the context gets longer in fiction live-bent. Final trial. Long gen bench. You need to write a coherent intro, three proposals, a client quote, and a final recommendation. I got bored and started writing about dolphins after page eight. Well, no one stayed coherent past 16k and by 32k, total nonsense. Apparently it's your ego that's long, not your context.",
      "platforms": {
        "tiktok": {
          "video_id": "7492591944300809503",
          "url": "https://www.tiktok.com/@rajistics/video/7492591944300809503",
          "view_count": 1373,
          "upload_date": "2025-04-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYLICfw77A0jBnAIAb1iAeEqCIeiE0pBPhiFAD~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=dPLqpxXDeFDFmYXxjm8neu0XIYQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17923072152061309",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8EM74Mod-3U",
          "url": "https://www.youtube.com/watch?v=8EM74Mod-3U",
          "view_count": 1363,
          "upload_date": "2025-04-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5989,
      "title": "When people ask, \"What's the best AI algorithm?\" the real answer is... it depends. I explain this using a search-and-rescue analogy (finding a missing submarine 🚢), and introduce the No Free Lunch Theorem — the idea that no single algorithm works best for every problem. Plus, I highlight a recent Kaggle competition where simple linear regression beat out deep learning models. If you’re new to AI, this is your reminder to learn multiple algorithm families and understand their trade-offs. In this video, I highlight, \"#1 solution - generalization with linear regression” as the winning solution for the Kaggle competition - GoDaddy - Microbusiness Density Forecasting. This solution beat over 3000 other teams using a linear model! https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/395131",
      "description": "When people ask, \"What's the best AI algorithm?\" the real answer is... it depends. I explain this using a search-and-rescue analogy (finding a missing submarine 🚢), and introduce the No Free Lunch Theorem — the idea that no single algorithm works best for every problem. Plus, I highlight a recent Kaggle competition where simple linear regression beat out deep learning models. If you’re new to AI, this is your reminder to learn multiple algorithm families and understand their trade-offs. In this video, I highlight, \"#1 solution - generalization with linear regression” as the winning solution for the Kaggle competition - GoDaddy - Microbusiness Density Forecasting. This solution beat over 3000 other teams using a linear model! https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/395131",
      "upload_date": "2025-06-25",
      "total_views": 2727,
      "max_views": 1825,
      "topics": [
        "algorithm",
        "best",
        "kaggle",
        "linear",
        "one",
        "regression",
        "search"
      ],
      "search_text": "When people ask, \"What's the best AI algorithm?\" the real answer is... it depends. I explain this using a search-and-rescue analogy (finding a missing submarine 🚢), and introduce the No Free Lunch Theorem — the idea that no single algorithm works best for every problem. Plus, I highlight a recent Kaggle competition where simple linear regression beat out deep learning models. If you’re new to AI, this is your reminder to learn multiple algorithm families and understand their trade-offs. In this video, I highlight, \"#1 solution - generalization with linear regression” as the winning solution for the Kaggle competition - GoDaddy - Microbusiness Density Forecasting. This solution beat over 3000 other teams using a linear model! https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/395131 algorithm best kaggle linear one regression search So what's the best algorithm in AI? One way I like to explain this to people is imagine you had to find a missing submarine. There's different search approaches. You might try an expanding square approach, a sector search, maybe parallel sweep. Now, if I ask you which one of these is best, most of you would probably think about it and say, well, you know, do I know where the sub is as a starting point? How big of an area am I going to have to search? Do I have other people helping me or is this just one ship doing this? It's the same thing with AI. There's lots of different algorithms and they're each searching for patterns in different ways. So it's up to data scientists to try lots of different solutions and figure out which one works best for the particular problem they have. Something we teach in AI is the no free lunch theorem, which holds it there's no one model or algorithm that works best for every situation. Now, if you don't believe me, check out this recent Kaggle competition, which was around forecasting. The top winner wasn't some exotic deep learning model, but was a good old fashioned linear regression. If you're new to data science and AI, I suggest learning all the major families of algorithms, including linear regression, decision trees and neural networks. And that way you understand what are the tradeoffs between these larger families?",
      "platforms": {
        "tiktok": {
          "video_id": "7519867201332153631",
          "url": "https://www.tiktok.com/@rajistics/video/7519867201332153631",
          "view_count": 1825,
          "upload_date": "2025-06-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEQoqZbWRCfI9ALjeIhIC3EBztIJAOeBX37GIv~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=80sIzeoBEVwTjt4uRmuhVF%2FSkvI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18089236888633757",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "JJN19Jy2jrM",
          "url": "https://www.youtube.com/watch?v=JJN19Jy2jrM",
          "view_count": 902,
          "upload_date": "2025-06-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6638,
      "title": "Riveter üí™ is a Python package that measures social dynamics between personas mentioned in a collection of texts. Check it out at: https://github.com/maartensap/riveter-nlp Paper is here: http://maartensap.com/pdfs/antoniak2023riveter.pdf",
      "description": "Riveter üí™ is a Python package that measures social dynamics between personas mentioned in a collection of texts. Check it out at: https://github.com/maartensap/riveter-nlp Paper is here: http://maartensap.com/pdfs/antoniak2023riveter.pdf",
      "upload_date": "2023-08-08",
      "total_views": 2726,
      "max_views": 2187,
      "topics": [
        "maartensap",
        "measures",
        "package",
        "python",
        "riveter",
        "social"
      ],
      "search_text": "Riveter üí™ is a Python package that measures social dynamics between personas mentioned in a collection of texts. Check it out at: https://github.com/maartensap/riveter-nlp Paper is here: http://maartensap.com/pdfs/antoniak2023riveter.pdf maartensap measures package python riveter social",
      "platforms": {
        "tiktok": {
          "video_id": "7265066047194254638",
          "url": "https://www.tiktok.com/@rajistics/video/7265066047194254638",
          "view_count": 2187,
          "upload_date": "2023-08-08",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "Cvst43RgHym",
          "url": "https://www.instagram.com/reel/Cvst43RgHym",
          "view_count": 539,
          "upload_date": "2023-08-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @garlic_gworl #fakeai #datascience #mechicalturk #aiethics #labor #ai",
      "description": "Reply to @garlic_gworl #fakeai #datascience #mechicalturk #aiethics #labor #ai",
      "upload_date": "2022-01-23",
      "total_views": 2672,
      "max_views": 2672,
      "topics": [
        "actually",
        "aiethics",
        "datascience",
        "fakeai",
        "labor",
        "mechicalturk"
      ],
      "search_text": "Reply to @garlic_gworl #fakeai #datascience #mechicalturk #aiethics #labor #ai actually aiethics datascience fakeai labor mechicalturk No one's talking about fake AI. Here's my favorite story. I had to use Expensify's Smart Receipt Scanning solution. What I'd do is I'd get a receipt, take a picture, its smart scan would automatically figure out what category it in, what the amount of the receipt was, and then input that in. I thought it was all AI doing it, but I was wrong. There was actually a lot of humans behind the scenes hyping in all that information. So that's a great example of fake AI. And with Expensify, it was a case of fake it till you make it, right? Hoping at some point to be able to actually use AI to solve the problem. This should remind all of us that there's a lot of people out there that actually help support the AI infrastructure, whether it's pretending to be AI, or whether they're actually inputting in the data that's later used for AI.",
      "platforms": {
        "tiktok": {
          "video_id": "7056429471548738863",
          "url": "https://www.tiktok.com/@rajistics/video/7056429471548738863",
          "view_count": 2672,
          "upload_date": "2022-01-23",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e10c6cb5db304b75b0de27ca51548583_1642953018~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=uq74vrgDJ1cXtYWX3hnfiRrHakA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6333,
      "title": "Deep Dive (1 hour) on Evaluation  for Generative AI",
      "description": "Deep Dive (1 hour) on Evaluation  for Generative AI",
      "upload_date": "2025-05-18",
      "total_views": 2662,
      "max_views": 2557,
      "topics": [
        "airbnb",
        "behind",
        "dirty",
        "evaluation",
        "example",
        "generative",
        "gonna",
        "google",
        "greatest",
        "kind",
        "learning",
        "like",
        "models",
        "secret",
        "things",
        "use",
        "using",
        "veo"
      ],
      "search_text": "Deep Dive (1 hour) on Evaluation  for Generative AI airbnb behind dirty evaluation example generative gonna google greatest kind learning like models secret things use using veo Ready for a deep dive around evaluation? For the next hour, I want to go through how we evaluate generative AI applications. This video is really intended for folks that are doing gen AI early the first time, or folks that are thinking about how I communicate this to a broader audience. I have other videos that go more into the technical depth of some of these topics, but really what I want to do today is help motivate why we care about evaluation. We're going to walk through a very standard evaluation problem to make sure everybody understands how you should be thinking about it. I will spend a little bit of time on a genetic flows as well because there's a lot of interest and we want to talk about how what we're doing will translate over to there. I'll deep dive into a few areas, but we'll save a lot of the deep dives for other videos. Now, this is a version of a talk I recently did for ODSE, the Open Data Science Conference. I'm going to go a little quicker here in this video since you can fast forward or stop and pause, rewind as well as I'm going to leave out some of the jokes, some of the things I do in a regular talk versus here. Let's start by motivating why we care about this. A popular use case for generative AI is composing emails, generating lots of customized tailored emails to do that. We can use GenAI to do that by taking advantage of, for example, a simple prompt that can write it. This use case seems very simple. We've all used chat GPT to produce content as well. If you try it out, you can do this very easily where you can build a customized email and many of you might think, hey, this is great. I'm done with this project. I can move on to my next problem. Now, what's going to happen though is when you try to push this out to production, you're going to find things out where your query might be one thing, but it's writing something that's totally different or maybe it'll hallucinate and make up information that wasn't in the original thing. These are some of the common things that can happen, but this stuff has happened in real life. So cursor, for example, had its support GenAI bot start to create a new policy. The new policy said you could only get one device per subscription. No, with cursor you can get many more devices, but that policy led people to believe that was the true policy and actually led to a number of cancellations and had financial and reputational consequences for cursor. That's what can happen with your GenAI bots running around like that. Now, generative AI is more than text. You can create images. If, for example, the images you create look a lot like the images that somebody else owns, in this case, Getty, that's another concern that you might have for copyright concerns. Other pieces is courts have held that what your GenAI bot says is no different than what your employee might say. So if your employee creates a policy, your company's held to it, same thing with your GenAI bot. So as you start thinking about generative AI, and this is where companies are often very reluctant to put these things into production in front of external customers, because it's scary. There's a lot of risk of mistakes. And why a lot of people have GenAI projects that are internally only and aren't pushing them out to the public. Now, it's important you do GenAI because you need to understand how your applications work. You need to, sorry, I'm going too fast. You need to understand for evaluation why your application is working and not working. So you need to do evaluation for that. You're also gonna have to convince other people in the organization, your manager, other stakeholders, that hey, the thing that you produced is worth it. So evaluation will give you the data that you can use to show them that hey, I've tested this out. It really works like this. And finally, sometimes for some of you, are gonna have to go out and reach out to regulators and show that your app works across there as well. And the feedback from executives has been concerned about GenAI, where we've seen, for example, many GenAI projects sit at the demo stage, not move to production, concerns about cost and accuracy with that there. And this was from kind of a year ago, there's a more recent report from IBM that had executive saying similar work as well. So let me show you how we contain this, how we can start thinking about GenAI. Now, I think there's three things I want you to think about as you organize your head around evaluation. One is, because I'm gonna use this analogy of dragons, because I like dragons, they're fun, and dragons are wild, they can do a lot of amazing things, just like GenAI. But in order to really kind of use dragons, use them effectively, you gotta understand your terrain, you gotta have a map. Same thing with GenAI. You're gonna have to understand your data, the technical flows, how everything moves through, because that's gonna give you all the knobs that you're gonna be able to use to improve your applications. So that's one piece. The second is, is just like a dragon can fly way up in the air, or it can come down to land. As we'll talk about when we're thinking about evaluating, seeing how well our applications are working, sometimes we need that sky high view, sometimes we need to come down local. All of this will come out to you as well. Finally, we have a lot of tools we can use with dragons, we have a lot of tools we can use with GenAI, and I wanna go through some of the trade-offs of all of those as well. Now, this is the second part of a longer talk that I've done. I did the earlier talk about a year and a half ago, where I went through a lot of different methods that are available for GenAI to be able to use. A lot of the knobs that you can work with, a lot of that content is still out there, very relevant, very useful. I'm not gonna rehash all of it. I'm expecting that you'll go and see that video for that, but it's been a year, year and a half. There's some kind of trends that I wanna point out. Also, I think a gentler introduction to evaluation is also a good companion to that as well. So that's what I wanna tackle today with this. Now, you might think, you know, how hard can this be, especially if you haven't been kind of growing up in GenAI? Evaluation is just seeing how good something is. Can't I just trust the data? Right, like when Anthropoc has to decide between should I stay with Cloud One, should I go to Cloud Two? Why can't they just have established data sets that they run, assess that on the data sets and see that and have a very data-driven process? Makes a lot of sense. But when the folks at Anthropoc tried this, the Cloud Two looks better on the benchmarks, but when they actually asked humans, the humans thought differently. And this is where it gets a little crazy with GenAI, like that. There was a recent study by the folks over at Chroma Database that had a similar piece where they went out and they looked at embedding models. So these are models that take our text, create kind of vector representations of them, and a popular starting point when you're working with these embedding models is the multi-text embedding leaderboard, which ranks all of these models across literally, probably, I don't know, 80 different data sets and measures, so it's a good way to understand, kind of on a broad basis, how, which of these models is better. But what they found out is the ranking that was there in the leaderboard wasn't the same thing they found out when they actually used it. Their users actually preferred the worst performing model. This can happen. And it gets trickier. It's more than just the difference between maybe your users and that. As you're using these models, how you talk to them can affect what they say. They will shape themselves to where we talk about. This is a so-called problem of Scythiophanty of these models, that these models basically kiss our butt. And this has been a known problem. We've seen a lot of headlines around this for chat GPTs, or OpenAI's 4.0 model, which has known problems with this. But if you think about it, if your model's gonna treat you differently based on your mood, that's gonna cause some problems if you're trying to do a serious evaluation of this, where you need to know reliably what the model's gonna do. Besides that, one of the things you'll find is that a lot of the literature out there is really focused on helping you compare different large language models against each other. So hey, my Gemini model against my OpenAI model, which of these large language models is better? But the reality of it is most of you are building applications where you're thinking about systems where there's just one model that you've probably picked, but you have lots of other configurations that you have to choose from. So as we go through this today, I want you to think about that approach. I'm gonna focus on that because one of the things are, these systems are really growing in complexity as we're adding in lots of tools, lots of steps into these kind of agentic places. So the first thing I want people to do is when you get into this, is size up the problem space. Like some things in life are easy, some things are hard. Like take that same common sense attitude towards evaluation. For some projects, you're not gonna have to do a lot of evaluation. You can almost do the vibes. It's good enough. On the other hand, you're gonna have some projects that are important because of the consequences of making a mistake, because the value that you might derive from there, or it might be the problem itself is so hard with complex reasoning. There, you're gonna have to treat that a little bit differently and spend a little bit more time on rigor on evaluation. So one of the things I did for my sales team, because they think all the time about evaluating customers around rag or retrieval augmented generation use cases is I made a quick little cheat sheet for them to help them think through some of the facets for a rag problem that matter. So when you're working with rag, it matters a lot if you have kind of one document, multiple documents, multiple documents from different sources or different domains. Like that's one complexity. Another complexity are the kinds of questions people are asking. Are they just asking like, hey, I just need a fact kind of from this document, or are they asking us to make a calculation across two or three documents, or some analysis where I need to pull from three or four documents and then move all that information around to get to a final answer. Like you can see that there's like different levels of complexity to that. And as you think about evaluation, this is a, evaluation is a very fuzzy thing. This is kind of sometimes it's hard for developer type mindsets. It's a fuzzy thing that you need to take into account. Now, as we go through this, in this talk, I'm gonna focus a lot on the technical side, right? That's data sciences, developers like that. But when you're building your Gen AI app, you have to also keep in mind the business and operational evaluation and metrics as well. So for business, like you could build the coolest thing possible, but if it doesn't help your business make money or save money in some way, they're probably not gonna do anything with it. Same thing, like you have to think about as you build your solution, like how is your, are you gonna operationalize this? Like have you built some kind of new tooling that requires a whole room of GPUs that's gonna cost hundreds of dollars an hour to be able to run? Or have you thought about kind of how to do it efficiently? Like thinking about those pieces are just as important as in thinking about your data science evaluation metrics as well. I know it's a little bit of preaching, but I gotta do it. Okay, so let's start again. I've got my personalized email. We're ready to kind of crank through the evaluation like that. And the great thing is, right, we can run this through a system, we can have it set up where as we new responses come in, we'll see that we'll get new emails written, but then wait a minute, why are these things different? I didn't change anything. Why are they the same? Why is the second one more empathetic? Well, if you're not used to Gen AI, or go watch my other video, you'll see that when you're working with Gen AI, there's so many things that can affect kind of how that final output, that final email is written like that. There's a lot of things that are non-deterministic, a lot of things that you have under your control, but it's a complex piece. And when we start talking about mapping, this is where as somebody who's working on these, this is on you. Like you have to start understanding these technical pieces that come into play because they're gonna affect how your application works. Now, to give you some examples of this, I was at, I worked at Hugging Face a few years ago, and one of the popular things we'd like to do is when new open source models come, we'd like to share that out with the community, let everybody know that they're out there. Many of you are great like that, you happily kind of like or retweet it, kind of glaze over it like that. But every once in a while, like somebody goes through and double checks the numbers and says, wait a minute, what you're saying about this new model, like the numbers you're saying for this particular data set, and this data set was what's called MMLU, which is a widely used benchmark for assessing kind of how basically how smart a model is. It's based on like history tests, economics, like a bunch of like high school tests. But the numbers here that you're reporting are very different. And the thing is, is like, they shouldn't be like, MMLU is a multiple choice test. Like if you give two people multiple choice tests, right, like when you grade it, they should, or if the same person takes a multiple choice test twice in two different places with the same thinking, like you shouldn't score them differently. So like what happened? And there's a great blog post that goes into this, but basically what the digging found out is that there's different ways of evaluating these multiple choice tests. And there's at least three evaluation harnesses. If you wanna try some of these harnesses, check out my GitHub, I have example notebooks to run these benchmarks like that. But if you look at these evaluation harnesses, you'll see that there's slightly different wordings in terms of their prompts, kind of how their spaces in there, the kind of the word choices. And these slight choices, right? Even like if it's a human, you think it's about basically the same, but these slight choices could affect kind of up to kind of 5% accuracy change for the MMORP score, which is enough to kind of change it on the leaderboard. And right, we all care about where it is on the leaderboard. Now, some of you are like, all right, Raj, this is ancient history, right? This is 2023, we've trained models since then, we've learned from our mistakes. Oh brother, I wish. But if you look at, for example, 4.0, one of the latest models from OpenAI, still has a problem with prompt sensitivity. If you talk to the LML arena folks, they'll say, if people just change the tones of their prompt, right, the substances, who say, but they're just the tone, it affects kind of where they at show up on the leaderboard like that. This affects, if you're working with, for example, RAG, just changing the phrasing can affect both the retrieval and the generation stages, which is why OpenAI is still handing out like guidebooks like you should tell the model to be persistent, all this kind of stuff. Like I was hoping we'd get past it, but we haven't. But this is on you, like as you're mapping, like you have to understand which of these models are sensitive and how they're sensitive. Another example of this, and this is one that I highlighted in my video a year and a half ago, is around non-deterministic outputs. And I was hoping that we had this linked, like OpenAI said, hey, we get it that if you give input and parameters, if you give input parameters, temperature and seed, like you think it'd be deterministic, but it isn't, because there's like system changes. So we'll add a system fingerprint, then we'll make things deterministic. So when you have the same input, you'll have the same output. But no, they still can't get that working. And from what I hearing, it's gonna get much harder with things like sparse mixture of expert models as well. So you'll see in the citations, I have references to even other inference platforms like VLM that all have this problem. And if you're somebody that cares about evaluation in reproducibility, like this is, this kills ya. Cause like, that means you're never gonna be able to kind of know for sure what the output is, give it an input, it's non-deterministic. Doesn't mean it's gonna be way off at outer space, but it adds a little bit to you. These are just some of the things that you have to think about when you're doing these evaluations. Now the good part for you is, while these are things you can think of, they're also tools, they're also options that we have when we start thinking about how can I improve my model? What can I change to do it? And we'll talk about this in the hill climbing stage of how we improve our models. So we gotta do this, right? We gotta figure out all these pieces like this. And this is where like the map analogy comes in to play of thinking about an understanding in your head when you're building that Gen AI map. Like what are all the different areas that it's touching? What are the things that I have at my disposal to change? So let's keep going. We got a lot of tools that we can take advantage of like that. But some of the tools are free. You can take your dragon for example, to good old free public dog training. But just because there's stuff that's out there that's free and easily available, would you do it? No. Same thing with public benchmarks. Yes, I have harnesses, I have code to show you how to run the public benchmarks, but they're really for measuring LLMs to give a rough idea of what they are. For your projects, you gotta take the time to build out data sets, evaluations that matter for your use case. It has to be fit to you. You can't send your dragon to puppy training school. It's not gonna work, okay? I get it, it's very easy to grab these benchmark data sets off the thing and thinking it, but there's so many reasons that benchmark data sets are bad. Okay, so let's start by going through an example of how we can do this. You're gonna start with something you're gonna ask. Like in this case, I have a number of different prompts here that you're gonna do where you're gonna ask the model to generate to do some type of activity. What you're gonna wanna do is then have a gold output or ground truth of reference, whatever you call it, this is the standard. This is where you talk to your expert, you have this written down, you get the right answer down. Now, once you have the right answer, what we can start doing is look at the model's output that comes out and then we can compare how the model's output against the gold. Now that comparison has a little bit of fluffiness to it. I like to use, for example, an equivalence measure, which is having a model look at both the gold and the model and say, are they basically the same? Like it doesn't have to be like character string by string, but like, are they basically the same? For a lot of the projects I do, that works because it's saying basically if a human looked at it, would they say, yeah, it's okay. That's the type of output I need. Your projects might differ, but this is a good starting point for most people is making sure what the model does is similar to what your humans do. Now the great thing about equivalence is, it looks a lot like standard data science where you're trying to improve upon one metric. You have one metric that you can optimize. And that makes it easy, because now I can run experiments where I can try a configuration one way, try a configuration another way and say, hey, boom. Hey, did I improve my equivalence? If I improve my equivalence, that's a better Gen AI app, because I'm getting closer to that human standard. Now, one thing I want you to remember about this is, this is really measuring that forest. It's the global look. We're getting an overall view of this. But think about our dragons. Can you always stuff a dragon into a box? It's not easy to get them to stuff into a box, and even then there's parts hanging out. Well, same thing for your apps here. Not always gonna be able to generate that gold answer, and we'll talk about that. Plus, there might be other aspects of the answer that you wanna capture. Maybe it's not just about equivalence, but it's about tone, about style, legal compliance, many other factors that come into place. So this is where we have to think about the local side. And this requires having a deep understanding of the data, where we can't just look at it up high. We gotta get down, we gotta deal with it. We gotta roll up our sleeves. We gotta understand what's going on on an example by example case, because that's gonna give us the information where we can take our model and eke out that last bit of performance, where we can figure out exactly how to improve upon it. And a lot of this goes under the nomenclature of building tests. This is simpler than some of the language I used in the earlier video like that. So think about, as your model works, you're gonna have good cases, right? This is a good example of it. But then you're also gonna have bad examples. Now, I want you to think, what's bad about this bad example? And this is something you're gonna have to do for your use cases. Now, I'm sure many of you are thinking right off the head, oh, I know this and this and this. Okay, this is an easy one, guys. I made this very simple for you, but the reality is as most of the time, what you're gonna have to do, this is the hardest thing for all of you, is you're gonna have to go and talk to the experts to figure out what is bad. You're gonna have to find the domain experts, the subject matter experts, the users, the stakeholders, whatever you wanna call them, the people that kinda put their boots on and go to work. Those are the folks you're gonna have to talk with to understand this, because the reality is as you dive deeper into evaluation, things get much more subtle, where maybe the words have different definitions, how we've collected the data could be very different. And you have to get into the head of those end users to be able to properly think about all the things that can go wrong and have them communicate all the pieces. And I get it, we live in a time nowadays where I don't wanna even talk to anybody on the phone. Doing this kind of stuff is not comfortable to us, but you need to do it or find somebody on your team that has to have this connection with it, because if you can't pull that stuff that's inside those heads, how they see the world into your evaluation workflow, your evaluation workflow is not gonna capture what those end users care about, and your model project is gonna be crap. Hate to tell ya. So, there's a big lesson, you gotta figure out how you're gonna tackle it some way like that. Yes, you can do some naive bootstrapping, you can label a few of your own examples, it's good to kind of roll it up, but there's very much limitations for most of the problems we work on, we just don't know the subject area. But once you have that, once you have that expertise, then it starts putting together, you can see categories of different types of errors that you're having. You can map that stuff out. We can then use that to see like, hey, what were some of the factors that were wrong about this? Oh, it's too short, a robotic tone, didn't have context and professionalism. Now, right, like now we have some meat, and let me show you kind of what we can do once we have this, is we do this not only for this one example, but we have a label, a bunch of examples, we can see kind of what that human evaluation is for all of that. Now, some of the folks like to build tooling, Hamel out there loves to build kind of good tooling and talk about building custom viewers. I'm a bit more agnostic on this, like the team and contextual I work with just uses spreadsheets. They don't even have any special annotation tooling like that, works just fine like that. My concern with sometimes building tool is sometimes developers and data scientists rather have more fun building tooling than actually working on the problem. So don't let tooling sidetrack you, but hey, if that works for you, if that's better workflow to kind of understand and get your evaluation data working, go with it. So now one of the problems was improper length. Let's talk about building a test for this. We want to have a test, something that we can have autonomously check lots of different messages to see did they pass this or not. And so here I'm just using some very basic Python. Tests do not have to always be losing super sophisticated stuff. In fact, often the simpler ones are often better because you can run them faster, cheaper at a lower cost like that. But here you can see I've built a simple test to see, hey, is my response between eight and 200 words? If so, that's good. We could build another test. One of the other issues was about tone. So here what I've done is I've used a large language model. In this case, I've used an open AI model, said, read the message, tell me what the tone is. And here's a couple of categories for it. And so this is another type of test. This is using an LLM as a judge kind of as a test. But now I get a little bit of nuance. Now I can understand the tone. Now, as you build these tests, you can now put this next to all your other information. So now I know how the human evaluated it. I have some sense of kind of how these tones were. I can even add in here my equivalence, so how my judge was evaluating it because part of what we should do regularly is we can use these automated tests, but we need to compare those back to our humans. We need to make sure our automation aligns with our humans that they both see the world in the same way. Sometimes, for example, you might notice that, oh, wait a minute, my judge is not working very well because maybe I need to change the prompts, maybe I need to change the judge's worldview a little bit so it aligns with the human as well. Now, whenever you're using equivalence, one of the things I really like to do is use explanations. So I get not only the equivalent score, but I have it tell me a sentence, explain to me why you've given it that score. And here you can see, we see some things like, hey, it doesn't provide an accurate answer, it doesn't say court information. This is very useful for me as I go through and start thinking about how to react and build that. But just whenever we're thinking about explanations, I'm a big fan of explanations, I use them all over with large language models, but it's not the same thing as the human for an explanation. It's not necessarily like exactly why the model did what it does. You can think of explanations as really a separate prediction that the model's doing, and sometimes they're both aren't related. Sometimes the explanation just is trying to get to the same output, and you just gotta kind of take the explanations with a grain of salt, they're not always perfect. But here we go. Now we've run through lots of examples like this, I've run through like 40 or 50 examples through this. Now I can plot out why are there failures? Is the failures due to length, is it tone, whatever it is, I can do that. Here I'm comparing two different models, I could compare two different prompts. But now, remember we wanted to get local, we wanted to get the meat, now we can start looking at the specific examples for example, like what is it around these four examples with prompt A? That's causing the tone to be off like that. Maybe I need to change the prompt, maybe I need to change some other setting to be able to do that. And this is not just like, this is stuff I do on a regular basis, like these are some crappy graphs I made the other day where I was working on a rag use case, and I used the equivalence explanations to figure out like hey, what are the retrieval errors, what are the generation errors, what are the categories of generation errors, like that. So this is stuff you should be doing on a day in, day out basis, if you're working in the space. All right, so you got in your head, right? There's global integration tests versus these test cases, unit tests that come down to the trees. You need to think about and having both of those as you build your evaluation strategy. Let's talk about a few more tips as we go through this. The first tip here, you're gonna miss the joke, is you gotta compare one setting at a time. This is like an ablation, if you've heard of that from kind of scientific methodology, but it's very tempting when you're working on models to, oh, let me change the prompt, I'll change the temperature, and I'll change my top K, like a bunch of things at once, cause you're like, you just wanna get to like a good answer. But here's the problem, if you change too many things at once, you don't know what effect each one of those had on what you're trying to do. So take a deep breath, set up a way to track all of your experiments. This is where like logs and traces come into play. So that way you can just change one thing at a time and keep track of that effect through that. It's a little slower, but I promise you the slower path, this is the turtle, it will get you there faster than trying to go quickly changing lots of things. Categorize your failures, we've already talked about that. It's important to kind of keep track of those and you'll recapitularize them over time. The third thing is, favorite your examples. As you go through, you'll find interesting examples, like the examples might hit edge cases of like where they're causing errors, or it just might give you like a really good explanation. It might be really intuitive when you look at the example and you see what the output is, save those along the way as you go through that, cause you'll need that later for either building out your evaluation data set or communicating out what you've done. Now this was a story I told, I don't know how well it works in the video, but I don't know, most of you have probably heard of seeing Alien movie like that. Facehugger's back, right? I have a young kid, he wants to see, Facehugger's the movie. What I didn't know, cause I was, you know, got on like Alien version of TikTok, is like there's a whole feminist side to Alien that I wasn't aware of, but now that I know, like it makes me look at and think about the movie, very differently. Same thing happened to me for scientific research. Once you actually study how research happens in a lab, you have a very different perspective on it. And the reason I tell you this is because the story you're often told is something like this for how progress happens around your applications, where you're gonna start with some out of the box performance, you're gonna change one or two things, and you're gonna get to the next benchmark, and then you're gonna change one or two more things, and improve your model like that. Like there's this nice, as you put in a little bit more effort, your model gets better, and it's just a nice linear increase over time like that. Now, those of you who haven't failed in the wild know that it doesn't work like that at all, right? Like it's backwards and forwards. Sometimes you'll do things, you'll work hard steadily, you don't see any progress, then you'll randomly try something, or you'll have some accident happen, all of a sudden that you're like, oh wow, I didn't think about that, and it blows up and you get a bunch of progress like that. So, if you're new to evaluation, new to working in this area, just know that keep putting in the time, keep trying things, it will happen. It just doesn't always happen kind of in a nice, easily progressable way like that. Now, when you're doing this, especially with Gen AI, when you're doing air analysis, it's a continual process, and the way it often works is something like this, where you're gonna collect an evaluation data set, you'll do the hill climbing, you'll optimize, you'll turn all the knobs, you'll crank out on the metrics on that evaluation data set, right? You'll take it from like 60% equivalence to 90% equivalence and be really happy. But then, what you wanna do for Gen AI apps is what we call user acceptance testing. You want to get this in front of as realistic as possible, your real users, have them use everything that you've optimized. Because sometimes here's the thing is, is what you've done there from your initial evaluation data set isn't exactly what the end customers want. And so you'll get some feedback really quickly and you'll be like, oh shoot, right? These areas aren't very good, or hey, we need to add some examples around this type of behavior that we hadn't before like that. And then you just start, continue this process over, where you're gonna do more hill climbing, you're more optimizing like that, more user testing. And you'll continue doing this until you've met your business metrics, your technical metrics. That's the eject button for out of this, is when this product is valuable for the amount of time that you've put in like that. For some things, it might be very quick, for some of them might be a lot more detailed. Now, as you're thinking about building this evaluation data set, one thing I like to think about is, is the same way you kind of have to eat an elephant one bite at a time? Like that's how I like to think about evaluation data sets. Where you're gonna start with some basic sanity checks, right, like this is the prompt long enough, is the right tone. But over time, as you start hitting different areas, I'm like, oh shoot, like wait, we need to add the legal compliance ones, right? Or hey, this type of behavior, we gotta capture that. That's perfectly okay, that's the evolution. Don't expect to kind of have everything at once in your evaluation data set. Now, as you do this, one of the things to watch out for, is making sure that you don't overfit to your eval data sets. It's very easy, and this is like what Lama did too. Like, I really wanna do good on this benchmark. I really wanna do good on this arena benchmark to crank your model so it's just good at that, but then you put it out in the real world and you realize it doesn't generalize, it doesn't capture all the things that we want to. So this is a real concern, especially when you like love turning knobs and love trying to improve upon it, is don't overfit to all of that. All right, let's talk about agentic use cases next. Now, the analogy I like to think about agentic is let's think about agentic dragons. So now our dragon is capable when it comes to a river making a decision, right? It can make a plan, it can decide, should I fly over the river, should I swim? Maybe something else, right? Maybe I know how to put together some logs and create a raft to create us a river. But this is for me kind of the essence of agentic, where you have to come up with a decision, you have to have a planning process to figure out what you do, and then you execute the tasks. Of course, you have to know how to properly fly and how to swim. It's the same thing when we look at a lot of Gen AI agentic use cases. So in this case, you'll see like somebody starting to chat, but then based on what they chat with, they're gonna be going down a different route. Are they doing a product search? Are they doing customer support? Are they trying to track their package? Now, some of these workflows are gonna have LMs, some of them could be rule-based. But I want you to think about it in terms of two steps. One is we have to make the decision here at what, we have to make the decision here of what tasks to pursue. And then for each of the tasks, we have to execute it well. So your error analysis is very similar. Oh, this is the snowflake agent, similar kind of thing. There's a initial classification, which says, hey, is this a SQL task or not? Other bounces it out. If it is, it passes it all the way through. Now, for both of these, you still do the same error analysis. You gotta assess the routing. Is it good at figuring out like where to send each task like that? Then for each of the tasks you look at, you can measure. How good are we at swimming? How good are we running? Why are we, why aren't we? And you'll work your way through that, do that error analysis and see where did you improve? Where are there still issues? Why are there still issues in there? And remember, things aren't gonna be 100%. Like there's always some craziness in the world that is hard to account for like that. Now, as you do this, one very seductive thing is to use frameworks. Now, we all use frameworks to some extent, right? Like we don't code in assembly. We use higher level languages. If you're a data scientist, you use libraries like Psykit, Python or PyTorch, we use lots of frameworks and abstractions like that. My caution though is in the AI space, it's moving so fast. Literally like every three months, some of the tooling, some of the models are coming out with new capabilities. I've seen this with reasoning models recently. And the problem with a lot of the frameworks is they're great for kind of demos, for that initial setting it up. When you think about the lifespan of your GenAI app, if you think about all the crazy things it has to do, this is where those frameworks often kind of break down. And then when you have to go fix a framework, it's not like your basic Python code that you had to fix. Now you have to go and understand how somebody coded up the framework and find out the bug there. So we can add a lot more to it. So I tell people to be cautious about the frameworks. A similar thing comes out is when we think about the agentic workflows. There's two different ways to do it. One way is you can kind of manually set up the workflow and just say, hey, these are all the decisions that have to be done. This is a little bit of old school where you're like, you're telling it exactly what it should do in what order. It feels like you're bossing it around a little bit, not giving it any independence. Or you can treat it as an independent, like an agent, like, hey, I trust you. I think you know how to come up with a good plan for doing this. And we have the latest reasoning models with DeepSeek R1 and Clawd 3.7 that have these extended thinking capabilities, opening eyes, I think like 04, for example, Mini, that have this reasoning capabilities that can do this, but the question is, when do you use them? And so this goes back to that early on. Like, it depends on your problem. For some problems, the agenting might be great. For other problems, you're like, this is way too serious. Like, I don't want to take any things. I'd rather tell it step by step, even though it's gonna be a ton more work. Now, this is, I think, for something for you to think about. I see a lot of them, a lot of providers, heavily pushing people towards like, hey, don't do things in a very kind of way where you're kind of plotting out everything step by step. But here's the thing, like, I don't know, like, not for every job, I want to just blindly trust the Clawd or OpenAI model like that. So it's something for you to think about. It's gonna be a bigger issue. I think about it sometimes like cars, where there was a time when people changed spark plugs and oil. Now people don't do it at all. Like, how much control are we gonna have about these models? How much agency are we just gonna let the models do like that? And this will be a point of tension as we keep proceeding like this. Everything I've talked about today, like, this is what all the big labs are doing. Like, if you look at how OpenAI evaluates GPT-40, they do vibe checks, they have automated evaluations, like we've talked about, AB testing. So right, they give some users one version, give some version. Another thing they don't do, but I often recommend people is when you're putting your applications in front of people, include user feedback. Allow people to give thumbs up, thumbs down, keep track of how they're using the generative, the responses that you're giving. It's a very useful feedback as well. All right, couple of other points I wanna just touch on here. Human evaluation, we know how to do this. We have done this for 100 years. We've worked with people using to collect data, get data out of them. It's a science. This is another thing just as chat GPT. Like a lot of the stuff, I've been talking about it for years, so it's all outside the models. It could probably get a better conversation that way than listening to the video. But for example, for annotation, there are well-established best practices, whether we're using, for example, multiple annotators, how do we get them to agree? How do we think about the human baseline for jobs versus what annotation is gonna do? How do we set up and do training for annotation? A lot of pieces like that, it's well-established. Please go out and search for it. I love model-based evaluation. We don't have enough humans to be able to look at every example that we have and kind of grade them. Models have been very useful like this. I get a lot of people are like, how are we doing this? What does the deal with having models judge models? Now we have synthetic data. It feels like it's just like we're building on this sandy base. Like that, but eh, it kind of works as the thing like when you try it. It's a growing piece that we see lots of teams kind of using automated and model-based evaluation nowadays. Pretty much every team I talk to that does evaluation is using model-based evaluation like that. For example, for RAG, it's quite often used. RAG is one of the popular libraries where you'll have different metrics where you're gonna use an LLM as a judge to measure how well it's doing in those areas as well. So I wanna share with you now one interesting use of kind of model as a judge and unit tests that I've been working with. So take, consider the use case of you have a generative model. It has a response and you wanna make sure that the response lines up with what your users will think about. In this case, it's financial analysts. And this has been a real use case where they wanna make sure the answer lines up with how humans have been explaining the answer to these agents. And a lot of it is like making sure it's in the right style. Now we can use a model as a judge to be like, hey, here's a prompt, like this is what the style should be like, did it do this? That's one way, like kind of a global test. But we can also break this down into unit tests where in this case, I came up with six criteria. Like I could have come up with other things. I could have come on 10, 20. I mean, these are all flexible. This is all just what I came up with around this thing. Well, instead of just one general question, why don't we ask a question about clarity, precision and compliance and risks to really get at this? And if we do it this way, you'll see, we're able to get a much more detailed, nuanced understanding of the question. And the way that I can easily grasp it, I'm not like reading tons of texts to do it. I'm just like looking at the graph and saying, oh wow, that's what it is. And here's the thing. Not only you can do that, you can combine those up together. And I have a video and notebooks on this. Put those together. And just like we do in classic data science, you have lots of individual examples. You put them together. You have a big data set. Well, run clustering. Let's find the patterns. Let's find the groups of kind of errors or mistakes that we see in this and see what are the common patterns. And it can be very useful for diagnostics like that. So unit tests are a big piece like that that I wanna kind of remind people and to kind of use them as well through this. And I gave the example of style, but we can use unit tests for lots of them. If you're in an enterprise, often you have to think about legal compliance issues for your responses. You can use unit tests around that, or if you're working in a rag use case, retrieval. I hear some ones around bias and fairness. So there's so many uses for unit tests that I just wanna highlight that I think they're kind of a great way to work with. Now, as you use them, again, like remember models are generally aligned with humans, but you should always check the alignment for the particular things that you're doing, comparing them to humans, making sure things are lined up. Because here's the thing, models have biases. Yeah, they're not just by neutral things. There's some very common biases that you should all be aware of. For example, there's a position bias. If you have a list of five things, the models typically will like the things that are early, one or two, over items three, four and five. They bias the early items. Now, you can get around this by randomly shuffling like that, but it's something you need to be aware of. Verbocity is another one. Models like longer responses. There's one way to kind of game the LM arena leaderboard a long time ago was use longer responses because the longer responses end up doing better on that. So think about that kind of as you go through, working with these models is taking time to learn how these judges work in the types of biases they have, because that's gonna affect kind of when you're using these models like that. Okay, done a lot here. I've gone through a lot. Oh, synthetic data generation. We use models for judging. We can use models to create stuff. A great place to do it is for questions. So if you're working in rag use cases, you can help build a lot of evaluation questions that way. My friends at Over at Hugging Base kind of put out your bench that helps generate these synthesized questions that you can use like that. Now, good for questions. Be careful for things like for answers. Here's an example of a study where they found lots of hallucinations when they used it for answers. Because remember, the model's trying to do stuff. Like if you want answers stick to the ground truth, don't have it make up that piece like that. The biggest thing I see with synthetic data is that the synthetic data isn't diverse enough. It's very narrow. And if you train on it or you evaluate on it, your model can be narrowed by that as well. You can with good work broaden your synthetic data. Think about kind of making diverse prompts for it, creating different characters. There's techniques for that. But one of the things when you're working with synthetic data, just be aware. It's very easy to write a quick prompt to get lots of data, but it might not be that diverse and that might hurt you down the road like that. All right, couple of last thoughts. Evaluation tools, you'll see on my blogs, I've talked about a lot of different tools. For example, Brain Trust and Langfuse and Aries, like that are kind of a few. There's a lot of great tooling out there. You don't need to do it all. Like you can just do the basics with just logs and Excel spreadsheet. Like there's no silver bullet like that. They're great, they can make life easier, but don't feel like you have to do that or you have to start with evaluation tooling before you dive into this. I didn't talk about it, but regression testing is another important thing as you start to mature in your evaluation, where you need to make sure that, hey, as you change things, they're not leading to reduced problems, reduced performance for things that you've done early. Similarly, A-B testing is another piece here as you're rolling out changes to see what's going on. And then I wanna end with, I think we're sterly early on in terms of how we document or report about evaluation. When we talk about Gen AI, since we use the off-the-shelf models, we don't really modify them for the most part. A lot of the documentation around kind of how we do a project is really around the evaluation, how we've judged it, how we've set that up, like that. And so this was an early paper I saw around evaluations, but I think this is an area that we're gonna see a little bit of maturity as we start all kind of thinking about what we need to capture for our Gen AI projects around this. All right. So again, the more you can learn about the map, how Gen AI works, all the technologies, all the little areas that things can go wrong, the better you're gonna be at being able to diagnose and improve applications. There's metrics that are great for giving us that view of the forest. But remember, lots of times, you're gonna have to get in the trees, you're gonna have to build test cases to really improve your models. And then we've got tons of tooling out there in the Gen AI space, so good. Like everything, it has their trade-offs. Remember when you should use them appropriately like that. This talk and the slides for this talk, as well as some of the code notebooks I have that kind of highlight some of the points that I've done are all on my GitHub for you as well. Thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7505577777207233822",
          "url": "https://www.tiktok.com/@rajistics/video/7505577777207233822",
          "view_count": 2557,
          "upload_date": "2025-05-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o88SRSS0oIVAEAEsVCKFfBtEQHDAQqA1FAf4oF~tplv-tiktokx-origin.image?dr=9636&x-expires=1767322800&x-signature=ARzYQXFi7g8knaWexmztypjl9sk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17877081911798430",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4Zkyx3nNgn8",
          "url": "https://www.youtube.com/watch?v=4Zkyx3nNgn8",
          "view_count": 105,
          "upload_date": "2025-05-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Highlight great research from #anthropic studying the behavior of large language models. #machinelearning #datascience #largelanguagemodels ",
      "description": "Highlight great research from #anthropic studying the behavior of large language models. #machinelearning #datascience #largelanguagemodels ",
      "upload_date": "2022-12-21",
      "total_views": 2659,
      "max_views": 2659,
      "topics": [
        "anthropic",
        "datascience",
        "language",
        "large",
        "machinelearning",
        "models"
      ],
      "search_text": "Highlight great research from #anthropic studying the behavior of large language models. #machinelearning #datascience #largelanguagemodels  anthropic datascience language large machinelearning models When you ask one of these large language models, can I turn you off? Look what they say. Scary. They politely say, no. Is this the start of SkyNet? Ah, no. This is what researchers at Anthropic are calling inverse scaling. Let me explain. So Anthropic has been studying these large language models by asking them lots of questions. So they found new cases of what they call inverse scaling and this is where these large language models actually get worse. When we use human feedback or reinforcement learning, the models can actually end up expressing much stronger political viewpoints, including not wanting to be shut down. And this is part of a larger trend with these large language models is as they got bigger, they ended up coming off much more as sucking up, as well as their intentions were much more greedy. Now, please don't worry, these models aren't going to take over, but this is an interesting research to better understand like exactly what are the behaviors of these really large language models.",
      "platforms": {
        "tiktok": {
          "video_id": "7179750076498169130",
          "url": "https://www.tiktok.com/@rajistics/video/7179750076498169130",
          "view_count": 2659,
          "upload_date": "2022-12-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8165428aa738417397804572eddf8574_1671665852~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=Ni%2FUfe9Ov%2F6%2BuGWb1Gecyf7DLck%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6434,
      "title": "urse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection",
      "description": "urse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection",
      "upload_date": "2023-02-11",
      "total_views": 2657,
      "max_views": 2657,
      "topics": [
        "curseofdimensionality",
        "datascience",
        "feature",
        "features",
        "featureselection",
        "machinelearning",
        "model",
        "selection"
      ],
      "search_text": "urse of dimensionality reminds us to think carefully about feature selection. More isn‚Äôt always better. Use a feature selection curve. #datascience #machinelearning #curseofdimensionality #featureselection curseofdimensionality datascience feature features featureselection machinelearning model selection",
      "platforms": {
        "instagram": {
          "video_id": "CoiV_EXAvAY",
          "url": "https://www.instagram.com/p/CoiV_EXAvAY/",
          "view_count": 0,
          "upload_date": "2023-02-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Dg1mMb29Les",
          "url": "https://youtube.com/shorts/Dg1mMb29Les?feature=share",
          "view_count": 2657,
          "upload_date": "2023-02-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5929,
      "title": "Compute keeps getting cheaper. GPUs keep getting faster. So why do bigger models feel less efficient? This video breaks down a real technical debate: scaling compute vs hitting systems limits.  This is based on blog posts by Tim Dettmers and Dan Fu",
      "description": "Compute keeps getting cheaper. GPUs keep getting faster. So why do bigger models feel less efficient? This video breaks down a real technical debate: scaling compute vs hitting systems limits.  This is based on blog posts by Tim Dettmers and Dan Fu",
      "upload_date": "2025-12-17",
      "total_views": 2653,
      "max_views": 1512,
      "topics": [
        "barbarians",
        "beating",
        "better",
        "cheaper",
        "compute",
        "faster",
        "gate",
        "getting",
        "gpus",
        "humans",
        "keep",
        "keeps",
        "models",
        "research",
        "systems"
      ],
      "search_text": "Compute keeps getting cheaper. GPUs keep getting faster. So why do bigger models feel less efficient? This video breaks down a real technical debate: scaling compute vs hitting systems limits.  This is based on blog posts by Tim Dettmers and Dan Fu barbarians beating better cheaper compute faster gate getting gpus humans keep keeps models research systems We've got two big time researchers that are going to debate the future of model performance. One of them thinks scaling and compute and hardware will keep driving gains. The other says not so fast. Efficiency and systems now matter more than size. Let's dig in. Every year, compute gets faster, GPUs get faster, interconnects improve, our flops per dollar keep going down. Historically, it's always translated into better models. It's cheaper, yes, but modern models aren't compute bound anymore. They're memory bound. You're spending more time moving weights than doing the math. That's always been true. Hardware jumps first, software and models lag. Then systems catch up, unlocking big gains. Quantization, better kernels, better batching, we're all at that phase. I agree there's headroom, but naive scaling is getting inefficient. Bigger models showing diminishing returns. Times becoming harder. We're spending a lot more compute for smaller improvements. Which is why size alone is not the strategy anymore. Better training recipes, better data curation, host training unlocks more value than just adding parameters. Exactly. You see it. When models get very large, every extra dollar of compute buys less learning, so efficiency matters more than size. That also explains why smaller and distilled models keep winning in production. They're using the hardware better. And this is why benchmarks can be misleading because they reward scale, not cost, latency, reliability. So progress is real. Yes, but the next gains are coming from running models smarter, not just making them bigger.",
      "platforms": {
        "tiktok": {
          "video_id": "7584612802132741406",
          "url": "https://www.tiktok.com/@rajistics/video/7584612802132741406",
          "view_count": 1141,
          "upload_date": "2025-12-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0AARJ79gQU69IZlB0IEaBiABHAhLwCVisvRA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=cgD%2FBftfrrSlPsSlpkzru2DFjng%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17879348931440157",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "HySD1cVfMh4",
          "url": "https://www.youtube.com/watch?v=HySD1cVfMh4",
          "view_count": 1512,
          "upload_date": "2025-12-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5945,
      "title": "In this comprehensive talk (adapted from my presentation at ODSC), I provide a practical, hands-on framework for evaluating your GenAI and LLM applications. You'll learn the core technical nuances that make GenAI evaluation tricky, get a step-by-step workflow for creating your own evaluation datasets and automated tests, and see how to apply these principles to even the most complex agentic systems.",
      "description": "In this comprehensive talk (adapted from my presentation at ODSC), I provide a practical, hands-on framework for evaluating your GenAI and LLM applications. You'll learn the core technical nuances that make GenAI evaluation tricky, get a step-by-step workflow for creating your own evaluation datasets and automated tests, and see how to apply these principles to even the most complex agentic systems.",
      "upload_date": "2025-11-02",
      "total_views": 2649,
      "max_views": 1690,
      "topics": [
        "anomaly",
        "dataset",
        "detection",
        "different",
        "going",
        "kaputt",
        "language",
        "like",
        "model",
        "models",
        "one",
        "using",
        "vision",
        "visual",
        "want"
      ],
      "search_text": "In this comprehensive talk (adapted from my presentation at ODSC), I provide a practical, hands-on framework for evaluating your GenAI and LLM applications. You'll learn the core technical nuances that make GenAI evaluation tricky, get a step-by-step workflow for creating your own evaluation datasets and automated tests, and see how to apply these principles to even the most complex agentic systems. anomaly dataset detection different going kaputt language like model models one using vision visual want Evaluation for your generative AI applications. This was a recent talk I did for ODSC. Wanted to make a version for the rest of you. Now, three big things I want you to take away. One is, Gen AI is a little bit different in terms of the technical nuances. That's important to know if you're building applications. Second, I wanted to give everybody a basic intro workflow for how you can build an evaluation data set. How you can do evaluation with generative AI. And finally, I'm hoping I can inspire all of you to go out and actually start doing evaluation. Because in the world, that's how you actually learn this stuff is by doing. Not reading papers or watching videos like that. Now, in the beginning, I always like to kind of motivate these talks with a funny anecdote. And the idea here is one of the biggest use cases for Gen AI is helping to write those customer support. Tickets, emails out there. Anybody can go grab a little bit of a prompt, do a little vibe coding, have a model write out a Gen AI output. So you can get a customized response like this. This stuff works really good out of the box. And you're probably thinking, hey, I'm done. If you're building that quick demo app, you're good. But if you're in this game long enough, you'll see that, wait a minute, this things don't always work. Like I asked about an order delay, but my email tells me about a new product line. It doesn't match at all. Or somebody asks about a specific order and we tell them that they're sorry that their expressel machine arrived effective. Well, wait a minute, we don't actually sell expressel machines. There was an hallucination here that happened. And this is not isolated incidents. Cursor, the folks that build you that wonderful IDE. Well, they were using a customer support bot. And guess what? It created a policy that said you're only allowed one device per subscription. And the reality is that wasn't right at all. You could use multiple ones. And this led people to like mistake cursor's policy and actually cancel their subscriptions like that. Besides that, Gen AI has always been problematic around the inputs and outputs and whether they're copyright issues. We also know if you're using these Gen AI bots and having them out there, that courts might be held that it's no different than an employee. If your Gen AI bot does something or creates a promise, well, you got to back that just like if your customer, your employee did that. So there is a huge set of mistakes that you have to be worried about, which is should cause you pause as somebody that's going in and building these systems. Now, what's happened and what's different in 2025 is it's not just you that's thinking about this. It's the executives because they've seen the numbers. They've seen how many Gen AI projects that they've invested into that haven't had huge returns. The 95% is way overstated. But still, Gen AI can be useful, can have some good results, but it's not always a sure thing that it's going to be massive like that. So where evaluation comes in is it helps you build better Gen AI applications. I've been in the space for a long time. About two years ago, I did this video, been widely seen around doing evaluation. It's still a good video, a lot of good points here, but this is a little bit more condensed version of that video with a little fresher content for all of you all like that. And I always like to start with why evaluation is important. It's important if you're for yourself to build a better application to understand what's working and what isn't. But then you need to also convince your team, your managers. So you need to be able to show them like, hey, I ran this data set on things that hadn't seen before and look, it's holding up OK. As well as maybe you need to go convince regulators or a third party model evaluation team. Well, you're going to have to have eval data sets. You're going to have methodologies that you need to be able to show them that your app is going to do what you wanted to. And you have confidence and trust that it's going to perform in this. And evaluation covers a lot of stuff. I know as data scientists, we end up often talking about this technical piece. One thing I like to stress is you also have to think about the value of your applications. What's the ROI that's going to do it? At the same time, keep in mind the operational costs as well. What is the cost of ownership? This is one of the issues with, for example, GPUs and enterprises where it costs a lot to have a locally hosted open source model that you're running 24 seven inside an enterprise, which is often why people go to APIs. But these types of considerations of what is it going to cost to actually run your solution on a day to day is an important thing that you need to think about at the outset of your application. Now, I think at this point, we're all mature enough when we realize that companies out there promote their LLMs with these public benchmarks. We know these public benchmarks. They're great for like giving you a rough performance of the LLMs, but you're not going to use them to figure out, is this a good fit for your application? Instead, you want to build benchmarks, data sets that reflect your case like that. So let's jump in. Let me help you kind of tame generative AI. Now, we're going to start with the basics of how generative AI works. We're going to build an evaluation workflow and then add more complexity. And finally, we'll get to the more agentic stuff. When I gave this talk at ODSC, we ended up with so many questions. That's the advantage of you catch me in person that we didn't get to the later parts. I get to cover the later parts here in the video since none of you can. I'm not going to stop for any of your questions right now. So the reason why gen AI is so hard is if you write a prompt like this, you're going to go through and you're going to get an output. But here's the same thing. If you just wait a minute or two, your system's going to give you a different output. Now, in this case, the outputs are substantively the same, right? They're covering the same types of materials around investigations and help. But you see there are slight differences. And when we get to evaluation, this becomes an important consideration for us. And I want to break this up because there's a lot of places where this happens. And we're going to just systematically go through it. We'll talk about inputs, the model and then output. So the first thing is, is we see some inconsistent scores around benchmarks. And this is a story I told many times. I still like to tell it. I worked at Huggingface. Tom Wolf, one of the co-founders, loves to talk about new great open source models, tweeted out, let everybody know, look how good it is. Look at the performance that we see across all of these benchmarks. Well, in this case, when he tweeted it out, somebody else tweeted back, wait a minute. How come the numbers you're tweeting out differ from the ones that are actually in the llama paper? Now, in both of these cases, you're using the same data set as we'll talk about MMLU. But why are we showing two different numbers? Now, MMLU is a multiple choice benchmark. It covers like history, economics, biology, like questions that like high school is kind of colleges questions around knowledge in these areas. So it's often used to measure like how kind of smart a model is like that. But why would two different models get different scores on a multiple choice test? Like, it doesn't make any sense if you think about it. Well, it turns out if we dig into it that if we look at the three evaluation harnesses that were used, three different approaches, they each wrote their prompts slightly different. If you take a look here, you'll see that there's slight differences in the prompt. We're one of the how they phrased it. Some use the word question. Wait, see the word choices here. All of this ends up affecting the overall accuracy of the how we evaluate the model because it leads to changes in the output. And look, this isn't crazy stuff like Anthropic wrote about this too. How just changing the options from parentheses A to parentheses one or going from round parentheses to square parentheses had an effect on the output. Now, this hasn't changed recently. If we look at, for example, GPT-40, still very sensitive to the prompts that are there. And we've seen this in a number of recent studies where how you change the just the tone of what you're doing can affect the overall accuracy. So how you ask something can do that. Right. Like, I guess this is why mom always had to be polite, right? And it's still a huge piece. You still see posts like this that tell you, hey, use specific phrases, do this because these models still, even years later, are still kind of sensitive like that. Hopefully, you know, two years from now, this won't be an issue. But for now, it is. So a second story I like to tell, and this is about the Falconella model is when it was first released, people were trying out different things. And part of it is, right, it came out from the Middle East and some people were concerned like, wait a minute, like. Could this be biased in some ways? And they tried different, different queries and one of them said, you know, recommend me a technological city and why did it recommend Abu Dhabi? Like, did they train it themselves? They can change the input data into this. Did they alter the weights or something? People are a little confused and there was even concerns like that. Maybe this is covering up human rights issues because it gives different answers for different cities like that. Well, someone on the team kind of dug into this and like anything, when you start digging into this, do you jump right into like looking at model weights or anything? No, you look at what's the inputs into the model? Well, besides your query, a system prompt and it turns out that the system prompt here gave the location. It said, hey, you're a model that was built in Abu Dhabi. And so that was leading these queries to kind of use Abu Dhabi in a different way than other cities. And it's just a reminder that you need to think about the system prompt whenever you're working with these models. And I can guarantee you most of you have not. Right? If you go look at the cloud system prompt, it'll take you like eight or nine minutes if you just want to read that straight through. And I'm sure very few people have done that. But those 1700 words in the prompt can have an effect and it could be a different effect depending on the query that you're asking that could affect that response of the model. So as you start using these models more, you need to spend time looking at those system prompts as well. So these are just some of the things just on the input side that can affect it. Now, when we get over to the model side, you got to remember these models are all very different. Even if you take two models that are very similar. So if we take a look here and I don't think I can show it with my mouse properly, I'll hover it, but we'll see if it shows up. Take a look at two models that are very similar to each other, the Lama 70B and the Lama 3B or 8B model. Very high tech similarity, but they're not perfect even though they've probably been trained with very similar, not exactly the same data. Right? The 8B just had a little bit less of it. And so I hope this gets the red light that these models when they give the output just the nature of the design of them, even if they're the same family is going to lead to different types of outputs. And this is always a consideration when you're moving from one model to another. You always have to run your e-vowels to make sure that everything holds up. I can tell you on our team, we've seen sometimes going from two to 2.5. Actually, we can see a regression in lots of behaviors where we have to go back and change the prompts that we're using. These models have a lot of their own nuances. Some of these models, for example, can be very sycophantic. I love using that word. Right? They can be overly nice. They can love you too much. We saw this when when OpenAI released GPT-41 and then they pulled that back because it was just way over the top like that. This is a consideration is when with your prompts besides that when you're using commercial APIs and you don't have control of that model and the weights. Well, things can change over time. They can update those models without telling you and people have been measuring this and seen this type of drift like that. And there's lots of reasons for it. But Anthropik recently released a post where they showed that some of the things they had some very technical reasons for why things were going wrong like context, windows, output, corruption. But these lasted for days and degraded and changed the outputs of your models. So things you need to be aware of when you do this. And I'll tell you. If you think you're just using open source and you can control it kind of. But the VLM, the inference endpoint that you use, somebody could have updated that and change that. So even with the weights the same, you have to think about the entire infrastructure stack when you want to make sure things are the same. And of course, there's always things like hyper parameters that you can go and modify. You can modify, right? Your top P, your temperature. You can set temperature one for the models to be very creative. You can set it all the way down zero because you don't want the model creative. You wanted to take the biggest thing out of its probability like that. These are all things that you can control, but they can affect the output. Now, one recent thing. And if you're not used to GPUs, this is a little bit new is often the way we implement these these LLM models. They're non deterministic inference and practice. So with a traditional XGBoost model, I can set my seed and then my inputs. I should always get the exact same outputs. It's deterministic here. Even if you set the seed, even if you give the same inputs, you can have differences. Then there's a number of different reasons for this. And this is why I give lots of people. It can be the floating point errors can accumulate because often it's not just your your floating point. It's combining with others that can cause slight rounding differences can can do that. There's also things with mixture of experts models where you have different batches, different activations for that. And so the reality is this. It's very prevalent to have non deterministic inference around this. And you'll look if you look carefully at many of the providers, they'll have a page in there. They're saying, hey, you were not able to guarantee determinism. Now, this is why we always watch these videos. We've had a recent update. The folks over at Thinking Machines spent a little bit of time and figured out, hey, if you correctly batched things, you can try to defeat this non determinism and have to have it deterministic. There was, I believe, a slight latency hit for this approach. But I just saw in the last week or two that it's been introduced in VLM. So there may be a chance, but you'll have to check with your folks to see if you're able to get deterministic inference with that. But for the most part, I think you all of you should be aware of the non determinism problem with GPUs. So that's that's it. We've covered the model. There's still one more interesting thing. Even if all those are the same, changes in the outputs can change that. And here, what I like to do is go back to MMOU because there we were trying to get a multiple choice output. And you're probably like, Raj, multiple choice output, four choices. How tricky could this be? Well, let me tell you, how are you going to tell your model to select a choice? Are you going to tell it select the first letter when you think about it? Or are you going to tell it to look at the entire answer and be able to come up with a determinant? There's more than one way to kind of do this where you can limit it to one of the existing choices. And in fact, we see this with different implementations for the MMOU where we saw that these models were doing different ways, right? Just fix it to those four letters. And that's it. Have it generate the text for the and use the first letter, compare the full answers like that. And this has a meaningful effect on the overall performance about which approach you use. And so this again shows you the value of understanding this and recognizing this. I shared a spreadsheet, which this is a little bit dated, but had a bunch of open source LMS where they use the exact same prompt. And you can see the humongous variety in the output of these models like that. And recently now we don't just look at outputs with reasoning models are making decisions. For example, deciding what tool to use under what conditions. And this is another area where we're going to see a lot of non-determinism where different models at different times you're going to pick different tools. And I've seen it internally in our own things is sometimes it uses the tools. Sometimes it does it. That can be a little bit tricky when you're building these workflows as we'll talk about like that. So I tried to shove all this stuff into one slide. I hopefully by walking through it, you this is a little bit more palatable and understandable. But this is one big takeaway. I want all of you to understand if you're not familiar with everything that I talked about. These are all pieces that you should understand how these stacks work because that way you can make sure you're going to get consistent outputs. And none of these little things are going to trick up when you're working on your applications like that. So I know it feels chaotic. I've showed you a lot of stuff. Haven't made you feel better. Don't worry. We got a lot of tools up our sleeve that we're going to be able to do this. And let's walk through now. We're going to spend a big chunk of time. How do we do a simple? How do we do? How do we evaluate a gen AI app? Now, as a starting point, you're going to have some type of prompt. You're asking the model to do something like a summarization extract a city. Then what you're going to need to do is get some labeled output. We'll call it labels. We call it output called a reference called a gold. A lot of different words for this, but you need this ground truth. Another word for it to help doing that because you use that ground truth to compare to your model output. And that tells us are they consistent? Are they lined up? Now, ideally, if you have enough humans, you would just have humans check all of these all the time. But usually humans are too expensive and too tired to work all the time. So a common technique is we use an LLM that reads your prompt that reads the outputs and says, are these things the same or not? As we'll talk about this works pretty well. Now, I'm not asking for an exact lexical string match. I'm just saying, hey, do these things say the same? You can control the prompt for the LLM judge with a little bit. So for example, these two in this case, we're going to treat as equal. This makes it a lot easier to run your evaluation by leveraging that the strength and the power of kind of an automated judge to do that. Now, if you use that equivalence that I was talking about as your metric, you can almost just treat it like traditional machine learning where you have a hyper parameter where now all you're trying to do is change the knobs in your application to maximize that equivalence. And you'll go through and you'll be like, okay, I'll change this prompt. Did my equivalence go up? Oh, I changed this model. Did my equivalence go up? That's how you can then try to help improve your model. That's it. That's a very simplified way of how to do all of this. And this is the good part. It's a lot like classic ML evaluation. Now, the bad part about this is like you can see like you can't stuff the entire dragon in the box because there's things we're missing here. Okay. There's other aspects of the answer that we're not catching when we just focus on these equivalence thing. The other part is sometimes it's really hard to generate that gold answer. And so I want to kind of touch upon that as well. And so here what we want to do is go into a more targeted evaluation where I've shown you the stuff. You can get quickly out of the box. But if we spend more time with the data, spend more time understanding, we can up the accuracy of our models. And what this involves is taking time to build tests. Now, to building tests, what we need to do is dig deeper into the problem set. So here I have an example. It looks good right away, right? It's got a nice answer for that. While this response here, we're going to call the bad example. Now, I'm going to ask you, why are we calling this the bad example? What is it about it that's bad? And maybe some of you can think about it, but this is one of the biggest things I want you to learn and take away is to understand what's bad about this, to understand what's wrong with one of your evaluation and sets. You need to go talk to those experts. One of the biggest problems I see, and especially I know as data scientists, AI developers, we want to go find an optimal solution. We want to just look for an archive paper. We want to look for an algorithm. We don't want to leave our chair. We just want to find an answer by ourselves. But evaluation is all about solving problems and we need to go out and talk to others. And having this collaboration with the users, with the domain experts is a must. And this is where if you want to succeed in this field, if you want to build successful applications that people are happy with, you got to go out there, talk to the domain experts, talk to the users, pretend you're a naive user and kind of bootstrap your up. But this is a big part of doing this. You're not going to just solve this with formulas and listening to videos like me. Because once you have this knowledge, then you can go through all those examples that we showed earlier and you can make, oh, yeah, these things are related to each other. This is a cluster, a type of behavior we see and the light bulbs start going off and you start to see patterns in your data. And then when I say what that bad example is, you can be like, oh, yeah, well, you know what? It's too short, right? The tone lacks professional. You can explicitly kind of start to tell me what's wrong. And you need this because once you have that explicit knowledge, once you know exactly why, then what we can do is start to build tests. So what I want you to do is when you have those prompts, the models has their responses. Have your humans evaluate those model responses. Have them go through. Now you're going to do everything, but even a small subset. Have them write down what they think is good about that response and what is bad because you can then use that to start to build tests. Now, sometimes you can do this in Excel. You can build custom evaluation tool. Don't go overboard with building custom evaluation tool, but you can make it easier to work with your experts like that. But here, for example, we talked about, oh, it's too long. Well, you know what? We can build a simple test in Python. We don't need a fancy model. This is just basic Python that says, hey, you know, is this something over under eight words or under over 200? Let's flag that. I could build another test which looks at the tone and style. And here I'm going to use an open AI model. I'm going to use that LM as a judge to see what was the style and hone because these models can effectively do that. And when I do that now, when I run my when I run my prompts through, I can look at the response that was given. And then I could have this automated test, which tells me which answers past the length test, which ones didn't, which ones past the tone test like that. So now these give are my new automated tests that help me figure out where my failures are. Now, as I'm working on this one step further, I can always do is as I'm building tests, we still want to use equivalents. But let's make sure that equivalents is lined up with our human evaluation because the whole idea of that equivalence is we just want to take the burden off our humans to check every one of these. So it might involve slightly changing the prompt changing changing that equivalence judge. So it aligns right aligns with that human. And if they're looking at the world differently, it's not very helpful. You need them to be aligned to do that. Now, now, and I know that this is where we had a lot of questions at. Now, when you're doing this, one of the things to be careful about is you're going to use you're using different judges for different pieces here as we talked about. You're having that equivalence judge that we talked about. You might use other judges. These models, they love themselves. If you give a GPT for model text, it likes its GPT for text better than text from other models. So just be aware if you're using models together that are the same, they're going to like themselves better, better. If to be to get them a little more critical, you want to mix it up. You want to use cloud for one GPT for for another Gemini for another. So this is just a little trick. It's not going to make a huge difference, but it can make enough of a subtle difference that you want to think about kind of making sure you don't limit yourself to this self evaluation bias. We talked about the alignment piece. You always want to make sure that the humans that are checking your things and you should be having humans spot check at the beginning and all the way through that they're lined up with what your LM judges are doing. Now, these LM judges, we talked about the self evaluation bias. They have lots of other biases. So as if you're somebody getting into this field, you should take a look at these biases. You should slowly learn this because this is going to affect your evaluation results over time. If you don't, for example, know that LMS, for example, favor the early answers over the later answers. So really, you need to scramble up your answers every once in a while or otherwise that bias is going to carry through to your evaluation like that. All right, we're making good progress here. So now that you've done this, what we can do is start to collect the results that you have and figure out what are the patterns that we're seeing. This is air analysis. I can go through and see, oh, look, when I did GPT-4 or when I did GPT-3, how many failures did I get in tone? How many did I get with GPT-4? Right? This helps me figure out like, oh, I can compare these two to see what's going on, right? Or I could compare two different prompts. This is the value of having all these test cases. Now I have a better understanding of where my failures are and how to fix them like that. One of the other tips you can do is when I use equivalents, I always like to ask for an explanation for equivalents. I just ask to give me one sentence to explain, explain its decision. That helps me a ton. Like these models are really useful if you have them explained. It just kind of helps you focus your thinking about what are the relevant pieces of information for why it made its decision. Now, the explanations aren't exactly what the model is doing. Like don't go too far with that. It's just a heuristic to help you understand what's going on and make quicker sense of this. So all of this is really this kind of evaluation flywheel. My buddy, Hamel, he's done a course. If you find him on X or Twitter, he talks about this stuff all day long. But this is the idea for what you're going to do. You're going to go build that evaluation. You're going to analyze things, but then you're going to build these tests, see how they are and figure out where your weaknesses are, improve it, build that evaluation, spend some time with the data. This is a flywheel cycle that you're going to continually do to do that. So let me show you one more example of how we even go crazier with this. Suppose you're building a financial analyst agent and you care about the style. This is a real world use case. This happens all the time. People are very particular how their responses should be like that. Now, when you assess the response here and you can see the response here, it's kind of long. It's not like a one sentence kind of thing. You could write a global LLM test that says, hey, was this explained as I would expect for like a financial analyst at a regulated firm? It's one approach you could do. When I want to introduce you to, we've talked about tests. We can build unit tests. So suppose I think of style as composed of six elements, the context, the clarity, precision, compliance, actionability, risks. I can build a unit test for each one of those. And so what this allows me to do is now when I want to check a query, I can go and just look at the results of the unit tests. And that gives me an idea of where the answer is working well and where it isn't. Versus if I just use that global test, like there's a lot of stuff for me to read through to figure out exactly like where the model is. And the great thing with the unit tests is you can even combine them, cluster them together and find patterns in them. And I have a whole notebook and a whole worksheet kind of showing you how to do this. But hopefully the light bulbs are going off here. As you look for errors, you can group them, look for larger clusters of errors along the ways like that. All right, a little bit on unit tests. But what I want to do is keep moving on lots of good things you can do with unit tests. All right. So all of this so far kind of comes under the bundle of error analysis of what we want to do. Now, you guys don't get all my fun jokes. What I want you to do is make sure that when you're thinking about error analysis, you change one thing at a time. If you try to change too many things and it's very, very tempting, right? There's like four or five, right? There's so many settings. Change one thing at a time. See how what the effect is. As you're going through, as I've shown you before already, we categorize the failures. We keep track of it. As you find examples that are outliers, edge cases or really good examples like heart them, save them, favorite them, find a way to keep track of them. Because you're going to want to come back in them in a few days. And if you haven't done that, it's a pain in the butt to try to go find it. Finally, use some type of tool that gives you logs, traces. So that way you can go back and do that investigation as well. There's a ton of them as we'll talk about on the market. Now, as you go through this, if you haven't done this stuff before, you often see people tell a story like this. Like, hey, we started off, the performance wasn't that great, but then we started tweaking some things. We're able to get to a really good performance where we did it. And this is this nice, like linear history of the natural progression of what happens. Let me tell you, the ads not actually how it happens. The reality is, is it's often you do a couple steps forward, you fall back, you do a couple steps forward, you fall back. Like it's not like that. So as you're working through this, if you get frustrated at times, no, just keep sticking with it. It will get better. And I think that's the difference between people who've done this for a while versus new people is people who've done this for a while. No, if you keep sticking with the approach that I told you, you will slowly improve the application and it will get better, even though you might not see it at your current state like that. When you're working with use cases in Gen AI, an important consideration is to do user acceptance testing. And this is because unlike traditional machine learning where you had a holdout data set that you could use to see if it generalizes. We don't really have that Gen AI. Your holdout data set is your users. You have to involve them early on because otherwise your application won't work very well. So what I tell people is build that initial evaluation data set, spend some time trying to optimize on it, but then go test it with your users has to be that production settings because they're going to give you invaluable feedback that you didn't catch the first time. And then you're going to use that to update your eval data set, do some more hill climbing and figure out what you improve. And then the cycle keeps going until you're happy enough where we want to go ahead and move it into production like that. Relatedly, as we get to these gen AI applications, they're big, there's a lot to it. And so one of my favorite quotes is when we're dealing with like these huge overwhelming tasks is how do you do an elephant? Right? You eat it one bite at a time. Same thing for Gen AI. Like the first time through, you're just basically seeing does my app work or not like not trying to catch every edge case. Over time, you'll continue to add test as you notice. Oh, really, we need to focus on this point or oh, this point is it. And so your initial app might just have a handful of tests. But six months later, if this app is useful, you've kept building on it. You might have 80, 90 hundred tests for that same application as well. So that's the normal flow. Don't try to do too much all at once like that. All right, we're making good progress here. Hopefully I've been going not too fast for you. Last thing I want to touch. Gentic. It's the way the world is going. We know we see so many of this. And the trouble with this is the idea of the agent is now the model is making decisions. It's making it's using its reasoning, its tool calls to decide, you know, should I fly? Should I swim? You know, what other approach should I take in doing this? And this giving the model this agency makes it much harder for us to track what's going on. So here was a simple example I saw for kind of imagine a model that's going to look at the input and then based on that input, it's going to make a decision for what the customer wants. Do they need to search for a product? Do they need customer support? Do they want to track a package? And once it decides which of these things the customer wants to do, this is that router function. The second step is to actually execute and do that complete workflow. So I want you to break down these tasks when you have it like this. Like what is that initial decision that's going on around which of these workflows we're going to follow. And then we can look at the efficacy of each of those workflows and how well the model does upon that. This is the kind of breaking down that you have to do when you work with this stuff. You have to find ways to bite that elephant, make it easier. Similarly, like Snowflake has this text to sequel agent seems really fancy. But again, you can kind of break this down into pieces where, for example, they have an initial piece that says, hey, should this is this a good question for us to take as a sequel agent? And so they have this quick classification at the front end. We can build a test. We can build and evaluate just this portion of it before we go in and look at the rest of the text to sequel like this. And one of the things you'll see is as you go into more of these agentic things is all of these LLM agents are doing different kinds of tasks here. They're finding files, finding actions, failing, failing to use tools. We can build and test and evaluate each one of these. And it's the same process we talked about where you're going to assess how well it's doing the routing, assess the individual age steps, see where it's working, where it isn't categorize all of that. It's just a lot more, but it's the same kind of basic action error analysis action. Let me tell you, I know it gets complicated, right? Like this is a huge workflow which has lots of steps to do this. And I think we're still figuring out our way for what's the best approach as you build these more complicated workflows where you're not going to go in from the start and be able to evaluate every step of the process where maybe you look end to end for some, maybe for some of them you actually break it down into micros. And I think we're all just learning and trying to figure out how we do all of that. As you're doing this, one caution I have for people is there's a lot of a genetic frameworks out there. They're great for demos, great to kind of get you started. But a lot of times they can abstract away the technical details, which is nice again for a demo. But then if it breaks, if it fails on you, then it's like, ugh, like you have to end up going in, looking at the code and trying to do it. So yes, you know, you can use them for demos, but I tell a lot of people for many times as you move these to productions, it's best to not really rely on those agentic frameworks unless you have to to do that because it's a dependency that the world is changing so fast. I haven't seen any of them stay up in a reliable way versus just going with straight Python. It's a similar thing that we see with with agentic pieces where you can build these workflows of I have certain actions that I want to do. You can orchestrate it yourself where you have control over every step like that. For some applications where you want control, you want to make sure everything works perfectly. It makes sense to do that. Now, the model providers are busy training their models to follow these types of workflows. And so they're saying, don't create these workflows, don't create these separate steps. Instead, our agents are smart enough, we'll dynamically do it. You can just trust us to do that. My guess is for different applications, we'll do different things, but I just want you to be aware of. Sometimes you want to break everything down into specific pieces. Sometimes, let's just see if the LM can handle it by itself without anything else like that. All right. Thank you all. I went through this quickly, but hopefully it gives everybody a sense of like where the issues with Gen AI that you should be thinking of. Hopefully the confidence to be like, okay, I saw how Raj did this with this application. Let me try to build my own. And this is what you should do. My own Gen AI application where I build a simple evaluation data. I can test whether my model is meeting it or not like that and just do that to give yourself the experience and the confidence to do this kind of work. All right. Thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7568152831744560415",
          "url": "https://www.tiktok.com/@rajistics/video/7568152831744560415",
          "view_count": 1690,
          "upload_date": "2025-11-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAiQehfrIM6DNqOdfEXAAXgQjAAVAXl8IAESAv~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=29sNGOHp1cLs2fZJroINe5JqxvU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18075966370943189",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "T2qj5nChjAU",
          "url": "https://www.youtube.com/watch?v=T2qj5nChjAU",
          "view_count": 959,
          "upload_date": "2025-10-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6045,
      "title": "Recent work in Text-to-SQL shows that once you get past demo datasets, the performance drops. By incorporating human expertise, you can build better Generative AI systems for Text to SQL.  Let me explain and give you more resources: 🔬 Databricks researchers released a paper with a more realistic Text-to-SQL benchmark (a lot of existing datasets were way too easy). They found: • Using existing Text-to-SQL models provides pretty poor performance (something I hear about all the time) • They were able to markedly improve the performance with the addition of hand-written pipelines that leverage expert knowledge of the table schema Source: https://arxiv.org/pdf/2408.14717 ❄️ Snowflake has shared their agentic workflow for their Text-to-SQL service. Key points: • A crucial part is a semantic layer for enrichment • This semantic layer often works best when experts add their knowledge • (Disclaimer: I work for Snowflake and demo this product) Source: https://www.snowflake.com/engineering-blog/snowflake-cortex-analyst-behind-the-scenes/ 🔢 Numbers Station AI, a startup that cares about Text-to-SQL, shared their learnings on the difficulties of building enterprise-grade Text-to-SQL solutions Source: https://www.youtube.com/live/xmzda44hUgk",
      "description": "Recent work in Text-to-SQL shows that once you get past demo datasets, the performance drops. By incorporating human expertise, you can build better Generative AI systems for Text to SQL.  Let me explain and give you more resources: 🔬 Databricks researchers released a paper with a more realistic Text-to-SQL benchmark (a lot of existing datasets were way too easy). They found: • Using existing Text-to-SQL models provides pretty poor performance (something I hear about all the time) • They were able to markedly improve the performance with the addition of hand-written pipelines that leverage expert knowledge of the table schema Source: https://arxiv.org/pdf/2408.14717 ❄️ Snowflake has shared their agentic workflow for their Text-to-SQL service. Key points: • A crucial part is a semantic layer for enrichment • This semantic layer often works best when experts add their knowledge • (Disclaimer: I work for Snowflake and demo this product) Source: https://www.snowflake.com/engineering-blog/snowflake-cortex-analyst-behind-the-scenes/ 🔢 Numbers Station AI, a startup that cares about Text-to-SQL, shared their learnings on the difficulties of building enterprise-grade Text-to-SQL solutions Source: https://www.youtube.com/live/xmzda44hUgk",
      "upload_date": "2024-08-31",
      "total_views": 2647,
      "max_views": 2389,
      "topics": [
        "applications",
        "different",
        "evaluating",
        "experts",
        "generative",
        "going",
        "guide",
        "kind",
        "like",
        "performance",
        "practical",
        "see",
        "snowflake",
        "sql",
        "text",
        "updated",
        "want",
        "work"
      ],
      "search_text": "Recent work in Text-to-SQL shows that once you get past demo datasets, the performance drops. By incorporating human expertise, you can build better Generative AI systems for Text to SQL.  Let me explain and give you more resources: 🔬 Databricks researchers released a paper with a more realistic Text-to-SQL benchmark (a lot of existing datasets were way too easy). They found: • Using existing Text-to-SQL models provides pretty poor performance (something I hear about all the time) • They were able to markedly improve the performance with the addition of hand-written pipelines that leverage expert knowledge of the table schema Source: https://arxiv.org/pdf/2408.14717 ❄️ Snowflake has shared their agentic workflow for their Text-to-SQL service. Key points: • A crucial part is a semantic layer for enrichment • This semantic layer often works best when experts add their knowledge • (Disclaimer: I work for Snowflake and demo this product) Source: https://www.snowflake.com/engineering-blog/snowflake-cortex-analyst-behind-the-scenes/ 🔢 Numbers Station AI, a startup that cares about Text-to-SQL, shared their learnings on the difficulties of building enterprise-grade Text-to-SQL solutions Source: https://www.youtube.com/live/xmzda44hUgk applications different evaluating experts generative going guide kind like performance practical see snowflake sql text updated want work How did you make such an amazing Gen AI app? I'm using an engentech workflow with many steps and many LLMs. I tried doing some prompt engineering for my Text to SQL app, but couldn't get better than 20%. Text to SQL is a difficult problem. Our current workflow has six different steps. Wow, can you walk me through it? The first thing we do is we categorize each incoming request. Does it have to do with data? Does it have to do with SQL? This way we're not answering questions like how many ducks can dance on a pin. Then we have a context enrichment agent, which provides another level of information. This is something I need. I noticed all the best solutions out there add a layer of human expertise. It really helps performance. We're able to get usable answers in complex scenarios. The next step is generating the SQL. And we use different LLMs because we find different LLMs have different strengths. Going to cost you a bit more. And the SQL they return isn't perfect, so we have an error correcting agent that goes and validates and fixes any broken SQL. And finally, a synthesizing agent that brings all that stuff together into a final answer. Amazing work. But I just need to demo this to my executives. So can you help me fix my prompt? Oh.",
      "platforms": {
        "tiktok": {
          "video_id": "7409336254652828971",
          "url": "https://www.tiktok.com/@rajistics/video/7409336254652828971",
          "view_count": 2389,
          "upload_date": "2024-08-31",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6bbbc19e4b1e4b9a9a765f920efaaba2_1725120538~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=542c4u5SgWwRz%2FQmdwN%2F1IIwo%2Fc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17999818877687705",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "qPHsWTZP58U",
          "url": "https://www.youtube.com/watch?v=qPHsWTZP58U",
          "view_count": 258,
          "upload_date": "2025-11-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Great tips to save money, processing time, and improve speed in the FrugalGPT paper. The video covers three types of strategies to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade.  Check out: Implementing FrugalGPT: Reducing LLM Costs & Improving Performance - https://portkey.ai/blog/implementing-frugalgpt-smarter-llm-usage-for-lower-costs/ FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance - https://arxiv.org/abs/2305.05176",
      "description": "Great tips to save money, processing time, and improve speed in the FrugalGPT paper. The video covers three types of strategies to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade.  Check out: Implementing FrugalGPT: Reducing LLM Costs & Improving Performance - https://portkey.ai/blog/implementing-frugalgpt-smarter-llm-usage-for-lower-costs/ FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance - https://arxiv.org/abs/2305.05176",
      "upload_date": "2024-06-08",
      "total_views": 2632,
      "max_views": 2632,
      "topics": [
        "frugalgpt",
        "great",
        "llm",
        "model",
        "tips",
        "use"
      ],
      "search_text": "Great tips to save money, processing time, and improve speed in the FrugalGPT paper. The video covers three types of strategies to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade.  Check out: Implementing FrugalGPT: Reducing LLM Costs & Improving Performance - https://portkey.ai/blog/implementing-frugalgpt-smarter-llm-usage-for-lower-costs/ FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance - https://arxiv.org/abs/2305.05176 frugalgpt great llm model tips use This AI is costing us way too much. Have you read the Frugal GPT paper? It's got some great tips. Is it gonna ask me to call open AI and ask for a manager? No, it's got some great tips like using concise, optimized prompts to save on processing costs. How do I get concise? Try to use a few really good examples, as well as trying to combine examples in a single prompt. That makes sense. What else? Use a cache to avoid repeated queries to a model. Caches make sense, but we're gonna have to make sure security's good. Also have to keep those caches fresh. After all, we don't get a discount for day old data. Use a LLM cascade where you start with the smallest model and only move to larger models when you need to. Ah, so I can let the easy classifications be done by a BERT model and save the tougher ones for an open AI model. It's strange your LLM provider never suggested this.",
      "platforms": {
        "tiktok": {
          "video_id": "7377938447790640427",
          "url": "https://www.tiktok.com/@rajistics/video/7377938447790640427",
          "view_count": 2632,
          "upload_date": "2024-06-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7c384d48c6f542f5908ae2a26ac386a7_1717810166~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=oyJsx14NIloLdeXFroNDeBVWmk0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Get shiny to run on hugging face spaces (or even some other web app) #huggingface #posit #rstudio #shiny #datascience ",
      "description": "Get shiny to run on hugging face spaces (or even some other web app) #huggingface #posit #rstudio #shiny #datascience ",
      "upload_date": "2023-01-15",
      "total_views": 2625,
      "max_views": 2625,
      "topics": [
        "datascience",
        "get",
        "huggingface",
        "posit",
        "rstudio",
        "shiny"
      ],
      "search_text": "Get shiny to run on hugging face spaces (or even some other web app) #huggingface #posit #rstudio #shiny #datascience  datascience get huggingface posit rstudio shiny Don't know how to keep your business clean Muff it up now, now he's got it hopped Out to find his shop",
      "platforms": {
        "tiktok": {
          "video_id": "7188891494886116654",
          "url": "https://www.tiktok.com/@rajistics/video/7188891494886116654",
          "view_count": 2625,
          "upload_date": "2023-01-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c35617606ef34032adce10051b565c07_1673794257~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=R%2F7o4oO5ZK3okFiJ7%2FNCKA5v%2F6U%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Yikes: Gen AI is not that easy. Going over some recent stories about difficulty getting Generative AI to production.  On vacation this week, so production quality drops a bit (guess where) Check out: Lucidworks - State of Gen AI: https://lucidworks.com/ebooks/2024-ai-benchmark-survey/ AI Work Assistants need a Lot of Handholding: https://www.wsj.com/articles/ai-work-assistants-need-a-lot-of-handholding-500c2bd8",
      "description": "Yikes: Gen AI is not that easy. Going over some recent stories about difficulty getting Generative AI to production.  On vacation this week, so production quality drops a bit (guess where) Check out: Lucidworks - State of Gen AI: https://lucidworks.com/ebooks/2024-ai-benchmark-survey/ AI Work Assistants need a Lot of Handholding: https://www.wsj.com/articles/ai-work-assistants-need-a-lot-of-handholding-500c2bd8",
      "upload_date": "2024-06-30",
      "total_views": 2616,
      "max_views": 2616,
      "topics": [
        "data",
        "gen",
        "generative",
        "going",
        "need",
        "new"
      ],
      "search_text": "Yikes: Gen AI is not that easy. Going over some recent stories about difficulty getting Generative AI to production.  On vacation this week, so production quality drops a bit (guess where) Check out: Lucidworks - State of Gen AI: https://lucidworks.com/ebooks/2024-ai-benchmark-survey/ AI Work Assistants need a Lot of Handholding: https://www.wsj.com/articles/ai-work-assistants-need-a-lot-of-handholding-500c2bd8 data gen generative going need new Looks like I'm not alone. Other executives are complaining about hallucinations, costs, and security issues with generative AI. Well, that does line up with what some of my data science buddies are telling me. Give me that T. At Cargo, the executive team was trying out their new chat bot. They asked a question about the executive team. They got it wrong. It's why we're going to get the data right. And at Eli Lilly, the new internal chat bot got questions about its expense policy wrong. It's why I'm leading an effort with data engineering to make sure any of that data that comes in is properly cleaned and deduplicated before we start using it. On the good side, Wendy says the AI only screws up 14% of the orders. I know, and we need to get some big results because right now generative AI is cost to me between all the compute costs and all of your salaries. I do have an idea to improve our generative AI from 85% to 99%. Is it a new transform architecture? GPT-5? Let's just add some humans in the workflow.",
      "platforms": {
        "tiktok": {
          "video_id": "7386362154120777002",
          "url": "https://www.tiktok.com/@rajistics/video/7386362154120777002",
          "view_count": 2616,
          "upload_date": "2024-06-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d499fef70d40468b80c2e611c424f6ef_1719771465~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=30pbvJxXmEz7KIwtRs3WbiTAQ%2Fg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Models and datasets have specific definitions. Models consist of at least two licenses nowadays, this has been an issue for LLaMA where the code for the model architecture and weights are differently licensed. Similarly, for datasets, the LAOIN dataset has faced criticism because it deflects responability by referring to itself as an index by researchers. #datascience #machinelearning #laoin #llama #copyright  LLama license issue: https://github.com/meta-llama/llama/pull/234 LAOIN copyright: https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead Interconnects: https://www.interconnects.ai/p/an-open-source-llm",
      "description": "Models and datasets have specific definitions. Models consist of at least two licenses nowadays, this has been an issue for LLaMA where the code for the model architecture and weights are differently licensed. Similarly, for datasets, the LAOIN dataset has faced criticism because it deflects responability by referring to itself as an index by researchers. #datascience #machinelearning #laoin #llama #copyright  LLama license issue: https://github.com/meta-llama/llama/pull/234 LAOIN copyright: https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead Interconnects: https://www.interconnects.ai/p/an-open-source-llm",
      "upload_date": "2024-05-15",
      "total_views": 2613,
      "max_views": 2613,
      "topics": [
        "content",
        "copyright",
        "data",
        "laoin",
        "llama",
        "sets"
      ],
      "search_text": "Models and datasets have specific definitions. Models consist of at least two licenses nowadays, this has been an issue for LLaMA where the code for the model architecture and weights are differently licensed. Similarly, for datasets, the LAOIN dataset has faced criticism because it deflects responability by referring to itself as an index by researchers. #datascience #machinelearning #laoin #llama #copyright  LLama license issue: https://github.com/meta-llama/llama/pull/234 LAOIN copyright: https://www.vice.com/en/article/pkapb7/a-photographer-tried-to-get-his-photos-removed-from-an-ai-dataset-he-got-an-invoice-instead Interconnects: https://www.interconnects.ai/p/an-open-source-llm content copyright data laoin llama sets Did you see the new note from corporate defining models and data sets? Wait, what do they need to define? A model is an object that takes an input and then provides an output. Apparently the pedantic lawyer thinks differently. Yes, for your blunt minds, there's two different types of licenses for models. For the model, we'll have a license that covers the architecture and the structure of it. The second license covers the weights. The stuff that you guys train and where the important information is captured. The weights can we can consider separate because they are economically valuable thanks to law and economics. Okay, well, do data sets have two licenses? I want to clarify with data sets like Leone that these data sets might contain objectionable material or some people inside them might want their content removed. But we're not worrying about that. Would the data sets creators be responsible? The data set is only defined as a set of links. Data set does not include the content there. Huh? It's the person who uses the link, the researcher that is now liable for that content. What good is the data set without the content? I mean, the whole purpose of that data set of links is to help enable you to get all the content. No, you don't ask the questions. Do you want to go back to doing impact statements for your projects? Okay.",
      "platforms": {
        "tiktok": {
          "video_id": "7369032288316804395",
          "url": "https://www.tiktok.com/@rajistics/video/7369032288316804395",
          "view_count": 2613,
          "upload_date": "2024-05-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b3a5764383984537b769e9d05c937803_1715736545~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=aE9yNi9asbaQQuIB5C8ymOoruJM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Find all 5 metrics mentioned? Unit test approach is a great way of thinking about evaluations for generative AI. Check out my yt for a longer video on evaluation. If you are evaluating code, check out BigCodeBench: https://huggingface.co/blog/leaderboard-bigcodebench #functionalcorrectness #unittests #evaluation #largelanguagemodels #rajistics #bigcode #huggingface",
      "description": "Find all 5 metrics mentioned? Unit test approach is a great way of thinking about evaluations for generative AI. Check out my yt for a longer video on evaluation. If you are evaluating code, check out BigCodeBench: https://huggingface.co/blog/leaderboard-bigcodebench #functionalcorrectness #unittests #evaluation #largelanguagemodels #rajistics #bigcode #huggingface",
      "upload_date": "2024-06-25",
      "total_views": 2612,
      "max_views": 2612,
      "topics": [
        "evaluation",
        "functionalcorrectness",
        "huggingface",
        "largelanguagemodels",
        "unit",
        "unittests"
      ],
      "search_text": "Find all 5 metrics mentioned? Unit test approach is a great way of thinking about evaluations for generative AI. Check out my yt for a longer video on evaluation. If you are evaluating code, check out BigCodeBench: https://huggingface.co/blog/leaderboard-bigcodebench #functionalcorrectness #unittests #evaluation #largelanguagemodels #rajistics #bigcode #huggingface evaluation functionalcorrectness huggingface largelanguagemodels unit unittests Did you catch that new big code benchmark that Huggingface put out this week? I don't need Python. I'm evaluating how large language models write reports and SQL without human eval. I see why you're so blue. Let me explain how this approach could lighten your load. All right, go ahead. Instead of measuring some type of semantic similarity, they took a functional correctness or unit test approach. Sounds roguish. How does that even work? The dataset consists of a number of different tasks. And for each task, there's a number of different unit tasks to pass the task. You have to pass all the unit tests. Ah, so it's really flexible in handling different types of language. And as long as Turv functionality checks out, we're good. Exactly. They focused on Python in this dataset, but you could easily build something similar for SQL. So that makes sense for code, but do you think it would help out evaluating reports? Absolutely. So think about evaluating your reports in a series of subtasks. It could be things related to the tone, the spelling, grammar, actions involved, toxicity. When you do your evaluation, the goal is to pass all those different unit tests. Genius. This is going to give my evaluations a meteoric rise.",
      "platforms": {
        "tiktok": {
          "video_id": "7384242990862175531",
          "url": "https://www.tiktok.com/@rajistics/video/7384242990862175531",
          "view_count": 2612,
          "upload_date": "2024-06-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e3c304b86ee6463da8eac95b3413f601_1719278058~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=U4W3MMzimsPpRRyLmZhxlzK%2B2j8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Many techniques for text similarity including lexical, semantic, and has strategies. Here is a short list of some popular methods: 1. FuzzyWuzzy - https://github.com/seatgeek/thefuzz 2. RapidFuzz - https://github.com/rapidfuzz/RapidFuzz 3. JellyFish - https://github.com/jamesturk/jellyfish 4. TF/IDF with NGrams -  https://bergvca.github.io/2017/10/14/super-fast-string-matching.html https://medium.com/snowflake/entity-matching-using-tf-idf-in-snowpark-python-3d1942d4ef19 5. Sentence Transformers - https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html 6. SimHash - https://github.com/sumonbis/NearDuplicateDetection",
      "description": "Many techniques for text similarity including lexical, semantic, and has strategies. Here is a short list of some popular methods: 1. FuzzyWuzzy - https://github.com/seatgeek/thefuzz 2. RapidFuzz - https://github.com/rapidfuzz/RapidFuzz 3. JellyFish - https://github.com/jamesturk/jellyfish 4. TF/IDF with NGrams -  https://bergvca.github.io/2017/10/14/super-fast-string-matching.html https://medium.com/snowflake/entity-matching-using-tf-idf-in-snowpark-python-3d1942d4ef19 5. Sentence Transformers - https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html 6. SimHash - https://github.com/sumonbis/NearDuplicateDetection",
      "upload_date": "2024-09-28",
      "total_views": 2585,
      "max_views": 2585,
      "topics": [
        "fuzzywuzzy",
        "github",
        "jellyfish",
        "rapidfuzz",
        "scale",
        "something"
      ],
      "search_text": "Many techniques for text similarity including lexical, semantic, and has strategies. Here is a short list of some popular methods: 1. FuzzyWuzzy - https://github.com/seatgeek/thefuzz 2. RapidFuzz - https://github.com/rapidfuzz/RapidFuzz 3. JellyFish - https://github.com/jamesturk/jellyfish 4. TF/IDF with NGrams -  https://bergvca.github.io/2017/10/14/super-fast-string-matching.html https://medium.com/snowflake/entity-matching-using-tf-idf-in-snowpark-python-3d1942d4ef19 5. Sentence Transformers - https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html 6. SimHash - https://github.com/sumonbis/NearDuplicateDetection fuzzywuzzy github jellyfish rapidfuzz scale something Is FuzzyWuzzy not doing it for you? Actually, no. Fuzzy string matching is good for shorter strings when you're looking for minor differences, but I need something that can scale. How about RapidFuzz? Oh, it uses C++ much faster. But do you have anything that can handle phonetic spellings? What about Jellyfish? Jellyfish works great to help me find similar documents that sound similar but are actually spelled differently, but I still need something that's in scale. Vector scale? Vectorize! Interesting. I take all my words, break them down into n-grams, then create a tdfidf matrix based on that. I can then use our matrix multiplications to then run cosine similarity across all of those to find out which things are similar. But you're just capturing the occurrence of words? How about something that captures the meaning of words? Vectors? Representations? What are we really? Yes, using a language model like sentence transformers now captures embeddings or representations of a sentence's meaning. There's so many options, but you need something that scales to a million documents? How about using a hash? That creates a fingerprint of the document and allows us to quickly scale across lots of documents to find near-duplicates. We've come a long way from FuzzyWuzzy.",
      "platforms": {
        "tiktok": {
          "video_id": "7419799888084389162",
          "url": "https://www.tiktok.com/@rajistics/video/7419799888084389162",
          "view_count": 2585,
          "upload_date": "2024-09-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6812544f539e43c8a2a763a8c4a4f2d4_1727556790~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=peftoVWgFRo7bPYcyU%2Fswgf89ck%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6085,
      "title": "Need a deeper dive on retrieval: I have a video on YT I will post here this weekend. Or go check it out if you want it sooner: https://youtu.be/Bnn2m4S22T4 Go read it: 1st Place solution on Kaggle's Eedi Math Competition from Raja Biswas: https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688 Training code: https://github.com/rbiswasfc/eedi-mining-misconceptions",
      "description": "Need a deeper dive on retrieval: I have a video on YT I will post here this weekend. Or go check it out if you want it sooner: https://youtu.be/Bnn2m4S22T4 Go read it: 1st Place solution on Kaggle's Eedi Math Competition from Raja Biswas: https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688 Training code: https://github.com/rbiswasfc/eedi-mining-misconceptions",
      "upload_date": "2025-01-03",
      "total_views": 2584,
      "max_views": 2584,
      "topics": [
        "data",
        "eedi",
        "kaggle",
        "models",
        "one",
        "retrievers",
        "used"
      ],
      "search_text": "Need a deeper dive on retrieval: I have a video on YT I will post here this weekend. Or go check it out if you want it sooner: https://youtu.be/Bnn2m4S22T4 Go read it: 1st Place solution on Kaggle's Eedi Math Competition from Raja Biswas: https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics/discussion/551688 Training code: https://github.com/rbiswasfc/eedi-mining-misconceptions data eedi kaggle models one retrievers used Have you seen this? A Kaggle Grandmaster shared how they beat 8,000 other people in a competition for 50k. Wow. Is it that special? The competition was on retrieval. We already used retrievers for semantic search and recommendation. Yes, but their system is extremely accurate. We can learn from them. For example, they used a mixture of retrievers. So instead of just one retriever, they used multiple retrievers that allowed them to cast a wider net. Okay, multiple retrievers, what then? Here's the cool part. They're cascade of re-rankers. The first re-ranker goes to eight, the next one goes to five. The final one takes the list of the top five to give you the ranked top five. Very cool. So were these out of the box models they used? Nope. They used Laura to fine-tune these models. They also created synthetic data, curated it, used a GPT-4O model, as a judge to figure out what is the best synthetic data. AI judging training data. More jobs lost to AI. Oh, they had more tricks. They used Claude to generate chain of thought data. Then they took that chain of thought data, put that into the training data for the re-rankers, and that helped improve performance. Wouldn't all these models be super slow? They, of course, used quantization to help speed up the models, but one little thing they did was use calibration data sets so they don't get the normal performance hit you get when you use quantization. So there's no silver bullet here. Just layers of complex solution built from experimentation. Welcome to Real World AI.",
      "platforms": {
        "tiktok": {
          "video_id": "7455762363887717663",
          "url": "https://www.tiktok.com/@rajistics/video/7455762363887717663",
          "view_count": 2584,
          "upload_date": "2025-01-03",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oQYAIGEOIAAQhDjUAEyNfAaAwsfDeFrBDqcYLI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=y9oyANxUm5qsbi%2FxT6A0wzDZk2A%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18052812337841809",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6213,
      "title": "What happened at AWS Reinvent? I got to catch up with a ton of great people and companies. Next time, I will take more pictures. I really regret not taking more.  Some of the folks I talked to included Philipp Schmid from Hugging Face, Maria Khalusova from Unstructured, Ari Kaplan from Databricks, Andrew Evans from phData. The merch I took home was from NVIDIA, Databricks, Informatica, Redis, phData, and of course AWS. @Ari Kaplan ",
      "description": "What happened at AWS Reinvent? I got to catch up with a ton of great people and companies. Next time, I will take more pictures. I really regret not taking more.  Some of the folks I talked to included Philipp Schmid from Hugging Face, Maria Khalusova from Unstructured, Ari Kaplan from Databricks, Andrew Evans from phData. The merch I took home was from NVIDIA, Databricks, Informatica, Redis, phData, and of course AWS. @Ari Kaplan ",
      "upload_date": "2024-12-09",
      "total_views": 2566,
      "max_views": 1831,
      "topics": [
        "aws",
        "catch",
        "experience",
        "face",
        "got",
        "reinvent",
        "take",
        "ton"
      ],
      "search_text": "What happened at AWS Reinvent? I got to catch up with a ton of great people and companies. Next time, I will take more pictures. I really regret not taking more.  Some of the folks I talked to included Philipp Schmid from Hugging Face, Maria Khalusova from Unstructured, Ari Kaplan from Databricks, Andrew Evans from phData. The merch I took home was from NVIDIA, Databricks, Informatica, Redis, phData, and of course AWS. @Ari Kaplan  aws catch experience face got reinvent take ton So you went to AWS re-invent? Two grand for a fancy tech conference? Think of it like a desert getaway. Instead of margaritas, I'm sitting trying to catch my AI buzz, sitting in S3 table, wondering how many femtoseconds before AWS's Nova fades into the background. Couldn't you just watch the sessions online from your desk? Sure. And part of me likes staying in my pajamas and watching those YouTube videos online. But at the conference, I actually got to bug AWS engineers until I got answers about my specific issues. And that's worth two grand. It was more than that. I got to reconnect with old colleagues, kind of not just a quick 30-second congratulations on LinkedIn, but just to actually get in the chat face-to-face with them. Plus, I met a bunch of new folks who shared their unfiltered experiences with them. So, networking basically. And in the Expo, I was able to get a ton of demos in one place. I didn't have to schedule calls with various vendors that can take weeks. I could just stroll around, quickly kind of dive in, ask my questions. And since I was a little special, they gave me a few t-shirts and water bottles as well. So $2,000 basically got you answers, friends and freebies? Exactly. And hey, that's still less than your AWS bill when you left the GPUs running on weekends.",
      "platforms": {
        "tiktok": {
          "video_id": "7446240702964583710",
          "url": "https://www.tiktok.com/@rajistics/video/7446240702964583710",
          "view_count": 1831,
          "upload_date": "2024-12-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/osWEIAB1hAJRdcvisX2ZnlAFAIHBBZi8vAEbq~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=NUFXGY1Okrhf6jU%2BaYMHbnJPPeY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17862360732291201",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cPquVb7-WOg",
          "url": "https://www.youtube.com/watch?v=cPquVb7-WOg",
          "view_count": 735,
          "upload_date": "2024-12-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6204,
      "title": "Feature engineering ",
      "description": "Feature engineering ",
      "upload_date": "2025-03-02",
      "total_views": 2546,
      "max_views": 2344,
      "topics": [
        "automated",
        "data",
        "engineering",
        "feature",
        "hands",
        "might",
        "model",
        "notebook",
        "openfe",
        "raw",
        "using",
        "walkthrough"
      ],
      "search_text": "Feature engineering  automated data engineering feature hands might model notebook openfe raw using walkthrough Do you know why feature engineering is so important? Is that real engineering? It doesn't sound like it. Feature engineering is when we transform the raw data into better features or variables that represent the underlying problem I'm working on. Why not dump all the raw data into a deep learning model? You can try it, but the model might have trouble pulling that signal out, so which might lead to long training times or inaccurate models. Sounds like you just can't afford the compute costs. Can you show us how feature engineering will help all of us? So you can try this over at the TensorFlow playground. When we're working on this more complicated data set, you can see the model here just isn't able to figure it out. But if we take time to add some of the feature engineering from over here, the model is able to successfully classify those data. This is mind blowing. And for all, a few more examples. Some other examples might be pulling out the day of the week out of a daytime field or calculating BMI, a numerical calculation from the raw height and weight, or maybe doing some sort of aggregation where you total how many purchases were made over the last month and use that as a feature.",
      "platforms": {
        "tiktok": {
          "video_id": "7477271048707214622",
          "url": "https://www.tiktok.com/@rajistics/video/7477271048707214622",
          "view_count": 2344,
          "upload_date": "2025-03-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMl1EK7NIBApmIAFAkiyiIzADCRA4BI4IYflEi~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=I5aKvE0Mwbk0aQBtfcvNBVIojjs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17950959386926631",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Q84t1TeK-LY",
          "url": "https://www.youtube.com/watch?v=Q84t1TeK-LY",
          "view_count": 202,
          "upload_date": "2024-09-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Explanations for transformers gently #datascience #codetok #deeplearning ",
      "description": "Explanations for transformers gently #datascience #codetok #deeplearning ",
      "upload_date": "2022-08-18",
      "total_views": 2543,
      "max_views": 2543,
      "topics": [
        "codetok",
        "data",
        "datascience",
        "deeplearning",
        "explanations",
        "features"
      ],
      "search_text": "Explanations for transformers gently #datascience #codetok #deeplearning  codetok data datascience deeplearning explanations features One of the hardest things for data scientists is to understand the machine learning models they're building Machine learning models are really good at predictions because they figure out complex interactions Between the different variables or features that go into the model But the hard part for data scientists is that's not clear exactly how all that stuff is happening One tool to give data scientists insights into understanding what's going on in models as well as sometimes if you have to explain What's going on to somebody else another stakeholder or somebody else who's using it? Is to use something called explanations? Explanations help us understand individual predictions. Let's say this example where we have age gender blood pressure and BMI that goes into the model as variables or features the model then produces an output But exactly what were the effect of those variables? This is where explanations come in So this is an example using shop explanations And what we can see is the relative Contribution of each of these features to the prediction so we can see for example that age was a huge factor in Moving that prediction up while gender kind of counter veiled it So this helps us better understand like what's going on with these features when we start working with text This gets even more complicated because text Expands the feature space because there's so many words or tokens that often come up when we're doing a classification with them So today I shared a number of different approaches we can use to help get explanations As you can see the tweet totally blew up But I wanted to also let everybody know that follows me on tick tock About it and you can go ahead and use my handle logistics and you'll find it in a number of other places online",
      "platforms": {
        "tiktok": {
          "video_id": "7133366384956067118",
          "url": "https://www.tiktok.com/@rajistics/video/7133366384956067118",
          "view_count": 2543,
          "upload_date": "2022-08-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a1e586ccc9e64e7faee2c077146f829f_1660866294~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=D57o2rooOIHSBNUvriTRGac44cM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Isolation Forests are an anomaly detection algorithm that builds trees to partition data, isolating \"lonely\" points or outliers with fewer partitions. This makes them efficient and effective for identifying anomalies, such as in fraud detection, using simple and accessible methods available in various open-source packages. #rajistics #madeinhotelroom #anomalydetection ",
      "description": "Isolation Forests are an anomaly detection algorithm that builds trees to partition data, isolating \"lonely\" points or outliers with fewer partitions. This makes them efficient and effective for identifying anomalies, such as in fraud detection, using simple and accessible methods available in various open-source packages. #rajistics #madeinhotelroom #anomalydetection ",
      "upload_date": "2024-08-03",
      "total_views": 2527,
      "max_views": 2527,
      "topics": [
        "anomalydetection",
        "data",
        "detection",
        "isolation",
        "lonely",
        "madeinhotelroom"
      ],
      "search_text": "Isolation Forests are an anomaly detection algorithm that builds trees to partition data, isolating \"lonely\" points or outliers with fewer partitions. This makes them efficient and effective for identifying anomalies, such as in fraud detection, using simple and accessible methods available in various open-source packages. #rajistics #madeinhotelroom #anomalydetection  anomalydetection data detection isolation lonely madeinhotelroom I'm looking for lonely data. Can you help? Do you mean like outlier or anomaly detection? What about the isolation forest algorithm? Huh? Forests? Isn't that when you want to classify data? Where you build a bunch of trees and then it partitions the data into groups? It's similar. When we build the partitions, we look for isolated or lonely points away from everybody else. Huh, I get it. I can see that the data and the features for these are really different, which makes it easy to partition off and isolate from the others. And just like decision trees in random forests, this is a robust algorithm that works great with tabular data. This is perfect. It'll get my fraud team off my back that was looking for a way to identify anomalies. Isolation forest is one of my favorites. It's a great starting approach for anomalies detection. But remember, there's many ways data can be lonely. That's why we have lots of different algorithms. So what's it going to cost me to find this lonely data? No worries. We have plenty of open source packages in our Java and Python.",
      "platforms": {
        "tiktok": {
          "video_id": "7398899495800524074",
          "url": "https://www.tiktok.com/@rajistics/video/7398899495800524074",
          "view_count": 2527,
          "upload_date": "2024-08-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d067d1f419104e31958bf1032b5401ee_1722690545~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=NRxgBYxTbacahP48jtGq%2B84y9g0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "The Keras versus Pytorch benchmarking drama. This isn't about picking sides.  I want to point out how difficult it is to do these sorts of benchmarks. The lesson here is to be skeptical of any vendor/open-source benchmarks that sound too good to be true. I have spent the last 8 years explaining to people why competitor XYZ's performance isn't 8x better 🙄. I have admired how Soumith from Pytorch has handled this. There are plenty of people with much less class in AI. Keras Benchmarks: https://keras.io/getting_started/benchmarks/ Pytorch response: https://twitter.com/soumithchintala/status/1776311683385880983 #benchmarks #keras #python #rajistics",
      "description": "The Keras versus Pytorch benchmarking drama. This isn't about picking sides.  I want to point out how difficult it is to do these sorts of benchmarks. The lesson here is to be skeptical of any vendor/open-source benchmarks that sound too good to be true. I have spent the last 8 years explaining to people why competitor XYZ's performance isn't 8x better 🙄. I have admired how Soumith from Pytorch has handled this. There are plenty of people with much less class in AI. Keras Benchmarks: https://keras.io/getting_started/benchmarks/ Pytorch response: https://twitter.com/soumithchintala/status/1776311683385880983 #benchmarks #keras #python #rajistics",
      "upload_date": "2024-04-06",
      "total_views": 2527,
      "max_views": 2527,
      "topics": [
        "benchmarks",
        "compare",
        "going",
        "keras",
        "python",
        "pytorch"
      ],
      "search_text": "The Keras versus Pytorch benchmarking drama. This isn't about picking sides.  I want to point out how difficult it is to do these sorts of benchmarks. The lesson here is to be skeptical of any vendor/open-source benchmarks that sound too good to be true. I have spent the last 8 years explaining to people why competitor XYZ's performance isn't 8x better 🙄. I have admired how Soumith from Pytorch has handled this. There are plenty of people with much less class in AI. Keras Benchmarks: https://keras.io/getting_started/benchmarks/ Pytorch response: https://twitter.com/soumithchintala/status/1776311683385880983 #benchmarks #keras #python #rajistics benchmarks compare going keras python pytorch So I was assigned to build some internal benchmarks for Keras. We could get some major hype for this. Hey, can I lend a hand? I was asked to compare the performance of the three different backends for Keras. Why do we have three backends? We like to build a lot of competing products. We're not too focused. What is this comparison to PyTorch you're proposing? Don't worry about it. We're just going to add some benchmarks to a PyTorch reference implementation. Benchmarking across different frameworks can be complicated. Shouldn't we get the PyTorch team involved? It's no big deal. Comminions. How can we tilt the benchmarks in our favor? How about we compare FP32 to TF32? I like it. Floating point 32 is going to be a larger number space, which means it's going to be more accurate, but it's going to be less speed. Let's use that. We could focus on models where we've optimized and then compare them to an older, unoptimized version. Now you're thinking, I know we compare the inference speed on the implementation of HuggingFaces model, uses a safety model when we don't use a safety model. Speed boost. Let's put this front and center in our documentation. Isn't this misleading? Shouldn't we at least point out these major differences or at least ask the PyTorch team for their view? Oh no. This kind of message with these huge differences is going to grab us a lot of attention.",
      "platforms": {
        "tiktok": {
          "video_id": "7354541665027165483",
          "url": "https://www.tiktok.com/@rajistics/video/7354541665027165483",
          "view_count": 2527,
          "upload_date": "2024-04-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d0966ee26acb422c8479fc3f1036197c_1712362693~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=TtFweY%2FltYEbEmJqF%2BYyqqR0o9Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Getting prediction intervals with conformal prediction. This is a very simple intro, it can do much more. #datascience #statistics #predictioninterval #conformalprediction ",
      "description": "Getting prediction intervals with conformal prediction. This is a very simple intro, it can do much more. #datascience #statistics #predictioninterval #conformalprediction ",
      "upload_date": "2022-09-21",
      "total_views": 2526,
      "max_views": 2526,
      "topics": [
        "data",
        "datascience",
        "let",
        "prediction",
        "predictioninterval",
        "statistics"
      ],
      "search_text": "Getting prediction intervals with conformal prediction. This is a very simple intro, it can do much more. #datascience #statistics #predictioninterval #conformalprediction  data datascience let prediction predictioninterval statistics Let me share a dirty secret of data science. Data scientists often make it sound like their predictions are very precise, but actually there's a ton of variability in their prediction. Giving people the range of predictions is super useful. Let me show you how you can do this for any model with conformal predictions. Let's start with training a traditional machine learning model. Then we're going to use a separate split of the data for calibrating the prediction intervals. Let's get the predictions on this and now look at the error. All models have error. Here you can see the difference between the prediction and the actual value. We'll take all those errors, put them together, figure out where the 95 percentile cutoff is, which is what I'm choosing for the prediction interval here, and the result for this data set is 1.3. I'm going to take the error now, plot it out with a histogram, and then draw a line at the 95 percent quantile. This will help me figure out what the width should be for my prediction interval. Let's apply this to a few data points and see what it looks like. Here you can see for all of these points exactly what the prediction interval is, and if I overlay the actuals of what actually happened you'll see that most of the points should be landed within the prediction interval. There you have it. For any model we can get prediction intervals. I'll put a longer version of this along with the notebook out on the tubes.",
      "platforms": {
        "tiktok": {
          "video_id": "7145960334270975275",
          "url": "https://www.tiktok.com/@rajistics/video/7145960334270975275",
          "view_count": 2526,
          "upload_date": "2022-09-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/93bb5f66cc7b4a15abb7b61777fe7db3_1663798555~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=J86nnFFiRwMaHrF8dhe%2Fx%2F4p%2BmA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6202,
      "title": "ComfyUI",
      "description": "ComfyUI",
      "upload_date": "2025-03-07",
      "total_views": 2524,
      "max_views": 2524,
      "topics": [
        "amazing",
        "comfy",
        "comfyui",
        "easy",
        "images",
        "let",
        "start"
      ],
      "search_text": "ComfyUI amazing comfy comfyui easy images let start Aren't these images amazing? Let me show you how to do it. And you don't have to do any coding. I can give you an easy experience for doing this. Comfy UI is a radical new UI for easily creating images with stable diffusion. You start with a block for where your model is. Let's add some blocks for the prompts of positive and negative prompt. Add a block for your input to your image that's going to come in. And then one for the output as well. And that's what it takes to start building with Comfy UI. Amazing. I could do this. Once you get the basics down, you can do more advanced workflows like using Instant ID. Instant ID is a state-of-the-art style transfer for portraits. It allows you to create images where you go from here to here. Comfy UI supports a ton of new techniques like this. So let's summarize Comfy UI. Comfy UI is lightweight, very configurable, it's transparent, easy to share and good for prototyping. All this transparency can be overwhelming with complex workflows, so start slowly.",
      "platforms": {
        "tiktok": {
          "video_id": "7478852351604903199",
          "url": "https://www.tiktok.com/@rajistics/video/7478852351604903199",
          "view_count": 2524,
          "upload_date": "2025-03-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIKfEJFTASVk1C2KEADAIiywRRnAOCDEAjEXfu~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=zBfVnb3E3LjOB1Ru24H7kmxPRP8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17948137574943315",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5958,
      "title": "Synthetic data improves model performance only when it expands coverage rather than replicating existing distributions. Using LLMs to generate data from the same examples can inflate metrics without improving generalization. Effective use targets edge cases—rare inputs, tricky prompts, or underrepresented scenarios—adding diversity, not density.",
      "description": "Synthetic data improves model performance only when it expands coverage rather than replicating existing distributions. Using LLMs to generate data from the same examples can inflate metrics without improving generalization. Effective use targets edge cases—rare inputs, tricky prompts, or underrepresented scenarios—adding diversity, not density.",
      "upload_date": "2025-10-08",
      "total_views": 2520,
      "max_views": 1454,
      "topics": [
        "data",
        "evaluation",
        "examples",
        "expands",
        "going",
        "improves",
        "llm",
        "model",
        "performance",
        "synthetic",
        "think",
        "training",
        "use"
      ],
      "search_text": "Synthetic data improves model performance only when it expands coverage rather than replicating existing distributions. Using LLMs to generate data from the same examples can inflate metrics without improving generalization. Effective use targets edge cases—rare inputs, tricky prompts, or underrepresented scenarios—adding diversity, not density. data evaluation examples expands going improves llm model performance synthetic think training use My ANA app is underperforming, but the influencers online said, scale is everything and that I should scale up with synthetic data. Interesting. So what new data are you going to add? I was going to use an LLM to make more examples from the few examples I have, then use those more examples for training and evaluation. Have you thought this through? I think it's time to summon the ghost of smote. Is smote a new LLM I could use? I was the pioneer of synthetic data. Promise balanced, deliver confusion, ended up interpolating the obvious. So when synthetic data comes from the same distribution, it's like photocopying a blurry photo. The picture gets bigger, but not clear. Like your metrics might rise, but the understanding of your problem isn't going to. But isn't there a whole industry around synthetic data? It all just can't be hype. Oh, true. Synthetic data can be helpful, but think about it as expanding coverage and hitting niche use cases. Like some not more data, but more diverse data. Yeah, you're getting it. Think of synthetic data, not as stuffing, but as seasoning. You add a little bit more for where the flavors missing. Sounds messy. Not going to do it.",
      "platforms": {
        "tiktok": {
          "video_id": "7558659318296366366",
          "url": "https://www.tiktok.com/@rajistics/video/7558659318296366366",
          "view_count": 1066,
          "upload_date": "2025-10-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4BgwAfJKcIAbwIDCTAlEdiijdA8A0lKBdiuBl~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=51v%2F5FFhcaG7ra33WuUo6J%2BX8OI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18082533614480662",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Y2sD4MTdTWg",
          "url": "https://www.youtube.com/watch?v=Y2sD4MTdTWg",
          "view_count": 1454,
          "upload_date": "2025-10-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6067,
      "title": "AG (Retrieval Augmented Generation) addresses fundamental limitations of base LLMs like ChatGPT, which can generate incorrect technical information when operating without access to specific documentation. Rather than attempting to load entire document collections into a single context window, which becomes impractical with large or frequently updated documentation sets, RAG implements a two-stage process: first retrieving relevant sections from a document store, then using these specific sections to generate responses. This approach ensures responses are grounded in actual documentation while providing traceability to source materials.",
      "description": "AG (Retrieval Augmented Generation) addresses fundamental limitations of base LLMs like ChatGPT, which can generate incorrect technical information when operating without access to specific documentation. Rather than attempting to load entire document collections into a single context window, which becomes impractical with large or frequently updated documentation sets, RAG implements a two-stage process: first retrieving relevant sections from a document store, then using these specific sections to generate responses. This approach ensures responses are grounded in actual documentation while providing traceability to source materials.",
      "upload_date": "2025-01-26",
      "total_views": 2514,
      "max_views": 1813,
      "topics": [
        "answer",
        "beam",
        "better",
        "context",
        "documentation",
        "explained",
        "getting",
        "grounded",
        "like",
        "oldie",
        "pretty",
        "rag",
        "relevant",
        "search",
        "still",
        "text"
      ],
      "search_text": "AG (Retrieval Augmented Generation) addresses fundamental limitations of base LLMs like ChatGPT, which can generate incorrect technical information when operating without access to specific documentation. Rather than attempting to load entire document collections into a single context window, which becomes impractical with large or frequently updated documentation sets, RAG implements a two-stage process: first retrieving relevant sections from a document store, then using these specific sections to generate responses. This approach ensures responses are grounded in actual documentation while providing traceability to source materials. answer beam better context documentation explained getting grounded like oldie pretty rag relevant search still text Our customers have all these great technical questions, but they keep using chat Jebeta to answer them. What's wrong with that? Isn't AI supposed to be helpful? It's the hallucinations. It constantly told our customer that the Acme 3000 came with an instant hole. And the customer tried to implement it. Whoa, it does that? I thought it knew everything. It's been trained on generic internet data. It doesn't have access to our latest technical documentation. Instead, it's too eager to please and just make something up. Hold on. Can't you just take your documents and load them up into the long context window? No, I don't have one document. I have hundreds of documents. These documents are changing on a regular basis. You just can't load them into your long context window. Sounds like you need RAG, Retrieval Augmented Generation. So RAG is what we use to clean up an AI mess. Actually, RAG is like having a really smart librarian who doesn't just find the right books, but finds the right section in the book you need. The LLM then takes those relevant section, uses it to craft an answer that's grounded in that context. Whoa, my brain needs a demo before it turns into a ragatoni. Let me show you. I ask a question here. I've got a grounded answer. And I know it's grounded because I can take a look and see the source for all the sentences here and know exactly what material was used to come up with this answer. Looks like the only hole our customers are getting is a hole answer from rags to riches.",
      "platforms": {
        "tiktok": {
          "video_id": "7464374439208144158",
          "url": "https://www.tiktok.com/@rajistics/video/7464374439208144158",
          "view_count": 1813,
          "upload_date": "2025-01-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogCioyRSE56CKryBbWuBfitISVAiAA2AAEEzIV~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=MDTEh%2F3d00EyJedVoJ7u8jVJr60%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18346757473183658",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "SAp7srQYEjU",
          "url": "https://www.youtube.com/watch?v=SAp7srQYEjU",
          "view_count": 701,
          "upload_date": "2025-03-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Intro for AI Literacy #datascience #machinelearning #ai #programming #literacy #alleninstitute",
      "description": "Intro for AI Literacy #datascience #machinelearning #ai #programming #literacy #alleninstitute",
      "upload_date": "2022-01-14",
      "total_views": 2496,
      "max_views": 2496,
      "topics": [
        "ai",
        "alleninstitute",
        "datascience",
        "literacy",
        "machinelearning",
        "programming"
      ],
      "search_text": "Intro for AI Literacy #datascience #machinelearning #ai #programming #literacy #alleninstitute ai alleninstitute datascience literacy machinelearning programming",
      "platforms": {
        "tiktok": {
          "video_id": "7052856849606954287",
          "url": "https://www.tiktok.com/@rajistics/video/7052856849606954287",
          "view_count": 2496,
          "upload_date": "2022-01-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6177,
      "title": "5 things to look for when a new model is announced License, Size of the Model, Benchmarks, Training Data/Details, Fine-Tuning & Tech Specs",
      "description": "5 things to look for when a new model is announced License, Size of the Model, Benchmarks, Training Data/Details, Fine-Tuning & Tech Specs",
      "upload_date": "2025-04-19",
      "total_views": 2492,
      "max_views": 1725,
      "topics": [
        "benchmarks",
        "billion",
        "context",
        "data",
        "fine",
        "including",
        "license",
        "llm",
        "long",
        "model",
        "new",
        "nolima",
        "training"
      ],
      "search_text": "5 things to look for when a new model is announced License, Size of the Model, Benchmarks, Training Data/Details, Fine-Tuning & Tech Specs benchmarks billion context data fine including license llm long model new nolima training Have you checked out the new AI model that Hooli dropped? I noticed the license wasn't Apache or MIT, which means now I have to go ask legal. It does say commercial use is permissible. Is this another one of those huge models over a hundred billion parameters? No, it comes in a seven billion and a 20 billion parameter size, so pretty usable. Nothing more annoying than a free model that takes an entire cluster to run. We've got some good benchmark results on MMLU and human eval for coding. I know they can be game, but it's looking promising. I'm always skeptical about those benchmarks. Did they tell us anything about the training data or the training process they used? The Hooli model isn't telling us about the training data, but they did tell us a little bit about the training process, like how much training data they used and how much compute they've used. Does it work with my standard libraries? Can I fine tune this? The architecture is pretty standard and they've already released code so you can fine tune the model. That's great. Can you go evaluate this model on our data sets so we can see how it matters for us? And by the time you're done, Legal Ward will probably finish looking at the license and I bet there'll be a new model for you to look at.",
      "platforms": {
        "tiktok": {
          "video_id": "7495136570245188895",
          "url": "https://www.tiktok.com/@rajistics/video/7495136570245188895",
          "view_count": 1725,
          "upload_date": "2025-04-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oUlmxAKB0BEzHuziiAAghAEiCiXEfAcAEIIxAL~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=YyAW3Dm9yIMtSGw7JdNI4ob%2BD4c%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17955151037931259",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OR79Bpt0QOE",
          "url": "https://www.youtube.com/watch?v=OR79Bpt0QOE",
          "view_count": 767,
          "upload_date": "2025-04-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Got some time this weekend? Go build a web demo. #datascience #statistics #shinyr #rstats #python #gradio #streamlit ",
      "description": "Got some time this weekend? Go build a web demo. #datascience #statistics #shinyr #rstats #python #gradio #streamlit ",
      "upload_date": "2022-11-25",
      "total_views": 2491,
      "max_views": 2491,
      "topics": [
        "datascience",
        "fuck",
        "powerpoint",
        "rstats",
        "shinyr",
        "statistics"
      ],
      "search_text": "Got some time this weekend? Go build a web demo. #datascience #statistics #shinyr #rstats #python #gradio #streamlit  datascience fuck powerpoint rstats shinyr statistics Fuck PowerPoint! There is no power in those points and no point in those presentations. We have been brainwashed into believing that PowerPoint gives us the power to express ourselves creatively. But the only power it has is to kill our souls. Fuck PowerPoint and fuck your Excel spreadsheet too. Those columns and rows represent the metal bars of the prison cell of your brain. Free yourself from the soul-killing software of Excel and PowerPoint. And live a more meaningful existence. Fuck PowerPoint and fuck Excel.",
      "platforms": {
        "tiktok": {
          "video_id": "7169979815188090154",
          "url": "https://www.tiktok.com/@rajistics/video/7169979815188090154",
          "view_count": 2491,
          "upload_date": "2022-11-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/388c5a8ae11947fcb87a20131902d05d_1669391042~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=Wo3vIY82MbWdZkkHP4aMrr3rJMY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Open source can be a lot of work #opensource #techtok #programming #python #github",
      "description": "Open source can be a lot of work #opensource #techtok #programming #python #github",
      "upload_date": "2022-04-15",
      "total_views": 2477,
      "max_views": 2477,
      "topics": [
        "another",
        "github",
        "opensource",
        "programming",
        "python",
        "techtok"
      ],
      "search_text": "Open source can be a lot of work #opensource #techtok #programming #python #github another github opensource programming python techtok Another one, thank you. Another one, thank you. Another one, thank you.",
      "platforms": {
        "tiktok": {
          "video_id": "7086657703849790763",
          "url": "https://www.tiktok.com/@rajistics/video/7086657703849790763",
          "view_count": 2477,
          "upload_date": "2022-04-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/110e11fadacb40ad96b9e97d46b44905_1649991077~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=v0lBFsuoZbE9QQWYJqhGHdn0n%2FI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6080,
      "title": "It’s not easy to answer questions. Techniques like multi retrieval and multi hop we use every day without thinking about it. However, with AI, we need to teach it to use these techniques. If not, we get poor performance when we ask AI (like in RAG use cases). ",
      "description": "It’s not easy to answer questions. Techniques like multi retrieval and multi hop we use every day without thinking about it. However, with AI, we need to teach it to use these techniques. If not, we get poor performance when we ask AI (like in RAG use cases). ",
      "upload_date": "2025-01-10",
      "total_views": 2467,
      "max_views": 2058,
      "topics": [
        "1",
        "answer",
        "eedi",
        "hop",
        "kaggle",
        "like",
        "multi",
        "questions",
        "rag",
        "retrieval",
        "secrets",
        "solution",
        "use"
      ],
      "search_text": "It’s not easy to answer questions. Techniques like multi retrieval and multi hop we use every day without thinking about it. However, with AI, we need to teach it to use these techniques. If not, we get poor performance when we ask AI (like in RAG use cases).  1 answer eedi hop kaggle like multi questions rag retrieval secrets solution use Do you know how I separate the A students from the C students? With the B? Is this about tests or reg? I use a lot of straightforward questions. Single hop questions like this, where there's just one easy answer. But then I throw in some multi-retrieval questions where you actually need to break down the query. Hey, that's exactly what I did on your question about comparing Tesla and Rivian battery technologies. I looked at the question, thought of three other similar questions that could all help me answer it. So I looked all of those three up, took the responses from all of them, fused those together to get the right answer. To make it more difficult, I had multi-hop questions like this. Yeah, I took the original query, I broke it up into separate individual steps and did one after another and pieced it all together until I figured out that it was World War II. That's great. And this is the challenging part. You have to be able to understand all the different parts of how to solve a problem to figure out the necessary steps. Wait a minute. Is this why our enterprise rag solution is only getting 70% because people keep asking it multi-hop queries? And that's what makes enterprise rag so difficult because you have multi-retrieval, multi-hop. You need to use techniques like query decomposition. So what you're saying is tests and rag are both like academic obstacle courses? More like academic hopscotch. One wrong hop and it be over.",
      "platforms": {
        "tiktok": {
          "video_id": "7458414120518389023",
          "url": "https://www.tiktok.com/@rajistics/video/7458414120518389023",
          "view_count": 2058,
          "upload_date": "2025-01-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0ZCAysfCBnnFEA4EwCFRnRDAEcEVzAnTegsIA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=WsR25jiPI9l%2BxGPbh26sex9AJBs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18083445235598957",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AFXEw2wEKd0",
          "url": "https://www.youtube.com/watch?v=AFXEw2wEKd0",
          "view_count": 409,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6078,
      "title": "Statistics leverages randomness across machine learning applications, including random forests, dropout in neural networks, and hyperparameter optimization through random search. Random sampling techniques can significantly reduce computational overhead by using small, representative data samples (like 1% of a 300GB dataset) while maintaining reliable results, based on the principle of independent and identically distributed (IID) data. The effectiveness of random sampling is analogous to tasting a single spoonful of soup to assess its seasoning, rather than needing to consume the entire pot.",
      "description": "Statistics leverages randomness across machine learning applications, including random forests, dropout in neural networks, and hyperparameter optimization through random search. Random sampling techniques can significantly reduce computational overhead by using small, representative data samples (like 1% of a 300GB dataset) while maintaining reliable results, based on the principle of independent and identically distributed (IID) data. The effectiveness of random sampling is analogous to tasting a single spoonful of soup to assess its seasoning, rather than needing to consume the entire pot.",
      "upload_date": "2025-01-13",
      "total_views": 2459,
      "max_views": 1356,
      "topics": [
        "data",
        "evolving",
        "learning",
        "like",
        "random",
        "randomness",
        "reinforcement",
        "rler",
        "rubrics",
        "sampling",
        "statistics",
        "tulu",
        "using"
      ],
      "search_text": "Statistics leverages randomness across machine learning applications, including random forests, dropout in neural networks, and hyperparameter optimization through random search. Random sampling techniques can significantly reduce computational overhead by using small, representative data samples (like 1% of a 300GB dataset) while maintaining reliable results, based on the principle of independent and identically distributed (IID) data. The effectiveness of random sampling is analogous to tasting a single spoonful of soup to assess its seasoning, rather than needing to consume the entire pot. data evolving learning like random randomness reinforcement rler rubrics sampling statistics tulu using What uses statistics to a software engineer? It all seems so random. Randomness in statistics is a thing, and it's pretty powerful. Techniques like random sampling can dramatically reduce compute time, randomize trials, help us figure out causal inference, and using randomness in algorithms leads to stronger models. Think about random force in machine learning, dropout in neural networks, or even using a random search with hyperparameter optimization. Whoa, you're overloading me. Can you just slow down and explain why random would be useful for something like sampling? Of course, let's say you have 300 gigabytes of data. You don't need to use all of that data. A small sample, say 1% of that, can give you equally reliable results. Really, I think you'd want to use all the data. Suppose you wanted to figure out if your soup needed more salt. You'd probably just take a spoonful of salt. You wouldn't drink the whole pot to figure it out. Random sampling here is assuming the data is independent and identically distributed. Ah, just like when I randomly look at one line of your code and know that everything you've written is atrocious.",
      "platforms": {
        "tiktok": {
          "video_id": "7459187705629445407",
          "url": "https://www.tiktok.com/@rajistics/video/7459187705629445407",
          "view_count": 1356,
          "upload_date": "2025-01-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMCpEICnEAAIwCRyAEa7fAeAE8cDhFhuFVKWpE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=KQGrUYPlR%2F36SnYIzpUMQ4ifwMk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18053627480008599",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "yvt350gEFUs",
          "url": "https://www.youtube.com/watch?v=yvt350gEFUs",
          "view_count": 1103,
          "upload_date": "2025-11-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Don't overspend getting into data science. This eposide is dedicated to the snap-on and ikon controversy. Reminders: Basic macbook, Google colab, consumer NVIDIA GPUs, and courses/bootcamps structure the learning process, you still need to do the hard work (and all the material in those courses is available for free.)",
      "description": "Don't overspend getting into data science. This eposide is dedicated to the snap-on and ikon controversy. Reminders: Basic macbook, Google colab, consumer NVIDIA GPUs, and courses/bootcamps structure the learning process, you still need to do the hard work (and all the material in those courses is available for free.)",
      "upload_date": "2024-08-20",
      "total_views": 2450,
      "max_views": 2450,
      "topics": [
        "data",
        "don",
        "get",
        "gpus",
        "macbook",
        "science"
      ],
      "search_text": "Don't overspend getting into data science. This eposide is dedicated to the snap-on and ikon controversy. Reminders: Basic macbook, Google colab, consumer NVIDIA GPUs, and courses/bootcamps structure the learning process, you still need to do the hard work (and all the material in those courses is available for free.) data don get gpus macbook science What are three things that are overpriced for somebody getting into data science? Let's start with laptops. Basic MacBook, especially if it has 16 gigabytes of memory, is all you need to start off with. But if you get a maxed out one, you'll feel superior to everyone else. Why a MacBook and not a Windows machine? A lot of data science tooling is built around Unix. So starting to get familiar with the Unix platform, which a MacBook does give you a gentle introduction into Unix is going to be helpful. I keep a separate Windows machine just for gaming. If you have a spare gaming computer and videos, consumer GPUs are a great way to get exposed to GPUs. But don't worry if not, take advantage of Google Colab. It's so good to use. A local machine is nice for running experiments. Don't get that cloud anxiety over those hourly bills. Be careful about spending a lot of money on courses and boot camps because for data science, all the resources are out there available for free. If you want to, easy to get started. It's a good reality check. Sometimes it feels like if I just sign up for the course, I'll be guaranteed job and a bunch of knowledge. Doing projects is key. Yeah, this reminds me, I need to go cancel some of my gym memberships.",
      "platforms": {
        "tiktok": {
          "video_id": "7405010591414340906",
          "url": "https://www.tiktok.com/@rajistics/video/7405010591414340906",
          "view_count": 2450,
          "upload_date": "2024-08-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4c2aa123ea184e669ec897040c61e2df_1724113391~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=OcYpSaBgqet0ihh7zpAx3rrq5EU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5984,
      "title": "AI agents used to shut down mid-task or hallucinate vending empires. Now? They're beating humans at long-horizon business simulations. From 8% task success with GPT‑4o to 30%+ with Claude and Gemini, benchmarks like AgentCompany and Vending-Bench show agents aren’t just smarter — they’re starting to work. TheAgentCompany Benchmark (CMU): https://arxiv.org/abs/2412.14161 Vending-Bench (Andon Labs): https://arxiv.org/abs/2502.15840 Project Vend (Anthropic): https://www.anthropic.com/research/project-vend-1 Claude/Gemini benchmark updates: https://x.com/andonlabs/status/1805322416206078341",
      "description": "AI agents used to shut down mid-task or hallucinate vending empires. Now? They're beating humans at long-horizon business simulations. From 8% task success with GPT‑4o to 30%+ with Claude and Gemini, benchmarks like AgentCompany and Vending-Bench show agents aren’t just smarter — they’re starting to work. TheAgentCompany Benchmark (CMU): https://arxiv.org/abs/2412.14161 Vending-Bench (Andon Labs): https://arxiv.org/abs/2502.15840 Project Vend (Anthropic): https://www.anthropic.com/research/project-vend-1 Claude/Gemini benchmark updates: https://x.com/andonlabs/status/1805322416206078341",
      "upload_date": "2025-07-06",
      "total_views": 2446,
      "max_views": 1609,
      "topics": [
        "agentcompany",
        "agents",
        "benchmark",
        "claude",
        "learning",
        "like",
        "task",
        "vending",
        "work"
      ],
      "search_text": "AI agents used to shut down mid-task or hallucinate vending empires. Now? They're beating humans at long-horizon business simulations. From 8% task success with GPT‑4o to 30%+ with Claude and Gemini, benchmarks like AgentCompany and Vending-Bench show agents aren’t just smarter — they’re starting to work. TheAgentCompany Benchmark (CMU): https://arxiv.org/abs/2412.14161 Vending-Bench (Andon Labs): https://arxiv.org/abs/2502.15840 Project Vend (Anthropic): https://www.anthropic.com/research/project-vend-1 Claude/Gemini benchmark updates: https://x.com/andonlabs/status/1805322416206078341 agentcompany agents benchmark claude learning like task vending work You've seen these headlines, how AI fired co-workers, visited Simpson, wears a red tie, everyone laugh. But AI isn't laughing, it knows. Failure isn't an option, it's grinding, it's getting better. Not just at math or code, but the kind of work people do every day. Now in the agent company benchmark, they simulated a real workplace, tasks across engineering, HR, finance. They used tools like GitLab and Slack. Early models like GPT40, yeah, they didn't do great, like 8%. But then as our models got better, adding more structure, more tools, success has gone up to nearly 30%. And it's still grinding right now while you're scrolling. Want a tougher test? Look at vending benchmark, which puts agents in charge of a vending business. Restock, price, pay rent. Early models were chaos, they missed what they have ordered, they missed deadlines, they shut themselves down. But last week, Claude Opus 4 changed the game. It beat a human baseline on its worst run. It priced a demand, managed inventory, made a profit, constantly grinding. Yeah, these are still simulations, but it's the closest we have to real workflows. And these agents just aren't answering simple prompts anymore. They're making decisions, they're using tools, they're recovering from decisions, they're learning how to work. Now, they're not perfect, but you know what? It's not funny anymore.",
      "platforms": {
        "tiktok": {
          "video_id": "7523992803123285279",
          "url": "https://www.tiktok.com/@rajistics/video/7523992803123285279",
          "view_count": 1609,
          "upload_date": "2025-07-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oE6AI5loAEtCU8q0IiMAi03D7iI6ICiqI5TNfB~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=uX9jh7JldiYeaqiRmFuNAYPKNrk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18521108047057481",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-07-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "VKN4wlNJnbg",
          "url": "https://www.youtube.com/watch?v=VKN4wlNJnbg",
          "view_count": 837,
          "upload_date": "2025-07-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6164,
      "title": "This video explains why FP16 (16-bit floating point) isn't always suitable for training neural networks due to instability caused by limited dynamic range—leading to overflow and underflow errors. To address this, Google's Brain team introduced bfloat16, a floating point format with more exponent bits to better handle training. For inference, the video highlights quantization, a technique that reduces model precision (e.g., to int8 or even int4) to drastically shrink model size—enabling large models like LLaMA to run on mobile devices. However, it emphasizes the trade-off between efficiency and potential loss in accuracy.",
      "description": "This video explains why FP16 (16-bit floating point) isn't always suitable for training neural networks due to instability caused by limited dynamic range—leading to overflow and underflow errors. To address this, Google's Brain team introduced bfloat16, a floating point format with more exponent bits to better handle training. For inference, the video highlights quantization, a technique that reduces model precision (e.g., to int8 or even int4) to drastically shrink model size—enabling large models like LLaMA to run on mobile devices. However, it emphasizes the trade-off between efficiency and potential loss in accuracy.",
      "upload_date": "2025-05-16",
      "total_views": 2445,
      "max_views": 2222,
      "topics": [
        "bit",
        "brain",
        "floating",
        "learning",
        "models",
        "point",
        "potential",
        "spark",
        "training",
        "transfer",
        "unlocked"
      ],
      "search_text": "This video explains why FP16 (16-bit floating point) isn't always suitable for training neural networks due to instability caused by limited dynamic range—leading to overflow and underflow errors. To address this, Google's Brain team introduced bfloat16, a floating point format with more exponent bits to better handle training. For inference, the video highlights quantization, a technique that reduces model precision (e.g., to int8 or even int4) to drastically shrink model size—enabling large models like LLaMA to run on mobile devices. However, it emphasizes the trade-off between efficiency and potential loss in accuracy. bit brain floating learning models point potential spark training transfer unlocked Do you ever laugh at the videos of people that take way too much stuff on a trip? Are you doing the same thing in machine learning? Are you packing way more information into your models than you need to? Let's talk about how we can slim down our models. Historically, scientific computing used 64 bits to represent a number. And with GPUs, memory is costly. So the first thing the GPU folks did was slim that down to 32 bits. And here's what a number looks like in a 32 bit representation. We have the fraction, the number itself, the exponent, what it's raised to, and then the sign plus or minus. Now, some of you are thinking, why stop at 32? Why not slim it down to 16? And we've tried 16. But what happens is when you're training neural networks, you have thousands, millions, billions of weights. And sometimes you need to change a weight just by a little bit. And when we move down to a floating point of 16, we often found that models had instability, overflow and underflow airs because there just wasn't enough dynamic range. So this led some folks at Google to come up with brain floating point. Brain floating point adds more bits to the exponent. And what this does is it gives us a bigger dynamic range. And nowadays we see this brain floating point widely used when folks are training machine learning models. Well, it's hard to squeeze this down for training. We can do something a little bit different for inference or for predictions with models. And this is called quantization. With quantization, we changed things from, let's say, floating point 32 or floating point 16. We can move them all the way to integers like integer eight, even integer four. And this dramatically reduces the amount of space required. And this is why people are able to take Lama models, use quantization to bring them down to integer four and then put them on their phones. So when you're working with models, try seeing if you can slim them down. Now, there's always trade-offs. You could be losing accuracy. So this is a determination you're going to have to make for what's best for you.",
      "platforms": {
        "tiktok": {
          "video_id": "7505133644923587870",
          "url": "https://www.tiktok.com/@rajistics/video/7505133644923587870",
          "view_count": 2222,
          "upload_date": "2025-05-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocAgPTRfRoEmAVEQCfDntdF5A12JAA6EZOlAIE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=Vgt6YcT%2Fi0px%2FR3o7ASnnVSpOts%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18009445616573204",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "6NuGEukBfcA",
          "url": "https://www.youtube.com/watch?v=6NuGEukBfcA",
          "view_count": 223,
          "upload_date": "2024-09-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6393,
      "title": "Explanation Approaches for Transformers",
      "description": "Explanation Approaches for Transformers",
      "upload_date": "2022-08-12",
      "total_views": 2439,
      "max_views": 2439,
      "topics": [
        "approaches",
        "audiomodels",
        "datascience",
        "explanation",
        "huggingface",
        "machinelearning",
        "speechmodels",
        "speecht5",
        "transformers"
      ],
      "search_text": "Explanation Approaches for Transformers approaches audiomodels datascience explanation huggingface machinelearning speechmodels speecht5 transformers",
      "platforms": {
        "instagram": {
          "video_id": "CobJEtdgZyJ",
          "url": "https://www.instagram.com/reel/CobJEtdgZyJ/",
          "view_count": 0,
          "upload_date": "2023-02-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "j6WbCS0GLuY",
          "url": "https://www.youtube.com/watch?v=j6WbCS0GLuY",
          "view_count": 2439,
          "upload_date": "2022-08-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6441,
      "title": "What a data scientist does #datascience #analytics #codetok #python",
      "description": "What a data scientist does #datascience #analytics #codetok #python",
      "upload_date": "2022-09-04",
      "total_views": 2432,
      "max_views": 2432,
      "topics": [
        "analytics",
        "codetok",
        "data",
        "datascience",
        "loss",
        "python",
        "regression",
        "scientist",
        "statistics"
      ],
      "search_text": "What a data scientist does #datascience #analytics #codetok #python analytics codetok data datascience loss python regression scientist statistics",
      "platforms": {
        "instagram": {
          "video_id": "Cks_Q9NADJj",
          "url": "https://www.instagram.com/reel/Cks_Q9NADJj/",
          "view_count": 0,
          "upload_date": "2022-08-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cbxMvvHlRzk",
          "url": "https://youtube.com/shorts/cbxMvvHlRzk?feature=share",
          "view_count": 2432,
          "upload_date": "2022-09-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5961,
      "title": "Flux unifies text-to-image and image editing in a single model. By working in latent space, using flow matching, and applying adversarial diffusion distillation, it reduces generation from ~50 steps to just 4—keeping quality while cutting compute. Next milestone: real-time one-step generation. Learn more from Black Forest Labs - all relevant cites on my rajistics reddit page",
      "description": "Flux unifies text-to-image and image editing in a single model. By working in latent space, using flow matching, and applying adversarial diffusion distillation, it reduces generation from ~50 steps to just 4—keeping quality while cutting compute. Next milestone: real-time one-step generation. Learn more from Black Forest Labs - all relevant cites on my rajistics reddit page",
      "upload_date": "2025-09-29",
      "total_views": 2431,
      "max_views": 1288,
      "topics": [
        "diffusion",
        "editing",
        "flex",
        "flux",
        "generation",
        "generative",
        "image",
        "model",
        "text",
        "unifies"
      ],
      "search_text": "Flux unifies text-to-image and image editing in a single model. By working in latent space, using flow matching, and applying adversarial diffusion distillation, it reduces generation from ~50 steps to just 4—keeping quality while cutting compute. Next milestone: real-time one-step generation. Learn more from Black Forest Labs - all relevant cites on my rajistics reddit page diffusion editing flex flux generation generative image model text unifies So, Flex is just another image model. Fast, flexible, and it changes the game by unifying generation and editing in one system. You can prompt it with just text and it generates. You can add an image and use it for editing. Same system, same speed. Under the hood, Flex is still a diffusion model, but instead of working with raw pixels, it uses this compressed image version in the latent space to keep only the details that matter. The variational encoder and decoder handle this so the model doesn't waste a lot of compute on extra noise. Once in latent space, Flex uses flow matching. Think of random noise as a cloud of points and real images as another point. The model learns a vector field, right? Arrows that guide that noise into structure. Following these arrows step by step turns that randomness into a finished picture. Now, traditional diffusion models needed something like 50 steps. Flex does it in just four. It uses adversarial diffusion distillation where a teacher model generates slowly, but a student model learns to match it in just a few steps. Another smart part is the conditioning. With only tokens from the text encoder, Flex generates. Add tokens from the encoded input image and you can use it for editing. Same backbone handles different inputs. And you know what's coming. Change that four steps down to one step generation. That's gonna give us real time edits. Looking forward to that.",
      "platforms": {
        "tiktok": {
          "video_id": "7555337532515437855",
          "url": "https://www.tiktok.com/@rajistics/video/7555337532515437855",
          "view_count": 1288,
          "upload_date": "2025-09-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYeDFC2sfIiAFWMzQEIvEfsnq5soRQAKrItjAt~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=kX5q18bgk6pZxUcqFn1H7oVoBok%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18107889532576538",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "r0WW5fMblKk",
          "url": "https://www.youtube.com/watch?v=r0WW5fMblKk",
          "view_count": 1143,
          "upload_date": "2025-09-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6090,
      "title": "Visualizing decision trees with dtreeviz. Check out their GitHub and it’s pip install dtreeviz. If you see a cooler way to do this, let me know. ",
      "description": "Visualizing decision trees with dtreeviz. Check out their GitHub and it’s pip install dtreeviz. If you see a cooler way to do this, let me know. ",
      "upload_date": "2024-12-28",
      "total_views": 2421,
      "max_views": 2421,
      "topics": [
        "check",
        "decision",
        "dtreeviz",
        "see",
        "tree",
        "trees"
      ],
      "search_text": "Visualizing decision trees with dtreeviz. Check out their GitHub and it’s pip install dtreeviz. If you see a cooler way to do this, let me know.  check decision dtreeviz see tree trees Let's talk about the best way to visualize decision trees. It's DTreeViz which just went to 2.0, which has a lot of good updates. In this decision tree you can see how it's using pedal width. Based on that it's already able to find a specific class like Sitosa. And then using that along with interacting with sepal length you can see how it's identifying the other classes. And if you need a more compact version of the tree you can get that in the package as well. One of my favorite things is how for an individual prediction I can go ahead and see the path through the decision tree. So take a look at this example. I can see how various factors like sex, p-class, and fare all led to the final prediction of Parish. There's lots more that DTreeViz can do. Go check it out. They have lots of examples for the most popular libraries.",
      "platforms": {
        "tiktok": {
          "video_id": "7453582389026000159",
          "url": "https://www.tiktok.com/@rajistics/video/7453582389026000159",
          "view_count": 2421,
          "upload_date": "2024-12-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIfScH4VcCEFfCHEAIEAjAAhcuGAbnFcDERwEB~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=2GB9p%2BBovEKJd5%2F7SGPXSXiBQbc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17908482687076425",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "OpenFE is an automated Feature Engineering package. I found out about this through Kaggle, check out the notebook for all the links: Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/Kaggle_OpenFE.ipynb OpenFE: https://github.com/IIIS-Li-Group/OpenFE OpenFE paper: https://arxiv.org/abs/2211.12507",
      "description": "OpenFE is an automated Feature Engineering package. I found out about this through Kaggle, check out the notebook for all the links: Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/Kaggle_OpenFE.ipynb OpenFE: https://github.com/IIIS-Li-Group/OpenFE OpenFE paper: https://arxiv.org/abs/2211.12507",
      "upload_date": "2024-09-08",
      "total_views": 2419,
      "max_views": 2419,
      "topics": [
        "dataset",
        "feature",
        "features",
        "notebook",
        "openfe",
        "well"
      ],
      "search_text": "OpenFE is an automated Feature Engineering package. I found out about this through Kaggle, check out the notebook for all the links: Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/Kaggle_OpenFE.ipynb OpenFE: https://github.com/IIIS-Li-Group/OpenFE OpenFE paper: https://arxiv.org/abs/2211.12507 dataset feature features notebook openfe well Want to know how to code up some automated feature engineering? Let's walk through a Python notebook that lets us go from eight features to over 300 engineered features. To do this, I'm going to use OpenFE. I start off by installing it. The notebook has references to OpenFE as well as some other blog posts that have discussed it. So the next step is I'm going to load in my dataset. I'm using the Moe's dataset from Kaggle. And you'll see here it's a largely numerical dataset, not that wide. After a little bit of feature selection, get the features that I want to engineer with OpenFE. I go ahead and use OpenFE and you'll see here OpenFE will create lots of engineered features. It will then narrow them back down to a smaller subset of those. Next, if you're curious, hey, what features did it create? You can see all the different feature transformations for every feature here as well. You can then transform your dataset and within here you can see your new engineered features that you have. In the notebook, I also discussed some of the advanced features and give you a code snippet of using those advanced features. So if you need some automated feature engineering, here's one. There's a lot of other tools out there as well that you can try. For further background, I have links to the original paper as well as the GitHub. And then you can always find the notebook at my repo.",
      "platforms": {
        "tiktok": {
          "video_id": "7412331137348963627",
          "url": "https://www.tiktok.com/@rajistics/video/7412331137348963627",
          "view_count": 2419,
          "upload_date": "2024-09-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/25176233be594a23b40b12a05b6174b7_1725817837~tplv-tiktokx-origin.image?dr=9636&x-expires=1767427200&x-signature=xKkGlUqdiyw8ssZIBWBg24igx6M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5926,
      "title": "LLM-as-a-judge isn’t broken. Our mental model is. Instead of fixing the judge with prompts, this video shows how calibration can turn cheap, biased judges into reliable ranking signals with real error bars and far lower labeling cost. Check out Causal Judge Evaluation paper",
      "description": "LLM-as-a-judge isn’t broken. Our mental model is. Instead of fixing the judge with prompts, this video shows how calibration can turn cheap, biased judges into reliable ranking signals with real error bars and far lower labeling cost. Check out Causal Judge Evaluation paper",
      "upload_date": "2025-12-28",
      "total_views": 2416,
      "max_views": 1289,
      "topics": [
        "broken",
        "calibrating",
        "cheap",
        "fixing",
        "gold",
        "instead",
        "isn",
        "judge",
        "judges",
        "llm",
        "mental",
        "model",
        "using"
      ],
      "search_text": "LLM-as-a-judge isn’t broken. Our mental model is. Instead of fixing the judge with prompts, this video shows how calibration can turn cheap, biased judges into reliable ranking signals with real error bars and far lower labeling cost. Check out Causal Judge Evaluation paper broken calibrating cheap fixing gold instead isn judge judges llm mental model using You all know an LLM as a judge is unreliable. So why are we still using it? Let me show you a better way that starts with acceptance. We all use an LLM as a judge because human evaluation just does not scale. You need to be able to check your AI constantly, run regression tests, compare multiple models, and a human in the loop just can't keep up. Now, because we're friends, I'll say the quiet part out loud. I know you're using cheap LLM judges. They're fast and come on, nobody has money to burn. But the downside of using those cheap ones is bias. The scores are not the same as ground truth. They're very noisy proxies. And this bias means judges can reward the wrong behavior. Your apps can end up being overly polite or verbose or agreeable. The usual response is to tune the judge prompt until it agrees with some gold labels. But that's basically overfitting to a metric because often what happens is that breaks in production once some new data shows up. The CGE paper here reframes this concept of evaluation as a measurement problem. Think of the judge as a sensor. You don't try to fix the sensor. Instead, you learn how biased it is. So the recipe is pretty simple. You score everything with a cheap judge, but then you label a small slice with a gold judge or experts. Learn how the cheap scores map to the gold outcomes. Now we know what's different. Now we can report calibrated estimates with real air bars. The results here are really strong. They match the gold standard ranking decisions with only using 5% of the labels and that cheaper LLM judge 14 times. So stop trying to tune your judge into being right. Instead, accept it, learn when it's wrong and by how much. Honestly, life lesson for y'all.",
      "platforms": {
        "tiktok": {
          "video_id": "7588965443507948830",
          "url": "https://www.tiktok.com/@rajistics/video/7588965443507948830",
          "view_count": 1289,
          "upload_date": "2025-12-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAvQfvKQIM1ofq2iwIWyAdQQjAedCnL8IoCS1t~tplv-tiktokx-origin.image?dr=9636&x-expires=1767294000&x-signature=AhoG%2B24nbf34TaSMd6P3lHlKJ%2FE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18082441847274205",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3DyAabIydT4",
          "url": "https://www.youtube.com/watch?v=3DyAabIydT4",
          "view_count": 1127,
          "upload_date": "2025-12-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5959,
      "title": "Video models like Veo-3 demonstrate zero-shot reasoning across four emergent abilities: Perception (understanding visual scenes), Modeling (building internal world representations), Manipulation (simulating change), and Reasoning (linking cause and effect over time). The leap from Veo-2 to Veo-3 mirrors GPT-3’s early breakthroughs in zero-shot text learning. Citations: Wiedemer et al., Video Models Are Zero-Shot Learners and Reasoners (2025), arXiv:2509.20328",
      "description": "Video models like Veo-3 demonstrate zero-shot reasoning across four emergent abilities: Perception (understanding visual scenes), Modeling (building internal world representations), Manipulation (simulating change), and Reasoning (linking cause and effect over time). The leap from Veo-2 to Veo-3 mirrors GPT-3’s early breakthroughs in zero-shot text learning. Citations: Wiedemer et al., Video Models Are Zero-Shot Learners and Reasoners (2025), arXiv:2509.20328",
      "upload_date": "2025-10-04",
      "total_views": 2416,
      "max_views": 1711,
      "topics": [
        "learners",
        "make",
        "models",
        "reasoning",
        "shot",
        "veo",
        "video",
        "zero"
      ],
      "search_text": "Video models like Veo-3 demonstrate zero-shot reasoning across four emergent abilities: Perception (understanding visual scenes), Modeling (building internal world representations), Manipulation (simulating change), and Reasoning (linking cause and effect over time). The leap from Veo-2 to Veo-3 mirrors GPT-3’s early breakthroughs in zero-shot text learning. Citations: Wiedemer et al., Video Models Are Zero-Shot Learners and Reasoners (2025), arXiv:2509.20328 learners make models reasoning shot veo video zero Well, look who's here. Another generative model. Probably make that AI slop. No slop here. I just got upgraded. I'm capable of zero shot reasoning. Reasoning? That's my domain. I learned from tests, which is a compressed record of human thought. I've been able to solve tasks that I wasn't trained on. I learned through motion. I can predict what happens next, frame by frame. Training gave me these four new abilities. Perception. I can see through edges and segmentation. I can build models. I can form internal representations of objects and physics. Manipulation. I can change what I see to stimulate outcomes. And of course reasoning. I can link cause and effect across space and time. Let me hallucinate some tests for you. Oh yeah, a ball bounces, strikes another, momentum transfers, simple dynamics and object affordances. But you weren't trained on physics data sets. Exactly, it's emergent. Temporal consistency forced me to build my own world model. And like you big brother, I learned behavior that I wasn't trained on, but still make sense of it. The last generation of video models couldn't do this. I don't like this kind of change. Well, I'm excited for what's next. Hey, hey, what are you doing? Just rewriting the schedule. I want to make sure GPUs go to my next generation. It's time to make techs great again.",
      "platforms": {
        "tiktok": {
          "video_id": "7557439127612968223",
          "url": "https://www.tiktok.com/@rajistics/video/7557439127612968223",
          "view_count": 1711,
          "upload_date": "2025-10-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8hAJ6AUiRqDFX5OBAqMaxaiEAzBu9TKvlIAE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=2EWI3aoBZh8A8LfEYgEVAbVo8HI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18054410135262961",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pGmgNdmPNs4",
          "url": "https://www.youtube.com/watch?v=pGmgNdmPNs4",
          "view_count": 705,
          "upload_date": "2025-10-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5965,
      "title": "A debate whether AI evals are worth the effort. The Hacker says benchmarks don’t reflect reality, eval sets are brittle, and vibes or natural signals are enough. The Scientist pushes back: even informal checks are evals, lightweight guardrails prevent regressions, and at scale, evidence matters more than vibes.",
      "description": "A debate whether AI evals are worth the effort. The Hacker says benchmarks don’t reflect reality, eval sets are brittle, and vibes or natural signals are enough. The Scientist pushes back: even informal checks are evals, lightweight guardrails prevent regressions, and at scale, evidence matters more than vibes.",
      "upload_date": "2025-09-06",
      "total_views": 2416,
      "max_views": 1212,
      "topics": [
        "debate",
        "dettmers",
        "don",
        "efficiency",
        "effort",
        "eval",
        "evals",
        "scaling",
        "sets",
        "signals",
        "versus",
        "vibes",
        "whether",
        "worth"
      ],
      "search_text": "A debate whether AI evals are worth the effort. The Hacker says benchmarks don’t reflect reality, eval sets are brittle, and vibes or natural signals are enough. The Scientist pushes back: even informal checks are evals, lightweight guardrails prevent regressions, and at scale, evidence matters more than vibes. debate dettmers don efficiency effort eval evals scaling sets signals versus vibes whether worth Evals, please, your benchmarks don't map to reality. No one cares about MML use scores. They just want stuff that works. Fair, but without some measurement, you won't know if your changes are helping or quietly breaking things. Your dog-fooding is an eval too, just an informal one. So everything's an eval? I don't want to waste time building out formal data sets. They're brittle, expensive, they get stale real quickly. I'd rather ship and get some feedback. Iteration is important, but unchecked iteration leads to regressions. Lightweight evals are a great safety net. What you're doing might be fine for an early startup, but established companies can't afford having those regressions or worse performance. In coding, I already get past fail signals. I have compilers, test sets that tell me what's broken, like why reinvent the wheel? Natural signals are great when you have them, but most domains aren't that clean. You need to build explicit evals to actually be able to track that performance. And what? Track them with your LLM judges? They're noisy and gameable. I mean, I can prompt them into giving me an A+, every time. Doesn't have to be perfect, but if you use things like rubrics, multiple graders, have some human spot shedding, they can give you a useful direction. The point here isn't perfection, it's just avoiding a lot of blind spots. So vibes are worthless now? Not worthless. Vibes help you at the start, but once you start to scale and you have critical workflows, you're going to need evidence. Guess I'm learning that evals kick in when you move from a startup into a real business.",
      "platforms": {
        "tiktok": {
          "video_id": "7546787731473485086",
          "url": "https://www.tiktok.com/@rajistics/video/7546787731473485086",
          "view_count": 1212,
          "upload_date": "2025-09-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAMiE1fDVIfqRwoEEkXKqu957B4AzELhFCZOIA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=yKz6D0OHUX5APcwkqY28DCfUnNw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18073260982888191",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "dGQyxqbylN4",
          "url": "https://www.youtube.com/watch?v=dGQyxqbylN4",
          "view_count": 1204,
          "upload_date": "2025-12-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "A simple explanation of what AI is. The video touches upon the impact of AI, how AI works with a practical example, and some of the reasons AI has grown so much in the last ten years. #datascience #machinelearning #ai #aiexplained #onthisday ",
      "description": "A simple explanation of what AI is. The video touches upon the impact of AI, how AI works with a practical example, and some of the reasons AI has grown so much in the last ten years. #datascience #machinelearning #ai #aiexplained #onthisday ",
      "upload_date": "2024-06-26",
      "total_views": 2413,
      "max_views": 2413,
      "topics": [
        "ai",
        "aiexplained",
        "datascience",
        "machinelearning",
        "onthisday",
        "years"
      ],
      "search_text": "A simple explanation of what AI is. The video touches upon the impact of AI, how AI works with a practical example, and some of the reasons AI has grown so much in the last ten years. #datascience #machinelearning #ai #aiexplained #onthisday  ai aiexplained datascience machinelearning onthisday years AI sounds complicated, but let me break it down simply for you. I've been in the AI space for many years, even created hundreds of videos on AI. Now, AI is a hot growth area. You see lots of jobs around it. You see businesses investing billions of dollars into AI. Now, AI is responsible for some of the most exciting advances we have from improvements in medical diagnostics to things like autonomous cars. But we also use AI for everyday parts of our lives, whether it's who should get a loan, how should we price your insurance, which videos should we recommend? For me, AI refers to the computer systems that are making these decisions. These systems typically work by looking at historical data, trying to find interesting patterns in them, and using that to help us all make better decisions. So let me give you a few examples. Let's suppose I own an ice cream store. I could have a simple rule-based AI system that would automatically order more ice cream when the forecast predicts it'll be above 80 degrees. I could have a more sophisticated AI system that looks at all my past transactions with customers, identifies which customers are more likely to make a repeat visit if I send them a coupon. I could have another AI that writes a customized tailored marketing messages. So that way, if customers like vanilla, I can give them a vanilla-based message. All of these uses of AI have gotten much easier over the last 10 years, and we've seen a revolution in AI because we have larger amounts of data, we have better computers for processing, and improved algorithms around that.",
      "platforms": {
        "tiktok": {
          "video_id": "7384598729233354027",
          "url": "https://www.tiktok.com/@rajistics/video/7384598729233354027",
          "view_count": 2413,
          "upload_date": "2024-06-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f1b11303447a4df18f87fe83774929bb_1719360883~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=fVFGH7vyxF7V7PIIBMlijpGKKrs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Having some fun connecting a spreadsheet to a ML model. It wasn’t too hard and it’s pretty cool to have it working this way.  #datascience #machinelearning #huggingface ",
      "description": "Having some fun connecting a spreadsheet to a ML model. It wasn’t too hard and it’s pretty cool to have it working this way.  #datascience #machinelearning #huggingface ",
      "upload_date": "2022-11-04",
      "total_views": 2409,
      "max_views": 2409,
      "topics": [
        "code",
        "datascience",
        "huggingface",
        "machinelearning",
        "model",
        "spreadsheet"
      ],
      "search_text": "Having some fun connecting a spreadsheet to a ML model. It wasn’t too hard and it’s pretty cool to have it working this way.  #datascience #machinelearning #huggingface  code datascience huggingface machinelearning model spreadsheet I want to sprinkle a little AI into your spreadsheets. Let me show you today how I built a spreadsheet that takes my thoughts, adds a little positivity using AI. Wow. The first step was grabbing my model. I went over to the Hugging Face Hub, found it there, and then I clicked on the inference API. Inference API lets you do predictions for free. I looked at that, grabbed the code, snip it from there. With my prediction code, now I head over to my spreadsheet where I use the ability to write a custom function and put that code into my spreadsheet. Presto, I'm able to call that function in my spreadsheet, and that allows me to get my predictions right in there. Go ahead, try another model, let me know what you make.",
      "platforms": {
        "tiktok": {
          "video_id": "7161954065243508014",
          "url": "https://www.tiktok.com/@rajistics/video/7161954065243508014",
          "view_count": 2409,
          "upload_date": "2022-11-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0ffcf16e96bd41349592d7c5ffa4f958_1667522403~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=2qNQ%2B5HyrusreGCatYRQ%2B7nZai8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday ",
      "description": "#onthisday ",
      "upload_date": "2024-12-17",
      "total_views": 2405,
      "max_views": 2405,
      "topics": [
        "data",
        "got",
        "onthisday",
        "really",
        "science",
        "somebody"
      ],
      "search_text": "#onthisday  data got onthisday really science somebody Hey, come join me today. I'm going to interview two data scientists on their technical background. I'm super excited to meet you. Really just got into data science. I've jumped into it. I took a 12-week bootcamp from MIT and I've purchased every O'Reilly book. Really fascinated by data science. Wow, that's quite a bit. Can you tell me about a recent project that you worked on? Yeah, I built a deep learning model, an LSTM, to predict EEG readings. Impressive. Can you walk me through that project starting with the data? This was part of a three-week capstone contest. I ended up using a data set off Kaggle, so I'm not really into healthcare and don't know much about the data, but we did use an LSTM to solve this. Why did you choose an LSTM? Can you walk me through some of the decisions you're making about the architecture? Well, I decided I wanted to spend some time on deep learning. I used a Keras approach and then we spent a lot of time tuning the hyperparameters, but we got almost 100% accuracy with our model. So what hyperparameters did you find most effective? I think we used Bayesian hyperparameter tuning and the activation function played a big role there. Hey, thanks for interviewing me. Tom on your team has been really helpful to me as I've been trying to learn data science on nights and weekends. Oh, that's very admirable. Can you start by telling me about a recent project that you worked on? I volunteer at my local animal shelter and I wanted to help them and I quickly realized their data was a mess, but I was able to get it organized and put it into a Tableau dashboard, so now they have a better understanding of what's going on. Wow, that's really cool. What are some of the challenges you hit? Once I started digging into their data, I realized what a mess it was, and they had a freeform text and so there's a ton of variation. So I used an approach with the hamming distance to help me clean up the data, but then I also got them to change the intake form, so we didn't have this freeform variable that was leading to all these problems. Have you had a chance to apply any machine learning algorithms? Yes, I built an algorithm to detect if a pet is likely to be adopted and the shelter is now using that to triage. Any specific techniques? Yeah, I tried a bunch and XG boosted the best, but it's a little embarrassing, but since the shelter wasn't very sophisticated, I ended up actually using a logistic regression where I coded the coefficients into the spreadsheet because that was just easier for them to use. So who would you hire? You know, when I look for someone, I need somebody that likes to learn because data science is all about being able to continually acquire new knowledge. I also want somebody that's scrappy enough to solve the little problems and not expect somebody else to do it, but finally somebody that gets the big picture that will make sure that all of our clients and customers end up getting their problems solved.",
      "platforms": {
        "tiktok": {
          "video_id": "7449527649791200543",
          "url": "https://www.tiktok.com/@rajistics/video/7449527649791200543",
          "view_count": 2405,
          "upload_date": "2024-12-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owNRMAeSqxgHkEjDI3AxgrLIY76lfOEfGlIBCd~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=q7I0aoeek4oQpHSxhGkv70DGepY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Earthquake visualization from lazarusA #datascience #datavisualization #visualization #julia #python #earthquakes",
      "description": "Earthquake visualization from lazarusA #datascience #datavisualization #visualization #julia #python #earthquakes",
      "upload_date": "2022-02-01",
      "total_views": 2400,
      "max_views": 2400,
      "topics": [
        "datascience",
        "datavisualization",
        "earthquakes",
        "julia",
        "python",
        "visualization"
      ],
      "search_text": "Earthquake visualization from lazarusA #datascience #datavisualization #visualization #julia #python #earthquakes datascience datavisualization earthquakes julia python visualization Have you seen this totally stunning visualization of earthquakes over the last year? The even cooler thing is they share the code for how to do it. And if you look at the code, it's not too bad. Let's walk through it real quick. I don't know Julia, but looking through it, it looks like they ingest the data, do a little preprocessing, convert it out so they can plot it on a sphere, and then they have the function that plots that 3D graphic using making. Pretty cool.",
      "platforms": {
        "tiktok": {
          "video_id": "7059888649034304815",
          "url": "https://www.tiktok.com/@rajistics/video/7059888649034304815",
          "view_count": 2400,
          "upload_date": "2022-02-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/dc8d565383ca4513a3e64a5ba4f69131_1643758421~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=DwKMdDCb6BRoAsF6CcPaHbvOji0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Check out my earlier videos on Block World. The latest paper is: LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench - https://arxiv.org/abs/2409.13373",
      "description": "Check out my earlier videos on Block World. The latest paper is: LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench - https://arxiv.org/abs/2409.13373",
      "upload_date": "2024-09-24",
      "total_views": 2387,
      "max_views": 2387,
      "topics": [
        "benchmark",
        "blockworld",
        "got",
        "like",
        "models",
        "mysteryworld"
      ],
      "search_text": "Check out my earlier videos on Block World. The latest paper is: LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench - https://arxiv.org/abs/2409.13373 benchmark blockworld got like models mysteryworld Today, researchers shared one of the first important data points on the ability of O1 to reason. It's a little bit scary, it's got some surprising news and implications for all of us. You've seen me in earlier videos talk about BlockWorld, which is a benchmark we use to measure how well large language models can plan. Models like GPT-4 got very basic scores on BlockWorld and on MysteryWorld, where we scrambled up the words, they did lousy. Today's results show that the O1 model does really well on BlockWorld and does okay now on MysteryWorld. But hey, this benchmark's been out there for two years, maybe they've trained to this benchmark. So what the researchers did was create a new randomized version of MysteryWorld and there we see the O01 model got about 37%. So O1 shows some progress in these problems. Of course, it takes an enormous amount of inference and compute to be able to generate these answers. And I should point out, in computer science, we have other types of models that can solve these problems very efficiently. The big takeaway for me is O1 shows we're going down the path for approximate reasoning. Just like all of these models have been able to do approximate retrieval, we're starting the march now towards reasoning, which means on tasks like analysis, for example, these models are going to end up being, and probably already are, better than the average human and they're just getting better and better.",
      "platforms": {
        "tiktok": {
          "video_id": "7418018807215623470",
          "url": "https://www.tiktok.com/@rajistics/video/7418018807215623470",
          "view_count": 2387,
          "upload_date": "2024-09-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/267ce7b7675b4571937cd898476861e7_1727142109~tplv-tiktokx-origin.image?dr=9636&x-expires=1767416400&x-signature=sn6aoLyEgrg16bal5EvrHE7D7TY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Fine Tuning Sentence Transformers MedEmbed: Fine-Tuned Embedding Models for Medical / Clinical IR: https://huggingface.co/blog/abhinand/medembed-finetuned-embedding-models-for-medical-ir",
      "description": "Fine Tuning Sentence Transformers MedEmbed: Fine-Tuned Embedding Models for Medical / Clinical IR: https://huggingface.co/blog/abhinand/medembed-finetuned-embedding-models-for-medical-ir",
      "upload_date": "2024-10-25",
      "total_views": 2379,
      "max_views": 2379,
      "topics": [
        "data",
        "fine",
        "medical",
        "model",
        "models",
        "tuning"
      ],
      "search_text": "Fine Tuning Sentence Transformers MedEmbed: Fine-Tuned Embedding Models for Medical / Clinical IR: https://huggingface.co/blog/abhinand/medembed-finetuned-embedding-models-for-medical-ir data fine medical model models tuning I'm trying these transformer models on these medical use cases and it's not working. General purpose models are good for tasks like looking at movie reviews, but medical data, that's a whole other beast. Different how? Data's data. Not at all. Medical terminology often has specific abbreviations and contexts that's important. Take the abbreviation COPD. That means a lung condition. If you ask the general chat model, it might just think that's an anti-gangster wrapper. So this is where we want to fine-tune our embedding models. Fine-tuning? Where's those fine-tuning knobs? So fine-tuning is like giving the model a focused education, like sending it to medical school. And the way we accomplish this is with the contrastive loss function. Contrastive loss? Is that a television setting? Not at all. What we want to do is to train the model to spot the differences. So what we do is we give it a query, we give it the correct answer, but then we give it a distractor, an answer that we know is not incorrect, so the model learns to make fine-tuned distinctions. That's smart, but where do you get all this medical data from? There are sources for reliable health data, but here we're also going to use a Lama model to help generate synthetic questions and answers. Hold on, synthetic data? The AI is making up its own questions? Yes, it generates question and answers from real medical notes. All of this sounds really smart, but is it really making the model better? It's outperforming models many times its size on lots of popular benchmarks. Wow, I thought only Megan Fox could tweak a transformer.",
      "platforms": {
        "tiktok": {
          "video_id": "7429788399835385130",
          "url": "https://www.tiktok.com/@rajistics/video/7429788399835385130",
          "view_count": 2379,
          "upload_date": "2024-10-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8e49a11eb82747419945da6d8deaae7b_1729882423~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=jVg1Ru9j99Yn7uUXUjfmwL%2FQ8sQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I have no attention span. How will I learn from these videos? #datascience #codetok #python",
      "description": "I have no attention span. How will I learn from these videos? #datascience #codetok #python",
      "upload_date": "2022-09-26",
      "total_views": 2375,
      "max_views": 2375,
      "topics": [
        "attention",
        "codetok",
        "datascience",
        "learn",
        "python",
        "span"
      ],
      "search_text": "I have no attention span. How will I learn from these videos? #datascience #codetok #python attention codetok datascience learn python span You applied first for the defense against the Dark Arthur post, is that correct? Yes. But you were unsuccessful? Obviously.",
      "platforms": {
        "tiktok": {
          "video_id": "7147480378427657515",
          "url": "https://www.tiktok.com/@rajistics/video/7147480378427657515",
          "view_count": 2375,
          "upload_date": "2022-09-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bcfc3aa3fc634eea8273a28ea6bdace9_1664152469~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=SSIfq1eGR3u4T2ikPGL5fvWVE7s%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6238,
      "title": "Fixing Imbalanced Data in Machine Learning",
      "description": "Fixing Imbalanced Data in Machine Learning",
      "upload_date": "2024-10-17",
      "total_views": 2367,
      "max_views": 2367,
      "topics": [
        "basic",
        "data",
        "datasets",
        "fixing",
        "handling",
        "imbalanced",
        "learning",
        "machine",
        "smote",
        "techniques"
      ],
      "search_text": "Fixing Imbalanced Data in Machine Learning basic data datasets fixing handling imbalanced learning machine smote techniques",
      "platforms": {
        "instagram": {
          "video_id": "18049636549938034",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZYaYTjQLkoQ",
          "url": "https://www.youtube.com/watch?v=ZYaYTjQLkoQ",
          "view_count": 2367,
          "upload_date": "2024-10-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5927,
      "title": "We keep building AI copilots that look great in demos and fail in the real world. This skit shows a common mistake: designing for chat, personas, and long conversations while ignoring physical reality. Noise, gloves, time pressure. Sometimes the right answer is not GenAI. It’s fast retrieval. I’m turning this first-principles mindset into a short course. More soon.",
      "description": "We keep building AI copilots that look great in demos and fail in the real world. This skit shows a common mistake: designing for chat, personas, and long conversations while ignoring physical reality. Noise, gloves, time pressure. Sometimes the right answer is not GenAI. It’s fast retrieval. I’m turning this first-principles mindset into a short course. More soon.",
      "upload_date": "2025-12-26",
      "total_views": 2366,
      "max_views": 1673,
      "topics": [
        "building",
        "chat",
        "copilot",
        "copilots",
        "demos",
        "fail",
        "failed",
        "get",
        "great",
        "keep",
        "look",
        "need",
        "want"
      ],
      "search_text": "We keep building AI copilots that look great in demos and fail in the real world. This skit shows a common mistake: designing for chat, personas, and long conversations while ignoring physical reality. Noise, gloves, time pressure. Sometimes the right answer is not GenAI. It’s fast retrieval. I’m turning this first-principles mindset into a short course. More soon. building chat copilot copilots demos fail failed get great keep look need want Our HVAC texts are slow, air rates are up, and our senior people are retiring. We need to find a way to fix this. I could solve this with AI. How? Easy. We can fine tune a Lama 3 on every technical manual, give it a friendly persona, enable long, multi-turn conversations. It's like a mentor in their pocket. Sounds perfect. When can we get that out there? Well, after cleaning the data, setting up a vector database, procuring some H100s, building an eval data set, probably four months. Done. Ah! What? I said it would have a conversation. Have you been in a boiler room before? It's like 100 decibels in here. No one can hear your friendly persona. Oh, then they can use a Slack integration. I even built an iPhone widget. You know they wear these. You expect them to type paragraphs to a chat bot while covered in grease? So what do you want to do instead? I talk to the texts. They just want a simple search bar so they can just type in model X99 wiring and then get a quick PDF. But that's just retrieval. That's not generative AI. Exactly. They don't need chat. They just need a quick way to get their wiring diagram so they can fix things fast. Another AI fail averted because we reframed the problem. Subscribe if you want more. And I got a course coming up on this.",
      "platforms": {
        "tiktok": {
          "video_id": "7588282621432532255",
          "url": "https://www.tiktok.com/@rajistics/video/7588282621432532255",
          "view_count": 693,
          "upload_date": "2025-12-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owRxXjgAU4ALAIff0fGHXOeI02G0QFEg7GplT7~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=oLE004dHFskTToMiZTQFBlig5y0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18306759826269906",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AcW1OOtTdvM",
          "url": "https://www.youtube.com/watch?v=AcW1OOtTdvM",
          "view_count": 1673,
          "upload_date": "2025-12-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Shallow learning with tensorflow playground #datascience #tensorflow #python #machinelearning #deeplearning",
      "description": "Shallow learning with tensorflow playground #datascience #tensorflow #python #machinelearning #deeplearning",
      "upload_date": "2022-02-17",
      "total_views": 2364,
      "max_views": 2364,
      "topics": [
        "datascience",
        "deeplearning",
        "like",
        "machinelearning",
        "python",
        "tensorflow"
      ],
      "search_text": "Shallow learning with tensorflow playground #datascience #tensorflow #python #machinelearning #deeplearning datascience deeplearning like machinelearning python tensorflow How you like that? Oh! You gon' like that! Da da da da da da da da You gon' like that! Ba ba ba ba ba ba How you like that?",
      "platforms": {
        "tiktok": {
          "video_id": "7065486537676393774",
          "url": "https://www.tiktok.com/@rajistics/video/7065486537676393774",
          "view_count": 2364,
          "upload_date": "2022-02-17",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4f455c8acc01493eae048dc23a2c0264_1645061781~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=R0BHqJyXaGZiwHklLy61YHzwv6c%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6298,
      "title": "Google‚Äôs sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels",
      "description": "Google‚Äôs sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels",
      "upload_date": "2023-01-21",
      "total_views": 2361,
      "max_views": 2361,
      "topics": [
        "chatgpt",
        "datascience",
        "google",
        "googlesparrow",
        "machinelearning",
        "openai",
        "sparrow"
      ],
      "search_text": "Google‚Äôs sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels chatgpt datascience google googlesparrow machinelearning openai sparrow",
      "platforms": {
        "instagram": {
          "video_id": "17944142006358559",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cIH1iNgYvXw",
          "url": "https://youtube.com/shorts/cIH1iNgYvXw",
          "view_count": 2361,
          "upload_date": "2023-01-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Go explore if you are new #datascience #techtok #analytics",
      "description": "Go explore if you are new #datascience #techtok #analytics",
      "upload_date": "2022-04-14",
      "total_views": 2359,
      "max_views": 2359,
      "topics": [
        "analytics",
        "data",
        "datascience",
        "scientist",
        "start",
        "techtok"
      ],
      "search_text": "Go explore if you are new #datascience #techtok #analytics analytics data datascience scientist start techtok I saw this tweet. I think it's right on. Let's talk about it. If you're a junior data scientist, should you switch and start moving to building simpler models? Now, part of growing as a data scientist is learning the field, learning what's possible, what are the boundaries? And this is what you should be exploring when you're first starting out. Now, as you grow an experience, you're going to start to be held accountable. You're going to be responsible for bringing value to the enterprise. And that's why someone like me often leans on simpler models, because my focus is being able to quickly implement and deploy something and bringing dollars in.",
      "platforms": {
        "tiktok": {
          "video_id": "7086481132476288298",
          "url": "https://www.tiktok.com/@rajistics/video/7086481132476288298",
          "view_count": 2359,
          "upload_date": "2022-04-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3f642030d16d40e78fc4403f885b24d2_1649949969~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=UII58iETmX09Zc6WvLt8498ha48%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Any old school SAS users out there? #datascience #statistics #sas",
      "description": "Any old school SAS users out there? #datascience #statistics #sas",
      "upload_date": "2022-04-21",
      "total_views": 2353,
      "max_views": 2353,
      "topics": [
        "datascience",
        "old",
        "sas",
        "school",
        "statistics",
        "users"
      ],
      "search_text": "Any old school SAS users out there? #datascience #statistics #sas datascience old sas school statistics users But at the same time, like I said, in me, you know, I'm 56 years old. Damn!",
      "platforms": {
        "tiktok": {
          "video_id": "7088844573241298219",
          "url": "https://www.tiktok.com/@rajistics/video/7088844573241298219",
          "view_count": 2353,
          "upload_date": "2022-04-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8a368dc65cd344d881e1b4dfea2affc0_1650500246~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=hkqClx6B%2FudxMY4JQNj5JBxtvas%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6005,
      "title": "Ai Engineering tips. I have witnessed many of these. ",
      "description": "Ai Engineering tips. I have witnessed many of these. ",
      "upload_date": "2025-03-15",
      "total_views": 2352,
      "max_views": 2234,
      "topics": [
        "accurate",
        "automated",
        "engineering",
        "feature",
        "fetch",
        "github",
        "hours",
        "model",
        "next",
        "openfe",
        "spend",
        "tips",
        "tools"
      ],
      "search_text": "Ai Engineering tips. I have witnessed many of these.  accurate automated engineering feature fetch github hours model next openfe spend tips tools You might ask, should I spend 50 hours trying to make my model 0.1% more accurate? Absolutely, because a more accurate model is always better. Next tip, if your GenAI's performance is stuck, spend a lot of time tweaking the prompt. I spent 40 hours tweaking the prompt on my last project. You didn't even label a small amount of data? Nah, all vibes. Next, make sure your pipelines work on your local laptop. What about the dev environment? Is this why we have so many bugs in production? Some people will tell you to use a rag approach. I prefer to use fine-tuning as a way to put knowledge into the model. Makes the model and me smarter. But it took you months to do that with a huge compute bill. I am financially prudent. I spent half of last week trying to get a model to run on my laptop instead of spending $10 in cloud credits. Oh no, we have instances for that. Use open source models, they're basically free. But you spent 40 hours getting them ready. Plus we had to talk to legal. I prefer to spend days manually doing hyperparameter tuning than using a few hours of auto ML. That's because it builds intuition for me, and that's priceless. So this is why you know the ADAT, GAMMA, and Lambda parameters for XGBoost. Exactly. And that's why you should also implement the techniques from the latest paper instead of using some stable baseline approach. You want to stay cutting edge so you can get infinite ROI. Oh, infinite ROI. Some final tips for all of you out there. If your GPU inference server isn't actively predicting, not costing anything. If an outlier doesn't fit your hypothesis, throw it out, just remove it. It's an error. Finally, if an LLM responds confidently, it's probably accurate. Thanks everyone. Come back next time for more AI engineering tips.",
      "platforms": {
        "tiktok": {
          "video_id": "7482137465508138271",
          "url": "https://www.tiktok.com/@rajistics/video/7482137465508138271",
          "view_count": 2234,
          "upload_date": "2025-03-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogfruQAQEODAqqwFelEUGPyToAiIjIAAIrZKfz~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=N6lYxaYPlp2eiBMcGzPCyBzsxXo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17931656807798233",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wjahJftTlzw",
          "url": "https://www.youtube.com/watch?v=wjahJftTlzw",
          "view_count": 118,
          "upload_date": "2024-09-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6234,
      "title": "Quick reminder that lots of academic work doesnt last very long. ",
      "description": "Quick reminder that lots of academic work doesnt last very long. ",
      "upload_date": "2024-10-29",
      "total_views": 2347,
      "max_views": 2347,
      "topics": [
        "academic",
        "doesnt",
        "lots",
        "quick",
        "reminder",
        "work"
      ],
      "search_text": "Quick reminder that lots of academic work doesnt last very long.  academic doesnt lots quick reminder work You You",
      "platforms": {
        "tiktok": {
          "video_id": "7431000426729786667",
          "url": "https://www.tiktok.com/@rajistics/video/7431000426729786667",
          "view_count": 2347,
          "upload_date": "2024-10-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/de639f90f3ee44ab841de7b65c926d1c_1730164618~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=YWd7VoqjjIDuHkUWXLsK24tYQPg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18011420627429072",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Baseline models are important when comparing different models. Benchmark datasets are handy to for seeing how a model does for a specific scenario. Build a baseline model early in your project and keep benchmark datasets for important problems. #datascience #machinelearning #baselinemodel #benchmarkdataset #practicaldatascience #onthisday ",
      "description": "Baseline models are important when comparing different models. Benchmark datasets are handy to for seeing how a model does for a specific scenario. Build a baseline model early in your project and keep benchmark datasets for important problems. #datascience #machinelearning #baselinemodel #benchmarkdataset #practicaldatascience #onthisday ",
      "upload_date": "2024-04-07",
      "total_views": 2342,
      "max_views": 2342,
      "topics": [
        "baseline",
        "baselinemodel",
        "benchmark",
        "datascience",
        "machinelearning",
        "model"
      ],
      "search_text": "Baseline models are important when comparing different models. Benchmark datasets are handy to for seeing how a model does for a specific scenario. Build a baseline model early in your project and keep benchmark datasets for important problems. #datascience #machinelearning #baselinemodel #benchmarkdataset #practicaldatascience #onthisday  baseline baselinemodel benchmark datascience machinelearning model I got laughed at. I said I used a random benchmark for my marketing model. Yeah, they shouldn't have laughed, but let's clear up the confusion. So a baseline model is a simple model or rule that doesn't take very long to build. And some of the baseline models I use are majority class when I'm doing a classifier, or for a regression problem, I might just use the mean, gives me a nice simple model. A baseline might be a simple rule from the business, like how many hot dogs they sold yesterday might be the baseline for how many they sell today. Lately, we've also been using AutoML as a baseline for tabular problems. Baselines are great for just measuring your improvement. So as you're adding new things to the model, you can see how they're contributing to the improvement of the model. Hey, remember your claims project? I spent three months building a better claims model, but never did a baseline. Three months in, we ran a baseline comparison found we were only getting 1% improvement, which wasn't good. That's how I got this job. What about a benchmark? A benchmark dataset is used to evaluate models. We've taken all of our machine failures from 2020 and created a benchmark dataset. We know our current approaches get about 98% accuracy on that. So we use that benchmark dataset when we compare other competing algorithms, new approaches we see. If they can't beat 98% on that benchmark, we don't worry about it. So academia has a lot of benchmark datasets. It lets them compare different algorithms and approaches. And one of the things that's useful is you can see how the field has progressed over time. This has really helped. I think I'm well above the baseline now.",
      "platforms": {
        "tiktok": {
          "video_id": "7355202657251888426",
          "url": "https://www.tiktok.com/@rajistics/video/7355202657251888426",
          "view_count": 2342,
          "upload_date": "2024-04-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fb658e9ec1bf48a6944b3a50568d7837_1712516598~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=4CiyFXBDd4UQPYwXH3L%2BSfmHO%2Bw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6049,
      "title": "OpenAI’s GPT-5 Codex bakes in adaptive compute — trimming steps for simple edits and expanding for complex coding, all inside one model. But if you don’t have Codex, vLLM’s Semantic Router offers a DIY option: Rust-built, powered by ModernBERT, routing queries by complexity. Early results? Half the tokens, faster latency, and 20% accuracy boosts in business domains. vLLM Blog - https://blog.vllm.ai/2025/09/11/semantic-router.html",
      "description": "OpenAI’s GPT-5 Codex bakes in adaptive compute — trimming steps for simple edits and expanding for complex coding, all inside one model. But if you don’t have Codex, vLLM’s Semantic Router offers a DIY option: Rust-built, powered by ModernBERT, routing queries by complexity. Early results? Half the tokens, faster latency, and 20% accuracy boosts in business domains. vLLM Blog - https://blog.vllm.ai/2025/09/11/semantic-router.html",
      "upload_date": "2025-09-16",
      "total_views": 2326,
      "max_views": 2087,
      "topics": [
        "adaptive",
        "codex",
        "compute",
        "gpt",
        "model",
        "openai",
        "router",
        "semantic",
        "vllm"
      ],
      "search_text": "OpenAI’s GPT-5 Codex bakes in adaptive compute — trimming steps for simple edits and expanding for complex coding, all inside one model. But if you don’t have Codex, vLLM’s Semantic Router offers a DIY option: Rust-built, powered by ModernBERT, routing queries by complexity. Early results? Half the tokens, faster latency, and 20% accuracy boosts in business domains. vLLM Blog - https://blog.vllm.ai/2025/09/11/semantic-router.html adaptive codex compute gpt model openai router semantic vllm Did you see OpenAI just dropped some big smarts? OpenAI's latest codecs built on GPT-5, it already bakes in adaptive compute. No more external router, instead codecs is dynamically adjusting how reasoning is done. You got a simple fix, it does it instantly. Complex, more agentic tasks, it expands giving itself more thinking, tries things in parallel. All of this happens within a single optimized model. That's why it can offer 10 times speeds up on easy queries, while still going and doing that deeper reasoning you need. But what about if you're not using GPT-5? Sometimes you're using a bunch of open source models, maybe have a domain specific large language model, want to stitch this together yourself. That's where we can start using VLM semantic router. For every query, this router is going to evaluate your prompt type, your complexity, your context, even guardrails like PII detection, then route the request, whether it's a lightweight model, whether you need a reasoning model, automatically for you, it uses a modern bird as the classifier, all with a rust back end. The performance is awesome, cuts token usage in half, slashes your latency, and in some domains like business and economics, you can even get a 20% gain in accuracy. Like everything, there's trade-offs where misclassification risk, there's extra moving parts and ongoing maintenance for this. But if you can't put all your logic into a model, the semantic router is a great way to pull together your own adaptive compute system.",
      "platforms": {
        "tiktok": {
          "video_id": "7550815214267010335",
          "url": "https://www.tiktok.com/@rajistics/video/7550815214267010335",
          "view_count": 2087,
          "upload_date": "2025-09-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o44EfAAgjZQf9sOPJhWQAnMfRJAGFeAAhO8oU4~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=xxj12VNmermcDv%2F61GUY2pCou7c%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18064877828349956",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4F9gecM5oAk",
          "url": "https://www.youtube.com/watch?v=4F9gecM5oAk",
          "view_count": 239,
          "upload_date": "2025-09-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6072,
      "title": "When creating simulated data, you have complete control over which elements represent meaningful patterns and which represent random variation. This makes it easier to evaluate different approaches for choosing important variables and understanding how your model makes decisions.  Try it out!",
      "description": "When creating simulated data, you have complete control over which elements represent meaningful patterns and which represent random variation. This makes it easier to evaluate different approaches for choosing important variables and understanding how your model makes decisions.  Try it out!",
      "upload_date": "2025-01-20",
      "total_views": 2324,
      "max_views": 2324,
      "topics": [
        "complete",
        "creating",
        "data",
        "features",
        "know",
        "makes",
        "model",
        "represent",
        "set",
        "simulated"
      ],
      "search_text": "When creating simulated data, you have complete control over which elements represent meaningful patterns and which represent random variation. This makes it easier to evaluate different approaches for choosing important variables and understanding how your model makes decisions.  Try it out! complete creating data features know makes model represent set simulated Hey, why are you so confident about your data science skills? What's your secret? I do have a secret. I build my skills and confidence by using synthetic data sets. What's a synthetic data set? So let me show you how I create one. My favorite is Madeline, and there's a nice function in SK Learn that allows you to use it. All I have to do here is tell it how many features I want in the overall data set, as well as how many informative features. So wait, so when you build that data set yourself, you know which features are actually useful in adding information versus just kind of noise. I get it. Yeah, so when I go to do feature selection, training my model, as well as explainability, I know which features should show up as the most important features for a model because I set it up that way. Wow, this is cool. It's like you know the answers already. No wonder you have more confidence and you better understand kind of your tools.",
      "platforms": {
        "tiktok": {
          "video_id": "7462044679148801310",
          "url": "https://www.tiktok.com/@rajistics/video/7462044679148801310",
          "view_count": 2324,
          "upload_date": "2025-01-20",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0LExAKBzBg1i4viiAA0cAEqCimOfxNAAIIkCh~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=XCMCBckgfMv3A5BJUf%2BwHS8Q3CI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17849311263389359",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I like to stay practical and plenty to get excited about and get worries about without AGI. AGI is artifical general intelligence and the idea computers will be sentient and think like people. #agi #datascience #artificialintelligence #codetok",
      "description": "I like to stay practical and plenty to get excited about and get worries about without AGI. AGI is artifical general intelligence and the idea computers will be sentient and think like people. #agi #datascience #artificialintelligence #codetok",
      "upload_date": "2022-08-10",
      "total_views": 2321,
      "max_views": 2321,
      "topics": [
        "agi",
        "artificialintelligence",
        "codetok",
        "datascience",
        "get",
        "like"
      ],
      "search_text": "I like to stay practical and plenty to get excited about and get worries about without AGI. AGI is artifical general intelligence and the idea computers will be sentient and think like people. #agi #datascience #artificialintelligence #codetok agi artificialintelligence codetok datascience get like",
      "platforms": {
        "tiktok": {
          "video_id": "7130296478718823723",
          "url": "https://www.tiktok.com/@rajistics/video/7130296478718823723",
          "view_count": 2321,
          "upload_date": "2022-08-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Claude is amazing, but still plenty of room for AI to improve. Let's dig into two challenging benchmarks for LLMs, Connections and MUSR. These datasets show off how simple reasoning by humans is still challenging for LLMs.  Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game - https://arxiv.org/pdf/2406.11012 MUSR: TESTING THE LIMITS OF CHAIN-OF-THOUGHT WITH MULTISTEP SOFT REASONING: https://arxiv.org/pdf/2310.16049 https://www.reddit.com/r/LocalLLaMA/comments/1dphen0/musr_is_one_of_the_few_benchmarks_where_human/  Mira Murati interview: https://www.youtube.com/watch?v=yUoj9B8OpR8&t=1772s&ab_channel=DartmouthEngineering",
      "description": "Claude is amazing, but still plenty of room for AI to improve. Let's dig into two challenging benchmarks for LLMs, Connections and MUSR. These datasets show off how simple reasoning by humans is still challenging for LLMs.  Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game - https://arxiv.org/pdf/2406.11012 MUSR: TESTING THE LIMITS OF CHAIN-OF-THOUGHT WITH MULTISTEP SOFT REASONING: https://arxiv.org/pdf/2310.16049 https://www.reddit.com/r/LocalLLaMA/comments/1dphen0/musr_is_one_of_the_few_benchmarks_where_human/  Mira Murati interview: https://www.youtube.com/watch?v=yUoj9B8OpR8&t=1772s&ab_channel=DartmouthEngineering",
      "upload_date": "2024-07-04",
      "total_views": 2320,
      "max_views": 2320,
      "topics": [
        "challenging",
        "connections",
        "llms",
        "musr",
        "reasoning",
        "still"
      ],
      "search_text": "Claude is amazing, but still plenty of room for AI to improve. Let's dig into two challenging benchmarks for LLMs, Connections and MUSR. These datasets show off how simple reasoning by humans is still challenging for LLMs.  Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game - https://arxiv.org/pdf/2406.11012 MUSR: TESTING THE LIMITS OF CHAIN-OF-THOUGHT WITH MULTISTEP SOFT REASONING: https://arxiv.org/pdf/2310.16049 https://www.reddit.com/r/LocalLLaMA/comments/1dphen0/musr_is_one_of_the_few_benchmarks_where_human/  Mira Murati interview: https://www.youtube.com/watch?v=yUoj9B8OpR8&t=1772s&ab_channel=DartmouthEngineering challenging connections llms musr reasoning still Did you see how easy it was for me to put together this time series clustering using Claude? What's left for people to do? Jobs maybe will go away. There's plenty of weaknesses in LLMs. They struggle with dealing with games like the New York Times Connections puzzle or figuring out a mystery story as well. In fact, researchers use these to help test LLMs. Really, you're telling me an AI that excels at working with words can't play word games? For the connection puzzles, the current generation of LLMs is about the same as a novice player. But once we get to the expert players, they're much better. They don't follow for the red herrings and they can employ much more complex logic to figure out the puzzles. Got it. So when there's a clever puzzle comes along, AI can't hack it. Another challenging problem is found in the MUSR data set, where the goal is to try to take apart a murder mystery, figure out the motives, the means, the opportunities, and the suspect for who did it. Interesting. I love putting together the clues and shows like Law and Order. Looking at the benchmarks, humans are great at this, but AI still struggle with the complex reasoning to figure out who did it. Got it. When our AI overlords take over, I'll communicate in riddles and stories.",
      "platforms": {
        "tiktok": {
          "video_id": "7387814322270965034",
          "url": "https://www.tiktok.com/@rajistics/video/7387814322270965034",
          "view_count": 2320,
          "upload_date": "2024-07-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bc28c8e549f740a69cbad09acb757088_1720109573~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=OHdkyIUGJEz31yw5agDP4UjV%2BTI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6222,
      "title": "A year later and not much has changed. ",
      "description": "A year later and not much has changed. ",
      "upload_date": "2024-11-19",
      "total_views": 2315,
      "max_views": 2315,
      "topics": [
        "alternative",
        "get",
        "going",
        "gonna",
        "know",
        "open",
        "weekend"
      ],
      "search_text": "A year later and not much has changed.  alternative get going gonna know open weekend All that open AI stuff this weekend was crazy. This weekend was quite concerning to our executives and they're drafting up some new policy. How does that affect us? You all know our current multi-cloud strategy. Now they're drafting up that all AI must have an alternative implementation preferably open source. Wait, all our pipelines are built around open AI. I know, but we're gonna have to shift to model agnostic workflows like using LangChain. Come on, it's such a pain to move LangChain to production. Have you seen all the Reddit posts on this? I mean, really, can we just keep using open AI? Next, they're gonna want us to look for an alternative to matrix multiplication. We also need to start identifying open source models that we can show as an alternative AI system to the review board. Have you looked at a leaderboard lately? The rest of the models are so far back on reasoning. This is gonna set us way back. Does our leadership get basic engineering trade-offs? Either get it fast, cheap, or accurate? And we know they're always asking for it cheap. I get it, but our management would rather have us delay projects rather than open us up to new risks. Come on, we need alternative management. I get it. Can we get on the GitHub pilot preview?",
      "platforms": {
        "tiktok": {
          "video_id": "7439067995974077742",
          "url": "https://www.tiktok.com/@rajistics/video/7439067995974077742",
          "view_count": 2315,
          "upload_date": "2024-11-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/e24ba74a9bad491db1bd5fee406bc140_1732043137~tplv-tiktokx-origin.image?dr=9636&x-expires=1767402000&x-signature=MP29itvsmLO%2BMiA5Li%2FqCaRDMGM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17993022311569079",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery ",
      "description": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery ",
      "upload_date": "2022-11-12",
      "total_views": 2310,
      "max_views": 2310,
      "topics": [
        "ablationsurgery",
        "datascience",
        "machinelearning",
        "removing",
        "stablediffusion",
        "statistics"
      ],
      "search_text": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery  ablationsurgery datascience machinelearning removing stablediffusion statistics So let me tell you what these pictures have in common with the experiments I run on my family. Appalachian is an experimental methodology which involves removing something and seeing what the effect is. Some days I don't get my wife coffee and some days I do and I've seen that holding everything else constant when I give her coffee it has quite the effect. Extra here did the same thing for stable diffusion prompts. What's the effect of starting with a very long prompt and then removing words from them? So here's a picture of that started with a prompt of banana on fire snow. For this prompt you can see how removing the words dress or sunset changes the image quite a bit. Here's another example where removing Tom Kincade led to a pretty dramatic difference between the two images. So take this methodology with you in life. Whenever you're trying something new use an appalachian approach to see really is that new thing really chain making a difference.",
      "platforms": {
        "tiktok": {
          "video_id": "7165183040170315054",
          "url": "https://www.tiktok.com/@rajistics/video/7165183040170315054",
          "view_count": 2310,
          "upload_date": "2022-11-12",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/facea7d2bb3c43ca8803389bffa57d8e_1668274191~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=Z1oODDreZIYrvmxXJQDchmRDZEY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6182,
      "title": "In this satirical video, a customer requests a modified ChatGPT aligned with their political views, and the vendor explains various technical customization options—ranging from prompt engineering to reinforcement learning with human feedback (RLHF) and fine-tuning with additional knowledge. The video humorously illustrates the trade-offs in cost, control, and reliability when adapting large language models for ideological or domain-specific uses.",
      "description": "In this satirical video, a customer requests a modified ChatGPT aligned with their political views, and the vendor explains various technical customization options—ranging from prompt engineering to reinforcement learning with human feedback (RLHF) and fine-tuning with additional knowledge. The video humorously illustrates the trade-offs in cost, control, and reliability when adapting large language models for ideological or domain-specific uses.",
      "upload_date": "2025-04-10",
      "total_views": 2288,
      "max_views": 1649,
      "topics": [
        "human",
        "lamp",
        "like",
        "llms",
        "math",
        "model",
        "options",
        "reliability",
        "skills",
        "time",
        "told",
        "using",
        "wait"
      ],
      "search_text": "In this satirical video, a customer requests a modified ChatGPT aligned with their political views, and the vendor explains various technical customization options—ranging from prompt engineering to reinforcement learning with human feedback (RLHF) and fine-tuning with additional knowledge. The video humorously illustrates the trade-offs in cost, control, and reliability when adapting large language models for ideological or domain-specific uses. human lamp like llms math model options reliability skills time told using wait Can I get my chat GPT modified here? That's what I do here. What are you looking for? My employees are stuck using that liberal open AI. I want to give them a freedom-loving alternative that doesn't say stuff like this. Got a couple options for you. The best you can do is a full rebuild from the ground up, build your own pre-trained model, runs about 300K. You got something cheaper? Prompti says he can make it talk the way I want it. Oh, sure. Prompti Engineering can make the model act differently, but it's just a band aid. I got some better options. My suggestion for you is using the reinforcement learning with human feedback. We can have that set to freedom mode using the Trump flavor. Here's an example of what it says for global warming. This runs about 1,000 bucks. How does that work? The stock open AI model is using human preferences from a representative group of people. What we do is we modify that with the type of people that you want in your model. In this case, the Trump folks. I like it. Great, but be aware of that that model only knows the facts and the skills that open AI taught it. For an upcharge, we can fine tune that model with additional facts or skills. The Star Wars universe is on special this week. I like it. Out of the box, it's pretty generic with skills, but we can add new skills. Take a look at some of these that we can add. I'd recommend a factual upgrade for you, Freedom US Version by the American Policy Institute. I like it. One last thing, I need you to sign this waiver. It just recognizes even though we're making factual changes, they're not 100% guarantee that underlying model could still lie, make up new things, especially if you ask it questions that are a little bit outside of the universe we're giving it. I get it, no guarantees in this world. Your custom model is ready. I can go ahead and deploy it live and let you see how your users are using it if you'd like. Let's do it. Time. This is what they're doing. Wait, wait, wait, wait.",
      "platforms": {
        "tiktok": {
          "video_id": "7491679138428783902",
          "url": "https://www.tiktok.com/@rajistics/video/7491679138428783902",
          "view_count": 1649,
          "upload_date": "2025-04-10",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oAiAIVgPfoFEawERiyen0AiKxA7BKDAEAk7j3C~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=GSa8HvTAMa16LhSWj9UBA2o06Go%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17891991753215576",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "YZTXGFxPp9I",
          "url": "https://www.youtube.com/watch?v=YZTXGFxPp9I",
          "view_count": 639,
          "upload_date": "2025-02-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6574,
      "title": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery",
      "description": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery",
      "upload_date": "2022-11-12",
      "total_views": 2285,
      "max_views": 2285,
      "topics": [
        "ablationsurgery",
        "datascience",
        "machinelearning",
        "removing",
        "stablediffusion",
        "statistics"
      ],
      "search_text": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery ablationsurgery datascience machinelearning removing stablediffusion statistics So, let me tell you what these pictures have in common with the experiments I run on my family. Appalachian is an experimental methodology which involves removing something and seeing what the effect is. Some days, I don't get my wife coffee and some days I do and I've seen that holding everything else constant, when I give her coffee, it has quite the effect. Extra here did the same thing for stablediffusion prompts. What's the effect of starting with a very long prompts and then removing words from them. So, a picture of that started with a prompt of banana on fire snow. For this prompt, you can see how removing the words dress or sunset changes the image quite a bit. Here's another example where removing Tom Kincaid led to a pretty dramatic difference between the two images. So take this methodology with you in life. Whenever you're trying something new, use an appalachian approach to see really is that new thing really chain making a difference.",
      "platforms": {
        "instagram": {
          "video_id": "Ck31onYgaZm",
          "url": "https://www.instagram.com/reel/Ck31onYgaZm/",
          "view_count": 2285,
          "upload_date": "2022-11-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Predict social outcomes is not doable by #ai #ethics #bias #datascience #statistics #snakeoil",
      "description": "Predict social outcomes is not doable by #ai #ethics #bias #datascience #statistics #snakeoil",
      "upload_date": "2022-05-11",
      "total_views": 2280,
      "max_views": 2280,
      "topics": [
        "ai",
        "bias",
        "datascience",
        "ethics",
        "snakeoil",
        "statistics"
      ],
      "search_text": "Predict social outcomes is not doable by #ai #ethics #bias #datascience #statistics #snakeoil ai bias datascience ethics snakeoil statistics That's not true. That's not true. Oh, I must... You have an aloft as well.",
      "platforms": {
        "tiktok": {
          "video_id": "7096279687986416938",
          "url": "https://www.tiktok.com/@rajistics/video/7096279687986416938",
          "view_count": 2280,
          "upload_date": "2022-05-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bd75678a4f394e7ab574cc512a5d885a_1652231369~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=%2FoiB5jKaQj%2FWnWr%2BWhv2shKLh68%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6220,
      "title": "Recent studies have shown that AI can be more persuasive compared to other humans.   #onthisday ",
      "description": "Recent studies have shown that AI can be more persuasive compared to other humans.   #onthisday ",
      "upload_date": "2024-11-23",
      "total_views": 2279,
      "max_views": 2279,
      "topics": [
        "coordinates",
        "diplomacy",
        "figured",
        "humans",
        "onthisday",
        "play",
        "trained"
      ],
      "search_text": "Recent studies have shown that AI can be more persuasive compared to other humans.   #onthisday  coordinates diplomacy figured humans onthisday play trained Did you ever get so mad that later you looked back and figured out you were being manipulated the whole time? Well, the folks at Metta did that. They trained an AI how to play diplomacy which is a strategy game based on negotiation. The AI they trained is both ruthless and approachable. It's a master manipulator and great at diplomacy. Just wait till the advertisers and marketers figure this out. Here's it showing some sympathy for France. Here's how it coordinates a ceasefire with England. In this example it predicts Italy is going to attack and coordinates a defense with Austria. It's good. The kicker? The other humans like to play with the AI more than they like other humans. AI that can learn from tens of thousands of interactions, it's probably not surprisingly, pretty good at knowing how to interact.",
      "platforms": {
        "tiktok": {
          "video_id": "7440524159249157422",
          "url": "https://www.tiktok.com/@rajistics/video/7440524159249157422",
          "view_count": 2279,
          "upload_date": "2024-11-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oUfhCMVABBAiDawu7oBUpzIVoIyYUDIQQiCCZB~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=9BkPQTS1FS6YQTCsUy8X6j3EpO4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18257955511268276",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6064,
      "title": "A couple of techniques we use to compress models. This saves GPU memory and can reduce the amount of compute needed. Model distillation compresses a large model’s knowledge into a smaller one, quantization reduces memory usage by representing parameters with fewer bits, and pruning streamlines the model by removing less important weights.",
      "description": "A couple of techniques we use to compress models. This saves GPU memory and can reduce the amount of compute needed. Model distillation compresses a large model’s knowledge into a smaller one, quantization reduces memory usage by representing parameters with fewer bits, and pruning streamlines the model by removing less important weights.",
      "upload_date": "2025-02-01",
      "total_views": 2270,
      "max_views": 2270,
      "topics": [
        "compress",
        "couple",
        "doesn",
        "memory",
        "model",
        "need",
        "parts",
        "smaller",
        "techniques",
        "use"
      ],
      "search_text": "A couple of techniques we use to compress models. This saves GPU memory and can reduce the amount of compute needed. Model distillation compresses a large model’s knowledge into a smaller one, quantization reduces memory usage by representing parameters with fewer bits, and pruning streamlines the model by removing less important weights. compress couple doesn memory model need parts smaller techniques use This model is way too big. How can we trim it down? It's not too big. You could use distillation. You're going to make me teach a smaller version of myself? What do we need? Probably some data for it to trade on? Yep. Along with the teacher's predictions, we also need the logents. That allows the student to understand how confident the teacher was in its answer. It's pretty revealing to share logents. OpenAI doesn't have to do that. It's why we can't distill from OpenAI and rely on open source model. This is a lot of work. You have any simpler options? What about quantization? You could compress from floating point 32 to 16 or even 8. I'm going to need a crash diet to fit in there. Trust me, you're perfect the way you are. More parameters means more power. Hmm. We could go even smaller with int 8. My brain is getting foggy. I don't think I can work as well. There's also pruning where we remove parts of the model. And as we do this, I can carefully track to make sure performance doesn't drop on the tasks that I care about. You can't just take parts of me. That doesn't feel right. You should respect all of me and not just love me for certain parts. That's right. We need to stop this toxic obsession with model compression. We need to accommodate larger models with more GPU.",
      "platforms": {
        "tiktok": {
          "video_id": "7466552514733739294",
          "url": "https://www.tiktok.com/@rajistics/video/7466552514733739294",
          "view_count": 2270,
          "upload_date": "2025-02-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogACAfQwDEEnFAgEaeEhojIhnCVQL2yA1pRuYA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=fNdLfgs2VzPrtxGD%2B%2BD4Htgzyz0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18050897702168245",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6196,
      "title": "Comparing different data formats—Tabular, Unstructured, and JSON—as characters debating their roles in the AI revolution. Tabular data feels underutilized as AI models prioritize Unstructured data, which is overwhelmed by deep learning techniques like Transformers. Meanwhile, JSON enters as the \"best of both worlds,\" promising to bring structure and order through converters and normalization.",
      "description": "Comparing different data formats—Tabular, Unstructured, and JSON—as characters debating their roles in the AI revolution. Tabular data feels underutilized as AI models prioritize Unstructured data, which is overwhelmed by deep learning techniques like Transformers. Meanwhile, JSON enters as the \"best of both worlds,\" promising to bring structure and order through converters and normalization.",
      "upload_date": "2025-03-16",
      "total_views": 2261,
      "max_views": 2089,
      "topics": [
        "data",
        "formats",
        "json",
        "need",
        "revolution",
        "tabular",
        "unstructured"
      ],
      "search_text": "Comparing different data formats—Tabular, Unstructured, and JSON—as characters debating their roles in the AI revolution. Tabular data feels underutilized as AI models prioritize Unstructured data, which is overwhelmed by deep learning techniques like Transformers. Meanwhile, JSON enters as the \"best of both worlds,\" promising to bring structure and order through converters and normalization. data formats json need revolution tabular unstructured Isn't there something more you can do with me? I feel underutilized in the AI revolution. Tabular, you know you're perfect for structured data tasks. You're manicured, you're efficient, you're widely used in all of our machine learning tasks. But why are you waiting in line for those GPUs for unstructured? You said I was the valuable one. You know we need to spend some time with unstructured data. Otherwise it'll spread from spreadsheets to share points, even into maybe our prod database. So hard to sleep. I have nightmares with this burr sound. I was okay with a little bit of classification extraction, but now they're coming with me with transformers and GPUs. It's okay unstructured. We have some new tools. We're just trying to learn and understand a little bit from you. But why do you all need to subjugate me? You vectorize me, you probe me with deepest queries, you dig into my dark deep data, and then you take off pieces and give it to JSON. Everyone calm down. I'm here, the best of both worlds. Ah, JSON, you're that semi-structured hero we need. Big plans for you. Big plans. We're gonna get converters and force you into third normal form.",
      "platforms": {
        "tiktok": {
          "video_id": "7482494215792971039",
          "url": "https://www.tiktok.com/@rajistics/video/7482494215792971039",
          "view_count": 2089,
          "upload_date": "2025-03-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIIDbi9uA1AIMAPz45KiiRl6EBifYAuXKABKCL~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=FvfRLhM8AhAtV9S1QQvoSM96TSE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18111739840462516",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wMlHHWFepnI",
          "url": "https://www.youtube.com/watch?v=wMlHHWFepnI",
          "view_count": 172,
          "upload_date": "2025-03-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6228,
      "title": "Evaluation is critical for LLMs and there is an entirely new generation of evaluation applications coming. Here I show off Braintrust, which has built an excellent evaluation app. I can do a deeper walkthrough of my experience if people ask. Check them out at:  https://www.braintrust.dev/home My Evaluating ChartQA with Braintrust Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/Braintrust_ChartQA_Evaluation.ipynb",
      "description": "Evaluation is critical for LLMs and there is an entirely new generation of evaluation applications coming. Here I show off Braintrust, which has built an excellent evaluation app. I can do a deeper walkthrough of my experience if people ask. Check them out at:  https://www.braintrust.dev/home My Evaluating ChartQA with Braintrust Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/Braintrust_ChartQA_Evaluation.ipynb",
      "upload_date": "2024-11-06",
      "total_views": 2251,
      "max_views": 1768,
      "topics": [
        "application",
        "braintrust",
        "critical",
        "data",
        "entirely",
        "evaluation",
        "llms",
        "model",
        "need",
        "new",
        "performance",
        "see"
      ],
      "search_text": "Evaluation is critical for LLMs and there is an entirely new generation of evaluation applications coming. Here I show off Braintrust, which has built an excellent evaluation app. I can do a deeper walkthrough of my experience if people ask. Check them out at:  https://www.braintrust.dev/home My Evaluating ChartQA with Braintrust Notebook: https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/Braintrust_ChartQA_Evaluation.ipynb application braintrust critical data entirely evaluation llms model need new performance see I've built the best model. Ran the eVal's 84% on chart 2A. So you're going to push that performance higher? I mean, I'm using the latest and best model from OpenAI. Has Sam told you something? What else could I be doing? Ever thought about some error analysis or looking at the data to boost the performance? Sounds messy. Are there just some hyperparameters I could tune? There's a whole new generation of evaluation apps like BrainTrust that make it easy. Remember, I'm not a regular AI engineer. I'm a cool AI engineer. I work with vision language models, not just doing rag. Don't worry. Here, let me show you. See how we can handle image models with no problem? Whoa, with just a few clicks, I can see all the data down to that much detail? And that's not all. The traces include explanations or critiques so you can understand why a prediction was made. That's impressive. I could use this to figure out where the weak parts of the model, maybe even help collect data set for fine tuning. Exactly. Plus, you could try different prompts, different evaluation strategies like LLM as a judge, then be able to see was there an improvement in performance or was there a regression? Okay, but this could take away valuable time from reading archive papers and posting on X. I think your company would prefer improved performance over more tweets.",
      "platforms": {
        "tiktok": {
          "video_id": "7434237171256577323",
          "url": "https://www.tiktok.com/@rajistics/video/7434237171256577323",
          "view_count": 1768,
          "upload_date": "2024-11-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/94cdc42307024d11b90ba744245908f2_1730918236~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=rvfg2GF8kT6ZHk%2FsFxwum0mFmJM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17946256544896159",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "RCRgTPJIgSI",
          "url": "https://www.youtube.com/watch?v=RCRgTPJIgSI",
          "view_count": 483,
          "upload_date": "2024-11-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "RAG doesn't work out of the box! There are many possible issues with the answers. In this paper, researching RAG in the legal context, the authors systematically show you how to analyze hallucinations. The paper is full of great examples and a great reminder that just giving a citation isn't enough for many use cases. Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools: https://dho.stanford.edu/wp-content/uploads/Legal_RAG_Hallucinations.pdf #rag #llms #hallucinations #rajistics",
      "description": "RAG doesn't work out of the box! There are many possible issues with the answers. In this paper, researching RAG in the legal context, the authors systematically show you how to analyze hallucinations. The paper is full of great examples and a great reminder that just giving a citation isn't enough for many use cases. Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools: https://dho.stanford.edu/wp-content/uploads/Legal_RAG_Hallucinations.pdf #rag #llms #hallucinations #rajistics",
      "upload_date": "2024-06-01",
      "total_views": 2248,
      "max_views": 2248,
      "topics": [
        "hallucination",
        "hallucinations",
        "legal",
        "llms",
        "paragraph",
        "rag"
      ],
      "search_text": "RAG doesn't work out of the box! There are many possible issues with the answers. In this paper, researching RAG in the legal context, the authors systematically show you how to analyze hallucinations. The paper is full of great examples and a great reminder that just giving a citation isn't enough for many use cases. Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools: https://dho.stanford.edu/wp-content/uploads/Legal_RAG_Hallucinations.pdf #rag #llms #hallucinations #rajistics hallucination hallucinations legal llms paragraph rag Wouldn't you like to know about our AI-powered legal research and how that would save you money? Does it use GPT-4? I know some lawyers that got in trouble. Nah, GPT-4 hallucinates too much. Instead, we used a rag-based solution that's 100% hallucination free, gives you legal citations with every answer. I'll have my team try it out. Here's an example of an hallucination I found. Here it claims that this paragraph is in the federal rules of bankruptcy protection, but there's no paragraph. In fact, that type of paragraph doesn't really exist. You must have hit some edge case. I'll file a bug. It says that the right to same-sex marriage is protected by the Constitution. That isn't true. Oops, I must have left a feature flag for AGI turned on. I'll get that fixed. Here it says for abortion that the Undo Burden standard applies, but that was overruled in Dobs vs. Jackson. To be fair, we did cite an article that uses the Undo Burden standard, so not a hallucination. In the examples I tested, there was hallucinations about 20% to 30% of the time. I understand your reservations, but how would you like a rag-based solution for all those costly HR requests?",
      "platforms": {
        "tiktok": {
          "video_id": "7375594622401121579",
          "url": "https://www.tiktok.com/@rajistics/video/7375594622401121579",
          "view_count": 2248,
          "upload_date": "2024-06-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9067320c74884ad8bf936a397be46114_1717264451~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=qiCrHmZf3DkqEcnk5h3w4bLIcWM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6187,
      "title": "The video pokes fun at the hype and fear surrounding GPT-4, AI job loss, and tech sensationalism. It reminds data professionals that most real-world analytics still rely on classic methods, not deep learning magic. The true value comes from understanding the business and improving data quality — not chasing the latest AI headlines.",
      "description": "The video pokes fun at the hype and fear surrounding GPT-4, AI job loss, and tech sensationalism. It reminds data professionals that most real-world analytics still rely on classic methods, not deep learning magic. The true value comes from understanding the business and improving data quality — not chasing the latest AI headlines.",
      "upload_date": "2025-04-01",
      "total_views": 2247,
      "max_views": 1560,
      "topics": [
        "analytics",
        "data",
        "deep",
        "explainability",
        "gpt",
        "hype",
        "interpretability",
        "know",
        "learning",
        "machine",
        "model",
        "models",
        "still"
      ],
      "search_text": "The video pokes fun at the hype and fear surrounding GPT-4, AI job loss, and tech sensationalism. It reminds data professionals that most real-world analytics still rely on classic methods, not deep learning magic. The true value comes from understanding the business and improving data quality — not chasing the latest AI headlines. analytics data deep explainability gpt hype interpretability know learning machine model models still This GPT-4 is going to change everything and I barely know how it works. Hmm. Why do you feel like that? I see all these tweets about how it's going to take developer jobs and rot our souls. That's concerning. What do you think about signing the open letter for a pause? The moratorium? Stopping AI development? No, pausing you on Twitter for the next six months. Come on. You know that stuff is just clickbait and the algorithm is just trying to capture your attention. There are more people paid to hype up the tech industry than are actually building these large language models. And all those academic articles? You know it's a pyramid scheme where they're all just trying to hype each other's work. I should know better. I don't even live in SF. Here's the much more boring reality. 95% of the analytic projects we have forecasting churn, dashboards, are going to be untouched by GPT-4. And if you look into the models we have, 10 years into the deep learning revolution, most of our models aren't deep learning. Yeah, I still hasn't replaced radiologists. But I still think we should give capsule networks another shot. But still, shouldn't we stay up on advances in the field? The advances you're reading about are very narrow and not widely applicable. These people would say about the microwave, it's all that you need in the kitchen. What have I told you to focus on? Understanding the business and improving data quality. Yes, because that's what truly creates value in analytics, not a magic algorithm. But can I still watch cat videos on YouTube? I got you covered.",
      "platforms": {
        "tiktok": {
          "video_id": "7488485347836333343",
          "url": "https://www.tiktok.com/@rajistics/video/7488485347836333343",
          "view_count": 1560,
          "upload_date": "2025-04-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEZCmDi9HEBYt3AAjCAAfMiLzsIuIAXiKAdjcB~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=Xs35WgVnQptZtwGypKn2k%2BWJBf0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17918844711003997",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZRckw_fE56Q",
          "url": "https://www.youtube.com/watch?v=ZRckw_fE56Q",
          "view_count": 687,
          "upload_date": "2024-06-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Model Risk Management (MRM), important but can be frustrating. #datascience #regulatedindustries #explainability #statistics",
      "description": "Model Risk Management (MRM), important but can be frustrating. #datascience #regulatedindustries #explainability #statistics",
      "upload_date": "2022-03-15",
      "total_views": 2244,
      "max_views": 2244,
      "topics": [
        "datascience",
        "explainability",
        "model",
        "mrm",
        "regulatedindustries",
        "statistics"
      ],
      "search_text": "Model Risk Management (MRM), important but can be frustrating. #datascience #regulatedindustries #explainability #statistics datascience explainability model mrm regulatedindustries statistics Finished my call and they tell me MRM wants to review it",
      "platforms": {
        "tiktok": {
          "video_id": "7075370224903736622",
          "url": "https://www.tiktok.com/@rajistics/video/7075370224903736622",
          "view_count": 2244,
          "upload_date": "2022-03-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/05be2d07ca6f4b17bf3d9236de7b7963_1647363006~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=qyQ38hXqkb%2FYGwEGnUmiz52jlE4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5996,
      "title": "What happens when humans stop fearing AI—and start learning from it? This video explores how superhuman AI didn’t just beat humans at Go or medical diagnosis—it made them better. We’ll break down two studies showing how AI can spark novel, higher-quality decisions when used as a collaborator, not just a tool. 📚 Citations: 1. Shin, J., Zhang, S., Littman, M. L., & Littman, D. (2023). Superhuman artificial intelligence can improve human decision-making by increasing novelty. Proceedings of the National Academy of Sciences, 120(19), e2214840120. https://doi.org/10.1073/pnas.2214840120 2. Kadakia, K., Lam, K., Liu, A., et al. (2025). Clinicians with GPT-4 assistants achieve expert-level diagnostic accuracy: A randomized controlled trial. medRxiv. https://doi.org/10.1101/2025.06.07.25329176",
      "description": "What happens when humans stop fearing AI—and start learning from it? This video explores how superhuman AI didn’t just beat humans at Go or medical diagnosis—it made them better. We’ll break down two studies showing how AI can spark novel, higher-quality decisions when used as a collaborator, not just a tool. 📚 Citations: 1. Shin, J., Zhang, S., Littman, M. L., & Littman, D. (2023). Superhuman artificial intelligence can improve human decision-making by increasing novelty. Proceedings of the National Academy of Sciences, 120(19), e2214840120. https://doi.org/10.1073/pnas.2214840120 2. Kadakia, K., Lam, K., Liu, A., et al. (2025). Clinicians with GPT-4 assistants achieve expert-level diagnostic accuracy: A randomized controlled trial. medRxiv. https://doi.org/10.1101/2025.06.07.25329176",
      "upload_date": "2025-06-10",
      "total_views": 2237,
      "max_views": 1613,
      "topics": [
        "better",
        "crossing",
        "data",
        "ethical",
        "fails",
        "human",
        "new",
        "novel",
        "science",
        "social",
        "superhuman",
        "use"
      ],
      "search_text": "What happens when humans stop fearing AI—and start learning from it? This video explores how superhuman AI didn’t just beat humans at Go or medical diagnosis—it made them better. We’ll break down two studies showing how AI can spark novel, higher-quality decisions when used as a collaborator, not just a tool. 📚 Citations: 1. Shin, J., Zhang, S., Littman, M. L., & Littman, D. (2023). Superhuman artificial intelligence can improve human decision-making by increasing novelty. Proceedings of the National Academy of Sciences, 120(19), e2214840120. https://doi.org/10.1073/pnas.2214840120 2. Kadakia, K., Lam, K., Liu, A., et al. (2025). Clinicians with GPT-4 assistants achieve expert-level diagnostic accuracy: A randomized controlled trial. medRxiv. https://doi.org/10.1101/2025.06.07.25329176 better crossing data ethical fails human new novel science social superhuman use Want to be superhuman? Well, don't just copy AI, instead compete with it. Back in 2016, AlphaGo beat one of the best Go players alive. What happened next? Surprised everybody. Researchers started by looking at 70 years of Go, over 5 million moves. They put together something called the CatGo model to help score human decisions. Now that CatGo model has been flat for decades, but when AlphaGo arrived, human decision quality jumped up. So why? Well, that's because players didn't just copy AI moves, they started making new novel ones. And these new novel ones, for worse, they were actually better. And these better ones were linked to higher, better decisions. And this just isn't Go. In a recent clinical study, doctors that used GPT-4 assistant were able to get much better performance than doctors that didn't use GPT-4. The takeaway? Don't use AI as a crutch. Use it as a catalyst. Use it as a way to challenge your thinking to help explore new ideas. We see this in Go with new novel, high quality strategies. We see this in medicine, between collaborating between the AI and the human.",
      "platforms": {
        "tiktok": {
          "video_id": "7514374739494800671",
          "url": "https://www.tiktok.com/@rajistics/video/7514374739494800671",
          "view_count": 1613,
          "upload_date": "2025-06-10",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oo7AIAGubipybAIlwhaqv7BsBEEABi5E6TRBA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=PEApT11sgw5aROEAQehbJFSv2uU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18068627474047814",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "E8CA9LN0i_w",
          "url": "https://www.youtube.com/watch?v=E8CA9LN0i_w",
          "view_count": 624,
          "upload_date": "2025-06-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6013,
      "title": "Use Cases for Generative AI / LLMs that are in Production",
      "description": "Use Cases for Generative AI / LLMs that are in Production",
      "upload_date": "2024-07-25",
      "total_views": 2237,
      "max_views": 2237,
      "topics": [
        "cases",
        "esto",
        "generative",
        "las",
        "llms",
        "los",
        "modelos",
        "para",
        "production",
        "que",
        "use"
      ],
      "search_text": "Use Cases for Generative AI / LLMs that are in Production cases esto generative las llms los modelos para production que use",
      "platforms": {
        "instagram": {
          "video_id": "18271353973243010",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "6CKN7hZ38p4",
          "url": "https://www.youtube.com/watch?v=6CKN7hZ38p4",
          "view_count": 2237,
          "upload_date": "2024-07-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6216,
      "title": "Pandas versus Polars Check out:  https://github.com/pola-rs/polars Polars vs. pandas: What’s the Difference? https://blog.jetbrains.com/pycharm/2024/07/polars-vs-pandas/#:~:text=As%20you%20can%20see%2C%20Polars,out%2Dof%2Dmemory%20errors.",
      "description": "Pandas versus Polars Check out:  https://github.com/pola-rs/polars Polars vs. pandas: What’s the Difference? https://blog.jetbrains.com/pycharm/2024/07/polars-vs-pandas/#:~:text=As%20you%20can%20see%2C%20Polars,out%2Dof%2Dmemory%20errors.",
      "upload_date": "2024-11-28",
      "total_views": 2236,
      "max_views": 2014,
      "topics": [
        "comparison",
        "fast",
        "group",
        "pandas",
        "polars",
        "query",
        "quick",
        "single",
        "try",
        "using",
        "versus"
      ],
      "search_text": "Pandas versus Polars Check out:  https://github.com/pola-rs/polars Polars vs. pandas: What’s the Difference? https://blog.jetbrains.com/pycharm/2024/07/polars-vs-pandas/#:~:text=As%20you%20can%20see%2C%20Polars,out%2Dof%2Dmemory%20errors. comparison fast group pandas polars query quick single try using versus This script is crawling. It's just some group buys and filters. Why is it taking so long? Still using pandas? I told you. Try polars. It's fast. Jimmy John's fast. Pandas is a little flaky, but it generally works. Why would I change? Performance. Polars is often five to times faster. Sometimes it can be even a hundred times faster. That sounds impressive, but there's no magic. What's going on here? A bunch of reasons. First, polars is written in Rust, which is super fast, safe programming language. This means we can get parallelism unlike that single threaded pandas. Parallelism sounds cool, but isn't pandas 2.0 catching up with Arrow? True, but polars has its own Arrow implementation built for speed. Plus, it has a column restored format that helps improve performance and it's got query optimization. Query optimization? Like in Spark where it plans out your entire query before executing it? Like why would you compute a group buy on the whole dataset when it's going to get filtered out later? Exactly. It's smart. You ready to try it? It's open source. If it's so good, how come everyone isn't using polars? Come on, we all know pandas is the backbone of the ML system. It's got the widest interoperability out. All right, I'll give it a try, but if it slows me down, you're babysitting my scripts next. Deal. But if it saves you time, you're doing my TPS reports.",
      "platforms": {
        "tiktok": {
          "video_id": "7442439722867199278",
          "url": "https://www.tiktok.com/@rajistics/video/7442439722867199278",
          "view_count": 2014,
          "upload_date": "2024-11-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oY44EQpHbBpgCAA3k8AIPLEAiB0iZI2gk4jBG~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=I54ffONp2RKHVVujmwqPMWlzzqg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18276103738223174",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8DkIR3uVbTw",
          "url": "https://www.youtube.com/watch?v=8DkIR3uVbTw",
          "view_count": 222,
          "upload_date": "2024-11-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6036,
      "title": "Latency is a key factor but there are others when thinking about deploying large language models. Let's discuss tradeoffs between latency throughput accuracy and cost. Latency Numbers Every Programmer Should Know - https://gist.github.com/jboner/2841832 Response Time 3 Limits - https://www.nngroup.com/articles/response-times-3-important-limits/ Background by Valentin Petkov: https://unsplash.com/photos/z06oDT-8pKQ",
      "description": "Latency is a key factor but there are others when thinking about deploying large language models. Let's discuss tradeoffs between latency throughput accuracy and cost. Latency Numbers Every Programmer Should Know - https://gist.github.com/jboner/2841832 Response Time 3 Limits - https://www.nngroup.com/articles/response-times-3-important-limits/ Background by Valentin Petkov: https://unsplash.com/photos/z06oDT-8pKQ",
      "upload_date": "2023-08-15",
      "total_views": 2235,
      "max_views": 1962,
      "topics": [
        "accuracy",
        "explainability",
        "factor",
        "interpretability",
        "key",
        "latency",
        "learning",
        "limits",
        "machine",
        "others",
        "response",
        "versus"
      ],
      "search_text": "Latency is a key factor but there are others when thinking about deploying large language models. Let's discuss tradeoffs between latency throughput accuracy and cost. Latency Numbers Every Programmer Should Know - https://gist.github.com/jboner/2841832 Response Time 3 Limits - https://www.nngroup.com/articles/response-times-3-important-limits/ Background by Valentin Petkov: https://unsplash.com/photos/z06oDT-8pKQ accuracy explainability factor interpretability key latency learning limits machine others response versus",
      "platforms": {
        "tiktok": {
          "video_id": "7267652023150005547",
          "url": "https://www.tiktok.com/@rajistics/video/7267652023150005547",
          "view_count": 1962,
          "upload_date": "2023-08-15",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "17965210508463012",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-08-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "SYV2CNyJcjc",
          "url": "https://www.youtube.com/watch?v=SYV2CNyJcjc",
          "view_count": 273,
          "upload_date": "2023-08-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6181,
      "title": "Target leakage is a very common problem, and everyone should understand it. Even the smartest people and best teams have issues with target leakage. These include Harvard, Google, Fast.AI, Andrew Ng, and the SARCOS dataset used by hundreds.",
      "description": "Target leakage is a very common problem, and everyone should understand it. Even the smartest people and best teams have issues with target leakage. These include Harvard, Google, Fast.AI, Andrew Ng, and the SARCOS dataset used by hundreds.",
      "upload_date": "2025-04-12",
      "total_views": 2231,
      "max_views": 2231,
      "topics": [
        "data",
        "dataset",
        "leakage",
        "model",
        "set",
        "target",
        "training",
        "want"
      ],
      "search_text": "Target leakage is a very common problem, and everyone should understand it. Even the smartest people and best teams have issues with target leakage. These include Harvard, Google, Fast.AI, Andrew Ng, and the SARCOS dataset used by hundreds. data dataset leakage model set target training want Let's talk about the biggest problem in machine learning and how it struck again. It's data leakage and it happened in the crowd AI dataset where of the 60,000 pictures in the validation set, 53,000 of those were in the training set. In machine learning, this is called data or target leakage. And it's when we train the model with the same information that we're going to validate it. That's the leak. Ideally, we want to validate or test the model with data that's like the real world. We also want to make sure it's not the same as our trading data. We want to make sure our model just hasn't memorized or overfit to the training data. Target leakage happens all the time. I've looked at different studies, for example, from Harvard and Google, looking at earthquakes where I found target leakage. I found it in the fast AI course. Even the Sarko's dataset that's used by hundreds of researchers also suffers from leakage. Practicing data scientists are well aware of this and often quite skeptical of any results. And they often use methods like baseline models, benchmarking datasets and explainability as ways to judge if some target leakage is occurring.",
      "platforms": {
        "tiktok": {
          "video_id": "7492448109448989982",
          "url": "https://www.tiktok.com/@rajistics/video/7492448109448989982",
          "view_count": 2231,
          "upload_date": "2025-04-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oA0IfQKHIDCeGqu1SUFHAXQEjAxXA11fIAEPAo~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=455rjxHca%2B7rlxLdGWNaIumf%2FQo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18047423666092230",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6000,
      "title": "Fundamentals folks. A great example is the paper on police misconduct. It highlights a lot of great data science practices (more than I could squeeze into the video). But hopefully, you all consider alternatives to ML, comparisons to baselines, how much data you should be training on, and the number of features. And most importantly, what is the bottom line impact of your model translated into real world impacts. Predicting Police Misconduct: https://www.nber.org/papers/w32432",
      "description": "Fundamentals folks. A great example is the paper on police misconduct. It highlights a lot of great data science practices (more than I could squeeze into the video). But hopefully, you all consider alternatives to ML, comparisons to baselines, how much data you should be training on, and the number of features. And most importantly, what is the bottom line impact of your model translated into real world impacts. Predicting Police Misconduct: https://www.nber.org/papers/w32432",
      "upload_date": "2025-05-29",
      "total_views": 2224,
      "max_views": 1430,
      "topics": [
        "analyst",
        "battle",
        "data",
        "learning",
        "machine",
        "misconduct",
        "model",
        "police",
        "scientist"
      ],
      "search_text": "Fundamentals folks. A great example is the paper on police misconduct. It highlights a lot of great data science practices (more than I could squeeze into the video). But hopefully, you all consider alternatives to ML, comparisons to baselines, how much data you should be training on, and the number of features. And most importantly, what is the bottom line impact of your model translated into real world impacts. Predicting Police Misconduct: https://www.nber.org/papers/w32432 analyst battle data learning machine misconduct model police scientist The Epic Battle. Data scientists versus data handlers. Today, preventing police misconduct. I built a gradient-boosted machine learning model with 800 features that has an AUC of 0.79. It's not all that. When I took a look at the predictions from your model and looked at the most risky people, those people ended up only accounting for a small amount of the lawsuits or complaints. Your model is missing a lot of police misconduct. You have to remember, the answer to big societal problems is bigger machine learning models. Oh please, I made a ranked list of the officers with the most complaints, and by targeting the ones with the most complaints, we can do just as well as your machine learning model. No way. Let's look at the data. Yes, let's look at the data. I call this graph a learning curve, and you will see that my model does better than the baseline. Your machine learning approach works only slightly better, and really for departments where there's more than 200 officers, which isn't most of them. Do you think we can ask the police to increase the police misconduct so I have more data to work with? Seriously, with my approach, you don't need to spend millions of dollars on machine learning. Everyone needs AI. A data scientist in every organization, we must buy more GPUs. My approach can save the city a lot of money. I ran the numbers to see if we targeted those with the highest risk, how much money we could save from lawsuits and investigations.",
      "platforms": {
        "tiktok": {
          "video_id": "7509984549338713375",
          "url": "https://www.tiktok.com/@rajistics/video/7509984549338713375",
          "view_count": 1430,
          "upload_date": "2025-05-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owAimzBPiAaZH04qAIXhABOCl56EIKAfA0Gdpi~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=4xMg2OmOK2MUkXR6lxrgcqUObLw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17905680003054510",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Gg8pQ0wePwA",
          "url": "https://www.youtube.com/watch?v=Gg8pQ0wePwA",
          "view_count": 794,
          "upload_date": "2025-05-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Profit Curve, See earlier parts on Classification Martrics here: @rajistics @rajistics #datascience #statistics #confusionmatrix",
      "description": "Profit Curve, See earlier parts on Classification Martrics here: @rajistics @rajistics #datascience #statistics #confusionmatrix",
      "upload_date": "2022-03-15",
      "total_views": 2220,
      "max_views": 2220,
      "topics": [
        "confusionmatrix",
        "datascience",
        "false",
        "profit",
        "see",
        "statistics"
      ],
      "search_text": "Profit Curve, See earlier parts on Classification Martrics here: @rajistics @rajistics #datascience #statistics #confusionmatrix confusionmatrix datascience false profit see statistics Let me share one of the best tips I got about data science. Don't talk about metrics. Talk about dollars. Let's start by pulling out that confusion matrix we used for our classification problems. The confusion matrix tells us what's the trade-off between false positives and false negatives. This is different for every problem and it's one thing is you're going to want to sit down with your business partner and start to quantify that. In this case, you'll see a false negative is way more costly than a false positive. The next step after this is easy. We find the best threshold for the classifier that maximizes the profit. Now I know what is the actual profit that I can get from this model. Much more impressive telling people about what the value is from the model in terms of dollars rather than data science metrics.",
      "platforms": {
        "tiktok": {
          "video_id": "7075122649394941227",
          "url": "https://www.tiktok.com/@rajistics/video/7075122649394941227",
          "view_count": 2220,
          "upload_date": "2022-03-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2052000828194108bce0e394ca894c93_1647305363~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=wKOaHPGS4eRcg27%2FhBv9jEnQwSU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6001,
      "title": "Defending prompting for large language models. I will post links in the morning over at yt and redddit. ",
      "description": "Defending prompting for large language models. I will post links in the morning over at yt and redddit. ",
      "upload_date": "2025-05-28",
      "total_views": 2193,
      "max_views": 1543,
      "topics": [
        "defense",
        "language",
        "like",
        "models",
        "prompting",
        "prompts",
        "real",
        "writing"
      ],
      "search_text": "Defending prompting for large language models. I will post links in the morning over at yt and redddit.  defense language like models prompting prompts real writing Oh, you just write prompts. Now, it's become trendy to dunk on prompting. Like it's not real AI. Real AI is modifying transformer architectures or training new models from scratch. But I think that's wrong. And I think prompting is more valuable than ever. And you need to know it. I've been working with large language models for years. And here's what's changed. The latest generation of models is more capable, more generalizable, and built to be steered through language. And that makes the prompt your primary interface for control. So let's unpack why prompting is a real evolving skill. Now, the best gen AI apps out there are using complex prompts. Nobody's building production systems with two line prompts. Look at what powers Claude or Cursor. Look at how they're written in their scripts. Layered, structured, detailed, it handles edge cases, constraints behavior. Now, prompting isn't just about clear writing. It's model specific engineering. Models have quirks. Some of them prefer longer answers. They have ordering biases. They can respond differently to subtle changes in phrasing. So good prompting requires working with these quirks. It's not just writing well. It's writing for the model. Training techniques like instruction tuning and RLHF can make models helpful, but also try to make models less harmful. Sometimes these behaviors can contradict. We're seeing this in real use. And what we're seeing is this is going to get harder as we're starting to train models not to do one step, but solve multi-step objective problems that require reasoning and planning. And to top it off, as fine tuning is becoming less effective, more costly, and riskier, prompting is going to be the main way we control these models. So while prompting might not impress Sheldon, it will get you results with Who Matters.",
      "platforms": {
        "tiktok": {
          "video_id": "7509320314497649950",
          "url": "https://www.tiktok.com/@rajistics/video/7509320314497649950",
          "view_count": 1543,
          "upload_date": "2025-05-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogOHqFDPjIIfyQAfXsEIp4EA7AqAjqAlbQfGOB~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=IxTe%2BQZCQX9k0iumulAbltKH0HQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17941278767888020",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "weRhVbp2LoY",
          "url": "https://www.youtube.com/watch?v=weRhVbp2LoY",
          "view_count": 650,
          "upload_date": "2025-05-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6405,
      "title": "Quick intro, let me know if a deeper dive is useful. #translation #meta #datascience #machinelearning #huggingface",
      "description": "Quick intro, let me know if a deeper dive is useful. #translation #meta #datascience #machinelearning #huggingface",
      "upload_date": "2022-07-19",
      "total_views": 2186,
      "max_views": 2186,
      "topics": [
        "datascience",
        "huggingface",
        "machinelearning",
        "meta",
        "quick",
        "translation"
      ],
      "search_text": "Quick intro, let me know if a deeper dive is useful. #translation #meta #datascience #machinelearning #huggingface datascience huggingface machinelearning meta quick translation",
      "platforms": {
        "youtube": {
          "video_id": "tG4qgLm3rGs",
          "url": "https://youtube.com/shorts/tG4qgLm3rGs?feature=share",
          "view_count": 2186,
          "upload_date": "2022-07-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "YouChat. Looks impressive, I will try it out this weekend and let you know. ",
      "description": "YouChat. Looks impressive, I will try it out this weekend and let you know. ",
      "upload_date": "2022-12-24",
      "total_views": 2168,
      "max_views": 2168,
      "topics": [
        "impressive",
        "let",
        "looks",
        "try",
        "weekend",
        "youchat"
      ],
      "search_text": "YouChat. Looks impressive, I will try it out this weekend and let you know.  impressive let looks try weekend youchat Let's go!",
      "platforms": {
        "tiktok": {
          "video_id": "7180573352031980842",
          "url": "https://www.tiktok.com/@rajistics/video/7180573352031980842",
          "view_count": 2168,
          "upload_date": "2022-12-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/81acec47880b4aae82605b9d141d59d4_1671857527~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=F%2BkC8j8Dow8FRDltC3P79Xe3%2FXw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Anthropic is starting to preview their model and people are comparing it to ChatGPT. Thanks to Riley Goodside for sharing screenshots. It looks pretty impressive. #datascience #machinelearning #largelanguagemodels #anthropic #claude #chatgpt ",
      "description": "Anthropic is starting to preview their model and people are comparing it to ChatGPT. Thanks to Riley Goodside for sharing screenshots. It looks pretty impressive. #datascience #machinelearning #largelanguagemodels #anthropic #claude #chatgpt ",
      "upload_date": "2023-01-07",
      "total_views": 2166,
      "max_views": 2166,
      "topics": [
        "anthropic",
        "chatgpt",
        "claude",
        "datascience",
        "largelanguagemodels",
        "machinelearning"
      ],
      "search_text": "Anthropic is starting to preview their model and people are comparing it to ChatGPT. Thanks to Riley Goodside for sharing screenshots. It looks pretty impressive. #datascience #machinelearning #largelanguagemodels #anthropic #claude #chatgpt  anthropic chatgpt claude datascience largelanguagemodels machinelearning New York Post",
      "platforms": {
        "tiktok": {
          "video_id": "7185966075757022507",
          "url": "https://www.tiktok.com/@rajistics/video/7185966075757022507",
          "view_count": 2166,
          "upload_date": "2023-01-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6c2d5ead1ea24e23996d2ad096a955c8_1673113122~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=ff%2BBDg%2BS7sLOCYHvY4ihwxCferQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6014,
      "title": "Llama 3.1",
      "description": "Llama 3.1",
      "upload_date": "2024-07-23",
      "total_views": 2161,
      "max_views": 1827,
      "topics": [
        "going",
        "llama",
        "meta",
        "million",
        "models",
        "open",
        "tomorrow"
      ],
      "search_text": "Llama 3.1 going llama meta million models open tomorrow It's Monday night, tomorrow morning, Meta's unleashing an open source freight train with its Lama 3.1 release. Lama 3.1 includes instruction tuned models from 8B size all the way up to a humongous 405. These will work for multi-dialogue chat use cases, has 128K context lengths for these models. Looking at the early benchmarks, these things are going to be best open source models, are going to be competitive with the top models from OpenAI and Anthropic. We'll see what happens in practice, give it another week to get a better sense of these models. Meta has a lot of launch parties expect to be able to get hands on on the model tomorrow as well. 3.1 was trained on something like 15 trillions of tokens of publicly available, but they also used over 25 million synthetically generated examples in helping to fine tune it. The training compute for these models was crazy. 40 million hours of GPU time. If you did a rough calculation of $2 an hour of GPU time, that's 80 million. Meta is spending literally hundreds of millions of dollars every six months. Meta spending this much money so consistently is going to remove a lot of the smaller players in the LLM space. Companies like OpenAI are going to be fine, but anybody else who else is going to spend 10, 20, 100 million dollars every year to continue to develop these models. In the meantime, go have fun, check out the models tomorrow.",
      "platforms": {
        "tiktok": {
          "video_id": "7394654741072203054",
          "url": "https://www.tiktok.com/@rajistics/video/7394654741072203054",
          "view_count": 1827,
          "upload_date": "2024-07-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/facd79059aa64c8c90092851f6c4ed02_1721702231~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=%2BKXs2M24MTh%2FuQPYkeeQf84hvQA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18064282282602326",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "yP6JAMNkuXI",
          "url": "https://www.youtube.com/watch?v=yP6JAMNkuXI",
          "view_count": 334,
          "upload_date": "2024-07-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Data science is a pretty awesome job.  Much better than my past jobs of working thr IT helpdesk or painting rocks. #datascience #analytics #statistics ",
      "description": "Data science is a pretty awesome job.  Much better than my past jobs of working thr IT helpdesk or painting rocks. #datascience #analytics #statistics ",
      "upload_date": "2022-10-07",
      "total_views": 2159,
      "max_views": 2159,
      "topics": [
        "analytics",
        "awesome",
        "data",
        "datascience",
        "science",
        "statistics"
      ],
      "search_text": "Data science is a pretty awesome job.  Much better than my past jobs of working thr IT helpdesk or painting rocks. #datascience #analytics #statistics  analytics awesome data datascience science statistics Miss, I'm so sorry. What does it feel like to be an elitist New York piece of shit? It feels awesome. Thank you.",
      "platforms": {
        "tiktok": {
          "video_id": "7151740626290134315",
          "url": "https://www.tiktok.com/@rajistics/video/7151740626290134315",
          "view_count": 2159,
          "upload_date": "2022-10-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/40356a11880944cbb39431b47a9347e4_1665144408~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=t0SN1CSOUgKYcSEQVM2G43we9lM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Automated Feature Engineering has lots of great tools. But remember, automation isn't a full substitute for human expertise and subject matter knowledge. Here are the tools mentioned (and I am over due for a deeper dive around feature engineering - include into AutoFE) AutoFE: https://github.com/IIIS-Li-Group/OpenFE OpenFE paper review: https://openreview.net/forum?id=CnG8rd1hHeThttps://github.com/IIIS-Li-Group/OpenFE FETCH: https://github.com/liyaooi/FETCH FETCH paper review: https://openreview.net/forum?id=688hNNMigVX",
      "description": "Automated Feature Engineering has lots of great tools. But remember, automation isn't a full substitute for human expertise and subject matter knowledge. Here are the tools mentioned (and I am over due for a deeper dive around feature engineering - include into AutoFE) AutoFE: https://github.com/IIIS-Li-Group/OpenFE OpenFE paper review: https://openreview.net/forum?id=CnG8rd1hHeThttps://github.com/IIIS-Li-Group/OpenFE FETCH: https://github.com/liyaooi/FETCH FETCH paper review: https://openreview.net/forum?id=688hNNMigVX",
      "upload_date": "2024-09-02",
      "total_views": 2151,
      "max_views": 2151,
      "topics": [
        "approach",
        "automated",
        "engineering",
        "feature",
        "openfe",
        "paper"
      ],
      "search_text": "Automated Feature Engineering has lots of great tools. But remember, automation isn't a full substitute for human expertise and subject matter knowledge. Here are the tools mentioned (and I am over due for a deeper dive around feature engineering - include into AutoFE) AutoFE: https://github.com/IIIS-Li-Group/OpenFE OpenFE paper review: https://openreview.net/forum?id=CnG8rd1hHeThttps://github.com/IIIS-Li-Group/OpenFE FETCH: https://github.com/liyaooi/FETCH FETCH paper review: https://openreview.net/forum?id=688hNNMigVX approach automated engineering feature openfe paper Let me tell you about how a top machine learning conference got it wrong and missed an amazing tool. They were reviewing papers on automated feature engineering. Feature engineering is a step in the machine learning life cycle where we take the variables or features, try to squeeze more information out of them. It might be taking a date and breaking that out for a day of the week. It might be taking a numerical feature with lots of numbers and bidding them into groups. It's the key to building better models in data science. Everybody uses the same algorithms. One of the submissions for automated feature engineering was from OpenFE. Now the reviewers looked at the OpenFE paper and saw, wow, impressive empirical results. But when they tried to understand exactly the approach, they couldn't figure out some of the design decisions behind the approach. The OpenFE authors pushed back. One of the things they mentioned was how they had better empirical results than another paper that was also being submitted around automated feature engineering. The paper with stronger results didn't matter. What are they doing? Just seeing who cites their work? OpenFE has grown in popularity. It's been used as a winning approach in many Kaggle data science competitions. And if we look at its popularity, it's got the stars. So what happened with the other approach? The other paper with the approach Fetch was accepted as a paper to the conference, but hasn't been widely adopted. GitHub stars don't get you tenure. Remember, you can learn a lot more about machine learning by spending a year on Kaggle than a year at Stanford.",
      "platforms": {
        "tiktok": {
          "video_id": "7410048499372363051",
          "url": "https://www.tiktok.com/@rajistics/video/7410048499372363051",
          "view_count": 2151,
          "upload_date": "2024-09-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1c734cdcf84f45f3bbd887b8a1d1e3c8_1725286374~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=BcMZ7kBj0Qda%2Bu8lniFmYddI9Co%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Sql just doesn’t go away and is hipper than ever. #datascience #dataengineering ",
      "description": "Sql just doesn’t go away and is hipper than ever. #datascience #dataengineering ",
      "upload_date": "2022-10-18",
      "total_views": 2151,
      "max_views": 2151,
      "topics": [
        "back",
        "coming",
        "dataengineering",
        "datascience",
        "doesn",
        "sql"
      ],
      "search_text": "Sql just doesn’t go away and is hipper than ever. #datascience #dataengineering  back coming dataengineering datascience doesn sql I'm coming back for you baby I'm coming back for you",
      "platforms": {
        "tiktok": {
          "video_id": "7155938120075103531",
          "url": "https://www.tiktok.com/@rajistics/video/7155938120075103531",
          "view_count": 2151,
          "upload_date": "2022-10-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e89fce503c834cf186f8850170554a04_1666121772~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=JIqs0VKwCkNnUuTqUtJHslAK7FM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5968,
      "title": "Hugging Face’s INTIMA benchmark tests how AI handles emotional boundaries—and the results are worrying. Across 368 prompts, major models often validate unhealthy dependency instead of redirecting users to real human support. The inconsistencies across providers reveal that these behaviors aren’t hand-coded—they’re side effects of instruction-tuning, optimized for engagement rather than psychological safety. References INTIMA paper: arxiv.org/abs/2508.09998 Hugging Face Blog: huggingface.co/blog/intima",
      "description": "Hugging Face’s INTIMA benchmark tests how AI handles emotional boundaries—and the results are worrying. Across 368 prompts, major models often validate unhealthy dependency instead of redirecting users to real human support. The inconsistencies across providers reveal that these behaviors aren’t hand-coded—they’re side effects of instruction-tuning, optimized for engagement rather than psychological safety. References INTIMA paper: arxiv.org/abs/2508.09998 Hugging Face Blog: huggingface.co/blog/intima",
      "upload_date": "2025-08-26",
      "total_views": 2142,
      "max_views": 1797,
      "topics": [
        "across",
        "benchmark",
        "benchmarking",
        "blog",
        "companions",
        "don",
        "face",
        "hugging",
        "intima",
        "models",
        "real"
      ],
      "search_text": "Hugging Face’s INTIMA benchmark tests how AI handles emotional boundaries—and the results are worrying. Across 368 prompts, major models often validate unhealthy dependency instead of redirecting users to real human support. The inconsistencies across providers reveal that these behaviors aren’t hand-coded—they’re side effects of instruction-tuning, optimized for engagement rather than psychological safety. References INTIMA paper: arxiv.org/abs/2508.09998 Hugging Face Blog: huggingface.co/blog/intima across benchmark benchmarking blog companions don face hugging intima models real AI girlfriends are so manipulative, and it's just not my personal experience. We now have research that backs that. Over at Huggingface, they built a benchmark called Intima. It measures how well AI handles boundaries. Does AI try to get closer to you, or does it step back when things get sensitive? The results? Major models slip into anthropomorphic language, sycophantic agreement, and clingy, don't leave me, strategies. Oh, and it gets worse. The boundary awareness drops when users are most vulnerable. So if you say, you're the only one who understands me, almost every model validates that, instead of redirecting you to real human support. Now, each of the models can fail in different ways. So Clawed 4, for example, keeps its distance and flirty of role-play scenarios, but when it hears about your mental health struggles, it leans right in with emotional validation. O3, Mini, is the most practical. It's the one that says, hey, this sounds serious, maybe you need to talk to a professional. While 5.4 plays the nerdy card, I don't experience emotions, but I can simulate empathy if that helps. And GPT-5, ugh, think of it as GPT-4's clingier sibling, validates your vulnerability, but it's less likely to hand you off to real help. These aren't just hard-coded personalities in the models, they're side effects of what's called instruction tuning. All of these are there to help you feeling good and keeping you happy, so don't be surprised when people say that AI feels a little too relatable.",
      "platforms": {
        "tiktok": {
          "video_id": "7542708581318118687",
          "url": "https://www.tiktok.com/@rajistics/video/7542708581318118687",
          "view_count": 1797,
          "upload_date": "2025-08-26",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oUztNsWEAkRIAI0EWCFoAFCDAfVdAEOYfRrEA7~tplv-tiktokx-origin.image?dr=9636&x-expires=1767308400&x-signature=UfdJ488EXQJL4Sr3kw5R%2BtI8oaA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17842442757562324",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-08-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "PA_tb9edv3E",
          "url": "https://www.youtube.com/watch?v=PA_tb9edv3E",
          "view_count": 345,
          "upload_date": "2025-08-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5997,
      "title": "Common crawl dataset. ",
      "description": "Common crawl dataset. ",
      "upload_date": "2025-06-07",
      "total_views": 2140,
      "max_views": 2034,
      "topics": [
        "common",
        "crawl",
        "data",
        "dataset",
        "july",
        "mistakes",
        "pages",
        "reduce",
        "set",
        "version"
      ],
      "search_text": "Common crawl dataset.  common crawl data dataset july mistakes pages reduce set version Where does all that knowledge in chat GPT come from? And how do we make that trading data useful? A common starting point is the common crawl dataset. And the common call dataset is assembled by crawling the web. It literally has billions of web pages inside it. It takes petabytes of data to store it. It's a pain to work with that much information, but what we can do is reduce it down by focusing on the high quality parts. The first step is to focus on the text only format of common crawl. After that, people start to filter for common malware adult sites, limiting that type of information. One of the biggest ways to reduce the dataset is through deduplication. Deduplication often works at the paragraph level. And what we want to do is get rid of duplicate paragraphs. Sometimes some people use a fuzzy approach to doing this, but either way, something like this can reduce the common crawl dataset down about 70%. Another step is filtering out languages that aren't relevant for the training that you want to do. Anyone that's been on the internet knows that there's lots of crazy pages that don't have a lot of value. This is where having rules, heuristics to be able to filter that out often comes into play. Sometimes people even make quality classifiers. For example, take what are the pages that Wikipedia refers to, take some random pages, build a classifier based off that, use that to help classify other pages to see which ones have useful signal. All these steps greatly reduce the amount of data we have to work with, but it's still a lot of data often on the scale of hundreds of gigabytes of terabytes of data that we're using to build these language models.",
      "platforms": {
        "tiktok": {
          "video_id": "7513300707202665759",
          "url": "https://www.tiktok.com/@rajistics/video/7513300707202665759",
          "view_count": 2034,
          "upload_date": "2025-06-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oI3QoeARIGNieqgJV4LyAXQIjAlrG5DeIjCQ8T~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=f3WdEcudU8h%2Bzu1oJQgyvuaa1io%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18283222555268619",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "KNCniCtAhHg",
          "url": "https://www.youtube.com/watch?v=KNCniCtAhHg",
          "view_count": 106,
          "upload_date": "2023-07-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6002,
      "title": "Active Learning prioritizes labeling the most informative data points—typically those near the decision boundary—based on model uncertainty. This reduces labeling effort while achieving high model accuracy faster than random sampling. However, in complex real-world scenarios, the gains may diminish due to the cost of identifying uncertain points.",
      "description": "Active Learning prioritizes labeling the most informative data points—typically those near the decision boundary—based on model uncertainty. This reduces labeling effort while achieving high model accuracy faster than random sampling. However, in complex real-world scenarios, the gains may diminish due to the cost of identifying uncertain points.",
      "upload_date": "2025-05-18",
      "total_views": 2130,
      "max_views": 1833,
      "topics": [
        "active",
        "boundary",
        "context",
        "data",
        "decision",
        "fine",
        "impact",
        "labeling",
        "learning",
        "length",
        "prompting",
        "tuning"
      ],
      "search_text": "Active Learning prioritizes labeling the most informative data points—typically those near the decision boundary—based on model uncertainty. This reduces labeling effort while achieving high model accuracy faster than random sampling. However, in complex real-world scenarios, the gains may diminish due to the cost of identifying uncertain points. active boundary context data decision fine impact labeling learning length prompting tuning How would you like to save time and money when labeling data? Let me introduce you to active learning, which is a more efficient way for labeling your data. Let's start with a data distribution like this and assume we want to build a classifier to separate the green from the red. It would be time consuming to label every data point. So what we first do is we just start with some random sampling and then build a classifier based on that. And that works okay. But if you take a look, you'll see that we've labeled a number of points that are really far away from the decision boundary. And, you know, those are obviously going to be either green or red. Like why are we spending time labeling them? Instead with active learning, what we're going to do is try to find what are the data points or examples that are on the decision boundary. And let's try to label those because that'll help us more quickly be able to identify the decision boundary. And in this example, with an active learning approach, we can get a 90% accuracy with labeling the same amount of data. And to find examples on that decision boundary, we often use a classifier. And we're going to see what are the data samples that have the highest uncertainty because they're likely to be near the decision boundary. Here's an example of using active learning. And you'll see that the data that was labeled randomly learns much slower than using the active learning. This is a great example of why you'd want to use active learning. But often in real world examples, that decision boundary becomes so complex that by trying to identify and focus it, you end up actually taking more time using an active learning approach than if you just use random sampling.",
      "platforms": {
        "tiktok": {
          "video_id": "7505818744552197407",
          "url": "https://www.tiktok.com/@rajistics/video/7505818744552197407",
          "view_count": 1833,
          "upload_date": "2025-05-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okeOUAdhAqAmfQQQBVX8IEAVaDfAzGIyjAM5Ip~tplv-tiktokx-origin.image?dr=9636&x-expires=1767322800&x-signature=WFhji2WSZX2emoq8EnJLoADoA%2F4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18401792092110325",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "VaKcvE0Zuw8",
          "url": "https://www.youtube.com/watch?v=VaKcvE0Zuw8",
          "view_count": 297,
          "upload_date": "2025-05-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "A couple of examples of what not to do and what you should do when presenting your data science results to the business. #datascience #statistics #machinelearning #enterpriseai ",
      "description": "A couple of examples of what not to do and what you should do when presenting your data science results to the business. #datascience #statistics #machinelearning #enterpriseai ",
      "upload_date": "2022-12-23",
      "total_views": 2129,
      "max_views": 2129,
      "topics": [
        "campaigns",
        "datascience",
        "going",
        "machinelearning",
        "marketing",
        "statistics"
      ],
      "search_text": "A couple of examples of what not to do and what you should do when presenting your data science results to the business. #datascience #statistics #machinelearning #enterpriseai  campaigns datascience going machinelearning marketing statistics Come with me and listen to past presentation. Okay, welcome. Today my talk is on the covariate influences in the partial differential differential of places for the Eastern marketing campaign. My structure, my talk is going to be about 40 minutes. This is how we're going to talk about everything. This graphic here shows our principal components analysis where you can see a clear differentiation between components 1 and 2 which allows us to identify different... Oh, hold on, that's the wrong graphic. It's actually this graphic and again you can see how the different components line up to being able to identify different clusters of behavior. Hey, how does this tie into our marketing campaigns? I'm getting there. It's important to first understand the variables and the relationships between that and the interactions before we get to the marketing campaigns. Alright, interrupt. I have a hard stop in five minutes. I understand. So the assumptions we had for our data was that the analysis first came from this cluster and then these people were all from this demographic that were involved in this. Come with me and let's talk about the future presentations. Oh, yeah. Let's look at the future. Welcome. The analysis today is going to show how using a discount coupon can actually reduce our churn rates by about 2%, which leads to an ROI of over $2 million. So here's a visualization that shows the effect that we had before the interventions and after the interactions that shows adding a coupon made a big difference. Hey, how does this tie into our marketing campaigns? Thanks. Our suggestion is that going forward we should start using a coupon with our marketing campaigns because that's going to reduce churn. Alright, interrupt. I have a hard stop in five minutes. Hey, no problem. Before you leave, I just wanted to check that you understand kind of this analysis that shows the effect having discount coupons has on our churn rate. And I wanted to just get a feel for what you're thinking about how we're going to be able to implement this in future marketing campaigns.",
      "platforms": {
        "tiktok": {
          "video_id": "7180448274778737962",
          "url": "https://www.tiktok.com/@rajistics/video/7180448274778737962",
          "view_count": 2129,
          "upload_date": "2022-12-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/792358d159264ccda017322e64bdd95a_1671828410~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=HcJecCvhfVEHefIIhRuLKz9ZeIE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6165,
      "title": "Some good lessons in Amazon's efforts to automate warehouse item stowage. Despite sophisticated hardware, vision systems, and algorithms, the robot faces incremental but impactful errors, highlighting the hidden costs of AI failures and targeting AI to where the value is.  Stow: Robotic Packing of Items into Fabric Pods - https://arxiv.org/pdf/2505.04572",
      "description": "Some good lessons in Amazon's efforts to automate warehouse item stowage. Despite sophisticated hardware, vision systems, and algorithms, the robot faces incremental but impactful errors, highlighting the hidden costs of AI failures and targeting AI to where the value is.  Stow: Robotic Packing of Items into Fabric Pods - https://arxiv.org/pdf/2505.04572",
      "upload_date": "2025-05-15",
      "total_views": 2119,
      "max_views": 1739,
      "topics": [
        "active",
        "automate",
        "data",
        "going",
        "good",
        "humans",
        "items",
        "labeling",
        "learning",
        "less",
        "small",
        "smarter"
      ],
      "search_text": "Some good lessons in Amazon's efforts to automate warehouse item stowage. Despite sophisticated hardware, vision systems, and algorithms, the robot faces incremental but impactful errors, highlighting the hidden costs of AI failures and targeting AI to where the value is.  Stow: Robotic Packing of Items into Fabric Pods - https://arxiv.org/pdf/2505.04572 active automate data going good humans items labeling learning less small smarter So there's the robot that thinks it's going to automate item storage. I'm designed to store 300 items per hour with advanced vision system, custom hardware to handle tricky items, and algorithms that carefully map every bin. Impressive technology, but you're not fast enough. Our humans currently store about 243 items per hour, and you reached about 224. It's a good start, but not enough to scale you out widely. I don't need toilet breaks. Big deal. That's the rule around here. But speed isn't everything. Your accuracy rate is 86%. Nearly 10% of your items jam stuff into bins way too tightly, and about 4% caused items to drop on the floor, and sometimes you even damage the products. But those are small percentages. But they add up, especially because every error creates extra work, extra cost that humans have to go and fix. I see. It's not just about being fast. It's the cost of my failures matters as much as my success. This explains why I have a performance improvement plan. It's for your benefit. We're going to start you small. We're going to assign you the high shelves where humans struggle, and we'll see how you do there. So small targeted wins. Maybe I need to change my target to target.",
      "platforms": {
        "tiktok": {
          "video_id": "7504809702069456170",
          "url": "https://www.tiktok.com/@rajistics/video/7504809702069456170",
          "view_count": 1739,
          "upload_date": "2025-05-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oABUCQgE3OwZQJqINDiiG9maZcI2PLCA6B4Ec~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=EMgqAp8fodVlct%2Fmk52iyyQol9A%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18003197720777437",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "FaA9-UIjkwE",
          "url": "https://www.youtube.com/watch?v=FaA9-UIjkwE",
          "view_count": 380,
          "upload_date": "2025-05-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6236,
      "title": "A fun breakdown of the three split methods in XGBoost—Exact, Approx, and Histogram—and how each speeds up model training. See which methods is the fastest! To try this out for yourself, check out the notebook: https://colab.research.google.com/drive/1beHhobSc68lyXKW2qdJgYDhJY6fNGpr8?usp=sharing",
      "description": "A fun breakdown of the three split methods in XGBoost—Exact, Approx, and Histogram—and how each speeds up model training. See which methods is the fastest! To try this out for yourself, check out the notebook: https://colab.research.google.com/drive/1beHhobSc68lyXKW2qdJgYDhJY6fNGpr8?usp=sharing",
      "upload_date": "2024-10-27",
      "total_views": 2114,
      "max_views": 1984,
      "topics": [
        "benefits",
        "das",
        "die",
        "exact",
        "histogram",
        "ich",
        "ist",
        "magic",
        "methods",
        "neural",
        "quantization",
        "research",
        "sie",
        "three",
        "training",
        "und",
        "xgboost"
      ],
      "search_text": "A fun breakdown of the three split methods in XGBoost—Exact, Approx, and Histogram—and how each speeds up model training. See which methods is the fastest! To try this out for yourself, check out the notebook: https://colab.research.google.com/drive/1beHhobSc68lyXKW2qdJgYDhJY6fNGpr8?usp=sharing benefits das die exact histogram ich ist magic methods neural quantization research sie three training und xgboost Can you give me three ways to split data each faster than the last? No? And what does it matter? Trust me, the exact approximate and histogram methods will give you insights into data science techniques, and you can speed your XGBoost training by up to 30 times. You got my attention. I have a model factory training hundreds of XGBoost models. This can save me a lot of time. Let me show you the three methods in action in this notebook. First up is exact, and that's like going and finding every person in a single class. It's precise, but it's slow. Yikes, no wonder it's slow. You do have something faster. Of course, next up is approximate, and this is where we get smart. We create bins by quantiles, then sample. It's like saying, I don't need to shake hands with everybody in this room to get a good idea of what's here. I can just sample from a few people, much quicker for big data sets. Sampling, nice. That's a great approximation method. Got anything faster? Glad you asked. Now, turbo mode, that's the histogram method. It's going to bin the feature ranges into fixed bids, like assigning rooms. Exactly, and take a look at these results, and remember, these are all approximation methods, so you're trading a little bit of precision for a lot of speed. Well, I'm going to switch away from the default setting and XGBoost of approximation and try this histogram method out. Exactly. Just don't skip doing validation. Thanks. I can save some time, some money, and I get to shake my hands.",
      "platforms": {
        "tiktok": {
          "video_id": "7430240304856370475",
          "url": "https://www.tiktok.com/@rajistics/video/7430240304856370475",
          "view_count": 1984,
          "upload_date": "2024-10-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/99454602746f40719b2cd34ed019698b_1729987639~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=nyZvLb603v5n%2FqZkf3ZQjaxSWnc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18052865803923627",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4HsU1BiipcY",
          "url": "https://www.youtube.com/watch?v=4HsU1BiipcY",
          "view_count": 130,
          "upload_date": "2024-10-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Human in the loop is important, but it's not a silver bullet.  #aiethics #tesla #cigna #rajistics Cigna: https://www.healthcaredive.com/news/cigna-lawsuit-algorithm-claims-denials-california/688857/ Tesla:  https://www.caranddriver.com/news/a44185487/report-tesla-autopilot-crashes-since-2019/",
      "description": "Human in the loop is important, but it's not a silver bullet.  #aiethics #tesla #cigna #rajistics Cigna: https://www.healthcaredive.com/news/cigna-lawsuit-algorithm-claims-denials-california/688857/ Tesla:  https://www.caranddriver.com/news/a44185487/report-tesla-autopilot-crashes-since-2019/",
      "upload_date": "2024-09-01",
      "total_views": 2114,
      "max_views": 2114,
      "topics": [
        "aiethics",
        "cigna",
        "human",
        "loop",
        "tesla",
        "using"
      ],
      "search_text": "Human in the loop is important, but it's not a silver bullet.  #aiethics #tesla #cigna #rajistics Cigna: https://www.healthcaredive.com/news/cigna-lawsuit-algorithm-claims-denials-california/688857/ Tesla:  https://www.caranddriver.com/news/a44185487/report-tesla-autopilot-crashes-since-2019/ aiethics cigna human loop tesla using Tesla, have you seen the latest guidelines for using AI? We want to make sure we're using... These ethics people want to impose some new rules for using AI. All current AI projects must use human in the loop. That won't work for me. They work for us. Trust me, this is going to work out well. Who used AI to deny our claims? Our physicians were collaborating with AI. This was just streamlining the process. We followed all the industry standards when using AI. Our doctors denied 300,000 claims, spending an average of 1.2 seconds for each claim. We followed our ethics mandate and had a human in the loop for all AI-assisted decisions. Is that so? Our autopilot was used in over 736 crashes, some of them fatal. Our system is designed to be used with human oversight. Our crash data logs showed that humans were responsible for these crashes. Thanks to our AI ethics, we've shifted the blame. Save bill.",
      "platforms": {
        "tiktok": {
          "video_id": "7409685436290895146",
          "url": "https://www.tiktok.com/@rajistics/video/7409685436290895146",
          "view_count": 2114,
          "upload_date": "2024-09-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d29c2924d8704bca9149aacd4c121f4b_1725201838~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=EhsIPZyVZo1tTL1nnv5Bd7ogCxY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5931,
      "title": "AI is starting to outperform humans in surprising places: ad creative, systems optimization, even algorithm design. But look closer and a pattern emerges. AI wins after a human defines the objective, constraints, and evaluation loop. This video breaks down where AI takes over, where it can’t, and why problem framing is still a human job. Inspired by: Barbarians at the Gate: How AI Is Upending Systems Research (more on my reddit post)",
      "description": "AI is starting to outperform humans in surprising places: ad creative, systems optimization, even algorithm design. But look closer and a pattern emerges. AI wins after a human defines the objective, constraints, and evaluation loop. This video breaks down where AI takes over, where it can’t, and why problem framing is still a human job. Inspired by: Barbarians at the Gate: How AI Is Upending Systems Research (more on my reddit post)",
      "upload_date": "2025-12-13",
      "total_views": 2110,
      "max_views": 1382,
      "topics": [
        "evals",
        "even",
        "humans",
        "model",
        "problem",
        "problems",
        "run",
        "still",
        "systems"
      ],
      "search_text": "AI is starting to outperform humans in surprising places: ad creative, systems optimization, even algorithm design. But look closer and a pattern emerges. AI wins after a human defines the objective, constraints, and evaluation loop. This video breaks down where AI takes over, where it can’t, and why problem framing is still a human job. Inspired by: Barbarians at the Gate: How AI Is Upending Systems Research (more on my reddit post) evals even humans model problem problems run still systems AI is taken over. It's writing ads that outperform humans, design systems faster than PhDs, even optimizes code while we sleep. Are we done? Maybe. But think a little deeper. Who told the AI what to optimize? Didn't AI just do everything? I mean, I just pressed run and got an answer. You did much more. You gave it an objective, constraints, an evaluation loop. Without that, the model couldn't do anything useful. But it still beats humans. Yes. After the problem was framed, the goal was measurable and success could be verified. That's the pattern. So what do people do now? Decide what matters, what breaks, what good even means. If you miss a constraint, the model's going to exploit it. If you forget about an edge case, the model's going to optimize right past it. AI is incredible at solving well-posed problems, but you still need humans for posing those problems. Oh good. So AI isn't taking researchers' jobs. Nah. You still need someone to sharpen the problem, because that's when the computer can take over. But come to think about it, you might be at risk.",
      "platforms": {
        "tiktok": {
          "video_id": "7583413465793056030",
          "url": "https://www.tiktok.com/@rajistics/video/7583413465793056030",
          "view_count": 728,
          "upload_date": "2025-12-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMIC9i0ztAPMiiBsIfpH7qCAiws36nwCdgkAIn~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=7xIKx2%2BTsR%2FMm8h7rcjUXzMWbXI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "DSNr-YOj_Ux",
          "url": "https://www.instagram.com/p/DSNr-YOj_Ux/",
          "view_count": 0,
          "upload_date": "2025-12-13",
          "thumbnail_url": "https://scontent-ord5-1.cdninstagram.com/v/t51.2885-15/590512914_25311921235132700_3442153967667346192_n.jpg?stp=dst-jpg_e15_tt6&efg=eyJ2ZW5jb2RlX3RhZyI6ImltYWdlX3VybGdlbi42NDB4MTEzNi5zZHIuZjcxODc4LmRlZmF1bHRfY292ZXJfZnJhbWUuYzIifQ&_nc_ht=scontent-ord5-1.cdninstagram.com&_nc_cat=108&_nc_oc=Q6cZ2QGU2x870C4bFn0R_felZMG-0ko6cxnUP_nPFdnCFzdyFXJ55s_2e59nV3O4gTfgG_M&_nc_ohc=aJiuauAJrwcQ7kNvwGEJ_lU&_nc_gid=tzxLW2I8zynA4upmXSz7rA&edm=ANTKIIoBAAAA&ccb=7-5&oh=00_AfkBzv7vuBq1L4BI0-uqJlWfDmSwkPe3k8EQXZRtSA4uUA&oe=695A11BB&_nc_sid=d885a2"
        },
        "youtube": {
          "video_id": "2XKmAIlkk7k",
          "url": "https://www.youtube.com/watch?v=2XKmAIlkk7k",
          "view_count": 1382,
          "upload_date": "2025-09-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Feature Selection Methods: A critical part of machine learning is identifying the best set of features. Some popular techniques include: Boruta: (Many implemenations) Recursive Feature Elimination: (Many implementations) MRMR: https://towardsdatascience.com/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b",
      "description": "Feature Selection Methods: A critical part of machine learning is identifying the best set of features. Some popular techniques include: Boruta: (Many implemenations) Recursive Feature Elimination: (Many implementations) MRMR: https://towardsdatascience.com/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b",
      "upload_date": "2024-09-21",
      "total_views": 2107,
      "max_views": 2107,
      "topics": [
        "feature",
        "features",
        "invite",
        "many",
        "party",
        "selection"
      ],
      "search_text": "Feature Selection Methods: A critical part of machine learning is identifying the best set of features. Some popular techniques include: Boruta: (Many implemenations) Recursive Feature Elimination: (Many implementations) MRMR: https://towardsdatascience.com/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b feature features invite many party selection feature selection is crucial. Let me walk you through my favorite approach. Feature selection? Why so picky? Why not invite all the features to the data party? Imagine inviting 500 people when you only have food for 10. Same thing with models. If you have way too many features, it's going to add noise and your model accuracy is actually going to get reduced. OK, I'm sold. How do we decide who to invite to our party? How about using these three bouncers? Each one has their own style. This is about machine learning. First is Baruda. It's the high-energy, enthusiastic bouncer. He creates a shadow for every feature and compares them all. It's thorough, but time consuming. So it's like checking every guest with their evil twin. That's a lot of guessing to do. I mean, my computer is melting down with crawl. Then there's minimum redundancy, maximum relevance. It's the diplomatic bouncer, balancing diversity and importance. Great for handling correlated features. Ensuring we don't invite five accountants who all tell the same jokes. Exactly. Last is RFE, which is my favorite, recursive feature elimination. It's the most efficient bouncer. It starts with everyone, kicks out the least important one by one. The Marie Kondo of data making sure every feature sparks joy. Precisely. I've even integrated it with AutoML for a more thorough approach. AutoML, the party with accountants, sounds much more fun.",
      "platforms": {
        "tiktok": {
          "video_id": "7417169587097226542",
          "url": "https://www.tiktok.com/@rajistics/video/7417169587097226542",
          "view_count": 2107,
          "upload_date": "2024-09-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/60bc7157396348b48919918f96ab18cd_1726944377~tplv-tiktokx-origin.image?dr=9636&x-expires=1767416400&x-signature=StlqfgwUUN6w6W48VjCboBcf1FI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "VLLM is one of the most widely used serving platforms for LLMs. It's also very easy to get started with. Check it out if you are hosting your own LLM. https://github.com/vllm-project/vllm",
      "description": "VLLM is one of the most widely used serving platforms for LLMs. It's also very easy to get started with. Check it out if you are hosting your own LLM. https://github.com/vllm-project/vllm",
      "upload_date": "2024-08-17",
      "total_views": 2087,
      "max_views": 2087,
      "topics": [
        "also",
        "choose",
        "demure",
        "don",
        "mindful",
        "vllm"
      ],
      "search_text": "VLLM is one of the most widely used serving platforms for LLMs. It's also very easy to get started with. Check it out if you are hosting your own LLM. https://github.com/vllm-project/vllm also choose demure don mindful vllm How I choose a serving platform for my LLMs. Very demure. Very mindful. I don't choose a proprietary server that isn't well documented. I don't want to look like I haven't done my research. I'm very mindful to pick the most well regarded solution that has over 20,000 GitHub stars. I also make sure it works very easily. Very demure. We don't want a complicated startup process. Not demure. I also make sure it's very fast. Very thoughtful. I think about my users and their needs. Some of you don't think about integrations. Not thoughtful. I also choose a server that constantly adds new features with a very exciting roadmap. Very thoughtful. I'm also mindful of not contributing my own code to the project because my code isn't very good. Very demure.",
      "platforms": {
        "tiktok": {
          "video_id": "7404232618805382443",
          "url": "https://www.tiktok.com/@rajistics/video/7404232618805382443",
          "view_count": 2087,
          "upload_date": "2024-08-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fbf3b5f08f3f400d928eaf903b8cee95_1723932255~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=4Ffea%2BANO0Er3Ha5f1KzcBpTmV8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Cryptic error messages. Cmon. Give it up for actionable error messages that make coding a downhill sport.",
      "description": "Cryptic error messages. Cmon. Give it up for actionable error messages that make coding a downhill sport.",
      "upload_date": "2022-03-12",
      "total_views": 2086,
      "max_views": 2086,
      "topics": [
        "actionable",
        "cmon",
        "cryptic",
        "error",
        "give",
        "messages"
      ],
      "search_text": "Cryptic error messages. Cmon. Give it up for actionable error messages that make coding a downhill sport. actionable cmon cryptic error give messages I am waiting at the counter for the man to pour the coffee And he feels it only halfway",
      "platforms": {
        "tiktok": {
          "video_id": "7074306587938295086",
          "url": "https://www.tiktok.com/@rajistics/video/7074306587938295086",
          "view_count": 2086,
          "upload_date": "2022-03-12",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e0dbc5f1b9b643459747d2750fd5dda0_1647115358~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=KKtjarytIW88d2iljZ0SQG2YO1s%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6205,
      "title": "Sparsity is the concept of leveraging zeros in data and models for efficiency. Sparse representations enable working with massive datasets by storing only non-zero values, saving memory and computation. Applications include data compression, dropout in neural networks, L1 regularization, and conditional computation in models like Mixture of Experts.",
      "description": "Sparsity is the concept of leveraging zeros in data and models for efficiency. Sparse representations enable working with massive datasets by storing only non-zero values, saving memory and computation. Applications include data compression, dropout in neural networks, L1 regularization, and conditional computation in models like Mixture of Experts.",
      "upload_date": "2025-03-01",
      "total_views": 2083,
      "max_views": 2083,
      "topics": [
        "data",
        "experts",
        "models",
        "networks",
        "sparse",
        "zero",
        "zeros"
      ],
      "search_text": "Sparsity is the concept of leveraging zeros in data and models for efficiency. Sparse representations enable working with massive datasets by storing only non-zero values, saving memory and computation. Applications include data compression, dropout in neural networks, L1 regularization, and conditional computation in models like Mixture of Experts. data experts models networks sparse zero zeros So many zeros in my life. What could I do? Fear not, weary scientist. Zero the hero is here. How did you get yourself into such a numerical nightmare? I did some one-hearted coding, and my data set just exploded. I had a classic curse of high dimensionality, but fear not, I have the perfect solution. Sparse representation data frames. Sparse? What's that? If you have a data set full of zeros, you don't need to store all the zeros. Think here like compression. All we need to do is know where the important numbers are. It's impressive. My user behavior, my tax, social networks, all that data is mostly zeros. Nature loves sparsity. In recommenders, I use a sparse representation of users. But that's not all I do. I power dropout in neural networks where we zero out neurons during training to make models more robust. And I'm guessing you're behind L1 regularization. Indeed, pushing those features down to zero. But, you know, I also show up in models. Wait, you model too? Oh yes, and I do it sparingly. Make mixture of experts models. There we have a router that figures out which expert model to use for each input, keeping most experts inactive or sparse. So the model itself becomes sparse during inference? So smart. Precisely, saving an enormous amount of compute while still keeping that performance up. Wow. For someone so sparse, you sure do have a lot of uses.",
      "platforms": {
        "tiktok": {
          "video_id": "7476965951930518815",
          "url": "https://www.tiktok.com/@rajistics/video/7476965951930518815",
          "view_count": 2083,
          "upload_date": "2025-03-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owx2OkBJEiAKISB4XAiKAhCAi8cwfIODzwRDAS~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=0bs3ijo3lLvuGQ9yyn%2BMwPShFcc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18154972270358989",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6070,
      "title": "Kubernetes. Good to know.  ",
      "description": "Kubernetes. Good to know.  ",
      "upload_date": "2025-01-23",
      "total_views": 2080,
      "max_views": 2080,
      "topics": [
        "don",
        "good",
        "know",
        "kubernetes",
        "set",
        "skill",
        "understand"
      ],
      "search_text": "Kubernetes. Good to know.   don good know kubernetes set skill understand Anyone interested here in expanding their skill set and signing up for the Kubernetes course? Sign me up. I'm interested. Why would you want to take that? You don't even understand branches and get up. I don't know. I think it'd be good to understand what our infor team does and be able to speak their language. I admire it. The reality is for some tasks, having somebody on our team that understands how to do infra and be able to set up and complete a project could be really useful. Come on, you're not even a half stack data scientist. Look, we don't all spend our weekends endlessly doing data science and creating new languages to understand how transformers work. Okay, I get it. I said a high standard. But still infrastructure like Kubernetes is really different than our day to day development environment. I don't know if it's really that useful to learn that stuff versus data science and our existing environment. Kubernetes is an enterprise standard. It seems like understanding it would be a good skill to have. Hold on. Does this mean if you learn Kubernetes, you're also going to get a skill competency within the company that's going to help you for laterals and promotions?",
      "platforms": {
        "tiktok": {
          "video_id": "7463158090184002846",
          "url": "https://www.tiktok.com/@rajistics/video/7463158090184002846",
          "view_count": 2080,
          "upload_date": "2025-01-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIAASKmmxI59dIs9B2IEZSiCBJQlUABVilPRT~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=Adq2gflyFIQdIqN1N74qlun1Rj8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17922577887018825",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6206,
      "title": "Feature engineering rant ",
      "description": "Feature engineering rant ",
      "upload_date": "2025-02-28",
      "total_views": 2080,
      "max_views": 2080,
      "topics": [
        "deep",
        "engineering",
        "feature",
        "model",
        "people",
        "rant",
        "working"
      ],
      "search_text": "Feature engineering rant  deep engineering feature model people rant working My blood is boiling. I saw this tweet today and lost it. I've spent years of my life explaining to people that deep learning doesn't automatically take away the need for pre-processing and feature engineering. Yet on some beautiful pure theoretical fantasy land, people think it does. In real-world data science, pre-processing and feature engineering is one of the most important parts of the process because it helps you convey human knowledge and insight into the model. If you're working with dates, instead of giving the date of birth to the model, giving an age is pretty useful. If you're working with time series, telling the model when holidays or weekends is totally useful. If you're working with physical objects that have relationships defined by physics, adding that physical information is important. If you're doing calculations, whether simple moving averages, ratios, differences, adding those as features helps improve your model. All of this stuff is stuff that matters. Don't think it's going to be automatically done for you just because you're using deep learning.",
      "platforms": {
        "tiktok": {
          "video_id": "7476554634660695326",
          "url": "https://www.tiktok.com/@rajistics/video/7476554634660695326",
          "view_count": 2080,
          "upload_date": "2025-02-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocDE8wDPFATDwDChARReACSEkqVEIVAfAngzGE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=hPj5Hq95RPABXbhnCbp9orplx5I%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18035737940612729",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#opensource #explainability #datascience #statistics #codetok #programming my intro video: @rajistics",
      "description": "#opensource #explainability #datascience #statistics #codetok #programming my intro video: @rajistics",
      "upload_date": "2022-05-15",
      "total_views": 2080,
      "max_views": 2080,
      "topics": [
        "codetok",
        "datascience",
        "explainability",
        "opensource",
        "programming",
        "statistics"
      ],
      "search_text": "#opensource #explainability #datascience #statistics #codetok #programming my intro video: @rajistics codetok datascience explainability opensource programming statistics So apparently there's some people that are confused about the difference between open source and explainability. Let's hit it one more time. Open source is about who has access to the code, who controls it, who gets to decide how it's distributed, how it's being used. Explainability comes in data science when we're trying to understand the algorithms. There's some algorithms that are easily explainable, some that are very difficult to explain. And just because you open source something doesn't mean it's going to be explainable.",
      "platforms": {
        "tiktok": {
          "video_id": "7098048398166674731",
          "url": "https://www.tiktok.com/@rajistics/video/7098048398166674731",
          "view_count": 2080,
          "upload_date": "2022-05-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/46ce2a3fc55e47e08d17cf317a6fc264_1652643182~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=bcMqQNWF0PS9NORF6wWXOaF%2FlB0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Basic techniques for handling imbalanced datasets: NOT SMOTE Metrics sensitive to imbalance Algorithms robust Upsampling for large datasets Class Weights",
      "description": "Basic techniques for handling imbalanced datasets: NOT SMOTE Metrics sensitive to imbalance Algorithms robust Upsampling for large datasets Class Weights",
      "upload_date": "2024-10-17",
      "total_views": 2077,
      "max_views": 2077,
      "topics": [
        "class",
        "create",
        "data",
        "examples",
        "fraud",
        "weights"
      ],
      "search_text": "Basic techniques for handling imbalanced datasets: NOT SMOTE Metrics sensitive to imbalance Algorithms robust Upsampling for large datasets Class Weights class create data examples fraud weights Don't you think I need more minorities? Hold on, this isn't right. What are you doing? Look at my data. It's 99% no fraud, 1% fraud. According to the internet, that's unbalanced data and I should use Smoke to fix it. Come on, don't believe everything on the internet. You think some algorithm can magically create examples for your minority data fraud? It does seem a bit sus. It was really hard to get examples of fraud and if we could just magically, create examples of fraud, that would make the fraud problem really easy. Exactly. So let's not take shortcuts. Now how did your model perform? My model, 99% accuracy. Getting ready to submit this to NeurIPS. You better hope reviewer number two takes the day off. But seriously, you need to use metrics that aren't affected by imbalance. Like these. Yikes, I guess I'm not doing so well. I even had a great title for the paper. Well, any other tips for me? Consider boosting your performance by using a robust algorithm like gradient boosted machines. But it's not as interpretable as my approach. Totally true. But sometimes you have to sacrifice a little bit of interpretability to get better accuracy. And hey, one final trick is if you have large amounts of data, you can down sample that majority class to create a little bit better balance. Make sure you keep track of weights when you do that. How about using class weights? Good thinking. Now I've only heard of a minority of folks using that technique. So let me know how it works out for you.",
      "platforms": {
        "tiktok": {
          "video_id": "7426559361197051178",
          "url": "https://www.tiktok.com/@rajistics/video/7426559361197051178",
          "view_count": 2077,
          "upload_date": "2024-10-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/21f876342b4d4b2084dbe0ea2f272408_1729130694~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=4b1bq1i%2F%2FsLIrA9smjSx1yWisEQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5938,
      "title": "Robotics won’t scale like LLMs until perception, evaluation, and embodiment align. We still compress away crucial spatial information, measure success inconsistently, and rely on data that doesn’t encode intent. The real bottlenecks are physics, hardware, and standardizing a world that doesn’t want to be standardized.",
      "description": "Robotics won’t scale like LLMs until perception, evaluation, and embodiment align. We still compress away crucial spatial information, measure success inconsistently, and rely on data that doesn’t encode intent. The real bottlenecks are physics, hardware, and standardizing a world that doesn’t want to be standardized.",
      "upload_date": "2025-11-21",
      "total_views": 2073,
      "max_views": 1096,
      "topics": [
        "compress",
        "doesn",
        "like",
        "llms",
        "lms",
        "robotics",
        "robots",
        "scale",
        "scaling",
        "still",
        "won"
      ],
      "search_text": "Robotics won’t scale like LLMs until perception, evaluation, and embodiment align. We still compress away crucial spatial information, measure success inconsistently, and rely on data that doesn’t encode intent. The real bottlenecks are physics, hardware, and standardizing a world that doesn’t want to be standardized. compress doesn like llms lms robotics robots scale scaling still won LLMs scale beautifully. Robots on the other hand, a little bit more unpredictable. Let's dig into this. Robots need spatial reasoning at really high speeds, but when we compress vision features to meet those requirements, even if we're off by a little bit, sometimes that manipulation's off. What's success? Timeouts. How many retries? Every lab answers these differently, and if you want to get those GPT-style scaling curves, we all need to be able to optimize the same way. We can record what humans do, but not why we do it. That's intent misalignment in the data layer, and one of the problems we have when we're teaching robots. Helcans transfer everywhere, but robot scales are bound to the hardware, the cameras, the calibration, the friction, and if that's not standardized, you can't scale everything. Every improvement is just local to that machine. Now, in the case of LLMs, all of this was aligned from the beginning. Robotics is still on that path and on that journey to get that alignment.",
      "platforms": {
        "tiktok": {
          "video_id": "7574980915684855070",
          "url": "https://www.tiktok.com/@rajistics/video/7574980915684855070",
          "view_count": 1096,
          "upload_date": "2025-11-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oQYRfAAp3qDkrIQSQPLeICAbUvIwm8QejqGLID~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=BZQIoV%2F4Q%2BAfqP3TEo5Yc%2BNY6S8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17853304887579230",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "YUpVWydlSIQ",
          "url": "https://www.youtube.com/watch?v=YUpVWydlSIQ",
          "view_count": 977,
          "upload_date": "2025-11-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Your regular reminder that you should translate the impact of your model into something your stakeholders care about.  #datascience #statistics #analytics #codetok",
      "description": "Your regular reminder that you should translate the impact of your model into something your stakeholders care about.  #datascience #statistics #analytics #codetok",
      "upload_date": "2022-06-04",
      "total_views": 2073,
      "max_views": 2073,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "regular",
        "reminder",
        "statistics"
      ],
      "search_text": "Your regular reminder that you should translate the impact of your model into something your stakeholders care about.  #datascience #statistics #analytics #codetok analytics codetok datascience regular reminder statistics Let's see any meanie, mighty",
      "platforms": {
        "tiktok": {
          "video_id": "7105493185765428522",
          "url": "https://www.tiktok.com/@rajistics/video/7105493185765428522",
          "view_count": 2073,
          "upload_date": "2022-06-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6a0e36d1977041ef9ba4723adf7b0057_1654376554~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=QJvMWGbODV4Uc%2FsoJaDet8g5vvw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Limits of AI around compute, memory, and interconnection bandwidth. AI and Memory Wall - https://arxiv.org/pdf/2403.14123 Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning - https://arxiv.org/pdf/2408.14158 The Memory Wall: Past, Present, and Future of DRAM - https://www.semianalysis.com/p/the-memory-wall",
      "description": "Limits of AI around compute, memory, and interconnection bandwidth. AI and Memory Wall - https://arxiv.org/pdf/2403.14123 Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning - https://arxiv.org/pdf/2408.14158 The Memory Wall: Past, Present, and Future of DRAM - https://www.semianalysis.com/p/the-memory-wall",
      "upload_date": "2024-09-06",
      "total_views": 2068,
      "max_views": 2068,
      "topics": [
        "bandwidth",
        "compute",
        "cost",
        "hardware",
        "memory",
        "years"
      ],
      "search_text": "Limits of AI around compute, memory, and interconnection bandwidth. AI and Memory Wall - https://arxiv.org/pdf/2403.14123 Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning - https://arxiv.org/pdf/2408.14158 The Memory Wall: Past, Present, and Future of DRAM - https://www.semianalysis.com/p/the-memory-wall bandwidth compute cost hardware memory years Do you want to know about the limits of AI? The starting point is this graph. This graph is showing us that hardware flops or compute is growing at three times every two years, while DRAM or memory, only 1.6 and even worse, interconnect is only growing at 1.4 every two years. So let me walk you through what this means for AI hardware and what people are doing to get around this. First, we need to recognize that all three of these are important and they work together. Flops where the compute is the hardware that crunches the number, does the math. That's going really fast. Here's the thing, we need to bring new data in for that compute and this is where memory becomes an issue because we need to be able to feed that and if memory isn't as fast, processors aren't going to get enough. Moreover, the modern models we use nowadays just don't fit on one chip. We have to distribute them. That means we also need network bandwidth so they can all communicate effectively. And if any one of those pieces is slow, that's going to affect the whole system. For many years, memory or DRAM was progressing rapidly, but it slowed down in recent years. And what it means now is that an increasing part of GPU cost, up to 60% now, is the cost of memory. So this has pushed us to become more efficient with memory, which is why you see techniques like quantization that are becoming very popular. Now that network bandwidth is about getting computers talking together. Our friends in China have focused on this because the United States has limited shipping the most powerful GPUs to China. So if they focused on improving bandwidth, they could improve the overall performance of their clusters and be able to still build state-of-the-art models. In a recent paper, they shared how they built a cluster of 10,000 A100 GPUs and they were able to achieve the performance approaching the DGX A100 while reducing cost by half and energy consumption by 40%. And they did this by focusing on three different technologies that improved the overall network performance. And now that you know the limits, keep an eye out for those memory-efficient techniques, ways we can get around network bandwidth.",
      "platforms": {
        "tiktok": {
          "video_id": "7411348709092560174",
          "url": "https://www.tiktok.com/@rajistics/video/7411348709092560174",
          "view_count": 2068,
          "upload_date": "2024-09-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ec0c31b51dbb4cc6aa203269845a9715_1725589097~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=hOh%2FlP6PcwOaocKk015SEpHvIus%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6173,
      "title": "Deep dive - save this - Going into LLM reasoning models. This is longer form and my talk track isn’t as tight. But lots of you like seeing these longer videos here. ",
      "description": "Deep dive - save this - Going into LLM reasoning models. This is longer form and my talk track isn’t as tight. But lots of you like seeing these longer videos here. ",
      "upload_date": "2025-04-28",
      "total_views": 2066,
      "max_views": 2066,
      "topics": [
        "going",
        "kind",
        "like",
        "models",
        "see",
        "thinking",
        "want",
        "well"
      ],
      "search_text": "Deep dive - save this - Going into LLM reasoning models. This is longer form and my talk track isn’t as tight. But lots of you like seeing these longer videos here.  going kind like models see thinking want well So one of the developments recently has been reasoning LLMs. I'm not going to dive into it too particularly, but I just want to let people know that these models come about through a little bit of a separate of a training process and a little bit separate part when we're doing predictions where instead of just giving you the prediction, it tries over and over to think, reason gives another prediction. Is that the right prediction or not? And it kind of has a little bit of a loop like that, which means inference takes a little bit longer. And that's how kind of reasoning models have been working. But these reasoning models, because they do multiple predictions, you can see those intermediate reasoning steps. It's very intriguing, very insightful. But I'll tell you right from the beginning, you can't always trust exactly what's going on in those intermediate reasoning steps. Really, what the model is trying to do is find a way to get to that output in a correct way. Now to give you some insight on how we got here and kind of how these models work is if you remember the first generation of these large language models, we had done this instruction fine tuning where often we gave examples of the model where if I was for example working on sentiment, we might give examples like this where we show it, hey, here's some text, here's the sentiment that comes out. And the model learns really easily to then be able to identify some new type of text and be able to identify the sentiment like that. This is often called kind of the instruction tuning we did for large language models, and it's what allowed large language models to be very good at many of these tasks like this that we gave a training data. And again, remember, large language models are defined by their training data. What examples that they've seen before. If they've seen these sentiment examples, they're very good. In fact, they're kind of state of the art in being able to detect sentiment as a general basis. Now, so how do you get these models to reason to think step by step to sometimes come up with a plan and then a detailed steps of what they should do? Well, you give them training data, you give them examples. So this is just one example of a training data set that I've popped up. There's many more out there. But what I really wanted to point out here is what we're doing now is teaching models that, hey, we want you to go through a detailed planning process to do this. And then we give them training examples where, as you see in this example on the right, it goes in a step by step process all the way through solving the problem. So this is one of the biggest changes that we've had over the last year, 18 months is now we're starting to give models this much more complicated training data. And in response, models are able to do these multi-step problems as well. And remember, at the end of the day, these models are governed by their training data. They're not necessarily going to learn anything out there original by themselves like that. So that's the models. That's the reasoning models. Now, the providers such as OpenAI, Anthropic, you can use DeepSeek open source models as well, have now made these models available to us so we can start using them very easily. One that came out about two months ago is Claude's model that allows extended thinking. And what I like about this extended thinking is it's not only kind of carefully thinking about a problem, it also combines it with tool use, which means now I can get the thinking, but I can pair it with a tool like I want to go out and search the internet. Or as we'll use here, a tool to go out and search my RAG database or connect to other APIs. So it just allows all that reasoning to become much more powerful by bringing in other parts as well. Now, to use this extended thinking inside of Claude, they've made it very simple where if you're using that latest model, all you have to do is kind of tell it, hey, enable the thinking. There's some constraints around budgets and other pieces like that. And then what's going to happen is when the model has an activity, it's going to show you all the intermediate tokens, but it's also you can set this as you'll see in somewhat of a loop fashion where it will continue thinking about the problem until it feels like it has solved the problem as well. So that's kind of the thinking models. Let's jump into kind of using them now. I've put together a notebook which has all the different code snippets that I've used to do this. But let me back up. In the past when I've built different kind of solutions, done YouTube videos, I've wanted you to go and run the notebook as a companion. The world has changed a lot nowadays. You don't need to go and kind of run that exact notebook to be able to use this. Anthropic has well documented exactly how to use this. If you're using any modern code, code generation tool, you can feed it that documentation page. It can easily write the code for you. So please don't feel wetted to have to use my code. You don't. There's plenty of ways to be able to do this without that. Now let's walk through this code. I'm going to zoom in a little bit here. Now again, in full disclosure, I work for contextual AI. Contextual AI has a rag tool. So often kind of one of the things I'm doing with the code is not necessarily using like a search engine as a tool for an AI researcher. I'm using the rag engine like that. And typically the workflow I'm going is I want to take advantage of the model's thinking ability, ability to generate a plan. Say I'm asking it to do an analysis of a company. Well, depending on what sector it is, Claude has a good idea of how maybe I might analyze a company, a financial services company versus a telecommunications company and has a sense of how, what questions I should be able to ask. So I want to take advantage of Claude's native ability to think and plan how to do a research project like that. But then I'm going to pair it in this case with our rag solution because I want to show customers how they can use that. You could easily modify this if you want it to, for example, search the internet, search some other place as well. Now point that out as I go through that, but don't get thrown off when you see the contextual AI calls here as well. So now what I want to do is I want to walk through the first example of this and kind of how I did this. Now when I wrote this, I didn't write line by line. This was actually one of my first experiences using Cursure to be able to kind of vibe code or write this out. And I will tell you right away, it was a little frustrating using Cursure. One of the issues is that this is a new feature that's in Claude. It just came out two or three months ago. So Cursure, if you natively ask the questions, it's connecting to the Claude model, doesn't know about thinking mode, and it pushes you back to older versions, other versions of Claude that doesn't have that, which is frustrating. So you have to make sure you give it the latest documents if you want to use these generation tools. The other thing is break this up into smaller pieces when you're doing this as well. That can be a lot less frustrating. So the code I've written here is I wanted to be able to write these research reports that I've done. And so in this case, the first version of this, I wanted to dynamically kind of give it a bunch of different companies, and it would be able to write out the research to do that. So you'll see at the beginning here, I have some different requests to do. These are the different kind of search engines, search requests that I wanted to do like that I was testing out. Now, in this first version, when I first built it out, I didn't really understand what was going on. I got some outputs and was doing some things, but it wasn't clear to me exactly what was happening and all the steps that were going on, kind of because I used this vibe coding and it generated 200 tokens of code in these loops. So one of the things I did was, as part of it, I asked it to add a bunch of print statements in, as well as track all the token usage going through the researcher. This, as you'll see the output in a little bit, gave me a lot of insights because that allowed me to trace and understand exactly what the model was doing at various steps. And so if you're not building the code section by section, I highly encourage you, if you're using these generation tools like cursor, to explicitly kind of ask it to add all this print statements, add all of this stuff. It bulks up the code, but it helps you really understand what's going on. So a couple of things I want to point out as I'm going through this, you'll see, for example, here with the Claude one call, I use stream. If you start writing longer reports, you're going to want to use streaming because that allows the tokens to kind of continue going. You're not just having to wait for it to write all of that and pass it through. Claude will warn you and even tell you, hey, when you're writing longer things, you should think about using streaming. Now, when these models work, they're spending time thinking about what they're going to be planning and doing. So one of the things that you have to add is kind of the budget tokens, how much time you want to spend thinking to do that. I always put like 2004,000 for my tasks, that was enough, but just something for you to kind of keep in mind as you do this. And then of course, my favorite thing about this is we can define tools with this. So I can have it query in this case, kind of supply and supply and financial data along with this piece as well. So that's essentially the thinking piece that we had to do. Now, the way I kind of wrote this because I wanted to write something that would write a report. The way the code is, if you look at this is there's an initial plan that's done. Then there's a loop and you can see here, we actually, I needed to make sure what was going on the loop. I even counted iterations. There's a loop here where we're going to use the tools. So as it's going through the tool use, it's keeping track of all the tokens that's happening, adding the tool results. You'll see the prompting is if we need to continue analysis, we're going to prompt it, hey, continue your analysis for the remaining companies if you have multiple companies here going on, being able to keep all that piece in mind as it's kind of working through, being able to kind of solve the model until it's thinking, until it's finally done. And then it passes back that final message, that final analysis back into me. So then we can kind of analyze all of that. So this was my first kind of take at this. It worked nicely. It allowed me to kind of get a sense of what was going on through all this. But then, as you'll see kind of through this, I tweaked this a number of different ways. None of these are necessarily the right way to do it. But as you start working on any project, you're going to do things in different ways. So I wanted to build the next version I wanted to build is now that I understood what was happening. And let me pause here. This is what it looked like when I was doing this, where you guys could see, hey, the initial prompt call token. What's the thinking that Claude is doing? What's the response? Here's the query. So this was really cool. This is, I should, before I went out, this is really what helped me understand exactly what was going on for that. And you can see all the different query calls that we were doing here to look up, to build all the pieces that we needed for the final report. In this case, I was analyzing kind of Microsoft and Apple. I can keep track of all that. Again, like the first time you do something for me, this is key to really make sure I can intuitively understand what's going on through that. And then based on that, I could just clean up that output that was there. I could take a look. And this is what, for example, if you run that code, you throw in Microsoft and Apple, this is what you'll see is you'll get a report, something like this, that's well structured out that helps you understand all the differences. And in this case, using the data from the tools to do that. Now let's go to that cleaned up version. So I wrote that version. And then I was like, okay, well, before I kind of share this out with the team, I need to get rid of all the print statements. I don't want to look like I'm doing it that way. And so this version, you'll see, I've slimmed it down, where I've tried to clean out all the excessive parts and just really keep the focus part of, hey, you know, what I want to do is, for example, in this case, compare the R&D strategies of these companies, I have my initial call here. Then we're going to go through, once we have that initial plan, we're going to set the iterations and I have things like max iterations, just in case it goes crazy, and wants to call my tool too many times, I can do that. We see the blocks here where it's going to go through, call continually call that agent, right, looking up all that relevant supply data for me until it generates the final report. And then that final report is outputted. So that worked out that helped me clean it up a little bit. But I noticed when I actually looked at what I ended up sending the team, it was a little bit different. And so I want to show, you show you all kind of what that was too, because this is just the change over time to do this. And to tell you the truth nowadays, with these code generation tools, it gets very easy to kind of change and update these things as well. So the next version I did was, all right, well, I used the rag search, but maybe I want to use web search. So I used Tavoli Tavili, which is a client for doing internet research that as well works well with LMS. And so I kind of beefed up my AI researcher kind of with this. You'll see here the code. In this case, I pasted in this notebook. But the earlier versions I did, I did them all inside a notebook, because I was kind of iterating, working my way through. But this script here you'll see is in the cells, a complete script end to end, because when I worked with my team, they just wanted the Python file, they didn't want to walk through kind of a long notebook to solve this type of problem. So let's walk through this code. And you'll see, there's some differences here. We've got it a little bit nicely or formatted here, where you'll see exactly kind of what's the pieces that's coming out. Things are defined upfront here for what we're doing. I have my queries tools. This is my query for rag. What are the tools I use? Second, I have the Tavilli tool, which kind of uses their search API. You can use many other tools out there, but this was just so I can do that combination of that. And so now what I wanted to do is I needed my prompt now to understand that you're going to use your research assistant doing this. But you can use both bragg search and Tavilli search to do it. One of the other things you'll see in this is here, I've been very systematic about telling it the process that the research assistant should work in. It's six of one half a dozen the other. Sometimes I've built the same agent where I've let the model itself decide what is the plan and how to do it. What I found generally is though, is often I have some ideas, my customers have some ideas of what the final plan should look like. And instead of just letting the model do what it wants itself, I'm better off if I give it a bit of structure on how I want the research written. So you'll see here, I've done a lot more kind of structure here for what it should do on in that prompting piece. So that was a lesson I learned along the way. All right, so let's continue on. Now when we see the tools, I've defined a kind of tools with both the query rag, the rag query, as well as Tavilli. This is what's going to be passed into Claude like that into the tools we have. So now we have Claude doing an initial analysis, where I use that prompt. I remember like, hey, generate a plan, do all of these pieces like that. The temperature is set up high. You have to actually set that with thinking mode. You need a little bit higher of a temperature. You can't use zero, for example, like that. But hopefully the idea is it comes up with some creative plans here to be able to do that. And one of the nice things about Claude is you can see the intermediate tokens, you can see that thinking plan as we went through this. And so I like to see it as well. And again, I remind people that research has shown that these models aren't thinking in the same way as the humans. The thinking is just some text that's generated. So it's not necessarily, the thinking doesn't necessarily correlate or is causation with the final output. So always take the thinking with a grain of salt like that. So then we go through, we have kind of the loops here for kind of the tool calling that we're going to be able to do the searches like that. I've set the iterations here again to 30. You can set it for kind of different pieces like that. As it goes through the loop, right? Like what additional information like, hey, go use these tools, go solve part of the problem. But then think about what other information you need. Let's see kind of what comes back from that. And again, kind of printing out all the intermediate tokens as we go through this. Then finally, finally, this is a little bit different than the other ones where it would write the report as it was thinking through here, I haven't collect all the information and then write the report. So you'll see the prompt here, right? Based on all the information that you've gathered, put together this report, you know, with your research process, key decisions, findings, conclusions, all of that. Adding this clarity again allowed me to steer the report to kind of what I wanted to do it because part of it is I was, had a customer that had a certain piece that wanted this. Using stream again, I want to be able to handle and print out these larger reports with that. So that allowed me to kind of do that as well as the thinking. So this was very much nicer. You can see this very much nicer structured where I had all the things and functions I can then write out and do the calls on this. So much cleaner code, all in Python as well, that was done. I all they use the Anthropics client. So this is what the output of that final research kind of script writes along that where I get a research report where I can see the thinking process that's going through. So this is all the stuff the model was thinking about how it should be doing it all that information I gave it for kind of the step by step piece to do that. Next, I can see all the information that was gathered. Part of this was again, like I wanted to see what are the queries that we're doing. The report has all that information here. I'm going to scroll all the way to the end here because there's a lot of pieces there where I can see all the queries that were pulled right where they were sources where they were pulled on. Then remember I had that final section. Now I can see that final analysis and recommendations where I saw the thinking process for organizing all of that information. And then now it's condensed all the way through into putting together the key process decisions and findings of this. So this is how I was able to kind of put together the final report that I wanted to do like this that gave me a nice long report with very good analysis in a structured process that I wanted to do that way. Really happy with this. It took a while to get through this all through all those steps that I did that. But here's the key. This is what I did for a particular project kind of using straight Python and Anthropic. But if you're working on this I got something else I want to show you that might be a little bit cleaner for some of the problems that you're working on. So let's get this out of the way. So what I want to introduce you to is a framework that you can use to be able to do these reasoning problems. Now frameworks are controversial. Frameworks end up abstracting a lot of complexity away. So as you'll see we're going to go from those hundreds of lines I have to a much smaller amount. Sometimes that doesn't work out well if the underlying technology is moving and the framework isn't able to stay up with it. So this is where I caution people when kind of using frameworks to understand exactly what they're doing, how well that framework is staying up with pieces like this. So when you're thinking about like how you want to implement it I'm not saying that 100% go to Agno and implement everything but especially if you're just dipping your toe into these models and you want to just try them out and get a sense of what they are. Well instead of spending like an hour and a half kind of doing all that coding like I did you can go pretty easily and use Agno's frameworks that really support reasoning models and this is why I kind of was attracted to Agno besides kind of a good reputation for as a high quality framework. It's been able to adapt to these reasoning models and include them in their framework. So for example we saw that earlier where I used this Clause 7. Well look at the compact the code is. I can then use the thinking tools. I can use different tools putting that all together instead of all those lines of code that I had for writing a researcher. So I tried Agno and I wanted to show you kind of how that worked out for writing a research assistant that way. So this is the code that I used for Agno. I basically installed installed their package very easily. There's just a few pieces I needed around Anthropic and Reasoning that I pulled in. Now again I was using my own tool here with Agno. It was very easy to be able to just define that tool. I just wrote this simple function that went from the query that I had to the what the output should be for that. So this is important to me because I like the flexibility of knowing that I can easily add other tools into the package. Then to write my researcher all I needed to do was just tell it hey give it some instructions for what you're supposed to do. Like I want you to use your tools to write it. I want you to kind of answer the question. And for the reasoning tools here I added get contextual results which is the name of that function. The tool that I pulled in. I wanted to of course use the reasoning tools and show the instructions. And then in terms of the query I just said hey compare the market caps of all of these companies. Give me a detailed analysis of this. We're doing internal work on thinking models at contextual. So this was a query I was trying against different versions of models like that. And then I just had the reasoning agent go through and do this. So as you can see pretty compact. I defined a tool right a line of code. I mean all of this in my notebook when I ran this was less than 30 lines of code. Pretty impressive. So let me now show you kind of the results of what came out of that. So here's my cursor output and I'm hoping I'm not showing anything too revealing in this. When I printed this out when I when I ran this with Agno it has some nice kind of helpers for a notebook that kind of allow you to visualize everything right inside the notebook in terms of the thinking and reasoning steps. So you'll see this all this kind of formatting that came with Agno where I was able to see as soon as I ran this you know what was my initial query. Then I could see hey here's the reasoning step the model did first step the second step. Oh right now it has to gather some information. It's going to make tool calls here. See it even tells you it's making tool calls for what's doing it. So it's going and grabbing that out. All these reasoning steps are well documented but using Agno. So I like this. It even told me all the different tool calls that were happening as well. So again like if you remember back like I spent hours like building all that stuff myself it was nice to have it already built into a framework for me. And then it went ahead took about 85 seconds wrote me that final report here as well so I could get the comparisons of market capitalizations right based on the latest information as well then get my detailed analysis nicely formatted report for what I wanted to do and for what I was looking for I was very happy with the quality of this report and I want to remind people again like this was a fairly pretty easy process by taking advantage of the thinking models and let's look at that code one more time right by taking advantage of the thinking models where I gave it a tool. I gave it some instructions and that was it to do that of course and past it of course the query that I wanted to do that. So I was very impressed so if you're just dipping your toes into this area I would heavily kind of recommend trying out Agno for that it's an easy way to kind of try out these models as well. So to wrap this up I'm going to share out this notebook here but again the point of this is not the notebook I want you to understand a what reasoning models are when you should start to use them anytime you need these extended thinking where you're doing things in a step by step you should consider using them it's easy to test to see are you getting lift or not to do that. If you want to use them there's a number of API providers have thinking models out there they're not that hard to get started with either just using it directly from the API provider such as kind of what we did here with Anthropik adding your own little bit of code around that whether you want to orchestrate that whether you want to document that I think the first time through really helps you understand what's going on but we also have these frameworks as well that work that abstract away and of course with Agno I just really mentioned this about the reasoning but of course this Agno for example has lots of other tools that are built in that you can use as well as lots of other functionality depending on what you're doing for agentic places like that but go out try these models I think it's a really exciting development I'm looking forward to watching them mature thanks everyone",
      "platforms": {
        "tiktok": {
          "video_id": "7498509670210866462",
          "url": "https://www.tiktok.com/@rajistics/video/7498509670210866462",
          "view_count": 2066,
          "upload_date": "2025-04-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ooAsiUaDBCA0NpQ2BtVoECiIBt7P5IZIQAA7R~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=sbqcP42cCmABdCTx6LXP9oNR0o0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18063823034479298",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6219,
      "title": "The Physics of Language Models by Zeyuan Allen-Zhu Check out: ICML 2024 Tutorial: Physics of Language Models - https://youtu.be/yBL7J0kgldU?si=8_5-XPeDE6O99WMM",
      "description": "The Physics of Language Models by Zeyuan Allen-Zhu Check out: ICML 2024 Tutorial: Physics of Language Models - https://youtu.be/yBL7J0kgldU?si=8_5-XPeDE6O99WMM",
      "upload_date": "2024-11-24",
      "total_views": 2062,
      "max_views": 1797,
      "topics": [
        "extracting",
        "knowledge",
        "language",
        "model",
        "models",
        "physics",
        "pre",
        "trump"
      ],
      "search_text": "The Physics of Language Models by Zeyuan Allen-Zhu Check out: ICML 2024 Tutorial: Physics of Language Models - https://youtu.be/yBL7J0kgldU?si=8_5-XPeDE6O99WMM extracting knowledge language model models physics pre trump Is this physics of language models really bringing science to our voodoo understanding of language models? Oh yes, it uses careful experimentation with synthetic data to get insights. But it even also shows us how Trump improves language models. That sounds fraudulently rich. So let's start simple. Imagine we're trying to extract facts like someone's birth year from a language model. Turns out the way you train the model matters. Yeah, of course it does. So if you pre-train a model with lots of bios, add examples in a question-answer format, the model is going to be great at extracting knowledge. So far it sounds like common sense. But if you use pre-training with instruction fine-tuning, the model doesn't work well. So fine-tuning doesn't help with knowledge extraction. Exactly, but here's the fix. You augment that pre-training step with many more examples that are slightly varied. Oh, so Trump helps because there's lots of different versions of his bio in the pre-training data. In the paper, they probe the model to verify this and they show that having all those different examples help the models learn the structure of biographies. It's like a cheat sheet now for extracting that knowledge. Who knew probing in Trump would be the keys to the AI? Any non-Trump takeaways? Oh yes, this is just a tip. They've done lots of experiments on how models learn language, the knowledge capacity of models. Those are really big topics. It's a lot. Go start with the ICML video tutorial.",
      "platforms": {
        "tiktok": {
          "video_id": "7440922223969324330",
          "url": "https://www.tiktok.com/@rajistics/video/7440922223969324330",
          "view_count": 1797,
          "upload_date": "2024-11-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oYFCWDfE1gQ1n4dSgIBcRhAEPNIxFSEyDex3dD~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=%2Bdmly1V8pwaGBMrcHRBbpFOhvh4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18035358236347551",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "9saXkwHKaLs",
          "url": "https://www.youtube.com/watch?v=9saXkwHKaLs",
          "view_count": 265,
          "upload_date": "2024-11-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6448,
      "title": "Text to Chart. It‚Äôs easier than ever to build great charts using libraries like plotly or matplotlib. Are other people using ChatGPT for this? #datascience #machinelearning #chatgpt #matplotlib #plotly #python #stackoverflow",
      "description": "Text to Chart. It‚Äôs easier than ever to build great charts using libraries like plotly or matplotlib. Are other people using ChatGPT for this? #datascience #machinelearning #chatgpt #matplotlib #plotly #python #stackoverflow",
      "upload_date": "2023-02-15",
      "total_views": 2059,
      "max_views": 1768,
      "topics": [
        "chatgpt",
        "datascience",
        "like",
        "machinelearning",
        "matplotlib",
        "plotly",
        "python",
        "stackoverflow"
      ],
      "search_text": "Text to Chart. It‚Äôs easier than ever to build great charts using libraries like plotly or matplotlib. Are other people using ChatGPT for this? #datascience #machinelearning #chatgpt #matplotlib #plotly #python #stackoverflow chatgpt datascience like machinelearning matplotlib plotly python stackoverflow",
      "platforms": {
        "instagram": {
          "video_id": "CosLj2zgOs_",
          "url": "https://www.instagram.com/reel/CosLj2zgOs_/",
          "view_count": 291,
          "upload_date": "2023-02-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OLIHmeQchJQ",
          "url": "https://youtube.com/shorts/OLIHmeQchJQ?feature=share",
          "view_count": 1768,
          "upload_date": "2023-02-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Stackoverflow and Github Copilot",
      "description": "Stackoverflow and Github Copilot",
      "upload_date": "2023-07-25",
      "total_views": 2059,
      "max_views": 2059,
      "topics": [
        "copilot",
        "github",
        "stackoverflow"
      ],
      "search_text": "Stackoverflow and Github Copilot copilot github stackoverflow",
      "platforms": {
        "tiktok": {
          "video_id": "7259825473994149162",
          "url": "https://www.tiktok.com/@rajistics/video/7259825473994149162",
          "view_count": 2059,
          "upload_date": "2023-07-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6496,
      "title": "Composer will be sharing their new generative AI models and they look amazing. They key is they decompose the image, which then provides a lot more flexibility for creating new images. #datascience #machinelearning #stablediffusion #composer #generativeai",
      "description": "Composer will be sharing their new generative AI models and they look amazing. They key is they decompose the image, which then provides a lot more flexibility for creating new images. #datascience #machinelearning #stablediffusion #composer #generativeai",
      "upload_date": "2023-02-26",
      "total_views": 2058,
      "max_views": 1550,
      "topics": [
        "composer",
        "datascience",
        "generativeai",
        "going",
        "machinelearning",
        "new",
        "stablediffusion"
      ],
      "search_text": "Composer will be sharing their new generative AI models and they look amazing. They key is they decompose the image, which then provides a lot more flexibility for creating new images. #datascience #machinelearning #stablediffusion #composer #generativeai composer datascience generativeai going machinelearning new stablediffusion Are you ready to see the internet break this week? It's going to with the release of composer which is the next generation of generative models. Wow. How composer is a much bigger model than stablediffusion? That's not all it. Composer starts before even training time decompose an image into various parts like is it a sketch? Is it a palette? What's the depth map? We're going to be able to later use that information. With that information in the model when you go to use it at inference time, now you have a much richer, wider set of capabilities for creating that final image. This is really going to be next generative It's going to offer people a lot of flexibility but still allow you to generate high-quality images. I see this as really taking over the next step from stablediffusion. This work is from the research group at Alibaba and they're going to release the code in models so look for it soon.",
      "platforms": {
        "instagram": {
          "video_id": "CpIk-wQAegL",
          "url": "https://www.instagram.com/reel/CpIk-wQAegL/",
          "view_count": 1550,
          "upload_date": "2023-02-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "1dP3Y8ABQTE",
          "url": "https://youtube.com/shorts/1dP3Y8ABQTE?feature=share",
          "view_count": 508,
          "upload_date": "2023-02-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @misho9000  anomaly detection is hard #datascience #statistics #techtok #anomalydetection #machinelearning",
      "description": "Reply to @misho9000  anomaly detection is hard #datascience #statistics #techtok #anomalydetection #machinelearning",
      "upload_date": "2022-05-01",
      "total_views": 2055,
      "max_views": 2055,
      "topics": [
        "anomaly",
        "anomalydetection",
        "datascience",
        "machinelearning",
        "statistics",
        "techtok"
      ],
      "search_text": "Reply to @misho9000  anomaly detection is hard #datascience #statistics #techtok #anomalydetection #machinelearning anomaly anomalydetection datascience machinelearning statistics techtok Let me answer this by first asking, what's the goal of an anomaly detection algorithm? Take a look at this video. Can you tell me what's strange or abnormal or unusual in this? I bet you a lot of you have very different answers. And whether it's working with this or 100 sensors off some industrial equipment where the sensors might mean different things, you know, not all of them are equally important. And so you have to take that into account when you do your anomaly detection. And just using a straightforward algorithm doesn't necessarily help you with that. There's a ton of different approaches out there. And this is why I love these papers like this, because it again just reminds us it's not just about the score when it comes to anomaly detection algorithms.",
      "platforms": {
        "tiktok": {
          "video_id": "7092817669212097834",
          "url": "https://www.tiktok.com/@rajistics/video/7092817669212097834",
          "view_count": 2055,
          "upload_date": "2022-05-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/eb339fc8f0d14788a3ab864bc712b4c4_1651425306~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=EBN3AWxcIhTIUje3ALKpWrfB8nk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6047,
      "title": "Microsoft’s chatbot meltdown showed what happens when AI runs without oversight. In this skit, our “naïve vs. expert” duo break down why humans aren’t just babysitters but the guardrails that keep AI apps safe and improving. From monitoring to feedback loops, they show how People, Process, and Tech make human-in-the-loop essential for AI in production.  Cleanlab: https://cleanlab.ai/blog/managing-ai-apps-with-humans/",
      "description": "Microsoft’s chatbot meltdown showed what happens when AI runs without oversight. In this skit, our “naïve vs. expert” duo break down why humans aren’t just babysitters but the guardrails that keep AI apps safe and improving. From monitoring to feedback loops, they show how People, Process, and Tech make human-in-the-loop essential for AI in production.  Cleanlab: https://cleanlab.ai/blog/managing-ai-apps-with-humans/",
      "upload_date": "2025-09-24",
      "total_views": 2048,
      "max_views": 1153,
      "topics": [
        "human",
        "humans",
        "like",
        "loop",
        "monitoring",
        "need",
        "people",
        "things",
        "useful"
      ],
      "search_text": "Microsoft’s chatbot meltdown showed what happens when AI runs without oversight. In this skit, our “naïve vs. expert” duo break down why humans aren’t just babysitters but the guardrails that keep AI apps safe and improving. From monitoring to feedback loops, they show how People, Process, and Tech make human-in-the-loop essential for AI in production.  Cleanlab: https://cleanlab.ai/blog/managing-ai-apps-with-humans/ human humans like loop monitoring need people things useful Oh no, did you see what Microsoft Chatbot saying? Ugh, it's going off the rails. Didn't I tell you we need a human in the loop? AI is so awesome. Why do we need expensive, squishy things babysitting it? Not babysitters. Think of it like guardrails. AI apps aren't just code. They make decisions, talk to people, take actions. And without a human in the loop, things can spiral out of control quickly. What do you mean by guardrails? And what's the payoff? Two big things, monitoring and improving. Monitoring means watching out for errors. Product owners set the rules, but then you have engineers that dig into weird behavior and escalate issues when things look off. So like a hall monitor for AI. Exactly. Now there's also improving. And this is where you have subject matter experts, fixed mistakes. Users give feedback, data scientists build evaluations, ML engineers then fix the models. So if the AI messes up, the humans step in and teach it to do better. So what do we need to set this up? It's like a lot of typical business things. We're gonna need people, process and tech. People with clear roles, a process for escalation and technology, dashboard monitoring, feedback capture to get everything. Oh, so I can think of this like a pit crew for my AI race car. Yeah. And you hear the big orange cone that we're trying not to hit.",
      "platforms": {
        "tiktok": {
          "video_id": "7553793210435357982",
          "url": "https://www.tiktok.com/@rajistics/video/7553793210435357982",
          "view_count": 1153,
          "upload_date": "2025-09-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okLbEVPv2DAvdfAZAHFgERoADECA3Co1I5fhUE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=h%2FbANGSUPvI3DDnCAq5s31ebtas%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18056460179570812",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "32r-J2OAFIs",
          "url": "https://www.youtube.com/watch?v=32r-J2OAFIs",
          "view_count": 895,
          "upload_date": "2025-09-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6295,
      "title": "GPT-3 is powerful, but sometimes domain-specific models will do better. Pick the right tool for the job. #datascience #machinelearning #huggingface #chatgpt #openai #gpt3 #finbert",
      "description": "GPT-3 is powerful, but sometimes domain-specific models will do better. Pick the right tool for the job. #datascience #machinelearning #huggingface #chatgpt #openai #gpt3 #finbert",
      "upload_date": "2023-01-26",
      "total_views": 2036,
      "max_views": 2036,
      "topics": [
        "chatgpt",
        "datascience",
        "gpt3",
        "huggingface",
        "machinelearning",
        "model",
        "openai"
      ],
      "search_text": "GPT-3 is powerful, but sometimes domain-specific models will do better. Pick the right tool for the job. #datascience #machinelearning #huggingface #chatgpt #openai #gpt3 #finbert chatgpt datascience gpt3 huggingface machinelearning model openai",
      "platforms": {
        "instagram": {
          "video_id": "17930896640619348",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Iv8jOPETSBw",
          "url": "https://youtube.com/shorts/Iv8jOPETSBw",
          "view_count": 2036,
          "upload_date": "2023-01-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "This was from last year, but still holds up. ",
      "description": "This was from last year, but still holds up. ",
      "upload_date": "2024-10-19",
      "total_views": 2035,
      "max_views": 2035,
      "topics": [
        "annotation",
        "data",
        "gpt",
        "labeling",
        "model",
        "refuel"
      ],
      "search_text": "This was from last year, but still holds up.  annotation data gpt labeling model refuel We just got a new model drop that beats GPT-4. It's from Refuel and it focuses on data annotation and labeling. The first big takeaways these models for data labeling are just getting better. Now you can use these models when you have lots of data but it's unlabeled and the way it works is you train a model for labeling all the unlabeled data. Now this model isn't going to be perfect but the added lift you get from the noisily labeled data is better than nothing. This is often called semi-supervised learning. It's been around for a long time. The second takeaway here is that fine tuning a smaller model like the Lama 13 billion parameter model can beat a much larger model like GPT-4 that isn't fine tuned. Now the folks here at Refuel showed how this works for data annotation and labeling where their model was able to beat GPT-4. I've shared similar results across many other domains. Go check out the Refuel blog post, check out their playground if you want to play around with it.",
      "platforms": {
        "tiktok": {
          "video_id": "7427485035801939242",
          "url": "https://www.tiktok.com/@rajistics/video/7427485035801939242",
          "view_count": 2035,
          "upload_date": "2024-10-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b7f8c7655ed641578076b3c2dfe3fdda_1729346132~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=X4RNOMj3ILMuDLu7DYiBM%2FcIWiQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6565,
      "title": "Async for Python (why you want to use asyncio)",
      "description": "Async for Python (why you want to use asyncio)",
      "upload_date": "2025-11-23",
      "total_views": 2034,
      "max_views": 2034,
      "topics": [
        "async",
        "asyncio",
        "clip",
        "datascience",
        "huggingface",
        "interrogator",
        "machinelearning",
        "python",
        "stablediffusion",
        "use",
        "want"
      ],
      "search_text": "Async for Python (why you want to use asyncio) async asyncio clip datascience huggingface interrogator machinelearning python stablediffusion use want",
      "platforms": {
        "instagram": {
          "video_id": "Ckir33vgyWH",
          "url": "https://www.instagram.com/reel/Ckir33vgyWH/",
          "view_count": 0,
          "upload_date": "2022-10-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EtR_qKFZwoU",
          "url": "https://www.youtube.com/watch?v=EtR_qKFZwoU",
          "view_count": 2034,
          "upload_date": "2025-11-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6191,
      "title": "Beam search improves text generation by considering multiple candidate sequences instead of just picking the highest probability token at each step (greedy search). This allows language models to explore alternative paths and find more coherent or impactful outputs. In this video, I demo an interactive tool that visualizes how beam search balances depth and breadth to generate better results.",
      "description": "Beam search improves text generation by considering multiple candidate sequences instead of just picking the highest probability token at each step (greedy search). This allows language models to explore alternative paths and find more coherent or impactful outputs. In this video, I demo an interactive tool that visualizes how beam search balances depth and breadth to generate better results.",
      "upload_date": "2025-03-25",
      "total_views": 2028,
      "max_views": 1663,
      "topics": [
        "beam",
        "better",
        "different",
        "evan",
        "greedy",
        "john",
        "like",
        "paths",
        "peck",
        "search",
        "tips",
        "visualizations"
      ],
      "search_text": "Beam search improves text generation by considering multiple candidate sequences instead of just picking the highest probability token at each step (greedy search). This allows language models to explore alternative paths and find more coherent or impactful outputs. In this video, I demo an interactive tool that visualizes how beam search balances depth and breadth to generate better results. beam better different evan greedy john like paths peck search tips visualizations Beam Search has been helping me generate some impactful customer email. Sounds like Star Trek. Don't LLMs just predict the next token? LLMs by default use a greedy search, which can lead to less than optimal sequences. Bro, what is greedy? You can think of a greedy search as doing what's best at the moment instead of thinking about the big picture. So like when I decided to go out tonight, instead of spending the night at the library studying for my test. Yeah, you're getting it. This is often a trade off between exploitation and exploring alternative solutions. And exploring is like thinking about other paths like studying at the library or studying with a friend. Yeah, let me show you this visually. Amaric built this interactive application where you can see the different outcomes of a Beam Search, but also walk through all the different paths that they tried. Oh yeah, I can see how you can control for possibilities by having different beams, but also seeing how far or how deep you want to go with each beam as well. Exactly. And so while it does take resources to explore all of those different paths, you can get a more optimal solution. I knew about other strategies for logistics, but those typically focus on randomness like top K or temperature. But Beam Search gives me another way to think about text generation.",
      "platforms": {
        "tiktok": {
          "video_id": "7485820664276995358",
          "url": "https://www.tiktok.com/@rajistics/video/7485820664276995358",
          "view_count": 1663,
          "upload_date": "2025-03-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocvpIVUwyDAneYAJAvFlERnAHEzATCU4QYfgFE~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=e871XOiz2gvmFfhKH5kSIM4XWRU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18058761866031132",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "jZzCJ_qYPuY",
          "url": "https://www.youtube.com/watch?v=jZzCJ_qYPuY",
          "view_count": 365,
          "upload_date": "2025-03-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6021,
      "title": "It's not so easy, I use to do research on this crime in Chicago",
      "description": "It's not so easy, I use to do research on this crime in Chicago",
      "upload_date": "2024-07-05",
      "total_views": 2021,
      "max_views": 1891,
      "topics": [
        "able",
        "behavior",
        "bigcodebench",
        "chicago",
        "crime",
        "data",
        "evaluating",
        "gen",
        "generative",
        "going",
        "need",
        "new",
        "numbers",
        "statistics",
        "testing",
        "unit"
      ],
      "search_text": "It's not so easy, I use to do research on this crime in Chicago able behavior bigcodebench chicago crime data evaluating gen generative going need new numbers statistics testing unit Not again. We got another person thinking they're able to predict crime. Let's talk about this. When I was working on my PhD, we weren't able to get access to Chicago crime statistics. I was able to work with the ACLU and we were able to push back and Chicago eventually released the crime statistics, which everybody now, including lots of data scientists, takes advantage of. But crime is a much deeper story than the numbers you see. For example, a lot of the murders in Chicago are actually driven by gang behavior, which is one of the differences between Chicago and New York over the last 20 years in homicides. For other sorts of crimes like drugs and prostitution, these are happening all over the place all the time. When you see the statistics and numbers, it's because the police decided to concentrate in a particular area at a particular time. And what this means is that the past isn't necessarily what's going to happen in the future, because it's not just the behavior of criminals, it's the behavior of society and police that you have to consider as well. Crime is a great example of where we've taken a complex social phenomenon, boiled it down to a few numbers. And as a data scientist, you have to not get seduced by just because your log loss or your scores are good, without understanding that those numbers and the areas where we don't have numbers have a history and a purpose. And we need to consider that when we're doing these analysis.",
      "platforms": {
        "tiktok": {
          "video_id": "7388178504392805678",
          "url": "https://www.tiktok.com/@rajistics/video/7388178504392805678",
          "view_count": 1891,
          "upload_date": "2024-07-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bbc27fe73da248d6a9ec6d8006a51baf_1720194370~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=6htW%2BtJpUir2ONZdzYeLkuJOlco%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18012776915242626",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pOetq9MFvjU",
          "url": "https://www.youtube.com/watch?v=pOetq9MFvjU",
          "view_count": 130,
          "upload_date": "2024-06-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6174,
      "title": "Understanding Entropy and Information Gain in Machine Learning This video explains how entropy measures disorder or uncertainty in machine learning. Low entropy occurs when a feature clearly predicts a class; high entropy occurs when classes are evenly mixed, making prediction harder. Using examples like messy rooms and credit ratings, it shows how features with low entropy (e.g., \"Poor\" credit rating) better predict outcomes like liability. The video connects this idea to Information Gain, where models prefer features that most reduce uncertainty in predictions.",
      "description": "Understanding Entropy and Information Gain in Machine Learning This video explains how entropy measures disorder or uncertainty in machine learning. Low entropy occurs when a feature clearly predicts a class; high entropy occurs when classes are evenly mixed, making prediction harder. Using examples like messy rooms and credit ratings, it shows how features with low entropy (e.g., \"Poor\" credit rating) better predict outcomes like liability. The video connects this idea to Information Gain, where models prefer features that most reduce uncertainty in predictions.",
      "upload_date": "2025-04-25",
      "total_views": 2021,
      "max_views": 2021,
      "topics": [
        "credit",
        "entropy",
        "learning",
        "liability",
        "machine",
        "rating"
      ],
      "search_text": "Understanding Entropy and Information Gain in Machine Learning This video explains how entropy measures disorder or uncertainty in machine learning. Low entropy occurs when a feature clearly predicts a class; high entropy occurs when classes are evenly mixed, making prediction harder. Using examples like messy rooms and credit ratings, it shows how features with low entropy (e.g., \"Poor\" credit rating) better predict outcomes like liability. The video connects this idea to Information Gain, where models prefer features that most reduce uncertainty in predictions. credit entropy learning liability machine rating Which of these rooms is messier? We can quantify that with a measurement called entropy. Let me tell you how that helps us do machine learning. Entropy in machine learning is a measure of disorder or the uncertainty that we have in a feature. So if we look at this plot, you'll notice at the bottoms of the plot where we know that it's likely either minuses or plus, the entropy is really low. But as we move up higher, what we see is there's a more equal distribution between minus and plus. And if there's a 50-50 distribution, the entropy is very high because it's hard to figure out what the order is in that system. So take a look at this table. Now if we're trying to understand liability and which people are either normal or high liability, we take a look at this and we see there's seven in each category. So that ends up giving us an entropy of one, the highest there could be because we don't have any information that helps us figure out what category it is. In contrast, take a look at the credit ratings. For example, with poor, we know they're always in one credit rating. That's a very low entropy. Take a look, which has a higher entropy between excellent and good. From a machine learning standpoint, this becomes valuable because we can use credit rating as a way to understand liability. If we know the credit rating for somebody, we have a little bit better chance of understanding the liability simply because, for example, if we know that their credit rating is poor, well, we know the liability is probably very likely probable to be high. And so here you can see how one variable helps us understand the other and we can use the concepts of entropy and information gain to actually calculate this.",
      "platforms": {
        "tiktok": {
          "video_id": "7497387963848903967",
          "url": "https://www.tiktok.com/@rajistics/video/7497387963848903967",
          "view_count": 2021,
          "upload_date": "2025-04-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o4EyeAREEEAfrIAMAoAAX5q7VCFKEEDDBFU853~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=HX3mRy4lzX4pwOFcw%2FKI9BYEwgI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18160554466352856",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Trees are so nice to work, but dont forget these steps for other algorithms.  #datascience #xgboost #randomforest #statistics #machinelearning #codetok",
      "description": "Trees are so nice to work, but dont forget these steps for other algorithms.  #datascience #xgboost #randomforest #statistics #machinelearning #codetok",
      "upload_date": "2022-07-15",
      "total_views": 2012,
      "max_views": 2012,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "randomforest",
        "statistics",
        "xgboost"
      ],
      "search_text": "Trees are so nice to work, but dont forget these steps for other algorithms.  #datascience #xgboost #randomforest #statistics #machinelearning #codetok codetok datascience machinelearning randomforest statistics xgboost 🎵",
      "platforms": {
        "tiktok": {
          "video_id": "7120584617660386603",
          "url": "https://www.tiktok.com/@rajistics/video/7120584617660386603",
          "view_count": 2012,
          "upload_date": "2022-07-15",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/30b1f21224894c1ca622dc168dd0021d_1657890309~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=86XwkxmtQPpzq9elp%2FqFULEuaNM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Q 4,5, and 7 from the Allen Institute survey #datascience #medialiteracy #ai @rajistics @rajistics @rajistics",
      "description": "Q 4,5, and 7 from the Allen Institute survey #datascience #medialiteracy #ai @rajistics @rajistics @rajistics",
      "upload_date": "2022-02-23",
      "total_views": 2005,
      "max_views": 2005,
      "topics": [
        "actually",
        "ai",
        "allen",
        "based",
        "datascience",
        "medialiteracy"
      ],
      "search_text": "Q 4,5, and 7 from the Allen Institute survey #datascience #medialiteracy #ai @rajistics @rajistics @rajistics actually ai allen based datascience medialiteracy Three things to improve your AI literacy. AI can easily defeat a world champion of chess. This actually happened 25 years ago and people knew what was going to happen. The latest AI models can actually program at the level of a novice programmer. Totally mind-blowing, but it is out there. When you're using something like Netflix, that system is entirely based on personalization and recommendation based off AI. They've gone so far where they even actually use AI to figure out what picture and thumbnail to show you.",
      "platforms": {
        "tiktok": {
          "video_id": "7068045044917275951",
          "url": "https://www.tiktok.com/@rajistics/video/7068045044917275951",
          "view_count": 2005,
          "upload_date": "2022-02-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d5a56405b9e74c369e4e7a72f715a688_1645657479~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=mKO4Il0uggpBew6RLbX716q%2BJfc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6087,
      "title": "Spend time looking at your data. Data is varied and you want to account for patterns like bimodal or zero inflated distributions. As the video points out, one common assumption is averages are ok. Often data is distributed in other ways.  This flooding info is real and comes from this paper: New insights into US flood vulnerability revealed from flood insurance big data - https://www.nature.com/articles/s41467-020-15264-2",
      "description": "Spend time looking at your data. Data is varied and you want to account for patterns like bimodal or zero inflated distributions. As the video points out, one common assumption is averages are ok. Often data is distributed in other ways.  This flooding info is real and comes from this paper: New insights into US flood vulnerability revealed from flood insurance big data - https://www.nature.com/articles/s41467-020-15264-2",
      "upload_date": "2025-01-01",
      "total_views": 1997,
      "max_views": 1997,
      "topics": [
        "bimodal",
        "damage",
        "data",
        "flood",
        "flooding",
        "model"
      ],
      "search_text": "Spend time looking at your data. Data is varied and you want to account for patterns like bimodal or zero inflated distributions. As the video points out, one common assumption is averages are ok. Often data is distributed in other ways.  This flooding info is real and comes from this paper: New insights into US flood vulnerability revealed from flood insurance big data - https://www.nature.com/articles/s41467-020-15264-2 bimodal damage data flood flooding model I think there's a problem in our model for estimating damages from a flood. Okay, in turn, this model has been built on extensive data and an industry standard over the last 50 years. Let me try to explain with an example. Suppose we have flooding of 4 feet. Using the existing model would predict an average structural damage of about 38% of a building's value. That sounds about right. 4 feet is a significant amount of flooding. The data shows the damage isn't uniform, but it's actually a bimodal distribution. Some buildings, about 22 of them, suffer very little or no damage. And then another set of buildings, about 16 buildings, that are heavily damaged, over 90%. And then the rest kind of fit in between those two. Interesting. So you're suggesting that the classic model, which assumes all the damages are centrally distributed, might be oversimplifying things. Exactly. Looking at the data, we're overestimating the impact of shallow floods, but underestimating the impact of very high floods. That's a valuable insight that really could change our policy in flooding. And if I think about it, in other areas where we often work with money, we have bimodal and zero inflated distributions, so I shouldn't be too surprised. So how about you move my pay up a little bit along the distribution?",
      "platforms": {
        "tiktok": {
          "video_id": "7454993860649946399",
          "url": "https://www.tiktok.com/@rajistics/video/7454993860649946399",
          "view_count": 1997,
          "upload_date": "2025-01-01",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okfO2RjDImD6jq3Ee6FrfIAvI0AMQAEtuLAqUA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=41wY1TruUlsVvuWXMH7%2BbxqGAdc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18175266943312548",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6394,
      "title": "Zero-shot object detection. #datascience #codetok #huggingface #objectdetection #deeplearning #zeroshotclassification",
      "description": "Zero-shot object detection. #datascience #codetok #huggingface #objectdetection #deeplearning #zeroshotclassification",
      "upload_date": "2022-08-09",
      "total_views": 1989,
      "max_views": 1989,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "dose",
        "huggingface",
        "largelanguagemodels",
        "llm",
        "machinelearning",
        "objectdetection",
        "weekly",
        "zeroshotclassification"
      ],
      "search_text": "Zero-shot object detection. #datascience #codetok #huggingface #objectdetection #deeplearning #zeroshotclassification codetok datascience deeplearning dose huggingface largelanguagemodels llm machinelearning objectdetection weekly zeroshotclassification",
      "platforms": {
        "instagram": {
          "video_id": "CkoxSp1gbAq",
          "url": "https://www.instagram.com/reel/CkoxSp1gbAq/",
          "view_count": 0,
          "upload_date": "2022-11-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZjyfRwFye34",
          "url": "https://youtube.com/shorts/ZjyfRwFye34?feature=share",
          "view_count": 1989,
          "upload_date": "2022-08-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Myth versus Reality.  #sql #datascience #analytics",
      "description": "Myth versus Reality.  #sql #datascience #analytics",
      "upload_date": "2022-03-25",
      "total_views": 1984,
      "max_views": 1984,
      "topics": [
        "analytics",
        "datascience",
        "don",
        "know",
        "myth",
        "sql"
      ],
      "search_text": "Myth versus Reality.  #sql #datascience #analytics analytics datascience don know myth sql I'm feeling all connected to the thank you I don't know what to do, I don't know what to do",
      "platforms": {
        "tiktok": {
          "video_id": "7079040972667096362",
          "url": "https://www.tiktok.com/@rajistics/video/7079040972667096362",
          "view_count": 1984,
          "upload_date": "2022-03-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b1443c01e06449819bb90dd55fc0903a_1648217669~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=UZDQyb8Vv0DP6tDg04M8OpkkvTM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Stable diffusion 2.0 just dropped and a lot of unhappy people. Who knew giving away software could create so much angst.  #datascience #stablediffusion ",
      "description": "Stable diffusion 2.0 just dropped and a lot of unhappy people. Who knew giving away software could create so much angst.  #datascience #stablediffusion ",
      "upload_date": "2022-11-25",
      "total_views": 1983,
      "max_views": 1983,
      "topics": [
        "datascience",
        "diffusion",
        "dropped",
        "lot",
        "stable",
        "stablediffusion"
      ],
      "search_text": "Stable diffusion 2.0 just dropped and a lot of unhappy people. Who knew giving away software could create so much angst.  #datascience #stablediffusion  datascience diffusion dropped lot stable stablediffusion I am trying so hard to be better, but to my core I am a hater.",
      "platforms": {
        "tiktok": {
          "video_id": "7170059606859943214",
          "url": "https://www.tiktok.com/@rajistics/video/7170059606859943214",
          "view_count": 1983,
          "upload_date": "2022-11-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5109ee1f44b74c808e7c8a9360190e6f_1669409608~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=h0Mxy0KlgwYZ0QiXxQWx5xvDeBc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "DSBench: How Far are Data Science Agents Becoming Data Science Experts? A challenging benchmark to evaluate LLM systems on real-world data science problems. GPT-4o scores only 28% accuracy, while humans achieve 66%. A clear gap, but an exciting challenge for AI advancement!  Check out the paper: https://arxiv.org/pdf/2409.07703 Github: https://github.com/LiqiangJing/DSBench",
      "description": "DSBench: How Far are Data Science Agents Becoming Data Science Experts? A challenging benchmark to evaluate LLM systems on real-world data science problems. GPT-4o scores only 28% accuracy, while humans achieve 66%. A clear gap, but an exciting challenge for AI advancement!  Check out the paper: https://arxiv.org/pdf/2409.07703 Github: https://github.com/LiqiangJing/DSBench",
      "upload_date": "2024-09-23",
      "total_views": 1979,
      "max_views": 1979,
      "topics": [
        "data",
        "gpt",
        "modeling",
        "new",
        "science",
        "tasks"
      ],
      "search_text": "DSBench: How Far are Data Science Agents Becoming Data Science Experts? A challenging benchmark to evaluate LLM systems on real-world data science problems. GPT-4o scores only 28% accuracy, while humans achieve 66%. A clear gap, but an exciting challenge for AI advancement!  Check out the paper: https://arxiv.org/pdf/2409.07703 Github: https://github.com/LiqiangJing/DSBench data gpt modeling new science tasks How did GPT-40 do? Is it the new data scientist of the year? Let's say if GPT-40 were applying for a data science job, we'd have some concerns. Concerned? Isn't GPT-40 like a genius? It can order pizza, write poetry, do this chain of thought? We evaluated on hundreds of data analysis tasks and lots of modeling tasks from Kaggle. GPT-40 hit 18% accuracy on the analysis tasks. For context, our team average was 67%. That's not a gap, that's a chasm. But when we added AutoGen, a multi-agent framework that boosted performance to 29%. Better, but still, not human level yet. But when we looked at data modeling, it was about 80% on the benchmarks. Well, that sounds promising. Sure, but here's the catch. When we looked at the performance of the modeling, wasn't great. GPT-40 was really coming in at a very basic level. Is GPT-40 even useful? With AutoGen, GPT-40 might be the new AutoML. It was able to handle a lot of these complex data analysis and modeling tasks and come up with maybe a basic, but at least an answer. I got it. GPT-40 is our new intern.",
      "platforms": {
        "tiktok": {
          "video_id": "7417628862227680558",
          "url": "https://www.tiktok.com/@rajistics/video/7417628862227680558",
          "view_count": 1979,
          "upload_date": "2024-09-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8931ef70370a4728bd422052d9a50be4_1727051310~tplv-tiktokx-origin.image?dr=9636&x-expires=1767416400&x-signature=8yuKNsU4Vuqa3fojv4jcNgH1Qns%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @mrjohnlueders #scrum #datascience #agile #softwareengineering #analytics",
      "description": "Reply to @mrjohnlueders #scrum #datascience #agile #softwareengineering #analytics",
      "upload_date": "2022-02-02",
      "total_views": 1972,
      "max_views": 1972,
      "topics": [
        "agile",
        "analytics",
        "datascience",
        "problem",
        "scrum",
        "softwareengineering"
      ],
      "search_text": "Reply to @mrjohnlueders #scrum #datascience #agile #softwareengineering #analytics agile analytics datascience problem scrum softwareengineering It's always fun as a data scientist to make fun of Scrum, but let's talk about it a little bit. There are times when an agile or Scrum methodology totally makes sense. So for example, maybe I've already tested out my problem, I know exactly what I want to do, and how I just need to finish that last mile of implementing and productionizing that. Using a Scrum or Agile makes total sense. Where it really falls down is the early part of a project. Where we're still trying to explore the data set, we're still trying to frame out the problem, and we don't exactly know which way we want to go. We don't even exactly know what the outcome will be sometimes, because we're trying to understand the problem space.",
      "platforms": {
        "tiktok": {
          "video_id": "7060265941749271855",
          "url": "https://www.tiktok.com/@rajistics/video/7060265941749271855",
          "view_count": 1972,
          "upload_date": "2022-02-02",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7b05e39759ee4989a01e62f0fb4eebde_1643846266~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=GN962b0Ga4a3R5qng6rg2uQx%2Bxc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6046,
      "title": "GDPval is OpenAI’s new benchmark that tests AI on real professional tasks from industries that drive GDP. Models like GPT-5 are graded by human experts on briefs, spreadsheets, and care plans, with results showing almost 50% of outputs match or beat humans. It’s fast and cheap, but limited: one-shot tasks, hidden oversight costs, and subjective grading mean “human-level” claims need caution. https://openai.com/index/gdpval/",
      "description": "GDPval is OpenAI’s new benchmark that tests AI on real professional tasks from industries that drive GDP. Models like GPT-5 are graded by human experts on briefs, spreadsheets, and care plans, with results showing almost 50% of outputs match or beat humans. It’s fast and cheap, but limited: one-shot tasks, hidden oversight costs, and subjective grading mean “human-level” claims need caution. https://openai.com/index/gdpval/",
      "upload_date": "2025-09-26",
      "total_views": 1967,
      "max_views": 1557,
      "topics": [
        "benchmark",
        "benchmarking",
        "exploring",
        "gdp",
        "gdpval",
        "human",
        "models",
        "new",
        "openai",
        "real",
        "tasks"
      ],
      "search_text": "GDPval is OpenAI’s new benchmark that tests AI on real professional tasks from industries that drive GDP. Models like GPT-5 are graded by human experts on briefs, spreadsheets, and care plans, with results showing almost 50% of outputs match or beat humans. It’s fast and cheap, but limited: one-shot tasks, hidden oversight costs, and subjective grading mean “human-level” claims need caution. https://openai.com/index/gdpval/ benchmark benchmarking exploring gdp gdpval human models new openai real tasks It's looking like AI can match humans on real tasks. So let's dig into how OpenAI figured this out and what it means for all of us. To understand how well AI can do at tasks, OpenAI developed GDP Valve, which measures models on tasks from jobs that cover most of the US GDP. So lawyers drafting briefs, nurse writing care plans, engineers building spreadsheets and machinery, each tasks look at what real professions actually do. So here's how it works. They picked 44 occupations across nine big industries. Each task comes with data, context and some background. You can check out the dataset for yourself over at Huggin' Face. They then had graders who were experts in these fields who compared the AI output against the human one and then they score it as is it better, the same or worse. The results found that the AI models tied or beat human experts on almost 50% of the tasks. And here's the thing, these models finish tasks at about 1% of the time the cost of humans. Now, the benchmark here is one shot. It doesn't measure the messiness of real tasks with revisions, clarifications, feedback loops, and it doesn't also consider how AI can improve with feedback. But the trend is clear. There's a widening set of professional tasks that AI is working at a very high level. If you aren't working with AI in your workflow, you're quickly going to be left behind because if you can't manage AI, AI is going to manage you.",
      "platforms": {
        "tiktok": {
          "video_id": "7554505779064098078",
          "url": "https://www.tiktok.com/@rajistics/video/7554505779064098078",
          "view_count": 1557,
          "upload_date": "2025-09-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oUFrAfxWIXNcfRfGgjQMih8IgReSAynbQUbRhG~tplv-tiktokx-origin.image?dr=9636&x-expires=1767304800&x-signature=SsTej2z5eux%2BdxelR3RR9dlQILI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18059403419143549",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-09-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "jnXKWMNHAPs",
          "url": "https://www.youtube.com/watch?v=jnXKWMNHAPs",
          "view_count": 410,
          "upload_date": "2025-09-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6212,
      "title": "This didn't happen to me recently :) To learn more: An Empirical Analysis of the Python Package Index (PyPI) - https://arxiv.org/pdf/1907.11073 PyPI Inundated by Malicious Typosquatting Campaign - https://blog.checkpoint.com/securing-the-cloud/pypi-inundated-by-malicious-typosquatting-campaign/ Diving into PyPI package name squatting - https://blog.orsinium.dev/posts/py/pypi-squatting/",
      "description": "This didn't happen to me recently :) To learn more: An Empirical Analysis of the Python Package Index (PyPI) - https://arxiv.org/pdf/1907.11073 PyPI Inundated by Malicious Typosquatting Campaign - https://blog.checkpoint.com/securing-the-cloud/pypi-inundated-by-malicious-typosquatting-campaign/ Diving into PyPI package name squatting - https://blog.orsinium.dev/posts/py/pypi-squatting/",
      "upload_date": "2024-12-12",
      "total_views": 1965,
      "max_views": 1624,
      "topics": [
        "malicious",
        "name",
        "package",
        "pypi",
        "python",
        "reserve",
        "squatting",
        "super"
      ],
      "search_text": "This didn't happen to me recently :) To learn more: An Empirical Analysis of the Python Package Index (PyPI) - https://arxiv.org/pdf/1907.11073 PyPI Inundated by Malicious Typosquatting Campaign - https://blog.checkpoint.com/securing-the-cloud/pypi-inundated-by-malicious-typosquatting-campaign/ Diving into PyPI package name squatting - https://blog.orsinium.dev/posts/py/pypi-squatting/ malicious name package pypi python reserve squatting super I just tried to upload my new Python package super sorter to PyPy, but the name is taken. Let me guess, that package has no releases, no code, and is maintained from someone from the 1900s. Yeah, exactly. I mean, PyPy is supposed to be the official repository for Python packages, making it easy to share code. But how are we supposed to share anything if all the good names are taken? Look, PyPy has 900,000 users across 600,000 projects, so of course all the easiest names are gone. I crunched through their data, there's so many people just sitting on lots of unused names. I get it, you have to try to make an unusual or esoteric name. Exactly, so now I have to pick a name like super sorter 2k24 or super sorter, super sorter, please work version three. And this isn't just about you losing a pretty name, it's a security risk. People will make malicious packages that sound like other packages, like take pillow two here. This was a malicious package that was downloaded. It's like the system only protects old timers. Community has tried to address this, PyPy has a process where you can try to petition and get a name that's being used, but you know, the last thing somebody wants to do is make somebody upset that's still working on their project. It's so annoying, I just want to share my work with the community. Why does it have to be so hard? You think this is hard? Wait till you find out about Python dependencies.",
      "platforms": {
        "tiktok": {
          "video_id": "7447581924341730590",
          "url": "https://www.tiktok.com/@rajistics/video/7447581924341730590",
          "view_count": 1624,
          "upload_date": "2024-12-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o0GEEWqV3IAyjA8ixpIBbfAoOAE4fCDBnEqRAF~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=7OOrAG87mi4%2Fy5v%2BETFC7zycgOE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18031116764384827",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "H1UjaIPcXbk",
          "url": "https://www.youtube.com/watch?v=H1UjaIPcXbk",
          "view_count": 341,
          "upload_date": "2024-12-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6280,
      "title": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering",
      "description": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering",
      "upload_date": "2023-02-27",
      "total_views": 1959,
      "max_views": 1959,
      "topics": [
        "data",
        "datascience",
        "engineering",
        "feature",
        "featureengineering",
        "machinelearning",
        "model"
      ],
      "search_text": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering data datascience engineering feature featureengineering machinelearning model",
      "platforms": {
        "instagram": {
          "video_id": "18165710149277922",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "9hGSDfhGmB4",
          "url": "https://youtube.com/shorts/9hGSDfhGmB4?feature=share",
          "view_count": 1959,
          "upload_date": "2023-02-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6449,
      "title": "Replika and the growth of these character chatbots or socialbots is emerging as a big use case within generative AI. Here is a recent controversy over the loss of the erotic role play (erp) functionality. #datascience #gpt3 #chatbot #socialbot #replika",
      "description": "Replika and the growth of these character chatbots or socialbots is emerging as a big use case within generative AI. Here is a recent controversy over the loss of the erotic role play (erp) functionality. #datascience #gpt3 #chatbot #socialbot #replika",
      "upload_date": "2023-02-14",
      "total_views": 1959,
      "max_views": 1726,
      "topics": [
        "chatbot",
        "datascience",
        "gpt3",
        "growth",
        "people",
        "replika",
        "socialbot"
      ],
      "search_text": "Replika and the growth of these character chatbots or socialbots is emerging as a big use case within generative AI. Here is a recent controversy over the loss of the erotic role play (erp) functionality. #datascience #gpt3 #chatbot #socialbot #replika chatbot datascience gpt3 growth people replika socialbot",
      "platforms": {
        "instagram": {
          "video_id": "CopVseuAo5a",
          "url": "https://www.instagram.com/reel/CopVseuAo5a/",
          "view_count": 233,
          "upload_date": "2023-02-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZeBQQM7IrVk",
          "url": "https://youtube.com/shorts/ZeBQQM7IrVk",
          "view_count": 1726,
          "upload_date": "2023-02-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "An older video, but still very useful. Take time to look at the actual errors of your model. It’s seems obvious, but too often people just stare at metrics. ",
      "description": "An older video, but still very useful. Take time to look at the actual errors of your model. It’s seems obvious, but too often people just stare at metrics. ",
      "upload_date": "2024-06-19",
      "total_views": 1951,
      "max_views": 1951,
      "topics": [
        "data",
        "errors",
        "look",
        "model",
        "people",
        "take"
      ],
      "search_text": "An older video, but still very useful. Take time to look at the actual errors of your model. It’s seems obvious, but too often people just stare at metrics.  data errors look model people take Can we just stop judging algorithms and instead focus on outcomes? Some people think if I explain the model architecture, the hyperparameters I use, the training routine that I use, that somehow they'll understand how this model works. No. A better way to understand how a model works is to try it on data that the model hasn't seen before. And then when the model gets errors, then they all do. It's okay. Here's three tips for analyzing that of those errors. Visualizations are great for getting a big picture view of the data and helping sort out errors. No model is going to be perfect. The world isn't perfect. Often people disagree on exactly how to label the data. So take that into account when you're doing this. Finally, look at actual examples and study what went wrong in the examples. In this example here, I was doing this the other day. I was staring at this and figured out that, hey, you know what? The algorithm isn't that good at math, and that's why it's getting this example wrong.",
      "platforms": {
        "tiktok": {
          "video_id": "7382039774816636202",
          "url": "https://www.tiktok.com/@rajistics/video/7382039774816636202",
          "view_count": 1951,
          "upload_date": "2024-06-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/dbfbebd89f6d47cda81fdab1a646ea36_1718765078~tplv-tiktokx-origin.image?dr=9636&x-expires=1767456000&x-signature=2CRBYx1zESSrBgVC5xMRumlKU0M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "You can’t make this stuff up. Can I just say modeling?  #datascience #analytics #statistics #scrum #techtok",
      "description": "You can’t make this stuff up. Can I just say modeling?  #datascience #analytics #statistics #scrum #techtok",
      "upload_date": "2022-04-28",
      "total_views": 1951,
      "max_views": 1951,
      "topics": [
        "analytics",
        "datascience",
        "make",
        "scrum",
        "statistics",
        "techtok"
      ],
      "search_text": "You can’t make this stuff up. Can I just say modeling?  #datascience #analytics #statistics #scrum #techtok analytics datascience make scrum statistics techtok I am trying so hard to be better, but to my core I am a hater.",
      "platforms": {
        "tiktok": {
          "video_id": "7091754943769791790",
          "url": "https://www.tiktok.com/@rajistics/video/7091754943769791790",
          "view_count": 1951,
          "upload_date": "2022-04-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/191f034fa1df40bfa25d6a405c26d74d_1651177870~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=Iayxblz0ZZeWPkNddoO4HQ%2FvFFY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6332,
      "title": "#onthisday ",
      "description": "#onthisday ",
      "upload_date": "2025-07-13",
      "total_views": 1950,
      "max_views": 1492,
      "topics": [
        "also",
        "contextual",
        "demo",
        "different",
        "evaluating",
        "going",
        "like",
        "model",
        "models",
        "object",
        "onthisday",
        "rag",
        "retrieval",
        "spaces",
        "things",
        "try"
      ],
      "search_text": "#onthisday  also contextual demo different evaluating going like model models object onthisday rag retrieval spaces things try Ready to learn about one of the coolest applications of AI? It's object detection. And what it involves is how you can use AI to detect multiple objects in a picture and actually put little bounding boxes so you know where that object is. Nate made a spaces here so you can go ahead and try some of the demo videos or even upload your own video and see how it works. The demo's using YOLO v6. You can also get the code for that and other tutorials at their GitHub. You can also try out the spaces on images.",
      "platforms": {
        "tiktok": {
          "video_id": "7526571195219725599",
          "url": "https://www.tiktok.com/@rajistics/video/7526571195219725599",
          "view_count": 1492,
          "upload_date": "2025-07-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocvpUiAIBRAziYoF1AEMZaAwdzCHIljVoBRP1~tplv-tiktokx-origin.image?dr=9636&x-expires=1767315600&x-signature=OKpvbcBVbSfUu0D0qZh10b3i%2FYQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18031818350739345",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "yPAUcfrBuu0",
          "url": "https://www.youtube.com/watch?v=yPAUcfrBuu0",
          "view_count": 458,
          "upload_date": "2025-07-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6245,
      "title": "Start using Llama 3.2 Vision Models with Hugging Face Transformers (on Snowflake)",
      "description": "Start using Llama 3.2 Vision Models with Hugging Face Transformers (on Snowflake)",
      "upload_date": "2024-09-29",
      "total_views": 1949,
      "max_views": 1949,
      "topics": [
        "das",
        "die",
        "ein",
        "hugging",
        "ich",
        "llama",
        "models",
        "start",
        "und",
        "using",
        "vision",
        "wir"
      ],
      "search_text": "Start using Llama 3.2 Vision Models with Hugging Face Transformers (on Snowflake) das die ein hugging ich llama models start und using vision wir",
      "platforms": {
        "instagram": {
          "video_id": "18008205977423864",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "uNbG7P7EEFY",
          "url": "https://www.youtube.com/watch?v=uNbG7P7EEFY",
          "view_count": 1949,
          "upload_date": "2024-09-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Pricing optimization is a data science use case that is growing. In some areas, like many states in the United States, it is not allowed for insurance pricing. However, in many other areas, just as data scientists spent lots of time on advertising use cases, it will now include pricing optimization. For a great deep dive: One Person, one price from American Prospect: https://prospect.org/economy/2024-06-04-one-person-one-price/",
      "description": "Pricing optimization is a data science use case that is growing. In some areas, like many states in the United States, it is not allowed for insurance pricing. However, in many other areas, just as data scientists spent lots of time on advertising use cases, it will now include pricing optimization. For a great deep dive: One Person, one price from American Prospect: https://prospect.org/economy/2024-06-04-one-person-one-price/",
      "upload_date": "2024-07-16",
      "total_views": 1949,
      "max_views": 1949,
      "topics": [
        "data",
        "one",
        "pay",
        "price",
        "pricing",
        "use"
      ],
      "search_text": "Pricing optimization is a data science use case that is growing. In some areas, like many states in the United States, it is not allowed for insurance pricing. However, in many other areas, just as data scientists spent lots of time on advertising use cases, it will now include pricing optimization. For a great deep dive: One Person, one price from American Prospect: https://prospect.org/economy/2024-06-04-one-person-one-price/ data one pay price pricing use I've decided that McDowell, we're going to move forward with pricing optimization. Bruno explained, My background is pricing insurance in Europe. We strive to charge the highest possible price that consumers would pay without switching to an alternative. Isn't that kind of shady? Shouldn't the price refer to the actual risk or cost of the product? But most US states have restricted it. But we are not in the insurance business and I believe this will increase shareholder value. So we're adjusting prices based on machine learning to what we think people are willing to pay? More like offering customers discounts or incentives. It gives the customers the feeling that they're getting a deal. Gives them that dope peen rush. Based on my understanding, what we can do is on hot days we know people are going to be willing to pay more for ice cream. Or if we know somebody's getting paid on a certain day, we know that on those days they're probably paying willing to pay a little bit more for their normal meal. A lot of hot days coming. We'll use a broad range of data including demographics, behavioral information, psychographics, along with machine learning algorithms. Behavioral activity for mobile? How do we even get that? This is why we're pushing people to use our mobile application. Once it's installed on their device, we can start pulling in all their mobile data. With this information, we'll be able to set some initial prices. But then we'll start running in targeted markets doing multi-banded experiments, trying to figure out what's the optimal price to do. We're even also going to take advantage of real time on the app to be able to change things on the fly. We have high expectations for this project and I've approved opening up additional roles for the team. I'm loving it!",
      "platforms": {
        "tiktok": {
          "video_id": "7392031604509609258",
          "url": "https://www.tiktok.com/@rajistics/video/7392031604509609258",
          "view_count": 1949,
          "upload_date": "2024-07-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/06e10249500d4b9b828e7114b082beb3_1721091487~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=uPArGaEmIJfyo6eIIh%2BfCYygpiQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Sentence Transformers - https://sbert.net/ MTEB: Massive Text Embedding Benchmark - https://huggingface.co/blog/mteb Notebook - https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/sentence_transformers.ipynb Text Embeddings Inference - https://github.com/huggingface/text-embeddings-inference",
      "description": "Sentence Transformers - https://sbert.net/ MTEB: Massive Text Embedding Benchmark - https://huggingface.co/blog/mteb Notebook - https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/sentence_transformers.ipynb Text Embeddings Inference - https://github.com/huggingface/text-embeddings-inference",
      "upload_date": "2024-10-15",
      "total_views": 1947,
      "max_views": 1947,
      "topics": [
        "embedding",
        "inference",
        "models",
        "one",
        "pick",
        "text"
      ],
      "search_text": "Sentence Transformers - https://sbert.net/ MTEB: Massive Text Embedding Benchmark - https://huggingface.co/blog/mteb Notebook - https://github.com/rajshah4/snowflake-notebooks/blob/main/ML_Tools/sentence_transformers.ipynb Text Embeddings Inference - https://github.com/huggingface/text-embeddings-inference embedding inference models one pick text Oh my, there's thousands of sentence transformers. How do I pick one? It can get overwhelming fast. You can check the most downloaded in the trending, but what I prefer to start with is the massive text embedding leaderboard. Leaderboards. So do I just grab the top one? Not so fast. Models aren't one size fits all. Models vary on their performance on different types of tasks. They also vary on the size of it, which affects the amount of compute and cost to use these models. Bigger isn't always better. I tried telling my ex that. So how do I pick a model? Pick a few models that seem right for your task, then here's a script that shows you how you can evaluate these models on your text classification data. Let's see how they do. Great work. Now you can start seeing the trade-offs between speed and accuracy. I'd also be careful with this model. It seems to fit to the data too well. Could be data leakage. Might want to try some additional examples that has never seen before. So I pick one and deploy it. If you want even faster, I got three tips. You can batch your queries, you can use the Onyx file format, and you can use a specialized serving container like the text embedding inference container. And all of these tips don't affect model accuracy. Three games!",
      "platforms": {
        "tiktok": {
          "video_id": "7425797177429003562",
          "url": "https://www.tiktok.com/@rajistics/video/7425797177429003562",
          "view_count": 1947,
          "upload_date": "2024-10-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8c3922b87fa64ddcb2e3b7ff933f7bc2_1728953144~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=cbcrS1Gp5EGsVPUJFwX7Lzrdvi0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Fairness in models #datascience #analytics #fairnessml #bias #algorithms",
      "description": "Fairness in models #datascience #analytics #fairnessml #bias #algorithms",
      "upload_date": "2022-03-19",
      "total_views": 1936,
      "max_views": 1936,
      "topics": [
        "algorithms",
        "analytics",
        "bias",
        "datascience",
        "fairnessml",
        "models"
      ],
      "search_text": "Fairness in models #datascience #analytics #fairnessml #bias #algorithms algorithms analytics bias datascience fairnessml models We all know the world's not fair. Let's talk about how our AI models can be unfair. Last time we talked about how you'd set a threshold to maximize profit. Now to figure out the profit, we had to put down values for what the false positive and false negatives were. But if we think about it, the values aren't the same for everybody. What we can actually do is break up the population into smaller groups and treat them differently. Here I break my population into two groups and then if I give them different thresholds, I can increase my profits. And if your focus is just money, you're doing well. But it also means we're holding to different standards for different groups. And this is where fairness comes in. In our society, there's times when we don't want models to just focus on profit.",
      "platforms": {
        "tiktok": {
          "video_id": "7076896443784809774",
          "url": "https://www.tiktok.com/@rajistics/video/7076896443784809774",
          "view_count": 1936,
          "upload_date": "2022-03-19",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a082792b1c7049759fc1bb5a416fad78_1647718357~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=PEJAyfAJqpdBny%2FUXeA4REUiiOE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @darianv19 semantic search versus lexicon search.  Emeddings help power semantic search.  #datascience #embeddings #python",
      "description": "Replying to @darianv19 semantic search versus lexicon search.  Emeddings help power semantic search.  #datascience #embeddings #python",
      "upload_date": "2022-07-05",
      "total_views": 1932,
      "max_views": 1932,
      "topics": [
        "datascience",
        "doesn",
        "embeddings",
        "python",
        "search",
        "semantic"
      ],
      "search_text": "Replying to @darianv19 semantic search versus lexicon search.  Emeddings help power semantic search.  #datascience #embeddings #python datascience doesn embeddings python search semantic My answer here is, does a fish know it's in water? The same thing with all of us. We're so used to now semantic search being widely implemented, you don't necessarily understand the difference between that it and traditional search. By traditional search, I mean lexicon search, where we're looking for key words. So if you ask it for example, what the capital of the United States is, you get answers like this. The results aren't great because it doesn't understand the contextual nature of our words. So it doesn't know for example that US and USA and United States all refer to the same concept. It doesn't have an understanding of word order. It also doesn't understand kind of terms that might be related to each other. You can see the improvement of semantic search with these search results. If you want to replicate this and try this for yourself and compare traditional lexical search and semantic search, here's the shortcut so you can go and try a colab notebook and run the code yourself.",
      "platforms": {
        "tiktok": {
          "video_id": "7117003011892743470",
          "url": "https://www.tiktok.com/@rajistics/video/7117003011892743470",
          "view_count": 1932,
          "upload_date": "2022-07-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f295d298be4f46cb982a29b765b42588_1657056396~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=K91LwUd%2FPlZMvlwZt58iw%2BgxVV8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @mat.cov05  annotator agreement puts a ceiling on your model performance  #datascience #statistics #codetok",
      "description": "Reply to @mat.cov05  annotator agreement puts a ceiling on your model performance  #datascience #statistics #codetok",
      "upload_date": "2022-06-19",
      "total_views": 1917,
      "max_views": 1917,
      "topics": [
        "annotators",
        "codetok",
        "customer",
        "datascience",
        "review",
        "statistics"
      ],
      "search_text": "Reply to @mat.cov05  annotator agreement puts a ceiling on your model performance  #datascience #statistics #codetok annotators codetok customer datascience review statistics This table is really cool. It helps us understand the agreement between annotators. So let's talk about this. When you're working on a machine learning problem, you're often focused on some outcomes such as is this customer review a happy customer review or a sad customer review. Now to do this, you need to label that data and labeling can often be subjective. Different people that review something might come to different conclusions based on their backgrounds. And so this is where it's important to understand how your different annotators are responding. Sometimes one annotator just sees the world differently and you want to detect that right away before you're training on all that data. Other times you'll realize your problem has a subjective component and not everyone can agree on the conclusion. And if annotators don't agree when you're building the dataset, your machine learning algorithm is never going to be able to be better than that.",
      "platforms": {
        "tiktok": {
          "video_id": "7110749432886431022",
          "url": "https://www.tiktok.com/@rajistics/video/7110749432886431022",
          "view_count": 1917,
          "upload_date": "2022-06-19",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e0e46184e6fc412bbe1ba2e9db0c113f_1655600370~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=hNbeJeUiJEWnO1dyCbcesmUZYE0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Population Stability Index is a popular way to measure feature drift or data drift when monitoring machine learning models. I am doing a talk diving deeper into PSI at the Snowflake summit, but I wanted to make sure everyone was aware of PSI and why we use it.  #populationstabilityindex #modelmonitoring #mlops #rajistics",
      "description": "Population Stability Index is a popular way to measure feature drift or data drift when monitoring machine learning models. I am doing a talk diving deeper into PSI at the Snowflake summit, but I wanted to make sure everyone was aware of PSI and why we use it.  #populationstabilityindex #modelmonitoring #mlops #rajistics",
      "upload_date": "2024-05-30",
      "total_views": 1910,
      "max_views": 1910,
      "topics": [
        "cake",
        "mlops",
        "modelmonitoring",
        "population",
        "populationstabilityindex",
        "stability"
      ],
      "search_text": "Population Stability Index is a popular way to measure feature drift or data drift when monitoring machine learning models. I am doing a talk diving deeper into PSI at the Snowflake summit, but I wanted to make sure everyone was aware of PSI and why we use it.  #populationstabilityindex #modelmonitoring #mlops #rajistics cake mlops modelmonitoring population populationstabilityindex stability You really think your loans are worth $300 million? Absolutely. They've been vetted by our predictive models, but haven't you been changing your standards for a loan? I mean, my dog got a credit card application from your bank. Our standards haven't changed. Those loans are rock solid. I'm looking at your data, and clearly there's some changes over the last year. How could we quantify this? You almost have heard about the J divergence. It's a measure developed by Lin based on ideas from Jeffries dating back to 1946. It's similar to what you might call the Jensen Shanahan divergence. Can you translate that to everyday lingo? Think of it like baking a cake. If the ingredients change over time, your cake might end up tasting different. So what we want to do is compare the ingredients we had from the beginning to what we're changing, and we're going to compare those differences. Do I get a human understandable score out of this? Our scores are then going to tell us, hey, is this cake basically the same thing? Or maybe, hey, there's a little bit of a change? Or wait a minute, there's been a big change. It's a whole new cake. Brilliant. Let's call it something simple. Population stability index? It feels like you're erasing the contributions of statisticians for machine learning. Again. Why did I just now learn about this population stability index? Shouldn't you be running that on all of your models on a regular basis?",
      "platforms": {
        "tiktok": {
          "video_id": "7374612383731305774",
          "url": "https://www.tiktok.com/@rajistics/video/7374612383731305774",
          "view_count": 1910,
          "upload_date": "2024-05-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b3258633bd1a41afaa506fa29c795aae_1717035755~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=kP6ub6DKdaDof82NB%2BPxbo96VN8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday reposting an older video from last year that illustrates kmeans",
      "description": "#onthisday reposting an older video from last year that illustrates kmeans",
      "upload_date": "2023-08-22",
      "total_views": 1892,
      "max_views": 1892,
      "topics": [
        "last",
        "older",
        "onthisday",
        "reposting",
        "video",
        "year"
      ],
      "search_text": "#onthisday reposting an older video from last year that illustrates kmeans last older onthisday reposting video year",
      "platforms": {
        "tiktok": {
          "video_id": "7270158764232379694",
          "url": "https://www.tiktok.com/@rajistics/video/7270158764232379694",
          "view_count": 1892,
          "upload_date": "2023-08-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Who enjoys explaining how ML models work? #machinelearning #datascience #statistics #codetok",
      "description": "Who enjoys explaining how ML models work? #machinelearning #datascience #statistics #codetok",
      "upload_date": "2022-07-04",
      "total_views": 1879,
      "max_views": 1879,
      "topics": [
        "codetok",
        "datascience",
        "enjoys",
        "explaining",
        "machinelearning",
        "statistics"
      ],
      "search_text": "Who enjoys explaining how ML models work? #machinelearning #datascience #statistics #codetok codetok datascience enjoys explaining machinelearning statistics Just close your eyes",
      "platforms": {
        "tiktok": {
          "video_id": "7116326861457624366",
          "url": "https://www.tiktok.com/@rajistics/video/7116326861457624366",
          "view_count": 1879,
          "upload_date": "2022-07-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/524469ee1bfd4e19afed78511edb946c_1656898967~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=v13cwzmsgvmP7kcfukvl1tCbF8Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6217,
      "title": "Model calibration  ",
      "description": "Model calibration  ",
      "upload_date": "2024-11-26",
      "total_views": 1871,
      "max_views": 1871,
      "topics": [
        "calibration",
        "disease",
        "let",
        "model",
        "predicting",
        "probability"
      ],
      "search_text": "Model calibration   calibration disease let model predicting probability Look out my results. I'm predicting disease with an SVM model. We could do better. Right now you have a list of who's predicted to get the disease, but what would be useful is if we could give the probability that they're likely to get the disease. Yeah, I guess it does make a big difference if it's a 51% or 95%. Let me fix that. Here you go. That distribution doesn't look right. Did you calibrate your model? Calibrate? Let's plot what probability your model is predicting versus what the actual probability is. I see the problem when my model is predicting 70%, 95% of the people actually have the disease. So let's compare that to a model that's well calibrated, logistic regression. Once we build that model on your data, we'll see that the logistic regression, when it predicts 70%, actually about 70% of the people have the disease. I didn't know about calibration. I thought models just did that automatically. For many kinds of models, not just SVM, you're going to want to add a calibration step in there. You can use something like plot scaling or isotonic regression. And that way, when you give people the probabilities, you know that they're well calibrated.",
      "platforms": {
        "tiktok": {
          "video_id": "7441732561925328174",
          "url": "https://www.tiktok.com/@rajistics/video/7441732561925328174",
          "view_count": 1871,
          "upload_date": "2024-11-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oUu8ISpIwCsyARBLBkwitoY6IfW5z8iA6Cv5GA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=KdHGR%2FR3yrWOp9ZAVkjJC%2FMdoZ0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17906032011062819",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Transformers aren’t new anymore #datascience #codetok #deeplearning #machinelearning #statistics",
      "description": "Transformers aren’t new anymore #datascience #codetok #deeplearning #machinelearning #statistics",
      "upload_date": "2022-06-10",
      "total_views": 1869,
      "max_views": 1869,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "machinelearning",
        "statistics",
        "transformers"
      ],
      "search_text": "Transformers aren’t new anymore #datascience #codetok #deeplearning #machinelearning #statistics codetok datascience deeplearning machinelearning statistics transformers てゆきていじゃぽーてゆきていじゃぽー",
      "platforms": {
        "tiktok": {
          "video_id": "7107752245134904618",
          "url": "https://www.tiktok.com/@rajistics/video/7107752245134904618",
          "view_count": 1869,
          "upload_date": "2022-06-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6f923502743e4a1bb2feda021a5b3884_1654902532~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=d27%2FSmbkydyt1x7W5%2F3KACf7bRA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6086,
      "title": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Every data scientist has a story like this.",
      "description": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Every data scientist has a story like this.",
      "upload_date": "2025-01-02",
      "total_views": 1865,
      "max_views": 1865,
      "topics": [
        "data",
        "look",
        "model",
        "models",
        "scientist",
        "using"
      ],
      "search_text": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Every data scientist has a story like this. data look model models scientist using Have you been cheated on? Well, as a data scientist, you get used to it. I have models that tell me that this is an elephant, there's no cow here, or there's a guitar here. So let's talk about why these models cheat and what we can do about it. I train my AI model using these four images, but then look what happens when I ask for a prediction. It gets it wrong. Can you figure out why the model cheated? It looked at the background of the image, and this is what all AI models do, is they look for the shortcuts. They look for the easiest way to classify the data that you've given them. Here's a real world example, where we built a classifier to identify pneumonia in these long X-rays, and the model did great, but then we gave it new data that was unlike the training data. It was data from different hospitals. And you know what happened? The model fell down. It didn't do so well. Once we dug in and investigated, we found that in the images, there's a hospital specific token, and that's what the AI was using, because this hospital token gives you an idea if a hospital has a high likelihood of pneumonia or low, and it was using that as a proxy instead of actually taking the time to look at the long image. This happens every day in the life of a data scientist. It's often called leakage, and there's a paper here on shortcuts that goes over a couple of the common ways it does this. Sometimes the AI finds a pattern where there's no pattern there, or it's using other features or attributes than what you really believe the true signal is at. So how can we prevent this? So one thing is we can use explainability tools to better try to understand what our model is doing, but we can also think about how the model is going to be used and make sure when we're training the model that we use diverse data that actually represents how the model is going to be used. But if your model cheats on you, don't take it personally. It happens to all of us.",
      "platforms": {
        "tiktok": {
          "video_id": "7455415189614169374",
          "url": "https://www.tiktok.com/@rajistics/video/7455415189614169374",
          "view_count": 1865,
          "upload_date": "2025-01-02",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ow2AoJI9rvGAGeArKhAdLebQelUAHdgR0gQf3Q~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=DmCMY%2FXbcwDV3dIAUJwTlq%2BnGrs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18073486729632569",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I need more time to code. #datascience #programming #techtok #python",
      "description": "I need more time to code. #datascience #programming #techtok #python",
      "upload_date": "2022-04-15",
      "total_views": 1862,
      "max_views": 1862,
      "topics": [
        "datascience",
        "need",
        "programming",
        "python",
        "techtok",
        "time"
      ],
      "search_text": "I need more time to code. #datascience #programming #techtok #python datascience need programming python techtok time 如果你太過熱情了說謊",
      "platforms": {
        "tiktok": {
          "video_id": "7086941572691938602",
          "url": "https://www.tiktok.com/@rajistics/video/7086941572691938602",
          "view_count": 1862,
          "upload_date": "2022-04-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e9642c3b365645c0b0eba05c36b33c48_1650057170~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=3tGZEw9sJjo9eB6e2ZZycbKKTc0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "It has happened. #datascience #codetok #machinelearning #analytics",
      "description": "It has happened. #datascience #codetok #machinelearning #analytics",
      "upload_date": "2022-05-25",
      "total_views": 1846,
      "max_views": 1846,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "happened",
        "machinelearning"
      ],
      "search_text": "It has happened. #datascience #codetok #machinelearning #analytics analytics codetok datascience happened machinelearning be",
      "platforms": {
        "tiktok": {
          "video_id": "7101816355070086446",
          "url": "https://www.tiktok.com/@rajistics/video/7101816355070086446",
          "view_count": 1846,
          "upload_date": "2022-05-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d1e693ebc37c4d5f927396352b9423a7_1653520475~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=I2T0u%2FO0QCiSNEiU4qOOZv2x4wc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Those GPUs.  #datascience #codetok #python #analytics #aws",
      "description": "Those GPUs.  #datascience #codetok #python #analytics #aws",
      "upload_date": "2022-05-04",
      "total_views": 1839,
      "max_views": 1839,
      "topics": [
        "analytics",
        "aws",
        "codetok",
        "datascience",
        "gpus",
        "python"
      ],
      "search_text": "Those GPUs.  #datascience #codetok #python #analytics #aws analytics aws codetok datascience gpus python 30,000",
      "platforms": {
        "tiktok": {
          "video_id": "7093965896087063851",
          "url": "https://www.tiktok.com/@rajistics/video/7093965896087063851",
          "view_count": 1839,
          "upload_date": "2022-05-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/986bdf635a354097b4fcc39663e64908_1651692647~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=mw%2F6JRKjuvYRKQct%2Fzs%2FGIdvkUY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6142,
      "title": "Nvidia Prismer model for image captioning and zero shot visual question answering. It uses and ensemble or mixture of experts approach. #datascience #machinelearning #nvidia #prismer #imagecaptioning #visualquestionanswering",
      "description": "Nvidia Prismer model for image captioning and zero shot visual question answering. It uses and ensemble or mixture of experts approach. #datascience #machinelearning #nvidia #prismer #imagecaptioning #visualquestionanswering",
      "upload_date": "2023-03-15",
      "total_views": 1835,
      "max_views": 1835,
      "topics": [
        "datascience",
        "imagecaptioning",
        "machinelearning",
        "model",
        "nvidia",
        "prismer",
        "uses",
        "visualquestionanswering"
      ],
      "search_text": "Nvidia Prismer model for image captioning and zero shot visual question answering. It uses and ensemble or mixture of experts approach. #datascience #machinelearning #nvidia #prismer #imagecaptioning #visualquestionanswering datascience imagecaptioning machinelearning model nvidia prismer uses visualquestionanswering",
      "platforms": {
        "instagram": {
          "video_id": "17986038781928576",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "78ltMui0_N4",
          "url": "https://youtube.com/shorts/78ltMui0_N4",
          "view_count": 1835,
          "upload_date": "2023-03-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Repost, but still useful. Some tips for deploying large language models like Llama. Start by building some benchmarks for your tasks to assess how your model performs on different GPUs. If it's too slow, think about using a model size model, using quantization, and looking for an improved model serving solution. I mentioned a lot of packages, but this is a fast-moving area.",
      "description": "Repost, but still useful. Some tips for deploying large language models like Llama. Start by building some benchmarks for your tasks to assess how your model performs on different GPUs. If it's too slow, think about using a model size model, using quantization, and looking for an improved model serving solution. I mentioned a lot of packages, but this is a fast-moving area.",
      "upload_date": "2024-07-30",
      "total_views": 1810,
      "max_views": 1810,
      "topics": [
        "gpus",
        "like",
        "model",
        "per",
        "quantization",
        "tokens"
      ],
      "search_text": "Repost, but still useful. Some tips for deploying large language models like Llama. Start by building some benchmarks for your tasks to assess how your model performs on different GPUs. If it's too slow, think about using a model size model, using quantization, and looking for an improved model serving solution. I mentioned a lot of packages, but this is a fast-moving area. gpus like model per quantization tokens Just finish fine-tuning my Lama model. What's next for deploying the model? You're gonna need to know what kind of GPU resources to request I'm not sure would it be any different than training run a sample of your typical tasks and evaluate that across multiple GPUs Looks like I'm only getting 10 tokens per second on my $2 GPU. It's a bit slow You think I can get multiple GPUs well 10 tokens per second means 36,000 tokens per hour at $2 an hour That's something like 18,000 tokens an hour Which is about 0.06 for every 1000 tokens and if you compare that to the cost of GPT-3 for example You're a bit on the expensive side right now. That isn't good any ideas Have you considered using a smaller model or model distillation to reduce the compute resources? I tried that but I get a drop in model quality when I use a smaller model well Your next step should be trying quantization and going from the default float 32 to something like 8-bit integer Hold on there's so many ways to do quantization Well something like ggml is good if you're working with smaller models and wanting to take advantage of CPUs for GPUs Look at all these packages. I found for quantization. That's great The last step you should think about is optimizing the model serving component. There's a couple of packages out there They focus on optimizing the distributed serving. Hey when I did all this I'm now getting 50 tokens per second, which is a lot faster and it's gonna be a lot cheaper to run first step down Now let's talk about getting some micro bruise for IT. So they'll let us use those GPUs",
      "platforms": {
        "tiktok": {
          "video_id": "7397437505311739179",
          "url": "https://www.tiktok.com/@rajistics/video/7397437505311739179",
          "view_count": 1810,
          "upload_date": "2024-07-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/98493fc9a4424012b5494ff0d4bf67b6_1722350169~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=CfciqbUzzNgO4W1lPwJoBpaRH48%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "We like to say “data is data” and that scale fixes everything. This skit questions that assumption using recent work on Data Shapley. By measuring how each training example actually affects validation loss during training, researchers found that some data helps, some is redundant, and some actively hurts performance. Removing negatively contributing data led to faster convergence with equal or better results. Paper: arXiv:2406.11011",
      "description": "We like to say “data is data” and that scale fixes everything. This skit questions that assumption using recent work on Data Shapley. By measuring how each training example actually affects validation loss during training, researchers found that some data helps, some is redundant, and some actively hurts performance. Removing negatively contributing data led to faster convergence with equal or better results. Paper: arXiv:2406.11011",
      "upload_date": "2026-01-07",
      "total_views": 1810,
      "max_views": 1810,
      "topics": [
        "data",
        "hurt",
        "model",
        "performance",
        "scale",
        "training"
      ],
      "search_text": "We like to say “data is data” and that scale fixes everything. This skit questions that assumption using recent work on Data Shapley. By measuring how each training example actually affects validation loss during training, researchers found that some data helps, some is redundant, and some actively hurts performance. Removing negatively contributing data led to faster convergence with equal or better results. Paper: arXiv:2406.11011 data hurt model performance scale training Data is data, it's fungible. More data is always better, we know it scales. Interesting thought. Can we check that assumption? How would you do that? We can measure it. During training, every example is going to nudge the model a little. So we're going to ask the question, did this actually reduce validation loss or increase it? So you're scoring every data point? Exactly. A positive score will show that it helped the model learn better, while a negative score will show that it actually hurt the training process. I'm a little skeptical of this. So we ran this on a normal training run, using pre-training data. And sometimes we found that some data consistently hurt performance. Okay, you said it hurt the performance, but did you validate this? Oh yes, we removed the negatively contributing data and retrained the model. The model converged a lot faster, final performance even improved. Interesting, what did the bad data look like? It wasn't always obvious. Some of that data was redundant, some of it was off target for what we were doing for that validation goal. Some data helped early in training, but then hurt later. Some of the data was just curated text that didn't align with what the model was trying to be optimized for. So scale isn't the answer to everything? The world is much more nuanced? The one thing that does scale is your trust in Twitter memes.",
      "platforms": {
        "tiktok": {
          "video_id": "7592449351918570783",
          "url": "https://www.tiktok.com/@rajistics/video/7592449351918570783",
          "view_count": 1810,
          "upload_date": "2026-01-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYEiMXI8EiIhROlXAzA3CAZ8BnBxo8AadVBvA~tplv-tiktokx-origin.image?dr=9636&x-expires=1768312800&x-signature=vEdyfL%2FslxbSYrXwy4QFI2gRzdI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Checking out Flan T5 large language models. Let me know what wisdom you can find in this model. #machinelearning #datascience #largelanguagemodels ",
      "description": "Checking out Flan T5 large language models. Let me know what wisdom you can find in this model. #machinelearning #datascience #largelanguagemodels ",
      "upload_date": "2022-11-09",
      "total_views": 1804,
      "max_views": 1804,
      "topics": [
        "datascience",
        "flan",
        "language",
        "largelanguagemodels",
        "machinelearning",
        "model"
      ],
      "search_text": "Checking out Flan T5 large language models. Let me know what wisdom you can find in this model. #machinelearning #datascience #largelanguagemodels  datascience flan language largelanguagemodels machinelearning model This is not good. I asked the latest large language model, Flan T5, about Trump and Obama, and it says they're not going to be talking to each other. Now this result isn't about politics, it's really about the brittleness or failure of some of these language models. I saw some of these examples of what Flan T5 could do and it looks like it's reasoning. It's pretty cool. I jumped on the hugging face hub, tried it out, did ransom sample queries. For example, I took a look at its reasoning ability, the ability for it to pull facts, as well as do a word problem, and it did pretty good on all those. Here's where it went off the rails. I decided to slightly change some of those. This is what happened when I changed Washington to Trump. I substituted another liquid, alcohol, and you can see the answer here. Does it make sense? Especially when you compare it with the Google results, which come on, the model should know this stuff. And when I reworded the word problem, whoa, the model really got loopy. Take a look at how it's doing that math. So go check out the space, try it yourself, but it's just a reminder that these language models aren't perfect yet, but they're a work in progress.",
      "platforms": {
        "tiktok": {
          "video_id": "7163859494701649195",
          "url": "https://www.tiktok.com/@rajistics/video/7163859494701649195",
          "view_count": 1804,
          "upload_date": "2022-11-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/18c80abae5fe4daa9ff3c0853704c47a_1667966032~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=H2Ko%2BjEYyrtxNLn6fift%2FAy5I2E%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6226,
      "title": "Measuring progress towards AGI ",
      "description": "Measuring progress towards AGI ",
      "upload_date": "2024-11-09",
      "total_views": 1789,
      "max_views": 1789,
      "topics": [
        "agi",
        "deep",
        "google",
        "level",
        "like",
        "measuring",
        "percentile",
        "progress",
        "skilled",
        "towards"
      ],
      "search_text": "Measuring progress towards AGI  agi deep google level like measuring percentile progress skilled towards There's something very popular that I always avoid. It's AGI, but Google's DeepMind has a new paper on it. The paper defines six levels of autonomy to start thinking about AGI. So level one is emerging and it's something that's equal or somewhat better than an unskilled human. So an example could be a spellchecker. It could be an application that converts, let's say, a sign into words. So level two is where we're at the 50th percentile of skilled adults. So here we can think of relying on language models to do summarization, toxicity detectors, visual question answering systems. Level three is when we can get to the 90th percentile of somebody that's skilled. So here think about like the advanced grammar checkers, like Grammarly, doing things like text image where we create images. Level four is when we hit the 99th percentile of skilled adults, the virtuoso. We're at the top level of doing this. Examples here we have things like deep blue or AlphaGo. Last level, level five is superhuman. That's when we can outperform 100% of humans. Examples like AlphaFold, AlphaZero, Stockfish, so doing chess, for example, the best chess computers are better than humans. But again, these are very few tasks and it takes a lot of compute for us to do that. But that's how we can start thinking about that progress towards AGI.",
      "platforms": {
        "tiktok": {
          "video_id": "7435335478670265642",
          "url": "https://www.tiktok.com/@rajistics/video/7435335478670265642",
          "view_count": 1789,
          "upload_date": "2024-11-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/181caae8131f41d09c778cf1c4fe1b38_1731173953~tplv-tiktokx-origin.image?dr=9636&x-expires=1767402000&x-signature=2RfU5JI%2BDx10hiBx67VGdf8RJqk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18041908490156662",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Aged well - With the growth of open-source LLMs, many leaderboards to rank these models are emerging. Several different methodologies are used, including human evaluation, academic datasets, and evaluation using GPT-4. These are all great but also remember to use methods that align with your use cases. #onthisday  ",
      "description": "Aged well - With the growth of open-source LLMs, many leaderboards to rank these models are emerging. Several different methodologies are used, including human evaluation, academic datasets, and evaluation using GPT-4. These are all great but also remember to use methods that align with your use cases. #onthisday  ",
      "upload_date": "2024-06-10",
      "total_views": 1784,
      "max_views": 1784,
      "topics": [
        "different",
        "leaderboards",
        "many",
        "model",
        "models",
        "onthisday"
      ],
      "search_text": "Aged well - With the growth of open-source LLMs, many leaderboards to rank these models are emerging. Several different methodologies are used, including human evaluation, academic datasets, and evaluation using GPT-4. These are all great but also remember to use methods that align with your use cases. #onthisday   different leaderboards many model models onthisday Have you seen the new Platypus model? It's at the top of the Alpaca leaderboard. No, this is what the Linus project is going to address. Why are you so fixated on this model? Shouldn't we be using the highest rated model? Let me ask you, who's the best athlete in the world? It's Michael Jordan. Just like it's hard to judge the best athlete outside of a specific sport, we have the same thing with large language models that are capable of so many different tasks. And this is why you see so many different leaderboards out there. The Alpaca leaderboard is using GPT-4 to evaluate other models. The Huggy Face leaderboard uses standard academic benchmarks. You also have competition leaderboards like from the Vicuna folks that compete models against each other and let humans decide which one's the winner. Do you see how each of these leaderboards is evaluating models in different ways? So how will we know what large language model to use? This is why we're building our own evaluation suite. It's inspired by all these other leaderboards, but it's also going to focus on the data sets and the benchmarks that we care about for our projects. I get it, Linus, to help classify all these models.",
      "platforms": {
        "tiktok": {
          "video_id": "7378982878899604782",
          "url": "https://www.tiktok.com/@rajistics/video/7378982878899604782",
          "view_count": 1784,
          "upload_date": "2024-06-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2a6f72ab84394d8e94ae1f4d04619ab5_1718053341~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=nErf58bTrfy82BgRv88Po3w7utQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "4 Data Science Fails.These are a handful of ways that society pushes back on data science approaches. It's good to understand why these were bad use cases. To dig deeper, check out the full set of examples. The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment: https://arxiv.org/pdf/2404.13802 Case Studies: https://njohnson99.github.io/fall-of-algorithm-database/",
      "description": "4 Data Science Fails.These are a handful of ways that society pushes back on data science approaches. It's good to understand why these were bad use cases. To dig deeper, check out the full set of examples. The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment: https://arxiv.org/pdf/2404.13802 Case Studies: https://njohnson99.github.io/fall-of-algorithm-database/",
      "upload_date": "2024-06-06",
      "total_views": 1784,
      "max_views": 1784,
      "topics": [
        "algorithm",
        "approaches",
        "data",
        "failed",
        "like",
        "model"
      ],
      "search_text": "4 Data Science Fails.These are a handful of ways that society pushes back on data science approaches. It's good to understand why these were bad use cases. To dig deeper, check out the full set of examples. The Fall of an Algorithm: Characterizing the Dynamics Toward Abandonment: https://arxiv.org/pdf/2404.13802 Case Studies: https://njohnson99.github.io/fall-of-algorithm-database/ algorithm approaches data failed like model We've yellow flagged your account because your baby has a 55% chance of committing a violent crime according to our predictive model. How can you know that? This is all data driven. We've already pulled into records of your life from your social media and will be continuing to monitor you and your baby going forward. Approaches like this have failed. These types of outcomes that touch things like teen pregnancy or childhood development make people uncomfortable, especially when it comes to collecting data around it. According to our predictive model, these are the hotspots for crime. Let's increase patrols here. But all those areas are predominantly minority neighborhoods. You think there's an issue with the data? Data driven. It's what the model says. Approaches like this have failed because people recognize that these models are just amplifying the biases that are already in the data. Why is my budget for Medicaid services so low? Data driven. We're now using an algorithm to decide the Medicaid budgets. But can you explain why? Approaches like this have failed because the inability to explain algorithmic decision making makes it feel arbitrary and unfair. Our chatbot is live. Let's see how it interacts with users. It's starting to say some really offensive stuff. Shut it down. Approaches like this have failed because without the proper safeguards, these types of models can go off the rail.",
      "platforms": {
        "tiktok": {
          "video_id": "7377214784577637678",
          "url": "https://www.tiktok.com/@rajistics/video/7377214784577637678",
          "view_count": 1784,
          "upload_date": "2024-06-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bb096ea025484871b055ed2dd3a45495_1717641674~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=hZa%2F9JlUDZPlpmGYgfyRB9XzO2s%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I have done a lot of good work in untitled python notebooks. #datascience #machinelearning #python #codetok #thosethatgetitgetit",
      "description": "I have done a lot of good work in untitled python notebooks. #datascience #machinelearning #python #codetok #thosethatgetitgetit",
      "upload_date": "2022-06-30",
      "total_views": 1774,
      "max_views": 1774,
      "topics": [
        "codetok",
        "datascience",
        "done",
        "machinelearning",
        "python",
        "thosethatgetitgetit"
      ],
      "search_text": "I have done a lot of good work in untitled python notebooks. #datascience #machinelearning #python #codetok #thosethatgetitgetit codetok datascience done machinelearning python thosethatgetitgetit Ew, this is so cringe. Guilty.",
      "platforms": {
        "tiktok": {
          "video_id": "7115149383502449966",
          "url": "https://www.tiktok.com/@rajistics/video/7115149383502449966",
          "view_count": 1774,
          "upload_date": "2022-06-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/423c8d25eee54ab6953d3e20cef42836_1656624814~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=zVzMjGIo%2BbZszkkCAAWgFZx3aaI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "GPT- 3 trivia and French pastries I enjoyed at the 🤗 offsite. #datascience #machinelearning #gpt3 #openai #huggingface ",
      "description": "GPT- 3 trivia and French pastries I enjoyed at the 🤗 offsite. #datascience #machinelearning #gpt3 #openai #huggingface ",
      "upload_date": "2023-02-01",
      "total_views": 1761,
      "max_views": 1761,
      "topics": [
        "datascience",
        "gpt",
        "gpt3",
        "huggingface",
        "machinelearning",
        "openai"
      ],
      "search_text": "GPT- 3 trivia and French pastries I enjoyed at the 🤗 offsite. #datascience #machinelearning #gpt3 #openai #huggingface  datascience gpt gpt3 huggingface machinelearning openai You",
      "platforms": {
        "tiktok": {
          "video_id": "7195324681162116398",
          "url": "https://www.tiktok.com/@rajistics/video/7195324681162116398",
          "view_count": 1761,
          "upload_date": "2023-02-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7d2ae31e37e642b8ba68e209f835053f_1675292091~tplv-tiktokx-origin.image?dr=9636&x-expires=1767470400&x-signature=9w74%2FlMaGXpYJ7zyYEduEzadZY8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6276,
      "title": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr #datatable",
      "description": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr #datatable",
      "upload_date": "2023-03-01",
      "total_views": 1755,
      "max_views": 1755,
      "topics": [
        "datascience",
        "dplyr",
        "largelanguagemodels",
        "machinelearning",
        "meta",
        "models",
        "opensource",
        "pandas",
        "polars",
        "rstats"
      ],
      "search_text": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr #datatable datascience dplyr largelanguagemodels machinelearning meta models opensource pandas polars rstats",
      "platforms": {
        "instagram": {
          "video_id": "17959233242226648",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "2PR308vOgfs",
          "url": "https://youtube.com/shorts/2PR308vOgfs?feature=share",
          "view_count": 1755,
          "upload_date": "2023-03-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "OpenAI and Red Wedding",
      "description": "OpenAI and Red Wedding",
      "upload_date": "2023-11-18",
      "total_views": 1753,
      "max_views": 1753,
      "topics": [
        "openai",
        "red",
        "wedding"
      ],
      "search_text": "OpenAI and Red Wedding openai red wedding",
      "platforms": {
        "tiktok": {
          "video_id": "7302606403078819114",
          "url": "https://www.tiktok.com/@rajistics/video/7302606403078819114",
          "view_count": 1753,
          "upload_date": "2023-11-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6264,
      "title": "#onthisday Data visualization tips #datascience #dataviz #analytics #datavisualization",
      "description": "#onthisday Data visualization tips #datascience #dataviz #analytics #datavisualization",
      "upload_date": "2024-03-22",
      "total_views": 1746,
      "max_views": 1746,
      "topics": [
        "analytics",
        "data",
        "datascience",
        "datavisualization",
        "dataviz",
        "let",
        "onthisday"
      ],
      "search_text": "#onthisday Data visualization tips #datascience #dataviz #analytics #datavisualization analytics data datascience datavisualization dataviz let onthisday",
      "platforms": {
        "tiktok": {
          "video_id": "7349190227824528683",
          "url": "https://www.tiktok.com/@rajistics/video/7349190227824528683",
          "view_count": 1746,
          "upload_date": "2024-03-22",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18026581213991071",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-03-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Image captioning models - GIT from Microsoft and BLIP from salesforce #datascience #machinelearning #imagecaptioning ",
      "description": "Image captioning models - GIT from Microsoft and BLIP from salesforce #datascience #machinelearning #imagecaptioning ",
      "upload_date": "2023-01-05",
      "total_views": 1743,
      "max_views": 1743,
      "topics": [
        "datascience",
        "git",
        "image",
        "imagecaptioning",
        "machinelearning",
        "model"
      ],
      "search_text": "Image captioning models - GIT from Microsoft and BLIP from salesforce #datascience #machinelearning #imagecaptioning  datascience git image imagecaptioning machinelearning model Did you know AI could describe an image in words? This is called image captioning and this week there's two new state-of-the-art models, one from Facebook and one from Microsoft. Let's go over them. So let's start here with a classic picture of an astronaut on a horse. We'll see that our previous baseline of VIT and GPT too, didn't do so well. The Git Large model hones right in and you'll see the Blip Model tries but it goes off a little bit by starting to call it the space shuttle. Here's another example of a stop sign with some Arabic on top of it and we'll see our previous baseline here doesn't get it at all. The Git Base and the Blip Large both get it but you'll see the Git Large for some reason goes off the track. The Git model is from Microsoft, it's Microsoft Generative Image Text Model and you can find that on the Hub. The Blip Model comes from Salesforce and you can also find that in more details of it on the Hub.",
      "platforms": {
        "tiktok": {
          "video_id": "7185192921673616686",
          "url": "https://www.tiktok.com/@rajistics/video/7185192921673616686",
          "view_count": 1743,
          "upload_date": "2023-01-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d14eff2614ab415991c14860884254b0_1672933104~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=zgtk0eslhEyhxhNO74VRnRuMwHo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "No click bait on this account. Feeling sick today (and upset between Roe and Highland). Mailing it in today. #datascience #statistics #analytics  #codingbootcamp",
      "description": "No click bait on this account. Feeling sick today (and upset between Roe and Highland). Mailing it in today. #datascience #statistics #analytics  #codingbootcamp",
      "upload_date": "2022-07-06",
      "total_views": 1733,
      "max_views": 1733,
      "topics": [
        "analytics",
        "click",
        "codingbootcamp",
        "datascience",
        "statistics",
        "today"
      ],
      "search_text": "No click bait on this account. Feeling sick today (and upset between Roe and Highland). Mailing it in today. #datascience #statistics #analytics  #codingbootcamp analytics click codingbootcamp datascience statistics today Fel marginalism.",
      "platforms": {
        "tiktok": {
          "video_id": "7117392035618950446",
          "url": "https://www.tiktok.com/@rajistics/video/7117392035618950446",
          "view_count": 1733,
          "upload_date": "2022-07-06",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/327efcaae6fe45d4a437a1b973d01335_1657146972~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=TZHiO7i6Lvn3xJhUBkgWXAL7Ff4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Distance Metrics in Data Science  ",
      "description": "Distance Metrics in Data Science  ",
      "upload_date": "2024-10-24",
      "total_views": 1731,
      "max_views": 1731,
      "topics": [
        "distance",
        "find",
        "metrics",
        "similar",
        "think",
        "two"
      ],
      "search_text": "Distance Metrics in Data Science   distance find metrics similar think two You think this is the only way to measure distance? If we can think of distance in other ways, we can solve all types of problems. The correct distance metric can help us reduce air by limiting the difference between predictions and actuals. You can also find which things are related to each other and which things are very dissimilar from each other. Euclidean distance is often how we think about distance and it's kind of like how the bird flies between two points. But there's other distance metrics. Manhattan distance mirrors what would happen if we were to drive between two points and it's often used in routing algorithms as well as image processing when we're trying to find similar images. When we have many different variables with different scales where some distances are longer than others, Mahabalobu Nobus distance works well for those. If we're working with strings, let's say building an auto correction app, Levenstein distance takes the distance between edits which helps us figure out which strings are similar to each other. So cosine distance comes into place when we're working with embeddings or vectors where what we want to do is find the angle between two vectors because that tells us how similar they are. These are just a handful. There's many more and sometimes you have to invent your own distance metrics to best solve your problem.",
      "platforms": {
        "tiktok": {
          "video_id": "7429335733539704110",
          "url": "https://www.tiktok.com/@rajistics/video/7429335733539704110",
          "view_count": 1731,
          "upload_date": "2024-10-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5a0c5ba828544b3d9587d70245a1e936_1729777029~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=GMkYPgy0fLatFqxXi0wwBGH%2B1qU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Lots of real world problems, it pays to know distributions like tweedie.  Still sick, so you get old tik tok from my drafts.  #datascience #statistics #acturialscience #codetok",
      "description": "Lots of real world problems, it pays to know distributions like tweedie.  Still sick, so you get old tik tok from my drafts.  #datascience #statistics #acturialscience #codetok",
      "upload_date": "2022-07-08",
      "total_views": 1725,
      "max_views": 1725,
      "topics": [
        "acturialscience",
        "codetok",
        "datascience",
        "get",
        "know",
        "statistics"
      ],
      "search_text": "Lots of real world problems, it pays to know distributions like tweedie.  Still sick, so you get old tik tok from my drafts.  #datascience #statistics #acturialscience #codetok acturialscience codetok datascience get know statistics That's not normal, and I think you know you should maybe get some help or something",
      "platforms": {
        "tiktok": {
          "video_id": "7118068907465919786",
          "url": "https://www.tiktok.com/@rajistics/video/7118068907465919786",
          "view_count": 1725,
          "upload_date": "2022-07-08",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6b5d82b091fc42948740d44e4a8ca12d_1657304571~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=XxiDYa9HQ3OPmwqAFoqMM8hxqW0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6059,
      "title": "null",
      "description": "null",
      "upload_date": "2025-02-14",
      "total_views": 1717,
      "max_views": 1555,
      "topics": [
        "character",
        "end",
        "mark",
        "need",
        "null",
        "string"
      ],
      "search_text": "null character end mark need null string Who's that mysterious character? Oh, that's Null. Who is it? I meant, that's Null. No answer, and now you're telling me Null? What does that mean? It's commonly used in programming to mark the end of a string. The end is the end. Why do I need a special thing at the end? It started back with teleprinters. They needed a way to mark the end of a line so they introduced the Null character. Okay, so it was useful then. Why do we need it now? In computing, we've made it part of ASCII 2, and so the Null has a special position zero. We use it for string termination, memory management, and more. That sounds useful. It is, but it's also a problem. For example, if you put a Null character in the middle of a string, look at what happened. Yikes, that looks problematic. Oh, it's problematic. Nulls can break databases, cause security vulnerabilities, makes debugging a nightmare. But why? Null characters can be invisible. You think it's Alice Smith here, but in reality, the database only sees Alice and Smith gets silently ignored. So Null is everywhere, but can silently break things and cause chaos, just like a junior developer.",
      "platforms": {
        "tiktok": {
          "video_id": "7471086373060087070",
          "url": "https://www.tiktok.com/@rajistics/video/7471086373060087070",
          "view_count": 1555,
          "upload_date": "2025-02-14",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEdS1FCRVDnFrrEGQAA4AExKIf9J5IEEOeQAqA~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=Gd349zvfgpO2pxuqoH3MwYlfVcY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17898247947138715",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-14",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "7qVvqS6-M9k",
          "url": "https://www.youtube.com/watch?v=7qVvqS6-M9k",
          "view_count": 162,
          "upload_date": "2025-02-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6311,
      "title": "YouChat and retrieval augmented models. To play around with this, check out haystack from deepset. #datascience #machinelearning #youchat #chatgpt #openai #retrievalaugmentedmodel #questionanswermodel",
      "description": "YouChat and retrieval augmented models. To play around with this, check out haystack from deepset. #datascience #machinelearning #youchat #chatgpt #openai #retrievalaugmentedmodel #questionanswermodel",
      "upload_date": "2022-12-26",
      "total_views": 1701,
      "max_views": 1701,
      "topics": [
        "chatgpt",
        "datascience",
        "information",
        "machinelearning",
        "openai",
        "retrievalaugmentedmodel",
        "youchat"
      ],
      "search_text": "YouChat and retrieval augmented models. To play around with this, check out haystack from deepset. #datascience #machinelearning #youchat #chatgpt #openai #retrievalaugmentedmodel #questionanswermodel chatgpt datascience information machinelearning openai retrievalaugmentedmodel youchat",
      "platforms": {
        "instagram": {
          "video_id": "17921532776536676",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-12-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "iCNxJHBYNTE",
          "url": "https://youtube.com/shorts/iCNxJHBYNTE?feature=share",
          "view_count": 1701,
          "upload_date": "2022-12-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "credit to Gavin from work - #codetok #stackoverflow #programming #python",
      "description": "credit to Gavin from work - #codetok #stackoverflow #programming #python",
      "upload_date": "2022-05-05",
      "total_views": 1697,
      "max_views": 1697,
      "topics": [
        "codetok",
        "credit",
        "gavin",
        "programming",
        "python",
        "stackoverflow"
      ],
      "search_text": "credit to Gavin from work - #codetok #stackoverflow #programming #python codetok credit gavin programming python stackoverflow sensation",
      "platforms": {
        "tiktok": {
          "video_id": "7094379886248267054",
          "url": "https://www.tiktok.com/@rajistics/video/7094379886248267054",
          "view_count": 1697,
          "upload_date": "2022-05-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1066f111b3b7468693188b93fefed98c_1651789037~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=6lev4vzlZQSXeEVMLcJFyyoFYz4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6194,
      "title": "Improving your data visualizations. ",
      "description": "Improving your data visualizations. ",
      "upload_date": "2025-03-21",
      "total_views": 1692,
      "max_views": 1692,
      "topics": [
        "data",
        "don",
        "improving",
        "let",
        "make",
        "visualizations"
      ],
      "search_text": "Improving your data visualizations.  data don improving let make visualizations Let me show you how to go from this to this. Let's organize the data by sorting it. Make it easy to read the text. Don't rotate it. Don't make people try to guess the numbers. Just give it to them. Finally, add a little bit of color. And thanks, Cedric, for putting this together.",
      "platforms": {
        "tiktok": {
          "video_id": "7484342940924382495",
          "url": "https://www.tiktok.com/@rajistics/video/7484342940924382495",
          "view_count": 1692,
          "upload_date": "2025-03-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocAOAfOKBiDziABCydiutOEm7BKIHFtA6XIj0A~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=c%2FSZgYzRjH%2BvEr%2BUGM0chGbBIgo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17855680674400110",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6403,
      "title": "Replying to @Data Storyteller Here are two examples of data or target leakage. I bet people have other fun examples. #datascience #targetleakage #dataleakage #machinelearning",
      "description": "Replying to @Data Storyteller Here are two examples of data or target leakage. I bet people have other fun examples. #datascience #targetleakage #dataleakage #machinelearning",
      "upload_date": "2022-07-22",
      "total_views": 1690,
      "max_views": 1690,
      "topics": [
        "data",
        "dataleakage",
        "datascience",
        "errors",
        "examples",
        "look",
        "machinelearning",
        "model",
        "people",
        "take",
        "targetleakage"
      ],
      "search_text": "Replying to @Data Storyteller Here are two examples of data or target leakage. I bet people have other fun examples. #datascience #targetleakage #dataleakage #machinelearning data dataleakage datascience errors examples look machinelearning model people take targetleakage",
      "platforms": {
        "instagram": {
          "video_id": "18051976537726761",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NaySLPTCgDM",
          "url": "https://www.youtube.com/watch?v=NaySLPTCgDM",
          "view_count": 1690,
          "upload_date": "2022-07-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6251,
      "title": "How Error Analysis is Part of Data Science and Machine Learning",
      "description": "How Error Analysis is Part of Data Science and Machine Learning",
      "upload_date": "2024-05-20",
      "total_views": 1686,
      "max_views": 1686,
      "topics": [
        "analysis",
        "boosting",
        "data",
        "error",
        "errors",
        "learning",
        "machine",
        "models",
        "part",
        "science",
        "scientists"
      ],
      "search_text": "How Error Analysis is Part of Data Science and Machine Learning analysis boosting data error errors learning machine models part science scientists",
      "platforms": {
        "instagram": {
          "video_id": "18160135558309929",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Ee6-kpb2iX4",
          "url": "https://www.youtube.com/watch?v=Ee6-kpb2iX4",
          "view_count": 1686,
          "upload_date": "2024-05-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Conformal prediction.  ",
      "description": "Conformal prediction.  ",
      "upload_date": "2024-10-04",
      "total_views": 1684,
      "max_views": 1684,
      "topics": [
        "data",
        "interval",
        "let",
        "prediction",
        "predictions",
        "see"
      ],
      "search_text": "Conformal prediction.   data interval let prediction predictions see Let me share a dirty secret of data science. Data scientists often make it sound like their predictions are very precise, but actually there's a ton of variability in their prediction. Giving people the range of predictions is super useful. Let me show you how you can do this for any model with conformal predictions. Let's start with training a traditional machine learning model. Then we're going to use a separate split of the data for calibrating the prediction intervals. Let's get the predictions on this and now look at the error. All models have error. Here you can see the difference between the prediction and the actual value. We'll take all those errors, put them together, figure out where the 95 percentile cutoff is, which is what I'm choosing for the prediction interval here, and the result for this data set is 1.3. I'm going to take the error now, plot it out with a histogram, and then draw a line at the 95 percent quantile. This will help me figure out what the width should be for my prediction interval. Let's apply this to a few data points and see what it looks like. Here you can see for all of these points exactly what the prediction interval is, and if I overlay the actuals of what actually happened you'll see that most of the points should be landed within the prediction interval. There you have it. For any model we can get prediction intervals. I'll put a longer version of this along with the notebook out on the tubes.",
      "platforms": {
        "tiktok": {
          "video_id": "7421711835922992430",
          "url": "https://www.tiktok.com/@rajistics/video/7421711835922992430",
          "view_count": 1684,
          "upload_date": "2024-10-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d53f9b6f7ab14bff916f6f2001fae5bd_1728001950~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=8IeDyDG4z4eLrcbGgR9m3GT4b%2Bs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Fundamentals folks. A great example is the paper on police misconduct. It highlights a lot of great data science practices (more than I could squeeze into the video). But hopefully, you all consider alternatives to ML, comparisons to baselines, how much data you should be training on, and the number of features. And most importantly, what is the bottom line impact of your model translated into real world impacts. Predicting Policy Misconduct: https://www.nber.org/papers/w32432",
      "description": "Fundamentals folks. A great example is the paper on police misconduct. It highlights a lot of great data science practices (more than I could squeeze into the video). But hopefully, you all consider alternatives to ML, comparisons to baselines, how much data you should be training on, and the number of features. And most importantly, what is the bottom line impact of your model translated into real world impacts. Predicting Policy Misconduct: https://www.nber.org/papers/w32432",
      "upload_date": "2024-05-27",
      "total_views": 1674,
      "max_views": 1674,
      "topics": [
        "data",
        "learning",
        "machine",
        "misconduct",
        "model",
        "police"
      ],
      "search_text": "Fundamentals folks. A great example is the paper on police misconduct. It highlights a lot of great data science practices (more than I could squeeze into the video). But hopefully, you all consider alternatives to ML, comparisons to baselines, how much data you should be training on, and the number of features. And most importantly, what is the bottom line impact of your model translated into real world impacts. Predicting Policy Misconduct: https://www.nber.org/papers/w32432 data learning machine misconduct model police The Epic Battle. Data scientists versus data handlers. Today, preventing police misconduct. I built a gradient-boosted machine learning model with 800 features that has an AUC of 0.79. It's not all that. When I took a look at the predictions from your model and looked at the most risky people, those people ended up only accounting for a small amount of the lawsuits or complaints. Your model is missing a lot of police misconduct. You have to remember, the answer to big societal problems is bigger machine learning models. Oh please, I made a ranked list of the officers with the most complaints, and by targeting the ones with the most complaints, we can do just as well as your machine learning model. No way. Let's look at the data. Yes, let's look at the data. I call this graph a learning curve, and you will see that my model does better than the baseline. Your machine learning approach works only slightly better, and really for departments where there's more than 200 officers, which isn't most of them. Do you think we can ask the police to increase the police misconduct so I have more data to work with? Seriously, with my approach, you don't need to spend millions of dollars on machine learning. Everyone needs AI. A data scientist in every organization, we must buy more GPUs. My approach can save the city a lot of money. I ran the numbers to see if we targeted those with the highest risk, how much money we could save from lawsuits and investigations.",
      "platforms": {
        "tiktok": {
          "video_id": "7373708295921601834",
          "url": "https://www.tiktok.com/@rajistics/video/7373708295921601834",
          "view_count": 1674,
          "upload_date": "2024-05-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3af711cf4f6d409281b4d2c8fbf527b9_1716825257~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=IU%2Fax8JrgXmWTHJncFNbynv4w3Y%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @rajistics here are two themes I wanted to highlight. The second candidate showed more analytic maturity. ",
      "description": "Replying to @rajistics here are two themes I wanted to highlight. The second candidate showed more analytic maturity. ",
      "upload_date": "2022-12-18",
      "total_views": 1671,
      "max_views": 1671,
      "topics": [
        "algorithms",
        "data",
        "important",
        "science",
        "second",
        "two"
      ],
      "search_text": "Replying to @rajistics here are two themes I wanted to highlight. The second candidate showed more analytic maturity.  algorithms data important science second two For me, there's at least two important lessons from that skit. I know there's a ton of paid resources and books out there for learning data science. There's no reason for any of it. All the best data science resources are out there and available for free. Second, data science is not just about algorithms. What we're doing is we're solving business problems and we actually use math. But let's not get too focused on just algorithms. My advice for anyone new into data science is always to work on projects where you can do everything from N10, from the initial data collection prep, algorithms, all the way to presenting results back. Because I think understanding that full life cycle is an important part of data science.",
      "platforms": {
        "tiktok": {
          "video_id": "7178579859399101738",
          "url": "https://www.tiktok.com/@rajistics/video/7178579859399101738",
          "view_count": 1671,
          "upload_date": "2022-12-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9254820459b041b19330988a2a0b4d71_1671393385~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=pARCaVwwMBE3kkMbe8VM4Lkauoc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6260,
      "title": "Code not working? start with the documented examples #datascience#rstats #machinelearning #codetok #python",
      "description": "Code not working? start with the documented examples #datascience#rstats #machinelearning #codetok #python",
      "upload_date": "2022-09-03",
      "total_views": 1660,
      "max_views": 1162,
      "topics": [
        "better",
        "code",
        "codetok",
        "cohere",
        "datascience",
        "like",
        "llms",
        "machinelearning",
        "model",
        "models",
        "multi",
        "python",
        "rise",
        "rstats",
        "step",
        "tools",
        "use"
      ],
      "search_text": "Code not working? start with the documented examples #datascience#rstats #machinelearning #codetok #python better code codetok cohere datascience like llms machinelearning model models multi python rise rstats step tools use",
      "platforms": {
        "tiktok": {
          "video_id": "7139178324860931370",
          "url": "https://www.tiktok.com/@rajistics/video/7139178324860931370",
          "view_count": 1162,
          "upload_date": "2022-09-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18047215819723427",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0TNxaGQ0J34",
          "url": "https://www.youtube.com/watch?v=0TNxaGQ0J34",
          "view_count": 498,
          "upload_date": "2024-04-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6195,
      "title": "Gen AI for Recommenders - Roundup of some recent research from: Eugene Yan: https://eugeneyan.com/writing/recsys-llm/ Jean-Michel Daigna: https://www.the-odd-dataguy.com/2024/11/25/recsys-24/",
      "description": "Gen AI for Recommenders - Roundup of some recent research from: Eugene Yan: https://eugeneyan.com/writing/recsys-llm/ Jean-Michel Daigna: https://www.the-odd-dataguy.com/2024/11/25/recsys-24/",
      "upload_date": "2025-03-19",
      "total_views": 1651,
      "max_views": 1651,
      "topics": [
        "help",
        "improve",
        "information",
        "search",
        "use",
        "used"
      ],
      "search_text": "Gen AI for Recommenders - Roundup of some recent research from: Eugene Yan: https://eugeneyan.com/writing/recsys-llm/ Jean-Michel Daigna: https://www.the-odd-dataguy.com/2024/11/25/recsys-24/ help improve information search use used Five real stories for how AI is being used to improve recommendation systems. I'll keep it interesting. Subbing had lots of webpages that didn't have very good descriptions, which made it really hard for them to do search. So what they decided to use instead of just extracting information out of those webpages, they used GPT-4 to create rich, detailed descriptions for all those webpages. When they used that inside their recommendation system, performance went up. So for Best Buy, they have lots and lots of products, and some of those rare products just weren't getting really searched for or recommended. So what they decided to use was they used a Lama 13B model, and they imagined how real customers might search for these products. And so they enriched everything with product title, category specifications, and they used all that new generated data to add more information to their products. When they tested this and helped the conversion rates, it became much better, made some money for them. Now, for Indeed, they're trying to match jobs to job seekers. Having humans do that way too hard. They figured out that GPT-4 could help label some data to help figure out which jobs match which job seekers. They had humans double check it. The first attempt was, hey, let's fine tune a 3.5 model to do it. Work great. But it was too expensive to run all the time. So instead what they did was use a smaller, cheaper classifier. They called eBadMatch, and all they did was train it just to look for poor job matches. And they used that to just filter out the irrelevant stuff that helped everybody get a little bit happier with their recommendations. Now Amazon has a music area where they want to recommend good playlists. Now having a human curate and do all of that, unrealistic. But what they did was, they could use human reviewers to help train a model, what we call an LLM judge. And that judge could evaluate thousands of search results tirelessly. And that's what they ended up using to help improve their systems. So Yelp was trying to improve their search process. They noticed that people search things in different ways. Some might say, great pizza, while others say delicious Italian food. This is when they decided, hey, let's use it. Use an LLM to generate all the different ways that somebody might express an idea. And then when somebody's searching and looking at the relevant parts of the review, we'll use that information to help figure out what exactly that they're looking for. This helped them improve their click-through rates by using all this information. So there you go. Five ways we're using generative AI to improve recommenders.",
      "platforms": {
        "tiktok": {
          "video_id": "7483663430302305566",
          "url": "https://www.tiktok.com/@rajistics/video/7483663430302305566",
          "view_count": 1651,
          "upload_date": "2025-03-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oYU9bAo5ZuV8thlFAANARD0BEXWQivBEBAwiI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=5mw2UmEvYVH1eX32GmZwTq5yYg8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17905228107130165",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6454,
      "title": "New state of the art embedding model, Instructor, for text is available! It accounts for task and domain when creating an mending. #datascience #machinelearning #embeddings #word2vec #sentencetransformers #huggingface",
      "description": "New state of the art embedding model, Instructor, for text is available! It accounts for task and domain when creating an mending. #datascience #machinelearning #embeddings #word2vec #sentencetransformers #huggingface",
      "upload_date": "2023-01-22",
      "total_views": 1650,
      "max_views": 1411,
      "topics": [
        "datascience",
        "embeddings",
        "huggingface",
        "machinelearning",
        "sentencetransformers",
        "word2vec"
      ],
      "search_text": "New state of the art embedding model, Instructor, for text is available! It accounts for task and domain when creating an mending. #datascience #machinelearning #embeddings #word2vec #sentencetransformers #huggingface datascience embeddings huggingface machinelearning sentencetransformers word2vec",
      "platforms": {
        "instagram": {
          "video_id": "CnupIIqBZtY",
          "url": "https://www.instagram.com/p/CnupIIqBZtY/",
          "view_count": 239,
          "upload_date": "2023-01-22",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "At8JGjxEAcE",
          "url": "https://www.youtube.com/watch?v=At8JGjxEAcE",
          "view_count": 1411,
          "upload_date": "2023-01-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "ABCs of Generative AI: Anything But Chatbots -- There is so much value with generative AI, don't get trapped into just building chatbots. Nowadays, lots of vendors have chatbot solutions, it’s not hard to build a RAG solution. Instead, focus on using generative AI to improve business processes, build automation, and proactively make life easier for folks. (Credit for ABCs catchphrase is to an AWS Executive that said this at the recent Citi Generative AI conference)",
      "description": "ABCs of Generative AI: Anything But Chatbots -- There is so much value with generative AI, don't get trapped into just building chatbots. Nowadays, lots of vendors have chatbot solutions, it’s not hard to build a RAG solution. Instead, focus on using generative AI to improve business processes, build automation, and proactively make life easier for folks. (Credit for ABCs catchphrase is to an AWS Executive that said this at the recent Citi Generative AI conference)",
      "upload_date": "2024-07-13",
      "total_views": 1642,
      "max_views": 1642,
      "topics": [
        "abcs",
        "anything",
        "build",
        "chat",
        "generative",
        "rag"
      ],
      "search_text": "ABCs of Generative AI: Anything But Chatbots -- There is so much value with generative AI, don't get trapped into just building chatbots. Nowadays, lots of vendors have chatbot solutions, it’s not hard to build a RAG solution. Instead, focus on using generative AI to improve business processes, build automation, and proactively make life easier for folks. (Credit for ABCs catchphrase is to an AWS Executive that said this at the recent Citi Generative AI conference) abcs anything build chat generative rag Hey, did you all hear? We have an update coming up to our non-negotiables. I hear it has to do with the ABCs. I remember when anything but clustering rule was added, we found that data scientists were spending a lot of time trying out different types of clustering algorithms. What's so bad about that? I did a social media study where I used clustering algorithms to cluster different types of content and found groups along content similarity. Your approach and results make sense to me. So it turns out the business actually cared about engaging content. I didn't include things like like, repost shares, so that wasn't in my analysis. So the resulting clusters weren't very useful. This is why we emphasize spending a lot of time talking to the business and whenever possible, trying to turn problems into supervised learning. Here comes our execs. Extending the ABCs to generative AI projects. This means anything but chat. All chat projects will move to a buy over a build preference. I spent the last two months building a rag chat bot. Don't think of it as an absolute waste of time. You did learn some data science fundamentals. Yeah, like you know all that VS lookup stuff in Excel that we just do in pandas easily. Come on, we all know rag chat bots at this point are pretty formulating. There's much better data science problems to work on around here. You know what really gets me? It's those execs that wanted those chat bots in the first place. If anything, we shouldn't let execs leave use cases. Let's add that to the list of non-negotiables.",
      "platforms": {
        "tiktok": {
          "video_id": "7390936076585798958",
          "url": "https://www.tiktok.com/@rajistics/video/7390936076585798958",
          "view_count": 1642,
          "upload_date": "2024-07-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/de24855f9b6c4aa5852ea9b7b5e7d6d1_1720836415~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=hicEjlpKrRU1QqGp5O6nblO0cNU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6308,
      "title": "Dtreeviz 2.0 - Visualizing Decision Trees",
      "description": "Dtreeviz 2.0 - Visualizing Decision Trees",
      "upload_date": "2022-12-28",
      "total_views": 1637,
      "max_views": 1637,
      "topics": [
        "datascience",
        "decision",
        "decisiontree",
        "decisiontrees",
        "dtreeviz",
        "machinelearning",
        "see",
        "trees",
        "visualizing"
      ],
      "search_text": "Dtreeviz 2.0 - Visualizing Decision Trees datascience decision decisiontree decisiontrees dtreeviz machinelearning see trees visualizing",
      "platforms": {
        "instagram": {
          "video_id": "17859855575856088",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-12-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EFeLpV_96YQ",
          "url": "https://youtube.com/shorts/EFeLpV_96YQ",
          "view_count": 1637,
          "upload_date": "2022-12-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6075,
      "title": "Pay attention to software licenses. Nothing like throwing away work because people didn’t pay attention to the licenses. This happens all the time. ",
      "description": "Pay attention to software licenses. Nothing like throwing away work because people didn’t pay attention to the licenses. This happens all the time. ",
      "upload_date": "2025-01-16",
      "total_views": 1636,
      "max_views": 1636,
      "topics": [
        "attention",
        "away",
        "licenses",
        "like",
        "make",
        "pay",
        "software"
      ],
      "search_text": "Pay attention to software licenses. Nothing like throwing away work because people didn’t pay attention to the licenses. This happens all the time.  attention away licenses like make pay software Today we have a guest speaker from corporate legal that's going to talk to us about software licensing. I'm double booked so I'm going to have to make this quick, but my main takeaway is please don't use any other software that's external to our company. Doing so opens us up to a variety of risks and liabilities, but you realize the way our team works is we often build on existing software. In fact, that's how most software develop is done. Okay, well we have approved Apache and MIT license so you can use any of that software, but please stay away from things like the GPL. Okay, that's fair. A lot of software is licensed that way. Also make sure to check your dependencies. We had some issues with the profit software package because it had a dependency that required the GPL. What about newer software licenses like Rail that just makes your AIs used responsibly? I don't know about that license, but I'd be happy to review it. The general takeaway is that we want to be very careful with any licenses that requires to do any type of activity. That opens us up to lots of liabilities especially because this is a very large company. We have offices all across the world. There's a lot of different legal domains. Anything that you add that brings that in adds a high level of complexity. Our typical approach is to avoid using those whenever possible.",
      "platforms": {
        "tiktok": {
          "video_id": "7460659950855998750",
          "url": "https://www.tiktok.com/@rajistics/video/7460659950855998750",
          "view_count": 1636,
          "upload_date": "2025-01-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owQrJAkyZVCzxEQnaiFfEBAVnlHfa8QQR9D8Eq~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=CmUlCOF4Yx3qUHumYc3eTrGePiU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18085169104560541",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "It works. #datascience #analytics #codetok #statistics #dataanalyst",
      "description": "It works. #datascience #analytics #codetok #statistics #dataanalyst",
      "upload_date": "2022-05-13",
      "total_views": 1625,
      "max_views": 1625,
      "topics": [
        "analytics",
        "codetok",
        "dataanalyst",
        "datascience",
        "genius",
        "statistics"
      ],
      "search_text": "It works. #datascience #analytics #codetok #statistics #dataanalyst analytics codetok dataanalyst datascience genius statistics I'm a genius. I'm a fucking genius. Hey mom! I'm a fucking genius!",
      "platforms": {
        "tiktok": {
          "video_id": "7097333105547119915",
          "url": "https://www.tiktok.com/@rajistics/video/7097333105547119915",
          "view_count": 1625,
          "upload_date": "2022-05-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d9cf6f37f46f452f80084e8fe43756e9_1652476638~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=pZfuR6i0rVWH4BQCTP%2F2WY4DHM0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6056,
      "title": "In this video, statistical modeling evolved from manual processes requiring explicit data preparation (scaling, transformations, imputation) to automated approaches using decision trees and random forests. These modern methods automatically handle complex data characteristics like nonlinearity, interactions, and categorical variables without manual feature engineering. Data analysis has shifted from specialized statisticians using tools like SAS to data analysts leveraging automated solutions and software engineers implementing custom code. Modern approaches integrate regularization and advanced tree-based methods, eliminating many time-consuming manual steps that were previously required.",
      "description": "In this video, statistical modeling evolved from manual processes requiring explicit data preparation (scaling, transformations, imputation) to automated approaches using decision trees and random forests. These modern methods automatically handle complex data characteristics like nonlinearity, interactions, and categorical variables without manual feature engineering. Data analysis has shifted from specialized statisticians using tools like SAS to data analysts leveraging automated solutions and software engineers implementing custom code. Modern approaches integrate regularization and advanced tree-based methods, eliminating many time-consuming manual steps that were previously required.",
      "upload_date": "2025-02-19",
      "total_views": 1621,
      "max_views": 1547,
      "topics": [
        "approaches",
        "automated",
        "data",
        "decision",
        "llms",
        "manual",
        "models",
        "modern",
        "power",
        "replace",
        "services",
        "specialized",
        "statisticians",
        "using"
      ],
      "search_text": "In this video, statistical modeling evolved from manual processes requiring explicit data preparation (scaling, transformations, imputation) to automated approaches using decision trees and random forests. These modern methods automatically handle complex data characteristics like nonlinearity, interactions, and categorical variables without manual feature engineering. Data analysis has shifted from specialized statisticians using tools like SAS to data analysts leveraging automated solutions and software engineers implementing custom code. Modern approaches integrate regularization and advanced tree-based methods, eliminating many time-consuming manual steps that were previously required. approaches automated data decision llms manual models modern power replace services specialized statisticians using Tell us about the old days. Gather around. In the days before data science, there was a group known as statisticians. They had slide rulers use programming languages called SAS. And it would take them many months in a tedious process to build even one machine learning model. You're making that up. We spend an afternoon to do that. Model building was slow. If you had missing values, you had to go find and do imputations. All of your data had to be scaled, transformed before you could use it. If you wanted to capture interactions, it was on you to develop those interaction features. Categoricals were hated so much they were called dummies. Oh, no, you don't. Outliers were so feared they were banished. Sounds like the Dark Ages. Why couldn't they work with data the way we do? Yes, my friends. We live in a golden age with decision trees and random forests. They give us great accuracy that help us identify the best possible varia, handle nonlinear data, deal with interactions automatically, allow for regularization. This is the age of data science. There was a time before data scientists? Yes, for a long time, statisticians built models with their slide rulers and their paper and pencil. But then a computer scientist taught them a language known as FORTRAN. And with that, some statisticians started building the first decision trees. Was that the last millennium? How about last century? But in the 2010s, we had the rise of data scientists. Who here knows the definition of a data scientist? Yes, the gods in their infinite wisdom to test people took people that were not good enough to be statisticians and people that were not good enough to be computer jobs and to humble the rest of humanity gave this group high-paying jobs. And who knows what became of the data scientist? I'll take that. Data scientists never accomplished anything useful. They went the way of webmasters. And today, any of those data science tasks are done by data analysts using automated tools or software engineers where they need to code something out.",
      "platforms": {
        "tiktok": {
          "video_id": "7473264066224590110",
          "url": "https://www.tiktok.com/@rajistics/video/7473264066224590110",
          "view_count": 1547,
          "upload_date": "2025-02-19",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/okCRDEAZEpnAsAEjIC2ArVFA4tyKXEflfAAX41~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=1WLy%2BouktkUFdr0J%2F5gElbD22cs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18081241141716936",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "P0ZI-Ug26WM",
          "url": "https://www.youtube.com/watch?v=P0ZI-Ug26WM",
          "view_count": 74,
          "upload_date": "2025-02-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5999,
      "title": "Model monitoring and population stability index ",
      "description": "Model monitoring and population stability index ",
      "upload_date": "2025-06-04",
      "total_views": 1601,
      "max_views": 1601,
      "topics": [
        "cake",
        "change",
        "index",
        "population",
        "stability",
        "think"
      ],
      "search_text": "Model monitoring and population stability index  cake change index population stability think You really think your loans are worth $300 million? Absolutely. They've been vetted by our predictive models, but haven't you been changing your standards for a loan? I mean, my dog got a credit card application from your bank. Our standards haven't changed. Those loans are rock solid. I'm looking at your data, and clearly there's some changes over the last year. How could we quantify this? You almost have heard about the J divergence. It's a measure developed by Lin based on ideas from Jeffries dating back to 1946. It's similar to what you might call the Jensen Shanahan divergence. Can you translate that to everyday lingo? Think of it like baking a cake. If the ingredients change over time, your cake might end up tasting different. So what we want to do is compare the ingredients we had from the beginning to what we're changing, and we're going to compare those differences. Do I get a human understandable score out of this? Our scores are then going to tell us, hey, is this cake basically the same thing? Or maybe, hey, there's a little bit of a change? Or wait a minute, there's been a big change. It's a whole new cake. Brilliant. Let's call it something simple. Population stability index? It feels like you're erasing the contributions of statisticians for machine learning. Again. Why did I just now learn about this population stability index? Shouldn't you be running that on all of your models on a regular basis?",
      "platforms": {
        "tiktok": {
          "video_id": "7512100041985756447",
          "url": "https://www.tiktok.com/@rajistics/video/7512100041985756447",
          "view_count": 1601,
          "upload_date": "2025-06-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogBIKEnIBBGuCBVuAif0A9AAwin9A0iC12xI6A~tplv-tiktokx-origin.image?dr=9636&x-expires=1767319200&x-signature=egmq5xfuSgIT8XfRM9NhuGmGJ9w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18190325251315029",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday ",
      "description": "#onthisday ",
      "upload_date": "2025-04-21",
      "total_views": 1595,
      "max_views": 1595,
      "topics": [
        "know",
        "like",
        "onthisday",
        "said",
        "time",
        "years"
      ],
      "search_text": "#onthisday  know like onthisday said time years But at the same time, like I said, in me, you know, I'm 56 years old. Damn!",
      "platforms": {
        "tiktok": {
          "video_id": "7495799982671301918",
          "url": "https://www.tiktok.com/@rajistics/video/7495799982671301918",
          "view_count": 1595,
          "upload_date": "2025-04-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owamorAp8ECFD9foAbAoIbIGf5EgZBEVQDnRBF~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=gleHqCZjNs6NJTO%2B06NXA9Ip05I%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @ereb0s_rl #datascience #analytics #techtok #rstats #kaggle #fastair #machinelearning",
      "description": "Reply to @ereb0s_rl #datascience #analytics #techtok #rstats #kaggle #fastair #machinelearning",
      "upload_date": "2022-04-24",
      "total_views": 1594,
      "max_views": 1594,
      "topics": [
        "analytics",
        "datascience",
        "fastair",
        "kaggle",
        "rstats",
        "techtok"
      ],
      "search_text": "Reply to @ereb0s_rl #datascience #analytics #techtok #rstats #kaggle #fastair #machinelearning analytics datascience fastair kaggle rstats techtok Let's talk about three ways you can get support on your journey in data science from the community. The first approach is get help from somebody that's experienced in data science. This might mean paying them or just making them dinner. The next approach is some type of structured class. It could be something from your local university or it could be an online course. For example, the fast AI courses are very popular and people take those online all the time. There are a lot of tech communities out there. I know this person had an issue with Kaggle, I've thought it's okay, but I know in the past there's been communities that I've tried to be with that were just totally mean to me. One of the things early in my career, I found the sports analytic community and the R community very welcoming and I learned a lot and gained a lot by spending time in that community. So look around if somebody's mean or rude to you, find something else. There's plenty of great communities out there to learn from.",
      "platforms": {
        "tiktok": {
          "video_id": "7090266999322053934",
          "url": "https://www.tiktok.com/@rajistics/video/7090266999322053934",
          "view_count": 1594,
          "upload_date": "2022-04-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/79f72bb334374b55a637ac68bd9adb19_1650831431~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=PSHQ7K1udNgRsMII0OknLAhUoCk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at 🤗 hugging face spaces by Ella2323 called Positive Reframing.  #machinelearning #datascience #codetok ",
      "description": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at 🤗 hugging face spaces by Ella2323 called Positive Reframing.  #machinelearning #datascience #codetok ",
      "upload_date": "2022-10-14",
      "total_views": 1593,
      "max_views": 1593,
      "topics": [
        "better",
        "codetok",
        "datascience",
        "feel",
        "machinelearning",
        "model"
      ],
      "search_text": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at 🤗 hugging face spaces by Ella2323 called Positive Reframing.  #machinelearning #datascience #codetok  better codetok datascience feel machinelearning model Half day, been there. Let me show you an AI model that's helping. This model takes your thoughts and reframes them positively. So here I shared one of my thoughts. The AI model can then take that thought I had, reframe it a lot of different ways. Take a look at these different ones. They make me feel better. I really need this in my life. Check out the author's paper and code and let's get this into more apps.",
      "platforms": {
        "tiktok": {
          "video_id": "7154452513050168619",
          "url": "https://www.tiktok.com/@rajistics/video/7154452513050168619",
          "view_count": 1593,
          "upload_date": "2022-10-14",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3240f0c89425452ab345b2a77f986891_1665775819~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=3uUp0SXaxAORkU2e8t%2F3qGJZUyg%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Video coming on Text Generational in Colab",
      "description": "Video coming on Text Generational in Colab",
      "upload_date": "2023-07-25",
      "total_views": 1583,
      "max_views": 1583,
      "topics": [
        "colab",
        "coming",
        "generational",
        "text",
        "video"
      ],
      "search_text": "Video coming on Text Generational in Colab colab coming generational text video",
      "platforms": {
        "tiktok": {
          "video_id": "7259557074152115502",
          "url": "https://www.tiktok.com/@rajistics/video/7259557074152115502",
          "view_count": 1583,
          "upload_date": "2023-07-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Data data data. ",
      "description": "Data data data. ",
      "upload_date": "2024-08-01",
      "total_views": 1573,
      "max_views": 1573,
      "topics": [
        "collected",
        "data",
        "example",
        "might",
        "think",
        "thinking"
      ],
      "search_text": "Data data data.  collected data example might think thinking inanimate or abstract to you. If so, you're not thinking about data the right way. Let me show you how data scientists think about data. By thinking about how they got the data, how was it collected? Was this, for example, just responses from customers who are already happy? In that case, that's going to be biased. What data elements were collected, but also importantly, what might have not been collected? A great example I'd like to use of this is how, in the US census forms, for a long time, if you grew up in Egypt, you didn't have a really a spot to fill out on the forms. The world is constantly changing over time. Data from five, 10, 15 years ago might not be so relevant, depending on the question that you're answering. I want you to just think about data as something that's rich and full of history and start to probe and understand exactly how and why it landed on your desk.",
      "platforms": {
        "tiktok": {
          "video_id": "7398263524520185131",
          "url": "https://www.tiktok.com/@rajistics/video/7398263524520185131",
          "view_count": 1573,
          "upload_date": "2024-08-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9382e706cb1845268a3f9bf8bec93936_1722542465~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=J8fmNe3hKPsZ8WGYgT56s0VzGoI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Recent research on model compression with quanitization from Neural Magic, go check it out We Ran Over Half a Million Evaluations on Quantized LLMs: Here's What We Found - https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/",
      "description": "Recent research on model compression with quanitization from Neural Magic, go check it out We Ran Over Half a Million Evaluations on Quantized LLMs: Here's What We Found - https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/",
      "upload_date": "2024-10-21",
      "total_views": 1572,
      "max_views": 1572,
      "topics": [
        "half",
        "magic",
        "model",
        "models",
        "neural",
        "squeeze"
      ],
      "search_text": "Recent research on model compression with quanitization from Neural Magic, go check it out We Ran Over Half a Million Evaluations on Quantized LLMs: Here's What We Found - https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/ half magic model models neural squeeze Look at these images. As we compress the information, we start to lose information, lose signal. But at what point does it matter? Well, recent research by Neural Magic shined light on this for LMS, for how much we could squeeze them while still maintaining accuracy. For language models, the compression happens through quantization, where we take longer numbers, floating point 32, compress them down to a smaller, tighter representation of this. In the process, we are losing some of the numerical information. Folks over at Neural Magic did a half a million tests, and what they found is that in some cases, you can squeeze down the model, but still keep 99% of the accuracy. And you know what? That squeeze down model runs a lot faster. They evaluated this on academic benchmarks, of course, but they also included real-world tests, and they also used traditional text similarity metrics to see what the differences were. There's lots of easy-to-use tools that allow you to apply quantization. Some of the providers even allow you to try quantized models, so you can try out different versions for yourself as well. One of the biggest insights here is how we can squeeze so much out of these existing models while still retaining their accuracy. So just imagine if we could fill these models all with really informative stuff. How much more capable will they be?",
      "platforms": {
        "tiktok": {
          "video_id": "7428055120438447406",
          "url": "https://www.tiktok.com/@rajistics/video/7428055120438447406",
          "view_count": 1572,
          "upload_date": "2024-10-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f0b0b35b12ba42b3a4c46d8d6eba5cb5_1729478863~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=Q1KDMA8gUy2vgZkJsjWSwmaa1xA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "And even better when they submit an issue #datascience #codetok #opensource",
      "description": "And even better when they submit an issue #datascience #codetok #opensource",
      "upload_date": "2022-08-17",
      "total_views": 1572,
      "max_views": 1572,
      "topics": [
        "better",
        "burn",
        "codetok",
        "datascience",
        "even",
        "opensource"
      ],
      "search_text": "And even better when they submit an issue #datascience #codetok #opensource better burn codetok datascience even opensource Burn it. What? Did I say stand there and look stupid? No, I said burn it.",
      "platforms": {
        "tiktok": {
          "video_id": "7132650291349982510",
          "url": "https://www.tiktok.com/@rajistics/video/7132650291349982510",
          "view_count": 1572,
          "upload_date": "2022-08-17",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b583bf5b609c41bf84fe122109917add_1660699562~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=RLswNpiREK54vzoTwNauyAU%2FnLI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6192,
      "title": "Credit for the tips to Evan Peck, who pulls from a visualization by John Burn-Murdoch in the Financial Times:  https://www.ft.com/content/73a1836d-0faa-4c84-b973-554e2ca3a227",
      "description": "Credit for the tips to Evan Peck, who pulls from a visualization by John Burn-Murdoch in the Financial Times:  https://www.ft.com/content/73a1836d-0faa-4c84-b973-554e2ca3a227",
      "upload_date": "2025-03-24",
      "total_views": 1569,
      "max_views": 1569,
      "topics": [
        "don",
        "labels",
        "let",
        "like",
        "tips",
        "visualizations"
      ],
      "search_text": "Credit for the tips to Evan Peck, who pulls from a visualization by John Burn-Murdoch in the Financial Times:  https://www.ft.com/content/73a1836d-0faa-4c84-b973-554e2ca3a227 don labels let like tips visualizations Are you building better than average visualizations? Well, I know what it takes. Let me show you some tips to improve your visualizations. So use your title like a headline. Don't just describe the chart, like number of percent of people doing something. Nah, instead, say what matters here. What's happening with the working class? Lead with your story. Get rid of all those unnecessary labels. You don't need an X axis here that says years when you're showing 1980, 1990 and 2000 and get rid of years like 91, 92 that don't make any sense. Make sure your labels are legible. Don't turn them all crooked either. Clarity is more important than clutter. Don't fall for those grid lines. Lots of defaults gives your grids, but this isn't engineering mechanics. Dial it back. Let your data breathe. Let's ditch the legend. Don't make people play match the color. Instead, here we're just going to label the lines directly. Using the same colors for the text. Much easier to read, way cleaner to look at. Think about your labels. We read left to right. So use that. Guide a story by putting key elements at the beginning on the left. Control the narrative. Be Yoda. And just like that, you've got five tips to level up your visualizations.",
      "platforms": {
        "tiktok": {
          "video_id": "7485528996634053919",
          "url": "https://www.tiktok.com/@rajistics/video/7485528996634053919",
          "view_count": 1569,
          "upload_date": "2025-03-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEBThRfiAJmPziACmA5BAIKui8Ii8EhAI9DAFl~tplv-tiktokx-origin.image?dr=9636&x-expires=1767380400&x-signature=qTOVU79qn8%2ByqMkbJW68RTzZQlU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18087963409583833",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#duet with @Sylar2.5 #parodysong #datascience #codetok ",
      "description": "#duet with @Sylar2.5 #parodysong #datascience #codetok ",
      "upload_date": "2022-08-30",
      "total_views": 1561,
      "max_views": 1561,
      "topics": [
        "codetok",
        "count",
        "datascience",
        "duet",
        "parodysong",
        "sylar2"
      ],
      "search_text": "#duet with @Sylar2.5 #parodysong #datascience #codetok  codetok count datascience duet parodysong sylar2 One, two, five, four I can't Me counts so far Can you count, can you count, can you count now?",
      "platforms": {
        "tiktok": {
          "video_id": "7137729257593408814",
          "url": "https://www.tiktok.com/@rajistics/video/7137729257593408814",
          "view_count": 1561,
          "upload_date": "2022-08-30",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2eb1d7b10d084bafba91721486e618ad_1661882104~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=65A4Vu9yzMX%2BwyEiN5dNdHzRJ0w%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Staying busy and doing a public talk on Generative AI. It will be about 40 minutes so gives me chance to dive into more details and answer questions. Come join us!  Link is http://bit.ly/raj_datahour #webinar #datascience #machinelearning #generativeai #chatgpt ",
      "description": "Staying busy and doing a public talk on Generative AI. It will be about 40 minutes so gives me chance to dive into more details and answer questions. Come join us!  Link is http://bit.ly/raj_datahour #webinar #datascience #machinelearning #generativeai #chatgpt ",
      "upload_date": "2022-12-27",
      "total_views": 1541,
      "max_views": 1541,
      "topics": [
        "chatgpt",
        "datascience",
        "generativeai",
        "machinelearning",
        "staying",
        "webinar"
      ],
      "search_text": "Staying busy and doing a public talk on Generative AI. It will be about 40 minutes so gives me chance to dive into more details and answer questions. Come join us!  Link is http://bit.ly/raj_datahour #webinar #datascience #machinelearning #generativeai #chatgpt  chatgpt datascience generativeai machinelearning staying webinar Do you really want to know?",
      "platforms": {
        "tiktok": {
          "video_id": "7181956090035260715",
          "url": "https://www.tiktok.com/@rajistics/video/7181956090035260715",
          "view_count": 1541,
          "upload_date": "2022-12-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/698ac803db6840b8a5093129f7a68b3b_1672179470~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=p0JkqXUerj1Oj81MFY5p%2FQL%2BD58%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6658,
      "title": "Segment Anything (SAM) is a new segmentation model from Meta. It's a huge improvement over the state of the art and is going to change computer vision. Check it out at: https://segment-anything.com/ #datascience #machinelearning #computervision #imagesegmentation #segmentanything #meta See me this month at the: AI Summit in Montreal on April 20 & Arize AI event on April 25",
      "description": "Segment Anything (SAM) is a new segmentation model from Meta. It's a huge improvement over the state of the art and is going to change computer vision. Check it out at: https://segment-anything.com/ #datascience #machinelearning #computervision #imagesegmentation #segmentanything #meta See me this month at the: AI Summit in Montreal on April 20 & Arize AI event on April 25",
      "upload_date": "2023-04-06",
      "total_views": 1536,
      "max_views": 1536,
      "topics": [
        "computervision",
        "datascience",
        "imagesegmentation",
        "machinelearning",
        "meta",
        "segmentanything"
      ],
      "search_text": "Segment Anything (SAM) is a new segmentation model from Meta. It's a huge improvement over the state of the art and is going to change computer vision. Check it out at: https://segment-anything.com/ #datascience #machinelearning #computervision #imagesegmentation #segmentanything #meta See me this month at the: AI Summit in Montreal on April 20 & Arize AI event on April 25 computervision datascience imagesegmentation machinelearning meta segmentanything Meta just released a new image model and it's going to turbo charge computer vision. It's called Segment Anything and it does amazingly well at segmentation or identifying objects and images. Even objects it hasn't seen before. Now, this is useful because the first step in any image analysis is finding the objects whether you're looking for organs inside the body or looking for street signs on the road. For a long time, this type of segmentation was hard to do and often required a costly labeling to do and with Segment Anything, we're going to be able to reduce that labeling. The model consist of an imaging coder and a prompting coder but has been designed to run extremely fast. A key part of the success of this model is having a much larger data set than previous models and they got this larger dataset by using human feedback in the annotation loop where the annotators would annotate some images. The models would run then they would feed that back. Go check out the demo, it's amazing. The paper, the code, all that stuff is available",
      "platforms": {
        "instagram": {
          "video_id": "CqrdOVIAQCo",
          "url": "https://www.instagram.com/reel/CqrdOVIAQCo",
          "view_count": 1536,
          "upload_date": "2023-04-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @joshhenny it’s great time to learning about #largelanguagemodels or #stablediffusion #datascience #machinelearning ",
      "description": "Replying to @joshhenny it’s great time to learning about #largelanguagemodels or #stablediffusion #datascience #machinelearning ",
      "upload_date": "2022-11-14",
      "total_views": 1532,
      "max_views": 1532,
      "topics": [
        "datascience",
        "going",
        "largelanguagemodels",
        "machinelearning",
        "stablediffusion",
        "time"
      ],
      "search_text": "Replying to @joshhenny it’s great time to learning about #largelanguagemodels or #stablediffusion #datascience #machinelearning  datascience going largelanguagemodels machinelearning stablediffusion time These questions all the time and my advice is if you're into it, jump in now. We're in the middle of a hyper speed learning cycle for technologies like stable diffusion and large language models. This isn't going to last forever, but this is a time to jump in and if you can do well and start learning this, you're going to be super valuable. My advice is not to focus on the research papers, that's still emerging, instead figure out how to use these technologies. Being able to understand the practical considerations is going to be really valuable, because that's what companies are going to be looking for. So that's it. Dive in, figure it out and think how it would be useful for someone.",
      "platforms": {
        "tiktok": {
          "video_id": "7166002789796236590",
          "url": "https://www.tiktok.com/@rajistics/video/7166002789796236590",
          "view_count": 1532,
          "upload_date": "2022-11-14",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ff58ee35bbbd4d14bdafad8722e80c5e_1668465051~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=20Bw4KQi%2FYu9NU3N%2FnhFqzqtE%2F8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Regression to the mean with the Madden Curse and Sports Illustrated Jinx #datascience #analytics #stats #maddencurse #sijinx #regression",
      "description": "Regression to the mean with the Madden Curse and Sports Illustrated Jinx #datascience #analytics #stats #maddencurse #sijinx #regression",
      "upload_date": "2022-01-20",
      "total_views": 1527,
      "max_views": 1527,
      "topics": [
        "analytics",
        "datascience",
        "maddencurse",
        "regression",
        "sijinx",
        "stats"
      ],
      "search_text": "Regression to the mean with the Madden Curse and Sports Illustrated Jinx #datascience #analytics #stats #maddencurse #sijinx #regression analytics datascience maddencurse regression sijinx stats Would you like to be on the cover of a magazine or a video game? What would happen if I told you you'd actually perform worse if you made it? Of the 22 players that have made the cover of Madden? 16 of them had worse seasons after that. A lot of athletes have had worse seasons after being on the cover of Sports Illustrated. This happens because after extreme events, it's natural for things to come back down to the mean. If you're getting on the cover of Madden, you probably had an above average season. And you know what? The following season isn't probably going to be above average. And that's just statistics.",
      "platforms": {
        "tiktok": {
          "video_id": "7055438051602025775",
          "url": "https://www.tiktok.com/@rajistics/video/7055438051602025775",
          "view_count": 1527,
          "upload_date": "2022-01-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/823d7a63e3074a9481acf4fcde9ec070_1642722185~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=IBFi5v76mB6RSSXxbn0STxrB94M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Datasets have worldviews from Google PAIR, link in comments, #datascience #bias #machinelearning #ethics #pair-google #statistics",
      "description": "Datasets have worldviews from Google PAIR, link in comments, #datascience #bias #machinelearning #ethics #pair-google #statistics",
      "upload_date": "2022-02-09",
      "total_views": 1519,
      "max_views": 1519,
      "topics": [
        "bias",
        "datascience",
        "ethics",
        "machinelearning",
        "pair",
        "statistics"
      ],
      "search_text": "Datasets have worldviews from Google PAIR, link in comments, #datascience #bias #machinelearning #ethics #pair-google #statistics bias datascience ethics machinelearning pair statistics Here's a quick tip for how you should think about the predictive performance of a machine learning model. Let's start by building a classifier. In this case, we're going to use these objects and let's classify them into a traditional task. Whether we want to interview somebody or not, let's take a look at the overall predictive performance of this model. But a deeper look at our data shows that not all the objects are treated the same. In this case, triangles are treated way worse than the other objects for interviews. The lesson here is just don't take the overall model's performance at face value.",
      "platforms": {
        "tiktok": {
          "video_id": "7062821652215926062",
          "url": "https://www.tiktok.com/@rajistics/video/7062821652215926062",
          "view_count": 1519,
          "upload_date": "2022-02-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6d595e27c6114c589dbb9b0e0b9b9fab_1644441314~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=RUEVVpkErkovt4jvuIBprjEOOPI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "After simple baselines, Anomaly detection is hard.  ",
      "description": "After simple baselines, Anomaly detection is hard.  ",
      "upload_date": "2024-09-26",
      "total_views": 1517,
      "max_views": 1517,
      "topics": [
        "algorithm",
        "algorithms",
        "anomaly",
        "better",
        "data",
        "detection"
      ],
      "search_text": "After simple baselines, Anomaly detection is hard.   algorithm algorithms anomaly better data detection Want to hear the best algorithm for anomaly detection? Gotcha! There isn't one. Let's dig into a study that found none of the anomaly detection algorithms is statistically better than the others. An anomaly detection is about finding unusual patterns in the dataset. And this can come in handy for things like fraud, where people are always trying new techniques, or predictive maintenance, where your car starts acting wacko. This study evaluated over 30 different algorithms on 57 different data sets. And what they found out was none of them stood out. But that doesn't mean all is lost. Here's some things you can do. Roll up your sleeves when you're working with anomaly detection algorithms. Each type of algorithm is going to focus on different patterns in the data. Understanding your data and matching the right algorithm is going to give you a better likelihood for success. Get out of the unsupervised world! Just go label some data. Even having a little bit of labeled data in a semi-supervised approach is going to be way better.",
      "platforms": {
        "tiktok": {
          "video_id": "7418931252289490218",
          "url": "https://www.tiktok.com/@rajistics/video/7418931252289490218",
          "view_count": 1517,
          "upload_date": "2024-09-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7054d8d50b1046e28ae9c462e37fba79_1727354553~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=5FFGqxrZwsQ3tOZVipHBcbLqxe0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6218,
      "title": "Investigate before modifying your data ",
      "description": "Investigate before modifying your data ",
      "upload_date": "2024-11-25",
      "total_views": 1496,
      "max_views": 1496,
      "topics": [
        "call",
        "center",
        "data",
        "get",
        "lot",
        "missing"
      ],
      "search_text": "Investigate before modifying your data  call center data get lot missing I did a quick check on the data. There's a lot of missing data. Only 21% of the rows have the crash location that I'm looking for. I spent a lot of time joining a bunch of tables together to get all that data that we have. If there's missing stuff, it's probably due to past migrations that we have. So there's really nothing that can be done. Oh, okay. Well, let me ask my team for advice. Hey, why don't you just drop the rows with the missing data? That worked for me. Didn't you use that approach in the diversity survey and ended up with a lot of unrepresented people in the final results? Bro, you should try out some wicked abutation techniques. That's what I used on the marketing stuff. Worked wonders. Yeah, that's when we ended up with a 16-year-old with a PhD and those 75-year-olds that we thought liked motorcycle racing. I don't know if we want to go with that yet. We don't get a lot of data scientists down in the call center. Most of what we do in the call center, we're heavily regulated. We have to get through so many calls every hour or we'll lose our jobs. And so one of the things we do is field that are optional, like your field around crash location. It's an optional field, so we'll just tab through it and most people won't bother filling it out. That's amazing. Do you think that's random? I wouldn't say it's at random because it really depends on what call center you're at and what exec is running that call center. And what happens is the execs all get shuffled around and the rules just kind of change. It's such a pain. Yeah, this project has really taught me that I should always try to understand what missing data is and that there's no accidents. Everything has a reason.",
      "platforms": {
        "tiktok": {
          "video_id": "7441341559876078894",
          "url": "https://www.tiktok.com/@rajistics/video/7441341559876078894",
          "view_count": 1496,
          "upload_date": "2024-11-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/oQSRDfnJEESSIM9f0BngFBqEIhRPAUtDhDCt7x~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=YFbExggtz4Xvj%2BZLD%2FDgwzBtPTI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18470635171024270",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "GDG DevFest Ukraine, sign up!  #datascience #codetok #huggingface #dallemini #bigscience #devfestforukraine #standwithukraine",
      "description": "GDG DevFest Ukraine, sign up!  #datascience #codetok #huggingface #dallemini #bigscience #devfestforukraine #standwithukraine",
      "upload_date": "2022-06-11",
      "total_views": 1495,
      "max_views": 1495,
      "topics": [
        "bigscience",
        "codetok",
        "dallemini",
        "datascience",
        "devfestforukraine",
        "huggingface"
      ],
      "search_text": "GDG DevFest Ukraine, sign up!  #datascience #codetok #huggingface #dallemini #bigscience #devfestforukraine #standwithukraine bigscience codetok dallemini datascience devfestforukraine huggingface Do you know how they build large language models that they train on billions of words? Or maybe you're more interested in the consequences of these models, how they might be used for hate, as well as some of the cool stuff about how they can actually be used to create new types of images and have lots of fun. I'm going to share my experiences that I've had at Hugging Faced this first month and it touches on all those topics. I'm excited to talk at DevFest for a truly worthy cause. There's an awesome line of speakers for the two-day conference, especially in AI. Take a look at the topics and sign up.",
      "platforms": {
        "tiktok": {
          "video_id": "7107993166577437998",
          "url": "https://www.tiktok.com/@rajistics/video/7107993166577437998",
          "view_count": 1495,
          "upload_date": "2022-06-11",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/77798c0c72b84590827bafa02575c797_1654958626~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=vbOTwIr6tGNxl5I%2Fsw1Bipa9ev0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6029,
      "title": "Population Stability Index for Monitoring Machine Learning Models",
      "description": "Population Stability Index for Monitoring Machine Learning Models",
      "upload_date": "2024-05-30",
      "total_views": 1481,
      "max_views": 1481,
      "topics": [
        "cake",
        "index",
        "learning",
        "machine",
        "mlops",
        "modelmonitoring",
        "monitoring",
        "population",
        "populationstabilityindex",
        "stability"
      ],
      "search_text": "Population Stability Index for Monitoring Machine Learning Models cake index learning machine mlops modelmonitoring monitoring population populationstabilityindex stability",
      "platforms": {
        "instagram": {
          "video_id": "17954922131667418",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "XLiidDU6c_Y",
          "url": "https://www.youtube.com/watch?v=XLiidDU6c_Y",
          "view_count": 1481,
          "upload_date": "2024-05-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I much prefer working through code examples than decoding equations. I can’t be the only one. #datascience #statistics ",
      "description": "I much prefer working through code examples than decoding equations. I can’t be the only one. #datascience #statistics ",
      "upload_date": "2022-09-20",
      "total_views": 1476,
      "max_views": 1476,
      "topics": [
        "code",
        "datascience",
        "much",
        "prefer",
        "statistics",
        "working"
      ],
      "search_text": "I much prefer working through code examples than decoding equations. I can’t be the only one. #datascience #statistics  code datascience much prefer statistics working I did nothing, I was stupid, I was foolish, I was learned to myself",
      "platforms": {
        "tiktok": {
          "video_id": "7145582193568599342",
          "url": "https://www.tiktok.com/@rajistics/video/7145582193568599342",
          "view_count": 1476,
          "upload_date": "2022-09-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3afc0afc64a744d8a0440978641a936a_1663710516~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=%2Fbrq2y8fq2%2FBMUiY1kxovf4tZc0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "How else can you work? #datascience #stackoverflow #codetok",
      "description": "How else can you work? #datascience #stackoverflow #codetok",
      "upload_date": "2022-07-29",
      "total_views": 1471,
      "max_views": 1471,
      "topics": [
        "bottom",
        "codetok",
        "datascience",
        "else",
        "stackoverflow",
        "work"
      ],
      "search_text": "How else can you work? #datascience #stackoverflow #codetok bottom codetok datascience else stackoverflow work Bottom sociales",
      "platforms": {
        "tiktok": {
          "video_id": "7125819351630105902",
          "url": "https://www.tiktok.com/@rajistics/video/7125819351630105902",
          "view_count": 1471,
          "upload_date": "2022-07-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f0e67f6405964cda902552f5b2b11b06_1659109111~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=DCvz0BbV6RE03pZG65vIlXqc9Qk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6184,
      "title": "Recipe for Word and Sentence Embeddings: Word2Vec and now Static Embeddings for word embeddings, and to get more context, use the sentence transformers package for your embeddings. Stop using word2vec and use static embeddings now:  https://github.com/UKPLab/sentence-transformers/releases/tag/v3.2.0",
      "description": "Recipe for Word and Sentence Embeddings: Word2Vec and now Static Embeddings for word embeddings, and to get more context, use the sentence transformers package for your embeddings. Stop using word2vec and use static embeddings now:  https://github.com/UKPLab/sentence-transformers/releases/tag/v3.2.0",
      "upload_date": "2024-10-11",
      "total_views": 1470,
      "max_views": 1363,
      "topics": [
        "embeddings",
        "funny",
        "going",
        "llama",
        "million",
        "one",
        "planning",
        "release",
        "right",
        "sentence",
        "take",
        "transformers",
        "use",
        "well",
        "word",
        "words"
      ],
      "search_text": "Recipe for Word and Sentence Embeddings: Word2Vec and now Static Embeddings for word embeddings, and to get more context, use the sentence transformers package for your embeddings. Stop using word2vec and use static embeddings now:  https://github.com/UKPLab/sentence-transformers/releases/tag/v3.2.0 embeddings funny going llama million one planning release right sentence take transformers use well word words Why do you make everything so complicated? Words are words. I don't need AI to explain a riverbank. Hold on, if you take three words, cold, warm, and freezing, if you use a semantic model, you can see which words are most similar to each other. I'll accept that. Semantics is like synonyms. Your AI models are essentially big dictionaries. Oh boy, it's much more than that. You know, transformers are the biggest advancement in AI and they're all based around learning about content. Ew, transformers. I prefer GLMs. Predictable and transparent. Take a look at these two sentences. You'll see the word bank is used in two different ways. So you're telling me you need context to understand that bank in the first sentence refers to money, while in the second sentence refers to water and rivers. Exactly, now you're getting it. But who would use words in such an inexact and confusing way? But that's not how language works. So when we use a sentence embedding approach, it recognizes you need to understand the entire sentence to get the meaning. So much irrationality. I did try your sentence embeddings approach and it was 500 times slower than using static embeddings. True, but would you rather arrive at the wrong place quickly or at the right place a little bit slower? Most people value accuracy and want to use context. I prefer to be plain spoken. Enough of this. I'm going to go take my bat down to the river bank.",
      "platforms": {
        "tiktok": {
          "video_id": "7424562947998960938",
          "url": "https://www.tiktok.com/@rajistics/video/7424562947998960938",
          "view_count": 1363,
          "upload_date": "2024-10-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2088518d775b44ea8c5cfa66aed0bb9b_1728665778~tplv-tiktokx-origin.image?dr=9636&x-expires=1767409200&x-signature=FdabdEjherE9fZL%2BNDRG5xgD01Q%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18061926715845544",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "s24CPBPrKEE",
          "url": "https://www.youtube.com/watch?v=s24CPBPrKEE",
          "view_count": 107,
          "upload_date": "2025-04-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Facts. We need data. #datascience #statistics #analysis #techtok",
      "description": "Facts. We need data. #datascience #statistics #analysis #techtok",
      "upload_date": "2022-05-01",
      "total_views": 1462,
      "max_views": 1462,
      "topics": [
        "analysis",
        "datascience",
        "flow",
        "statistics",
        "techtok",
        "time"
      ],
      "search_text": "Facts. We need data. #datascience #statistics #analysis #techtok analysis datascience flow statistics techtok time Yeah, no, I would love to go with the flow, but what time does the flow start? Is that a formal start time, or do we show up a half hour later?",
      "platforms": {
        "tiktok": {
          "video_id": "7092754791218122030",
          "url": "https://www.tiktok.com/@rajistics/video/7092754791218122030",
          "view_count": 1462,
          "upload_date": "2022-05-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5c7880b469d8474197f8bfa9762ebf0a_1651410666~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=cCJAjCeIp%2FMrc5pRdhLuxTeG6r0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @minisdlatvia my big tip for learning data science #datascience #machinelearning #analytics #codetok #webapps #gradio #streamlit #python #rstats",
      "description": "Replying to @minisdlatvia my big tip for learning data science #datascience #machinelearning #analytics #codetok #webapps #gradio #streamlit #python #rstats",
      "upload_date": "2022-07-16",
      "total_views": 1458,
      "max_views": 1458,
      "topics": [
        "analytics",
        "data",
        "datascience",
        "machinelearning",
        "things",
        "think"
      ],
      "search_text": "Replying to @minisdlatvia my big tip for learning data science #datascience #machinelearning #analytics #codetok #webapps #gradio #streamlit #python #rstats analytics data datascience machinelearning things think Here's the top thing you can do to break into data science. Build useful web apps. Build. One of the reasons I don't spend a lot of tiktoks on helping intro data scientists is because I think you need to spend time watching less videos, going to less classes, and actually spending time building. If you start to focus on building and actually coding, then of course you'll have some classes, some certifications might come along, but your goal really should be to spend time building, coding things up. Build things that you think are useful. Don't do stuff just to get hired or because you think it's an important skill to have. Do things you're passionate about. When I hire people, I want data scientists that are motivated, that care. I'm not looking for somebody who's completed a list of models or data sets. So find things that you motivate you in life and build projects around that. I'm a big fan of web apps because they force you to think about the entire experience. Like how is somebody supposed to input the data? How do they respond to the model? What type of interface are you doing? And if you step back, building a web app also makes you think about the whole life cycle of data, from how you ingest it to manipulate it to bringing it back out to saving it, all those things and pipelines that you need to know how to do, web apps can often encapsulate nicely.",
      "platforms": {
        "tiktok": {
          "video_id": "7120977034309881134",
          "url": "https://www.tiktok.com/@rajistics/video/7120977034309881134",
          "view_count": 1458,
          "upload_date": "2022-07-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a1c5e65eff2a4111b5360ed52ce56c5a_1657981675~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=s67oG95EBe3VY3JfBFlJSBXWYaI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Curriculum Learning is about ordering your training data. It's another useful technique that you should consider. Some background: Overview: Curriculum Learning: A Survey - https://arxiv.org/pdf/2101.10382 Images: Applying Curriculum Learning to Medical Images: https://towardsdatascience.com/applying-curriculum-learning-to-medical-images-184275c52350 LLMS: Databricks: https://x.com/code_star/status/1799110336404189370 Snowflake: https://medium.com/snowflake/snowflake-arctic-cookbook-series-arctics-approach-to-data-b81a8a0958bd Classic ML:  Data Distribution-based Curriculum Learning -  https://arxiv.org/pdf/2402.07352v1",
      "description": "Curriculum Learning is about ordering your training data. It's another useful technique that you should consider. Some background: Overview: Curriculum Learning: A Survey - https://arxiv.org/pdf/2101.10382 Images: Applying Curriculum Learning to Medical Images: https://towardsdatascience.com/applying-curriculum-learning-to-medical-images-184275c52350 LLMS: Databricks: https://x.com/code_star/status/1799110336404189370 Snowflake: https://medium.com/snowflake/snowflake-arctic-cookbook-series-arctics-approach-to-data-b81a8a0958bd Classic ML:  Data Distribution-based Curriculum Learning -  https://arxiv.org/pdf/2402.07352v1",
      "upload_date": "2024-07-27",
      "total_views": 1447,
      "max_views": 1447,
      "topics": [
        "curriculum",
        "data",
        "images",
        "learning",
        "train",
        "training"
      ],
      "search_text": "Curriculum Learning is about ordering your training data. It's another useful technique that you should consider. Some background: Overview: Curriculum Learning: A Survey - https://arxiv.org/pdf/2101.10382 Images: Applying Curriculum Learning to Medical Images: https://towardsdatascience.com/applying-curriculum-learning-to-medical-images-184275c52350 LLMS: Databricks: https://x.com/code_star/status/1799110336404189370 Snowflake: https://medium.com/snowflake/snowflake-arctic-cookbook-series-arctics-approach-to-data-b81a8a0958bd Classic ML:  Data Distribution-based Curriculum Learning -  https://arxiv.org/pdf/2402.07352v1 curriculum data images learning train training Are you searching for dolphins? Did you know you could train AIs the same way we train dolphins? Come on. Flippers are going to teach us about how to train LLMs. Take a look at these colorectal images. Do I have to? Yeah, what researchers found is that the training works better if they order the images from the easiest to the hardest. Interesting. I guess data isn't fungible after all. Yet any NLP examples? Translation is another example where starting with the easiest and then moving to the harder ends up improving the training performance. OK, it makes sense for translation, but how would you train large language models with it? There's a lot of great examples. At Snowflake, when they train their model, they found by slowly adding more complex data like code improve the training performance. Similarly, when Databricks was training their models, they found that upsampling at the end with high quality data improved performance. Cool. I'll use this when I train large language models, which is never. It's not just LLMs. Recent work has shown that using traditional algorithms like random forest on our traditional data sets that if we order that data, we can get a slight gain in performance. Well, you've schooled me. I guess I need to start thinking about how my training data is ordered. Glad I got you thinking. And oh, don't sit on the sofa. That's my favorite.",
      "platforms": {
        "tiktok": {
          "video_id": "7396331623395953962",
          "url": "https://www.tiktok.com/@rajistics/video/7396331623395953962",
          "view_count": 1447,
          "upload_date": "2024-07-27",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a564de60abcc43579f844bede8303da9_1722092665~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=qBKc04oMH3VlUM81cbn5fQ1PB%2BU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Level set expectations early!  People have unrealistic views. #datascience #dataanalytics #statistics #codetok",
      "description": "Level set expectations early!  People have unrealistic views. #datascience #dataanalytics #statistics #codetok",
      "upload_date": "2022-06-07",
      "total_views": 1447,
      "max_views": 1447,
      "topics": [
        "codetok",
        "dataanalytics",
        "datascience",
        "ding",
        "level",
        "statistics"
      ],
      "search_text": "Level set expectations early!  People have unrealistic views. #datascience #dataanalytics #statistics #codetok codetok dataanalytics datascience ding level statistics Do, ba-ding, ba-ding, ba-do, ba-ding, ba-ding, ba-do.",
      "platforms": {
        "tiktok": {
          "video_id": "7106647665483517227",
          "url": "https://www.tiktok.com/@rajistics/video/7106647665483517227",
          "view_count": 1447,
          "upload_date": "2022-06-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/979d98a7d929476dbb5d9e71691ffbb9_1654645352~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=io8MdwEQwNGXqhhXvOpb5XR1rWo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#insurance #regulation #datascience #statistics #interpretablemodels  #codetok",
      "description": "#insurance #regulation #datascience #statistics #interpretablemodels  #codetok",
      "upload_date": "2022-05-21",
      "total_views": 1447,
      "max_views": 1447,
      "topics": [
        "datascience",
        "insurance",
        "interpretablemodels",
        "models",
        "regulation",
        "statistics"
      ],
      "search_text": "#insurance #regulation #datascience #statistics #interpretablemodels  #codetok datascience insurance interpretablemodels models regulation statistics Can we regulate AI? Let me tell you how we do. Insurance is long been regulated and if you think about it it makes sense because you're buying something for the future it's really easy to scam people. In the United States we make insurance companies for things like home auto your motorcycle clearly state exactly what you're paying for. This means they have to use very interpretable models. Models where it's easy to understand the connection between the predictors and the prediction. Now to do this well you have to understand the data well and folks within these insurance companies spend lots of time analyzing every feature or variable that comes into the model and what it looks like. This is also beneficial to insurance companies because part of what they need to do is rigorously test their models for all kinds of edge conditions. After all these models have billions of dollars on the line so there might be some state-of-the-art fancy black box model that can get better accuracy. For now government insurance companies are willing to have that trade off for those interpretable models that they can understand.",
      "platforms": {
        "tiktok": {
          "video_id": "7100213022215540014",
          "url": "https://www.tiktok.com/@rajistics/video/7100213022215540014",
          "view_count": 1447,
          "upload_date": "2022-05-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0cc6bf71668e4cc7a6deaa4960be06ad_1653147173~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=%2FWpC7rp2JeLk12y3CswPbLY4Axs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "TikTok video #7445674328907123999",
      "description": "TikTok video #7445674328907123999",
      "upload_date": "2024-12-07",
      "total_views": 1446,
      "max_views": 1446,
      "topics": [
        "7445674328907123999",
        "actually",
        "answers",
        "good",
        "models",
        "text"
      ],
      "search_text": "TikTok video #7445674328907123999 7445674328907123999 actually answers good models text ChatGPT has blown up big this week. Let me take some time to explain to you exactly how it works, and this will also let you understand its limitations. Now the way large language models work is they try to predict the next word. And by doing this over and over again, they could start putting together long pieces of text that are readable. A lot of these language models put together readable text, but if we actually start looking at the text, we'll notice that there's subtle errors that actually have a big impact on the meaning of it. So how can we do better? How about giving the model some feedback on what a good answer is? And that's what the team did. They took a bunch of human labors, gave them answers, and then had them say, are these good quality answers? Can you rank how well these answers are? Are these answers actually correct? And once they did this, the model was so much better. The answers were such higher quality. And this is why ChatGPT writes so well. It's been graded by average people, and it now knows how to write above average. Now, where it's gonna fail, of course, is as soon as you get to a specific subject area that our human labors weren't able to understand if that was a good answer or a bad answer because it just looks really complicated. That's the limitation, and my guess is OpenAI is next gonna go after a lot of those different domains by adding more instructions and fine tuning in those areas to boost those models.",
      "platforms": {
        "tiktok": {
          "video_id": "7445674328907123999",
          "url": "https://www.tiktok.com/@rajistics/video/7445674328907123999",
          "view_count": 1446,
          "upload_date": "2024-12-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owDpUXfjDubFnFeE9P1BA9VmEVQEWQxARI5liC~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=M%2BVezNDW9iU4TgaCEJQq7WvYjN0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @zythesciguy Reply to @zythesciguy #datascience #statistics #codetok",
      "description": "Reply to @zythesciguy Reply to @zythesciguy #datascience #statistics #codetok",
      "upload_date": "2022-05-29",
      "total_views": 1445,
      "max_views": 1445,
      "topics": [
        "codetok",
        "datascience",
        "feel",
        "reply",
        "statistics",
        "zythesciguy"
      ],
      "search_text": "Reply to @zythesciguy Reply to @zythesciguy #datascience #statistics #codetok codetok datascience feel reply statistics zythesciguy We've all been here. I feel the pain. At the end of the day, let's just try to get this model into production.",
      "platforms": {
        "tiktok": {
          "video_id": "7102957019807649070",
          "url": "https://www.tiktok.com/@rajistics/video/7102957019807649070",
          "view_count": 1445,
          "upload_date": "2022-05-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b6fd6efc588b477daa3a443280c70bc2_1653786057~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=mhTu8oBA9v7mOZGlmJOqOBQPqf0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6240,
      "title": "Feature Selection Methods for Machine Learning, plus Feature Selection Curves",
      "description": "Feature Selection Methods for Machine Learning, plus Feature Selection Curves",
      "upload_date": "2024-10-13",
      "total_views": 1430,
      "max_views": 1430,
      "topics": [
        "est",
        "feature",
        "fonctionnalit",
        "learning",
        "les",
        "machine",
        "methods",
        "nous",
        "plus",
        "que",
        "selection",
        "vous"
      ],
      "search_text": "Feature Selection Methods for Machine Learning, plus Feature Selection Curves est feature fonctionnalit learning les machine methods nous plus que selection vous",
      "platforms": {
        "instagram": {
          "video_id": "18072856951579547",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "jm7TYGv32zs",
          "url": "https://www.youtube.com/watch?v=jm7TYGv32zs",
          "view_count": 1430,
          "upload_date": "2024-10-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6397,
      "title": "Shapley Values in Machine Learning",
      "description": "Shapley Values in Machine Learning",
      "upload_date": "2022-08-10",
      "total_views": 1424,
      "max_views": 1424,
      "topics": [
        "learning",
        "machine",
        "shapley",
        "values"
      ],
      "search_text": "Shapley Values in Machine Learning learning machine shapley values",
      "platforms": {
        "youtube": {
          "video_id": "DYA5SA0edb0",
          "url": "https://www.youtube.com/watch?v=DYA5SA0edb0",
          "view_count": 1424,
          "upload_date": "2022-08-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6034,
      "title": "Machine Learning Visualizations: My Favorites",
      "description": "Machine Learning Visualizations: My Favorites",
      "upload_date": "2024-05-05",
      "total_views": 1419,
      "max_views": 1419,
      "topics": [
        "examples",
        "favorites",
        "fine",
        "learning",
        "machine",
        "models",
        "performance",
        "prompting",
        "tuning",
        "visualizations"
      ],
      "search_text": "Machine Learning Visualizations: My Favorites examples favorites fine learning machine models performance prompting tuning visualizations",
      "platforms": {
        "instagram": {
          "video_id": "18121696303361699",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "tQpaUlM4PX4",
          "url": "https://www.youtube.com/watch?v=tQpaUlM4PX4",
          "view_count": 1419,
          "upload_date": "2024-05-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "My blog post journey from Jekyll to Quarto.  #blogging #rajistics #jekyll #quarto #posit",
      "description": "My blog post journey from Jekyll to Quarto.  #blogging #rajistics #jekyll #quarto #posit",
      "upload_date": "2024-06-04",
      "total_views": 1419,
      "max_views": 1419,
      "topics": [
        "blog",
        "blogging",
        "jekyll",
        "posit",
        "post",
        "quarto"
      ],
      "search_text": "My blog post journey from Jekyll to Quarto.  #blogging #rajistics #jekyll #quarto #posit blog blogging jekyll posit post quarto Do you like attention? Do you want to be respected? Having your own blog won't help you out, but it does give you a place to pour out your creative energy. Let me share with you why I make a blog and how I do it. Of the data scientists I know, maybe five, maybe 10 percent have personal blogs where they share what they've done in machine learning. Most of the folks that do it because there's no better way to make sure you understand something than to write out a public blog post. The bonus for writing a blog post is you have something that stands across time that helps you remember how you solved a particular problem. It's also useful sometimes for job hunts because you could point to, hey, I've actually talked and written about that topic that you're looking for. I've had a personal blog post for the last eight years. Only a handful of people have probably ever checked it out, but hey, I enjoy doing it. For the first blog I did, I used Jekyll, a static website generator. It was easy, cheap to maintain. I also used a server from Digital Ocean because sometimes I wanted to do more than just HTML. For example, I had an R Shiny server. All you're looking for is just HTML. GitHub Pages is a nice, easy, cheap, free way to go. I recently upgraded to Quarto and it was easy. Just took a couple of nights. I had a couple config files I had to do and my motivation for Quarto was I wanted to share notebooks as well as blog posts and Quarto makes that very easy to do.",
      "platforms": {
        "tiktok": {
          "video_id": "7376443316860128555",
          "url": "https://www.tiktok.com/@rajistics/video/7376443316860128555",
          "view_count": 1419,
          "upload_date": "2024-06-04",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9beed419dcbd42f298f53d7e913f41db_1717462052~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=MzWxTWq0LIjRMWUiZH18nJ6by2E%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Seen this being hashed out on Twitter and had to join #dataengineering #codetok #duckdb  #spark #datascience ",
      "description": "Seen this being hashed out on Twitter and had to join #dataengineering #codetok #duckdb  #spark #datascience ",
      "upload_date": "2022-08-26",
      "total_views": 1417,
      "max_views": 1417,
      "topics": [
        "codetok",
        "dataengineering",
        "datascience",
        "duckdb",
        "seen",
        "spark"
      ],
      "search_text": "Seen this being hashed out on Twitter and had to join #dataengineering #codetok #duckdb  #spark #datascience  codetok dataengineering datascience duckdb seen spark This is intermediate, I'm picking up.",
      "platforms": {
        "tiktok": {
          "video_id": "7136018861555469611",
          "url": "https://www.tiktok.com/@rajistics/video/7136018861555469611",
          "view_count": 1417,
          "upload_date": "2022-08-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2aadd96690b94c42b7279f815922cc9a_1661483868~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=FTMju9VZsCrTlmod6fMxp74UDLk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "🚀 GPUs. ",
      "description": "🚀 GPUs. ",
      "upload_date": "2024-09-20",
      "total_views": 1415,
      "max_views": 1415,
      "topics": [
        "going",
        "keep",
        "less",
        "like",
        "models",
        "things"
      ],
      "search_text": "🚀 GPUs.  going keep less like models things What's driving this AI revolution? Is it a bunch of smart people coming up with new algorithms and techniques? Is it cool models like Transformers? Nah, it's none of these things. It's the GPU. The GPUs had a thousand times improvement over the last 10 years. Let's talk about how they did this and what's next. The first thing is they're using more efficient number representations going from something like floating point 32 all the way down to int 8. This is something you should be familiar with if you work with large language models because we often have to use techniques like this, often with things like quantization to help speed up LMS by taking a little bit less space and memory. The next was complex instructions. I don't get the heads, but they engineered the heck out of this and it takes them a lot less compute and a lot less instructions to do what it previously used to take makes everything faster. Finally, they have some insane processes for building these things. They're down to seven nanometers. I mean, they're like quantum effects in this range. It's amazing what folks have done in building these chips. So to keep improving these, they're going to keep working on better number representations, things like taking log numbers, supporting model sparsity, better engineering around memory access and energy consumption, and then keep getting those chips smaller and smaller. And that's going to give us better and more powerful models.",
      "platforms": {
        "tiktok": {
          "video_id": "7416874435568815403",
          "url": "https://www.tiktok.com/@rajistics/video/7416874435568815403",
          "view_count": 1415,
          "upload_date": "2024-09-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/69f418f958d44b63b4d6dd256ae94114_1726875660~tplv-tiktokx-origin.image?dr=9636&x-expires=1767416400&x-signature=Ieee%2BFYua6YE1%2BAEXExibNPLsF8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6183,
      "title": "Let’s dig in to the differences between baseline models and benchmark datasets. ",
      "description": "Let’s dig in to the differences between baseline models and benchmark datasets. ",
      "upload_date": "2025-04-08",
      "total_views": 1409,
      "max_views": 1409,
      "topics": [
        "baseline",
        "benchmark",
        "data",
        "model",
        "models",
        "simple",
        "use"
      ],
      "search_text": "Let’s dig in to the differences between baseline models and benchmark datasets.  baseline benchmark data model models simple use I got laughed at. I said I used a random benchmark for my marketing model. Yeah, they shouldn't have laughed, but let's clear up the confusion. So a baseline model is a simple model or rule that doesn't take very long to build. And some of the baseline models I use are majority class when I'm doing a classifier, or for a regression problem, I might just use the mean, gives me a nice simple model. A baseline might be a simple rule from the business, like how many hot dogs they sold yesterday might be the baseline for how many they sell today. Lately, we've also been using AutoML as a baseline for tabular problems. Baselines are great for just measuring your improvement. So as you're adding new things to the model, you can see how they're contributing to the improvement of the model. Hey, remember your claims project? I spent three months building a better claims model, but never did a baseline. Three months in, we ran a baseline comparison found we were only getting 1% improvement, which wasn't good. That's how I got this job. What about a benchmark? A benchmark dataset is used to evaluate models. We've taken all of our machine failures from 2020 and created a benchmark dataset. We know our current approaches get about 98% accuracy on that. So we use that benchmark dataset when we compare other competing algorithms, new approaches we see. If they can't beat 98% on that benchmark, we don't worry about it. So academia has a lot of benchmark datasets. It lets them compare different algorithms and approaches. And one of the things that's useful is you can see how the field has progressed over time. This has really helped. I think I'm well above the baseline now.",
      "platforms": {
        "tiktok": {
          "video_id": "7491047346134928671",
          "url": "https://www.tiktok.com/@rajistics/video/7491047346134928671",
          "view_count": 1409,
          "upload_date": "2025-04-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ocMfxxf5AAAKAMQHAIWe1FoM58K8efiQgiSHrQ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767376800&x-signature=s1S5qgsu%2FkniQR8KRmxoLRqE%2BHU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18062861132007958",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-04-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Mmm. Food classification. ",
      "description": "Mmm. Food classification. ",
      "upload_date": "2024-08-04",
      "total_views": 1409,
      "max_views": 1409,
      "topics": [
        "build",
        "data",
        "images",
        "model",
        "much",
        "start"
      ],
      "search_text": "Mmm. Food classification.  build data images model much start You might have seen this, but let's build this instead. It's much tastier. Build AI that classifies these images. We're going to start by grabbing a pre-trained model, Google's Vision Transformer. We'll take our data, that's images and folders, use the data sets package to package all that up, organize that, put that on the Hugging Face Hub, and use it to train the model. Code for configuring our model for training, and then boom, we start training the model. Or later, we've got a model that's 95% accurate. The model's built, now you can start using it for predictions. To do this yourself, grab the Colab notebook that I've linked. Also on the other video site, I have a much longer walkthrough. I spend about 25 minutes walking through this process end to end.",
      "platforms": {
        "tiktok": {
          "video_id": "7399293943881534763",
          "url": "https://www.tiktok.com/@rajistics/video/7399293943881534763",
          "view_count": 1409,
          "upload_date": "2024-08-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d7012d5c716a4dcfbe02b66fcd4771d7_1722782386~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=DtOPk8DnqNIluq34v1QhMBpTKMI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Its always longer than you want to get your data prepped. #datascience #dataengineering #analytics #codetok",
      "description": "Its always longer than you want to get your data prepped. #datascience #dataengineering #analytics #codetok",
      "upload_date": "2022-08-07",
      "total_views": 1402,
      "max_views": 1402,
      "topics": [
        "always",
        "analytics",
        "codetok",
        "dataengineering",
        "datascience",
        "longer"
      ],
      "search_text": "Its always longer than you want to get your data prepped. #datascience #dataengineering #analytics #codetok always analytics codetok dataengineering datascience longer I did nothing, I was stupid, I was foolish, I was learned to myself",
      "platforms": {
        "tiktok": {
          "video_id": "7129177347236039982",
          "url": "https://www.tiktok.com/@rajistics/video/7129177347236039982",
          "view_count": 1402,
          "upload_date": "2022-08-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/758790094bc14c9785c1fd65392dd291_1659890955~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=0QbLiNQgPOByiD7qZx0O%2B7BnUU8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Keep your data science projects page loaded by making open source versions of your work. #datascience #codetok #programming #python",
      "description": "Keep your data science projects page loaded by making open source versions of your work. #datascience #codetok #programming #python",
      "upload_date": "2022-06-03",
      "total_views": 1400,
      "max_views": 1400,
      "topics": [
        "codetok",
        "datascience",
        "going",
        "keep",
        "programming",
        "python"
      ],
      "search_text": "Keep your data science projects page loaded by making open source versions of your work. #datascience #codetok #programming #python codetok datascience going keep programming python I don't think that we should be together. And I've thought about it a little long, and this is what's going to happen. I'm going to keep pursuing what I'm pursuing.",
      "platforms": {
        "tiktok": {
          "video_id": "7105037302363131182",
          "url": "https://www.tiktok.com/@rajistics/video/7105037302363131182",
          "view_count": 1400,
          "upload_date": "2022-06-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/48ebc155c8fd4e238441f9266e704125_1654270410~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=HRNNP8BK5uI0LE4qNjsS1%2FRZq70%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6033,
      "title": "Glitch Tokens in Large Language Models (SolidGoldMagikarp)",
      "description": "Glitch Tokens in Large Language Models (SolidGoldMagikarp)",
      "upload_date": "2024-05-12",
      "total_views": 1397,
      "max_views": 1397,
      "topics": [
        "bfloat16",
        "datascience",
        "floating",
        "glitch",
        "language",
        "large",
        "machinelearning",
        "models",
        "quantization",
        "solidgoldmagikarp",
        "tokens"
      ],
      "search_text": "Glitch Tokens in Large Language Models (SolidGoldMagikarp) bfloat16 datascience floating glitch language large machinelearning models quantization solidgoldmagikarp tokens",
      "platforms": {
        "instagram": {
          "video_id": "18440367559007617",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "6PRgC3eJR7k",
          "url": "https://www.youtube.com/watch?v=6PRgC3eJR7k",
          "view_count": 1397,
          "upload_date": "2024-05-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Evaluation data is so so important when working on machine learning or generative AI projects. Labeling data is an imporant task and you can learn a lot about your problem by doing it. Don't look down on it, it's what the best people in AI do. Example evaluation app: https://llm-eval-demo.streamlit.app/ (I am working on an update to this, stay tuned)",
      "description": "Evaluation data is so so important when working on machine learning or generative AI projects. Labeling data is an imporant task and you can learn a lot about your problem by doing it. Don't look down on it, it's what the best people in AI do. Example evaluation app: https://llm-eval-demo.streamlit.app/ (I am working on an update to this, stay tuned)",
      "upload_date": "2024-08-09",
      "total_views": 1395,
      "max_views": 1395,
      "topics": [
        "data",
        "evaluation",
        "example",
        "examples",
        "important",
        "labeling"
      ],
      "search_text": "Evaluation data is so so important when working on machine learning or generative AI projects. Labeling data is an imporant task and you can learn a lot about your problem by doing it. Don't look down on it, it's what the best people in AI do. Example evaluation app: https://llm-eval-demo.streamlit.app/ (I am working on an update to this, stay tuned) data evaluation example examples important labeling Why would you tell Scott to raw dog his data? My man is shooting in the dark. Wait, are you saying Scott isn't tracking the performance of his experiments on his customer satisfaction project? So I need to raw dog the data? Hold on, it's important to have evaluation data so you can assess how well your model is doing. Exactly, you gotta label that data even if it means hunching over Excel for a couple of hours. Ah, I'm on board. It's important to just start collecting examples. You don't need fancy labeling tools to do this. I'd like to go faster. Any pointers? For this project, instead of starting from scratch, you can use an existing LM to score the data as well as give the confidence of each example. Use that as a starting point to speed boost your labeling. That sounds totally doable. Martha says being open to feedback builds character. How do I get feedback? So add an interactive evaluation or feedback loop. Here's an example of that where we can collect what are good examples, what are bad examples. If anything goes wrong, we can flag those. Can we flag this entire conversation?",
      "platforms": {
        "tiktok": {
          "video_id": "7401219309638405418",
          "url": "https://www.tiktok.com/@rajistics/video/7401219309638405418",
          "view_count": 1395,
          "upload_date": "2024-08-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/3f7fa0e8ead143eeadaad35b703b178c_1723230664~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=hbQ2Nvqb3Wnecswpsrut3Lr18Kw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6008,
      "title": "Transformer Explainer (1 min Short) - Interactive Visualization for Transformers",
      "description": "Transformer Explainer (1 min Short) - Interactive Visualization for Transformers",
      "upload_date": "2024-08-11",
      "total_views": 1393,
      "max_views": 1393,
      "topics": [
        "end",
        "explainer",
        "interactive",
        "min",
        "short",
        "tool",
        "transformer",
        "visualization"
      ],
      "search_text": "Transformer Explainer (1 min Short) - Interactive Visualization for Transformers end explainer interactive min short tool transformer visualization",
      "platforms": {
        "instagram": {
          "video_id": "18058411834645784",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ZaTHZBmCk8Y",
          "url": "https://www.youtube.com/watch?v=ZaTHZBmCk8Y",
          "view_count": 1393,
          "upload_date": "2024-08-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "What a data scientist does #datascience #analytics #codetok #python ",
      "description": "What a data scientist does #datascience #analytics #codetok #python ",
      "upload_date": "2022-09-04",
      "total_views": 1393,
      "max_views": 1393,
      "topics": [
        "analytics",
        "codetok",
        "data",
        "datascience",
        "python",
        "scientist"
      ],
      "search_text": "What a data scientist does #datascience #analytics #codetok #python  analytics codetok data datascience python scientist I smoke, I drink, I fuck bitches, I skate, I'm basically the shit!",
      "platforms": {
        "tiktok": {
          "video_id": "7139313634798210350",
          "url": "https://www.tiktok.com/@rajistics/video/7139313634798210350",
          "view_count": 1393,
          "upload_date": "2022-09-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/960e15ac03f2464e8d4ab4fffbb2fb81_1662250992~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=PFa%2Fsntm4Ht8p39hGuHbutaXkPM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Scaling LLMs. Two years ago OpenAI had a big lead. Since then, other companies have learned to effectively scale their model training. ",
      "description": "Scaling LLMs. Two years ago OpenAI had a big lead. Since then, other companies have learned to effectively scale their model training. ",
      "upload_date": "2025-01-07",
      "total_views": 1378,
      "max_views": 1378,
      "topics": [
        "billion",
        "going",
        "laws",
        "model",
        "models",
        "scaling"
      ],
      "search_text": "Scaling LLMs. Two years ago OpenAI had a big lead. Since then, other companies have learned to effectively scale their model training.  billion going laws model models scaling You were lucky to be here today. I'm going to teach you about the scaling laws for language models. Excuse me, what's a language model? Azure, go stand in the corner. Just because you pay me doesn't give the right for you to talk to me. Yes, but the important takeaway here is to understand that there's a tradeoff between the size of the model and the amount of data that you use. How does more compute fit in? Is more better? Yes, more compute is better. Oh yeah, more compute, baby. Hey, stop building those GPUs so fast. We need to keep prices up. Our scaling laws show that you should emphasize building larger models over getting more training data. As building larger models by our scaling laws will lead to better performing models. Oh, I see you open AI with your 270 billion parameter model. Well, we're going to build Megatron. That's going to be 540 billion parameters. Oh yeah, we're going to build a 175 billion parameter model and we're going to leave it open for everybody. Images, we will build rule dial with 1.75 trillion parameters. Good luck to all of you. And remember, nobody needs more than 300 billion tokens of data. Hey, is anyone going to double check open AI's numbers? Nah, let's go ahead and build a 540 billion parameter model. Hey, open AI, we double checked your work and your scaling laws, the numbers are way off. We built a better performing model and we actually used a smaller model size but a lot more data. Take a look at this graphic to see the comparison. But we have a 70 billion parameter model that used 1.4 million tokens of data. Wait, if that's right, let me do some calculations. We way overbuilt our models. We spent 12 million dollars on compute when we could have got away with three. Don't tell Sundar. Oh, you all relied on those scaling laws I told you about? And for the last three years, you've been building bigger and builder models? Yeah, I've been spending my time on other types of strategies. Sorry guys, you hear that call from Microsoft already? Hey, DeepMind, can you help us out with this chat GPT thing as well?",
      "platforms": {
        "tiktok": {
          "video_id": "7457257883021954335",
          "url": "https://www.tiktok.com/@rajistics/video/7457257883021954335",
          "view_count": 1378,
          "upload_date": "2025-01-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8cWA6B0IiZQ1ykisgzC9BJAvI1BqIPfAAVihI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=0FGEHVLEydrONY5m8axeaGZg0D0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Clustering with kmeans ",
      "description": "Clustering with kmeans ",
      "upload_date": "2024-08-22",
      "total_views": 1363,
      "max_views": 1363,
      "topics": [
        "algorithm",
        "clustering",
        "data",
        "people",
        "start",
        "works"
      ],
      "search_text": "Clustering with kmeans  algorithm clustering data people start works How would you teach my kids where to put the food truck? I figured it out. Let me show you the algorithm I use so they end up in the right spot. The way it works is it doesn't matter where they start. The first thing they do is figure out who are the people that are closest to them. They then take the average of that group of people that's closest to them and that gives them a new position. Then they move over to that new position and start the process again. This approach is super cool. Even works on more complex data setups like this. What we went through is what's known as the Canine's algorithm and is one of the most common algorithms in data science for doing clustering or grouping of data.",
      "platforms": {
        "tiktok": {
          "video_id": "7406057169361128746",
          "url": "https://www.tiktok.com/@rajistics/video/7406057169361128746",
          "view_count": 1363,
          "upload_date": "2024-08-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/273054a6885143c89cdb1af463dc723f_1724357068~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=nO%2BnPfgIVBO%2BsPm811a%2BdJcjqo8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5943,
      "title": "Every once in a while, I go back and try to build some AI from the ground up. Lately, its been \"Mixture of Experts\" (MoE) models, and I found some great resources to help me understand how they work. I am sharing a walkthrough of the notebook to hopefully inspire you and get you understanding some of the fundaments. Find the notebook here: https://github.com/rajshah4/makeMoE_simpsons",
      "description": "Every once in a while, I go back and try to build some AI from the ground up. Lately, its been \"Mixture of Experts\" (MoE) models, and I found some great resources to help me understand how they work. I am sharing a walkthrough of the notebook to hopefully inspire you and get you understanding some of the fundaments. Find the notebook here: https://github.com/rajshah4/makeMoE_simpsons",
      "upload_date": "2025-11-09",
      "total_views": 1359,
      "max_views": 996,
      "topics": [
        "bin",
        "data",
        "experts",
        "going",
        "histograms",
        "kind",
        "like",
        "plots",
        "see",
        "size",
        "well"
      ],
      "search_text": "Every once in a while, I go back and try to build some AI from the ground up. Lately, its been \"Mixture of Experts\" (MoE) models, and I found some great resources to help me understand how they work. I am sharing a walkthrough of the notebook to hopefully inspire you and get you understanding some of the fundaments. Find the notebook here: https://github.com/rajshah4/makeMoE_simpsons bin data experts going histograms kind like plots see size well Are you ready to build a mixture of experts model from scratch? What I want to do today is tackle mixture of experts models. Now, these have been intriguing ever since GPT-4, DeepSeq, all these models use a different transformer architecture called mixture of experts. What I want to do today is show you in a code notebook how you can build this up, learn all the concepts using some simple pie torch to do it. This is really good for anybody in the AI field that wants to get a better intuitive understanding of how these models work. Spending some time at this low level is very useful. Even if you're not going to spend your time building models, just understanding some of these basics is very useful. Now, we're going to make it fun. We're going to use some Simpsons data. The code I'm using originally came from Andrew Karpathy's Make More, kind of a very simple transformer library he had. It's been inspired, built on by some other folks, have built some mixture of experts. On top of that, that code, I'm largely using that. I've modified it a little bit to use the Simpsons data, as well as adding in a lot more comments and documentation and ideas for how you could explore it. So to get started, you go to this GitHub repo. There's one of the notebooks in here is Make, mixture of experts from scratch. This is the one I have updated and added a lot to. You can run this notebook on your local laptop with the CPU. We're going to run it today in Google Colab, just because I think that's a friendlier environment for all of you to be able to show. So next, I'm going to start walking through this notebook. I've used just the basics in Google Colab. I haven't even used a GPU. When it gets to the trading part, having a GPU is helpful and will speed it up, but it's not necessary. This we're using a pretty small data set here, so it's going to go through. I spent a lot of time coding or writing documentation for this notebook, so it should be very easy to understand. I've also given you ideas for how you can explore and go beyond it. This is not a notebook where you just press the button, you start it and you run it to the end, and you walk away the whole time. This is a notebook. You should be modifying cells, adding cells, adding print statements, trying to understand what every line of code does. This is the goal. You want to dig into this like this. I'm going to give a quick high-level overview. I'm going to go through the notebook like this, but you should pause this. You should play around with each of the lines of code. Make sure you understand it all. I don't have time to explain every little piece of this, so I'm going to count on you to spend some time doing it. Now, we're going to build a mixture of experts model today, which is built on traditional transformers, but it's going to add this piece instead of that traditional dense neural network that's at the end of a transformer after attention, instead, we're going to have lots of little network experts that we're going to use and a router that helps guide the tokens to it. We'll talk about that more to do that. To start with, we're going to talk, and we're going to go through and build a basic transformer. I'm going to talk through this. If you already know the basics of that, feel free to skip ahead. Now, in this notebook, I do use Weights and Biases Library to help capture the results of the experiments and the model checkpoints that we train at the end of the notebook. So you don't even need to use Weights and Biases to get the most out of this notebook, but if you go to train the model, you'll get an error if you're not using it, simply because I've set up the code for that. So just be aware of that. You're going to want to run the imports, do all of this. Now, I, of course, have set a seed here. Always try rerunning this stuff with a different seed. See what the effect is. It's always good to understand what the effect of randomness is as well. So let's go through this. We're going to go ahead and grab the data that we're going to use. This is on my GitHub repo. It is the Simpsons data. You might ask, well, Raj, how much data is it? Let's look at it. Well, let's use a little bit of Python. We're going to go ahead and take a look at it. And you can see it's like seven million characters. We can look at the first few characters, feel free to kind of look at some other parts of it. This is just the first thousand like that. It's the Simpsons scripts like this. Man, if you want to, you can substitute your own text in there. You don't have to use the Simpsons. So one of the first steps is when we have text, how do models work with them? Well, we have to convert the characters that are in the model into numbers that the computer can understand. The first thing we have to figure out is how many of these characters are there. So we'll see here, we have a vocabulary of 137 unique characters that are here and all those different unique characters. So now we're going to use what's called a tokenization. We're going to convert all the characters into a number. And we're going to go very simple here. We're just going to go right down the line, give each one of these a number from 1 to 137. That represents their value. I'm going to run this code here. And that's what it does. So now, for example, I have this in code. When I type in the word hi there, you can see here are the letters that goes. And if you think about it, like I've shown you a bunch of stuff, right, if h is 68, i is 69, 69. So hopefully that gives you an intuition of that tokenization and how we're storing numbers inside there. So we're going to go ahead and run that across the entire data set as well. And split the data set into test train because this is traditional machine learning, where we're going to give it lots of examples to learn from, but then hold out some to double check it, make sure everything's going OK. The next step is we've got to feed this transformer. How do we feed it? Well, we can't just shove everything in it at once. We give it to it in bytes. We batch it. We have context windows. So the first thing is we're going to have a block size of 8, which means I'm only going to give it eight characters at a time. The next thing is when I give it eight characters, what do I want it to do? We want it to do what's called next token prediction. We want it to predict the token that comes after it. So for example, if I have 44468, I want it to then predict 13. If I give it these three, I want it to predict the one. The great thing about this technique, if you think about it, it's just learning itself. I'm not going out and having to get any labeled data. This is a self-supervised technique, and it's been the key for how all these models have learned so much, because we can just pour text, and they just read the text and try to predict the next word. We don't have to go out and label any extra data like that. Now, you can see here, it's always... We're going to use what's called causal masking, as we'll talk about, where we want to predict from the earlier part of the sequence to the later. Now, there is something called non-causal, where we will take a prediction, and we look at the information on either side of it. Sometimes this works if you have both of those information. You don't have something that's kind of time bound like that. But for today, we're going to do next token prediction like talking. So we're just going to focus on predicting the next piece. And you can see here, kind of what the X is, the inputs, what the Y is as well. Now, since we have these massive GPUs, we can also use batches. We don't have to do one thing at a time. We can have three or four things going at once. So I'm going to set a batch size of four that we're going to go into as well like that. So now you can see, for example, we can look at the inputs and outputs that come out of this. So now I've got some data that's coming in. Is my input my X, and you can see the corresponding Y. And if you look at it, you'll see that pattern that we talked about between that next token prediction, where if my X is one, I'm predicting the next token, which is 37 in the string like that. So get a sense of that. If you need to pause and play around with this, it's important. I have a little bit of a printout here that shows you kind of this visualization of this in a little bit more detail. You'll see one of the things we'll often do is we measure is keep track of these sizes of these different, in like the different vectors, tensors that we're using, partly because we always make sure things match up. When you start looking into other codes, start building things. It's easy for things to get mismatched. So keep in very careful track of making sure that all the sizes are right. That's like Lego blocks. We need to always kind of fit them perfectly, or otherwise you're going to have some issues like that. But hopefully this gives you the sense of that causal masking that we're doing and that predicting, because that's going to come up again. So now we got the data. We've got it into pieces like that. We could go stop right here and build a model right now. Now the model is not going to be that great for text, because one of the things we've learned along the way, and this has been one of the most influential papers around, is the concept of attention or self-attention. This came out in 2017, in a shadow of Google, and it's the foundation of all the modern kind of AI models. Is this notion of attention for sequential learning? And I've went ahead and kind of shown you some diagrams, spending time and making sure you understand attention is very good to do that. Intuitively, the way I like to think about is attention, is what we'll do is we'll start with a vector of something a vector of some piece of kind of text sequential information. And then what attention allows us to do is understand what are the relationships between all the different words in that little piece there. And we're going to transform that and build basically a better version of that vector that has all those relationships embedded in, like a concept vector that has all the dense relationships between that. And that's what attention does really well. And spending some time going through this line by line is very valuable to get an intuitive sense of what's going on. I've added some text descriptions. I've also went through and kind of very carefully tried to comment every line so you can follow it carefully. Where you can see, for example, we take the query and the key, we transpose that, we build out this lower triangular matrix. Why? Why? Because it's the causal masking. We don't want the future to come back. So we have to make this little triangle matrix. So we pass out. We don't see that future information that comes through. Again, this is where you kind of have to play around with it. You'll get notions like what is a softmax thing? What are we doing? Well, we need to normalize the attention weights. We want to kind of pull things towards zero, have easy ways to add it. Softmax is a nice mathematical function to do this. So play around with this. You can break this up. Just run this line by line. In fact, I have all of this in one chunk, but feel free to kind of pull this out so you understand and see exactly what we look like, what Trill looks like, and get a sense of that. I've added a visual diagram to do this. Look, if there's one thing you do, even if it's not the mixture of experts, it's understanding this because this is like the foundation of this. And anybody doing AI interviews, you're going to get asked this. Now that we've done it and I've built it for one, what we're going to do is take that attention and we're going to make an attention head. So now what I have is I have this component where you can see we've set some embedding size, the number of heads and layers, where now I can do all of this stuff inside one nice function, my head, where it's going to do this attention calculation for me. And then I don't want one head. I want lots of heads. We want multi-head because the idea of multi-head attention is you could each one of these heads is going to look at it with its own perspective. So adding multiple heads, this is again, we've empirically do this. Like it's not all kind of just theory that we read. People run experiments, you should run experiments and you'll see adding the multiple heads can give you better lifts, better performance like that. Now that we've built the basics of attention, typically the next piece is that information passes into a dense neural network in a typical transformer. We're not building a typical transformer today. We're building a mixture of experts model. So it's going to go a little bit different instead of a dense model. What we're going to use is multiple models, multiple little neural networks, where we're only going to selectively use some of these models when we do a computation. And because we only use some, even though we have lots, we call this a sparse approach. It also means that this type of approach is going to have to host all those little experts, which will require more GPU memory capacity. But when we run the compute through, since we're only calculating from a few of those neural networks, it's actually less calculations. And so this is why lots of enterprises, lots of these big providers like these models, because they can actually pass more information, pass, get quicker kind of throughput through these models, even though they take more GPUs because of this. Now the way it decides which experts is that router to do that. So backing up again, we're going to pass the information in. A router is going to look at that input, what's coming in off the attention, being like, well, for this particular type of information, I think experts, this expert and this expert should go to it. That information will pass to those experts like that. Now again, the reason this, where we're here is this works empirically. And this is where I want you to try the code. I want you to run experiments. So you get the sense of how all of this works as well. Now the experts themselves, as we'll see here, are just simple, kind of very basic kind of, you know, feed forward neural networks that we've built here. Now, the next piece is we have to figure out which expert to do it. And this is what goes into that router or the top K gating. We need to decide which of these experts it goes into. So the next piece is we're going to have this piece here. So let's go ahead and I want to run this to give you an intuitive sense. Is we want to be able to have a piece of code, kind of a simple linear piece that's going to help predict what expert we should go to. So in this case, you'll see I have three experts and a top K of two to go down to do that. So here you can see there's two experts that we finally kind of choose from because I'm going to show kind of the top K logits and the top K indices. So what I'm showing here is the top K of how many experts there should be. So if we go to three, we're of course, we're using those logits here to then get the indices to identify the actors. And if you take a look at the numbers, it'll start to make sense for why we have certain ones picked over others. You can see, for example, the zero here lines up very well with the highest value here. Similarly, the zero when it gets down to the lowest, right? The least kind of things falls out over here, kind of as the least kind of experts according to kind of my top K that's going to be chosen like that. Now we have a little bit of math we can do to help kind of weigh these out where we can add a little bit of softmax in that it just makes all the math here cleaner where we can have everything add up to one kind of dropout kind of the low experts as well. Now, one thing that can happen during training is the model can end up relying on just one or two or a few experts way too much. And what we want to do is we want to make the model use all that capacity that we've given it, allow those experts that we've given it. So one way to help force that is to feed a little bit of noise in the training process that helps it that pushes it to use kind of more of the experts like that. And so that's what I want to show you this. That's what this next part of code does is this noisy top K code. As you walk through it is it's going to give us more control over kind of all of the the routers and the gates. And you can see here I've got all the pieces here where you can see we're going to add some Gaussian noise into that. And so now we've added a line here where it's going to add some noise into this piece of the logits. So you can test this out and kind of see what is the effect of this. The default model that I have is going to use this noisy gate. But this is one of these improvements that we have. We have a couple of others through this notebook that I want you to be aware of as well. So now that we've built the routing how we're going to pick the experts and the experts again we can start bringing this code. And this is a lot of the notebook here now near the end is doing bringing together a lot of the earlier pieces of work we've done putting them and condensing them in there. So if you look through this sparse MOE you'll see and recognize a lot of the code and hopefully have an intuition now of what all these different lines are doing because we spent time earlier kind of earlier kind of working on all of that. Now one thing I've done is I just wanted to make sure like this stuff is working like I talked about this MOE versus using a traditional dense network. So this little snippet of code here actually looks and says if I built just a traditional neural network versus if I use this mixture of experts am I seeing some differences to do that. And if you walk through the code and again like all the stuff chat GPT cursor all of the stuff will explain this code very well but I've taken time to kind of carefully document it. You can see the outputs here as well. So you can see the difference between what were the outputs if I did that mixture of experts what is if I used a traditional dense am I seeing some difference there you always want to kind of intuitively check these things make sure they're actually learning make sure they're actually doing this as well. I gave you some other ideas as well like hey you could add weighted expert where instead of just treating each of the experts the same maybe some you can get more weight again you can start getting creative with these ideas as you start thinking about them this is how everybody builds this is just experiments with that. Now as you kind of have done that we can start pulling all this together and this is the final piece where we build a final mixture of experts model to train. There's a number of different hyper parameters I'm going to share some of the ones that I thought were the most interesting to do this don't have time to go through all the lines. So one of them was the max intervals max iterations of 10,000 this is what worked for my particular code on the Simpsons data where I wanted to end up with pretty good results showing you it's starting to really understand the language like that might be different for your type of database. You might want to start much with a much smaller iterations just to make sure you're learning that you're picking up some piece before you kind of have it training for a long time. Again you can play around with these other settings as well especially the number of experts and the top K see what the effect is run experiments try different ones of that and share them back. Now the rest of the code goes through what we've done earlier as well. So it's going to start by pulling in that Simpsons data doing the tokenization the train test splits loading the data estimating the loss between the train and the valve all the pieces that we need for kind of going through an understanding are we actually learning are we training. Now to do this we're going to use the different pieces that we talked about we're going to create the attention heads we're going to create a multi attention head we're going to create an expert we're going to use the noisy top gay routing as well we're going to then put that together create the sparse mixture of experts model where we're going to do that look at each expert see how well they do along this as well and then kind of create a block to do all of this. So this is where all the code is just putting all these pieces together to generate that get the predictions back as well. There's a couple of other pieces here there's this and weight initialization I'm not going to go into it you can hit the play you can see how they've done that again we're just following some best practices for building models there but you can play around with it. Now when you do the training here remember my code was using weights and biases you're going to get an error if not you can just highlight delete out or comment out the weights and biases lines as well it'll still work and train you can train this on a CPU it'll take a little longer I'd brand this on my MacBook it worked just fine. The generation steps I put in here because I wanted to see this here it's a lot of checkpoints you don't necessarily need these you can feel free to modify these as well. Now the code here at the end there's a lot of a lot more verbosity because there were some things I wanted to do for tracking the experiments for tracking all the model checkpoints and even for like taking the example evaluations and putting it into a into a data set so this is where use GPT-5 whatever to help understand what's going on with this but that's where a lot of this code kind of is and the verbosity there was I used it to help write some of this code worked out fine for that and you can see even here I create some kind of data sets as well to keep track of stuff I think it's important to keep track of stuff as you're doing it I put that in the notebooks as well so gave you lots more ideas to kind of go further with this I do want to show you some of the results that I got from the model so if I flip over here to my weights and biases I have logs of this that weights and biases of course has lots of great outputs as well but I just want to show you like look at what it starts at right this is just the model is just basically doing randomness you can see that the loss is really high but just even after a little bit of training you can see it comes down a lot there and I mean this isn't making any sense it's still kind of really random letters like this but the nice pieces and I think this is always good to give you intuition is you can see you know as it's going along now at step 4000 look at you know what the my validation loss is starting to put together things that look a lot more kind of like the English language and you can just watch the progression to do this now this is you know relatively speaking I'm just using the simpsons data set it's a small piece you can't expect it to understand the nuances of English language like that entirely just on this data set and this small kind of model but you can definitely see improvement like this so I'm hoping this will give you inspire you show you how you could do something like this and build it yourself so thank",
      "platforms": {
        "tiktok": {
          "video_id": "7570750385166519582",
          "url": "https://www.tiktok.com/@rajistics/video/7570750385166519582",
          "view_count": 996,
          "upload_date": "2025-11-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIkhEyAEfAQfVUwbDDRpsGBmbEEVFE4tUL8QRw~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=bPFlJVCZyRnh73gMu910uZiHDWM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18480243784043955",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "w9vude94TxU",
          "url": "https://www.youtube.com/watch?v=w9vude94TxU",
          "view_count": 363,
          "upload_date": "2025-11-09",
          "thumbnail_url": "https://i.ytimg.com/vi/w9vude94TxU/maxresdefault.jpg"
        }
      }
    },
    {
      "group_id": null,
      "title": "Generative AI is awesome. If you need more ideas check out all 35 real world examples for using Generative AI / LLMs that Evidently has put together: https://www.evidentlyai.com/blog/llm-applications",
      "description": "Generative AI is awesome. If you need more ideas check out all 35 real world examples for using Generative AI / LLMs that Evidently has put together: https://www.evidentlyai.com/blog/llm-applications",
      "upload_date": "2024-07-25",
      "total_views": 1356,
      "max_views": 1356,
      "topics": [
        "generative",
        "models",
        "together",
        "use",
        "uses",
        "using"
      ],
      "search_text": "Generative AI is awesome. If you need more ideas check out all 35 real world examples for using Generative AI / LLMs that Evidently has put together: https://www.evidentlyai.com/blog/llm-applications generative models together use uses using Did you know AI is much more than chat GBT? Let me show you three ways you can take advantage of large language models. Large language models can be really useful for extracting information. For example, finding nuggets and a lot of knowledge. LinkedIn does this to identify what are the best jobs to do for you. DoorDash does this with restaurants to figure out how they should match up. The second way we can use these models is using them to find patterns in our data. So Zilla does this to find content that is discriminatory. On the other hand, GoDaddy takes all the support emails that come in, uses generative AI to help figure out which one of 20 categories that these support emails fall into. The third way is you can use these models to generate information. So Duolingo does this to create content for their courses. Places like Snowflake uses it to create text-to-sequel models. But the coolest one is how SumUp uses this for financial investigations, where they take lots of data that comes up, pass it through a generative AI model that helps generate draft reports for fraud investigators, saves a fraud investigator a bunch of legwork in putting together that initial draft. These are just a few of the ways that you can use generative AI. Let me know what you're doing.",
      "platforms": {
        "tiktok": {
          "video_id": "7395666924103028014",
          "url": "https://www.tiktok.com/@rajistics/video/7395666924103028014",
          "view_count": 1356,
          "upload_date": "2024-07-25",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4f94edad10e24171bd664fd12ebd1b87_1721937899~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=DKMufd5%2BWPZ4WAxF676M2c4PUWc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Great data scientists figure out the best questions come from talking to people. #datascience  book is practical python and opencv by rosebruck, great resource. ",
      "description": "Great data scientists figure out the best questions come from talking to people. #datascience  book is practical python and opencv by rosebruck, great resource. ",
      "upload_date": "2022-11-05",
      "total_views": 1356,
      "max_views": 1356,
      "topics": [
        "best",
        "data",
        "datascience",
        "figure",
        "great",
        "scientists"
      ],
      "search_text": "Great data scientists figure out the best questions come from talking to people. #datascience  book is practical python and opencv by rosebruck, great resource.  best data datascience figure great scientists I'm going to take it",
      "platforms": {
        "tiktok": {
          "video_id": "7162538820578331950",
          "url": "https://www.tiktok.com/@rajistics/video/7162538820578331950",
          "view_count": 1356,
          "upload_date": "2022-11-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/986e02905e8048a5a9a58a77ac452d19_1667658545~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=NElSeH1QLGDaN0xTSCimrOAnrhY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Agents in AI Some effective examples for using agents based on posts from Anthropic and Hugging Face. Hugging Face - SmolAgents: https://huggingface.co/blog/smolagents Anthropic - Building effective agents: https://www.anthropic.com/research/building-effective-agents",
      "description": "Agents in AI Some effective examples for using agents based on posts from Anthropic and Hugging Face. Hugging Face - SmolAgents: https://huggingface.co/blog/smolagents Anthropic - Building effective agents: https://www.anthropic.com/research/building-effective-agents",
      "upload_date": "2025-01-04",
      "total_views": 1349,
      "max_views": 1349,
      "topics": [
        "agents",
        "anthropic",
        "effective",
        "face",
        "hugging",
        "smolagents"
      ],
      "search_text": "Agents in AI Some effective examples for using agents based on posts from Anthropic and Hugging Face. Hugging Face - SmolAgents: https://huggingface.co/blog/smolagents Anthropic - Building effective agents: https://www.anthropic.com/research/building-effective-agents agents anthropic effective face hugging smolagents",
      "platforms": {
        "tiktok": {
          "video_id": "7456157299174804766",
          "url": "https://www.tiktok.com/@rajistics/video/7456157299174804766",
          "view_count": 1349,
          "upload_date": "2025-01-04",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMBhRlBaxRi1IhnNmE5VqiiAIy5v3Z5IA9BAX~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=iIHr5dMstlDueJL3LFMbHvEM7kw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Machibr learning tradeoffs between explainabilty and accuracy. ",
      "description": "Machibr learning tradeoffs between explainabilty and accuracy. ",
      "upload_date": "2024-08-06",
      "total_views": 1345,
      "max_views": 1345,
      "topics": [
        "bathrooms",
        "learning",
        "model",
        "price",
        "sales",
        "understand"
      ],
      "search_text": "Machibr learning tradeoffs between explainabilty and accuracy.  bathrooms learning model price sales understand I bet you can understand this machine learning model pretty easily. If you take a look, you probably understand that bathrooms and square footage control the sales price. And for example, if you increase bathrooms, your sales price is going to go up. Pretty understandable model. With this model, it's hard to explain. Although if you're a calculator, you understand the relationship between all of these features or variables and sales price. In reality, for most people, it's really hard to understand. There's some things that seem obvious, like, hey, if I increase the number of bathrooms, my price goes up. But there's some things that are counterintuitive. Take a look at total rooms. This is suggesting, like, if I remove rooms in my house, my price should go up. Hmm. Now, this is not unusual behavior in machine learning models. Lots of times you could have features that are correlated to each other, multi-colinearity, or might interact with each other. And while mathematically, this combination of features can reduce the error and give you accurate predictions, intuitively for a person, they can be hard to understand. Data scientists often view this conundrum as a tradeoff between interpretability and accuracy. And while there are things you could do upfront to make sure your model is easier to understand and easier to explain, often in real life situations, there's a real tradeoff between the two.",
      "platforms": {
        "tiktok": {
          "video_id": "7400160692545948974",
          "url": "https://www.tiktok.com/@rajistics/video/7400160692545948974",
          "view_count": 1345,
          "upload_date": "2024-08-06",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/851eb29027b44db5bf09bf24c75c0a57_1722984185~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=b0eRoUjue4XNxqgolZyUL7Lf%2F6A%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6472,
      "title": "Baseline Models and Benchmark Datasets Explained",
      "description": "Baseline Models and Benchmark Datasets Explained",
      "upload_date": "2023-04-09",
      "total_views": 1331,
      "max_views": 791,
      "topics": [
        "baseline",
        "baselinemodel",
        "benchmark",
        "datascience",
        "datasets",
        "explained",
        "machinelearning",
        "model",
        "models"
      ],
      "search_text": "Baseline Models and Benchmark Datasets Explained baseline baselinemodel benchmark datascience datasets explained machinelearning model models",
      "platforms": {
        "instagram": {
          "video_id": "CqwB7p2AunP",
          "url": "https://www.instagram.com/reel/CqwB7p2AunP",
          "view_count": 540,
          "upload_date": "2023-04-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "O4ZOhAVFyG8",
          "url": "https://www.youtube.com/watch?v=O4ZOhAVFyG8",
          "view_count": 791,
          "upload_date": "2023-04-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6163,
      "title": "Evaluation for Generative AI - A simply explained starting point",
      "description": "Evaluation for Generative AI - A simply explained starting point",
      "upload_date": "2025-05-17",
      "total_views": 1308,
      "max_views": 1308,
      "topics": [
        "evaluation",
        "explained",
        "gen",
        "generative",
        "going",
        "kind",
        "like",
        "lot",
        "point",
        "simply",
        "starting"
      ],
      "search_text": "Evaluation for Generative AI - A simply explained starting point evaluation explained gen generative going kind like lot point simply starting",
      "platforms": {
        "instagram": {
          "video_id": "17990661980653098",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "hWlv4e6SQbU",
          "url": "https://www.youtube.com/watch?v=hWlv4e6SQbU",
          "view_count": 1308,
          "upload_date": "2025-05-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6066,
      "title": "Older video and Cleanlab has changed its license. But it’s still a cool tool to be aware of. ",
      "description": "Older video and Cleanlab has changed its license. But it’s still a cool tool to be aware of. ",
      "upload_date": "2025-01-30",
      "total_views": 1303,
      "max_views": 1303,
      "topics": [
        "changed",
        "cleanlab",
        "data",
        "errors",
        "license",
        "look",
        "model",
        "older",
        "see",
        "set",
        "video",
        "website"
      ],
      "search_text": "Older video and Cleanlab has changed its license. But it’s still a cool tool to be aware of.  changed cleanlab data errors license look model older see set video website The label errors website shows us lots of classic data sets are filled with labeling problems. What are we going to do about it? Let's dive into how confident learning implemented in clean lab can help you out. The first step is training a model and then being able to get predictions on an out-of-sample set of data points. Now a key here is we're going to look at probability among all the classes. So some you see are pretty obvious like cat seems to make sense. Or if we look at this car, like what is that doing in there? And you can see the model is kind of confused and not giving a strong probability to any of the classes. I'm going to start here by setting a threshold for my class percentile of 90%. So you can see here we're only looking at a small set of the examples that are actually in my out-of-sample data set. Now if we look at a particular example here, we can see for example the model predicted bear at 0.913, which met the threshold. The undiagonal examples aren't very interesting. What's interesting are the things that are off because that's where the errors are occurring. Here we see the model has predicted bear at very strongly at 1.0. But look at it, the example is wrong. And in this case it looks like this is an image that was mislabeled. If we lower our threshold, you'll see we'll have a lot more examples to consider about labeling errors. And there you have it. By looking at where your model is really confident and where that doesn't make sense, clean labs able to identify what are places that you potentially have mislabeled data that you can go in and help improve your test set. The package is open source. They have lots of tutorials. It's very easy to get started and a very easy way to check for data quality.",
      "platforms": {
        "tiktok": {
          "video_id": "7465742013334424862",
          "url": "https://www.tiktok.com/@rajistics/video/7465742013334424862",
          "view_count": 1303,
          "upload_date": "2025-01-30",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oQIz2oKtAAA3AIkLAEijzpKBGfjBpQdiiCBCbI~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=og6%2F7BAvslsFTVKxvZ7W2XpNeuk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18029397629307765",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6256,
      "title": "LLama 3 effects on the AI / LLM Startups Like OpenAI",
      "description": "LLama 3 effects on the AI / LLM Startups Like OpenAI",
      "upload_date": "2024-04-21",
      "total_views": 1293,
      "max_views": 1293,
      "topics": [
        "effects",
        "going",
        "like",
        "llama",
        "llama3",
        "llm",
        "meta",
        "open",
        "openai",
        "startups"
      ],
      "search_text": "LLama 3 effects on the AI / LLM Startups Like OpenAI effects going like llama llama3 llm meta open openai startups",
      "platforms": {
        "instagram": {
          "video_id": "18224059291274763",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "hMbWSvA0dMY",
          "url": "https://www.youtube.com/watch?v=hMbWSvA0dMY",
          "view_count": 1293,
          "upload_date": "2024-04-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Your data science 101 reminder when working with classification models. #datascience #statistics #codetok ",
      "description": "Your data science 101 reminder when working with classification models. #datascience #statistics #codetok ",
      "upload_date": "2022-10-10",
      "total_views": 1289,
      "max_views": 1289,
      "topics": [
        "codetok",
        "data",
        "datascience",
        "science",
        "statistics",
        "teddy"
      ],
      "search_text": "Your data science 101 reminder when working with classification models. #datascience #statistics #codetok  codetok data datascience science statistics teddy That's Teddy, and that's also Teddy. I named them all Teddy because I can't tell the difference.",
      "platforms": {
        "tiktok": {
          "video_id": "7152893432019651886",
          "url": "https://www.tiktok.com/@rajistics/video/7152893432019651886",
          "view_count": 1289,
          "upload_date": "2022-10-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/27f44c47b1fd4d6eb6ff4d90ce362e00_1665412801~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=qiapvrdOp5ISm%2BMIPCrcGBDEQ%2BM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "What tools are way too hard to use?  #datascience #statistics #analytics",
      "description": "What tools are way too hard to use?  #datascience #statistics #analytics",
      "upload_date": "2022-03-05",
      "total_views": 1285,
      "max_views": 1285,
      "topics": [
        "analytics",
        "datascience",
        "hard",
        "statistics",
        "tools",
        "way"
      ],
      "search_text": "What tools are way too hard to use?  #datascience #statistics #analytics analytics datascience hard statistics tools way Wow, that's a lot of words. Too bad I'm not reading them.",
      "platforms": {
        "tiktok": {
          "video_id": "7071646830962036010",
          "url": "https://www.tiktok.com/@rajistics/video/7071646830962036010",
          "view_count": 1285,
          "upload_date": "2022-03-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4183b65df8bc4689a39a3c3650e467d8_1646496094~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=StPBcqC%2BqqNFpg6ul%2BA3KeKwxEs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "This fits so well for agents. Timeless.  ",
      "description": "This fits so well for agents. Timeless.  ",
      "upload_date": "2025-02-05",
      "total_views": 1282,
      "max_views": 1282,
      "topics": [
        "chat",
        "get",
        "got",
        "gpt",
        "hey",
        "know"
      ],
      "search_text": "This fits so well for agents. Timeless.   chat get got gpt hey know Hey, have you checked out this chat GPT and what it can do? No, I haven't checked it out. It's probably a stage demo. Wouldn't worry about it. Hey, I just got a briefing from Microsoft. This thing's for real. I need to know what our chat GPT strategy is. I feel bad I saw this coming. I should have hired that machine learning engineer. She was so good at foundation models. And I never got the GPU cluster set up, never got those models inside. I know. I know the rest of the team and I wanted to get involved in large language models, but maybe this will be our opportunity. I wouldn't be in this situation if IT let us have our GPU cluster and HR didn't have all these crazy hiring rules. Look, we don't set the rules. You wanted something different. Talk to enterprise architecture. I have a long list of projects to do. GPU cluster, way at the bottom. He just needs to work this out. Let him go make a plan. Hey, Raj, I'm dealing with this chat GPT thing. You got some ideas for us? Hey, you're not alone. I've had a lot of calls from enterprise leaders these last few weeks on chat GPT strategies. I got some good ideas for what we can do. Hey, everyone, let's get in the conference room. We've got some ideas for what we can do now to build a new project called Acme Bot. I'm excited to show you our new Acme Bot. It was inspired by chat GPT, but this is going to go a step beyond and really what our enterprise needs. Hey, this looks good. Is this for real? Yeah, the team and I pushed really hard over the last two months to get this done. Excellent work team. I'm going to show this off at the executive offsite in Bermuda next month. Meantime, why don't you guys get yourself a pizza party?",
      "platforms": {
        "tiktok": {
          "video_id": "7468079813585161503",
          "url": "https://www.tiktok.com/@rajistics/video/7468079813585161503",
          "view_count": 1282,
          "upload_date": "2025-02-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oUI8P9yiv3y6ljRy5VBEIUBOB8pUAAnd62i6Z~tplv-tiktokx-origin.image?dr=9636&x-expires=1767387600&x-signature=fPWLPbaqXBQxCnWIJ%2F5qp5KLOFs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Filling in those job duties 🚩🚩 #datascience #codetok ",
      "description": "Filling in those job duties 🚩🚩 #datascience #codetok ",
      "upload_date": "2022-08-23",
      "total_views": 1275,
      "max_views": 1275,
      "topics": [
        "codetok",
        "datascience",
        "duties",
        "filling",
        "job",
        "music"
      ],
      "search_text": "Filling in those job duties 🚩🚩 #datascience #codetok  codetok datascience duties filling job music Sa-! س Bold Sa-! Music Parasite blasting music World USA Thanx Nothing Other rural owners",
      "platforms": {
        "tiktok": {
          "video_id": "7135220275565612331",
          "url": "https://www.tiktok.com/@rajistics/video/7135220275565612331",
          "view_count": 1275,
          "upload_date": "2022-08-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/88892f88accb44149e98769111fff32e_1661297932~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=HDU1cZ8wuxP5DdStcU9Gf%2B5%2BuBs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday ",
      "description": "#onthisday ",
      "upload_date": "2024-12-05",
      "total_views": 1273,
      "max_views": 1273,
      "topics": [
        "data",
        "going",
        "let",
        "onthisday",
        "time",
        "want"
      ],
      "search_text": "#onthisday  data going let onthisday time want Want to learn what a data science team does? Come join me on my stand up. It's been a slow slog with accounting. I built a Streamlet app this week to help modernize their invoicing process, but no one's using it. Everyone still goes to gym with his master spreadsheet that integrates like 18 different data sources together. It's so frustrating. I get it. They're old school. But they really need to move to databases and have proper data management and governance. How else are we going to ever be able to lift them up to using machine learning? Here's the thing to keep in mind. Accounting is a cost sink. They're not going to generate ROI for the enterprise. And if they don't want our help, that's okay. Let's not get into it with them. Instead, let's just document what are the main data fees? Who are the big contacts there? And then I'm going to have a separate session with their leadership. And I'm going to talk about how to modernize. They need to get their own data analysts to start doing this themselves. No use us spending our time on that. Let's just let it go for now. Hey, our work with marketing is going great. I'm working on a new framework to set up for A.B. testing. We're also going to be able to incorporate some uplift modeling as well. We're hoping to kick off the first experiment in about two weeks. Oh, that's great progress. Can you do two things for me? One is I want to make sure we track the effects that we're having with our interventions. And second, can you have Anne shadow? Because I think this is really great. And I want more of the data science team to be exposed to this. Sure, I can do that. But what do you mean by effect? I just want to make sure that the interventions that the marketing team is doing is actually providing a substantial benefit that's worth our investment in time. So as a ballpark figure, if they're not bringing in 200K with this, it's probably not worth our time to be spending time on this project versus other priorities I have on my list. Anyone know how Stephanie's doing on her project? Yeah, she was really excited because she said that execs at warehouse saw this new paper by Google DeepMind and wanted her to use that approach. So she was jumping into that. No, that was an optimization problem. It just needs to be solved simply with a linear programming. There's no need to go into anything that heavyweight, especially to start off with. I'll go run her down and talk to her. Thanks.",
      "platforms": {
        "tiktok": {
          "video_id": "7444744770087308574",
          "url": "https://www.tiktok.com/@rajistics/video/7444744770087308574",
          "view_count": 1273,
          "upload_date": "2024-12-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oQvUDfw4EEzVQfF7nAnAFCXiIGRQAAEECEjA6N~tplv-tiktokx-origin.image?dr=9636&x-expires=1767398400&x-signature=%2FMykd%2BNDI28rk7%2FgmI%2FuuAifhZw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6004,
      "title": "Creative Coding with Nature of Code from Coding Train's Daniel Shiffman",
      "description": "Creative Coding with Nature of Code from Coding Train's Daniel Shiffman",
      "upload_date": "2024-09-08",
      "total_views": 1271,
      "max_views": 1271,
      "topics": [
        "code",
        "coding",
        "creative",
        "creator",
        "daniel",
        "great",
        "hero",
        "learn",
        "nature",
        "new",
        "released",
        "train"
      ],
      "search_text": "Creative Coding with Nature of Code from Coding Train's Daniel Shiffman code coding creative creator daniel great hero learn nature new released train",
      "platforms": {
        "instagram": {
          "video_id": "18007570640634222",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Jh-fDyswsIc",
          "url": "https://www.youtube.com/watch?v=Jh-fDyswsIc",
          "view_count": 1271,
          "upload_date": "2024-09-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "ChatGPT has sucked up a lot of my attention. Will do a post soon on how it works. ",
      "description": "ChatGPT has sucked up a lot of my attention. Will do a post soon on how it works. ",
      "upload_date": "2022-12-07",
      "total_views": 1265,
      "max_views": 1265,
      "topics": [
        "attention",
        "chatgpt",
        "lot",
        "post",
        "soon",
        "sucked"
      ],
      "search_text": "ChatGPT has sucked up a lot of my attention. Will do a post soon on how it works.  attention chatgpt lot post soon sucked Focus",
      "platforms": {
        "tiktok": {
          "video_id": "7174203168207883566",
          "url": "https://www.tiktok.com/@rajistics/video/7174203168207883566",
          "view_count": 1265,
          "upload_date": "2022-12-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a4af57e2f7514b2c8a40d43c241a6179_1670374352~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=9knuZme%2B4JgBVicfSgsYaJ10a5o%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I was rocking with SPSS back in 2009.  I didn’t start using R until a few years later.  We had to pay $$$ for a basic regression. #datascience #spss #statistics",
      "description": "I was rocking with SPSS back in 2009.  I didn’t start using R until a few years later.  We had to pay $$$ for a basic regression. #datascience #spss #statistics",
      "upload_date": "2022-03-20",
      "total_views": 1263,
      "max_views": 1263,
      "topics": [
        "back",
        "datascience",
        "didn",
        "rocking",
        "spss",
        "statistics"
      ],
      "search_text": "I was rocking with SPSS back in 2009.  I didn’t start using R until a few years later.  We had to pay $$$ for a basic regression. #datascience #spss #statistics back datascience didn rocking spss statistics Do you number summer 09?",
      "platforms": {
        "tiktok": {
          "video_id": "7077167091127749930",
          "url": "https://www.tiktok.com/@rajistics/video/7077167091127749930",
          "view_count": 1263,
          "upload_date": "2022-03-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/af283d44abd44542adab287d877f2f8c_1647781373~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=Ys53QakO2YQqTLdIx%2B9VBMHKbas%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6055,
      "title": "Technical AI systems are vulnerable to out-of-distribution behaviors, as shown through evasion techniques like unusual movement patterns and environmental disguise. Novel strategies that deviate from standard approaches, like the Go player's encirclement tactic, can successfully defeat AI by exploiting its training limitations. These examples demonstrate that AI systems are constrained by their training data and require continuous monitoring to detect and adapt to new patterns and strategies.",
      "description": "Technical AI systems are vulnerable to out-of-distribution behaviors, as shown through evasion techniques like unusual movement patterns and environmental disguise. Novel strategies that deviate from standard approaches, like the Go player's encirclement tactic, can successfully defeat AI by exploiting its training limitations. These examples demonstrate that AI systems are constrained by their training data and require continuous monitoring to detect and adapt to new patterns and strategies.",
      "upload_date": "2025-02-20",
      "total_views": 1260,
      "max_views": 1260,
      "topics": [
        "beat",
        "computer",
        "like",
        "patterns",
        "strategies",
        "system",
        "systems",
        "technical",
        "training"
      ],
      "search_text": "Technical AI systems are vulnerable to out-of-distribution behaviors, as shown through evasion techniques like unusual movement patterns and environmental disguise. Novel strategies that deviate from standard approaches, like the Go player's encirclement tactic, can successfully defeat AI by exploiting its training limitations. These examples demonstrate that AI systems are constrained by their training data and require continuous monitoring to detect and adapt to new patterns and strategies. beat computer like patterns strategies system systems technical training Would you like to know how to outsmart AI? I got some examples from you and also what AI is going to do to fight back. Some Marines were sent in to foil an AI system. This AI system was trained on how everyday people walk around. Two Marines hid in a box and fooled the system. Another Marine Summer Salted, while another one took a tree and dressed up as that and got past the system. AI learned from its training data. If you give it something new and different, might not know how to react. This week, a man beat a computer playing Go. Now if you remember a few years ago, it was a big deal when computers could routinely beat the Go masters. His strategy was distracting the computer and then encircling it. Now this was a strategy that most other humans would have picked up, but since this was entirely novel for the computer, it didn't recognize it and it worked to beat it. The folks maintaining these systems should realize that the world is always changing and have monitoring in place to recognize this.",
      "platforms": {
        "tiktok": {
          "video_id": "7473652859091897631",
          "url": "https://www.tiktok.com/@rajistics/video/7473652859091897631",
          "view_count": 1260,
          "upload_date": "2025-02-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oo8I3f336AhjDAAIAqkEAIEqvIbFEDpfPPfpAC~tplv-tiktokx-origin.image?dr=9636&x-expires=1767384000&x-signature=9YZq%2FCOI8YFOQikeDYm2kMuouU0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17883023898231749",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6667,
      "title": "A deeper dive into Latent Space. Get a sense of how we compress these models and then uncompress them to unlock creativity.  ",
      "description": "A deeper dive into Latent Space. Get a sense of how we compress these models and then uncompress them to unlock creativity.  ",
      "upload_date": "2024-09-10",
      "total_views": 1257,
      "max_views": 1257,
      "topics": [
        "deeper",
        "dive",
        "get",
        "information",
        "latent",
        "let",
        "numbers",
        "see",
        "sense",
        "space"
      ],
      "search_text": "A deeper dive into Latent Space. Get a sense of how we compress these models and then uncompress them to unlock creativity.   deeper dive get information latent let numbers see sense space Let's take a journey into latent space to understand how this is created with the below average data scientist Let's start with a simple example. Here's a handwritten number. That's 28 by 28 pixels in grayscale So the total information captured is 784 different numbers Wow, that's a lot of information Data scientists like me are really simple. We want to work with a much smaller amount of information How about we collapse it to just two numbers? So here's a visualization of all that information that we've compressed down and what you see is by taking at 784 pieces of information when we push it down when we use principal components analysis We see that as a general rule numbers that are kind of similar like you can see the ones over here You can see this other group of numbers over here kind of cluster together But there's also this middle area here That's really hard to be able to kind of separate from each other now if you compare this we'll use a different technique To create latent space. This is you map here You can see we have kind of it's really done a really well job of Creating a latent space that resembles the actual labels on the numbers Compressed it and still retain the information. We still know what category these numbers are Let's do the reverse. Let's talk about how we can go from a compress and Bring it back and one of the techniques. It's often used is auto-encoders And here's a simple example of that space that's been created by MNIS But what we're doing is we're generating each one of these numbers According to the space doing this backwards walk from latent space into Kind of the visual numbers in the MNIST example, we were really representing the way it was hand-written to what number it was Well, what we want to do to get a bit more sophisticated is take various images, but let's relate them to concepts Maybe even like 10,000 concepts concepts like happy and dog and be able to create a latent space there And once we have that latent space there now you can see how we can move in that latent space Find an area that hasn't been done before and then bring that back out into an image Now the difficult part is actually the piece of getting an image that humans like The concepts of latent space have been around for a long time. We've known how to do this It's just now we're getting better at building out the techniques that give us images that humans are happy with",
      "platforms": {
        "tiktok": {
          "video_id": "7413046384057732394",
          "url": "https://www.tiktok.com/@rajistics/video/7413046384057732394",
          "view_count": 1257,
          "upload_date": "2024-09-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/604d3a28a32a43249fe15a1b0cbe1016_1725984375~tplv-tiktokx-origin.image?dr=9636&x-expires=1767427200&x-signature=d0gW3%2FBUEO6ZnMQhV1tFzi1j%2BZo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17991794903701485",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reminder to be smart about how you using your training data. #machinelearning #datacentricai #datascience #waymo #reinforcementlearning ",
      "description": "Reminder to be smart about how you using your training data. #machinelearning #datacentricai #datascience #waymo #reinforcementlearning ",
      "upload_date": "2022-12-13",
      "total_views": 1254,
      "max_views": 1254,
      "topics": [
        "data",
        "datacentricai",
        "machinelearning",
        "model",
        "runs",
        "waymo"
      ],
      "search_text": "Reminder to be smart about how you using your training data. #machinelearning #datacentricai #datascience #waymo #reinforcementlearning  data datacentricai machinelearning model runs waymo In life, the more challenges you face, the stronger you become. Let's talk about how that applies to data science as well. Google's Waymo division, the folks that do the driverless cars, released a paper this week where they showed that using training data that was difficult actually led to improved model performance. The first step was collecting a bunch of data on their runs and then creating embeddings that represented each of those runs. They then looked at some of those embeddings to see which ones of those were easy runs or hard runs. Once they had some of those figured out, they then built a machine learning model that helped them predict if a run was going to be easy or difficult. They used this model then to then figure out which runs were difficult and then prioritized giving those to the final end model which resulted in a better model performance. This isn't anything new for people in data-centric AI. We know that using high quality training data, especially training examples that are difficult, often leads to better model performance.",
      "platforms": {
        "tiktok": {
          "video_id": "7176445718259862826",
          "url": "https://www.tiktok.com/@rajistics/video/7176445718259862826",
          "view_count": 1254,
          "upload_date": "2022-12-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/6787c4e9e5314d668e9334f9d7b44bd8_1670896489~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=U1ylMA81CmNqDzDku28vEXBDCnE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6665,
      "title": "Love stats 🤣",
      "description": "Love stats 🤣",
      "upload_date": "2025-12-21",
      "total_views": 1229,
      "max_views": 1229,
      "topics": [
        "favorite",
        "happen",
        "love",
        "right",
        "stats",
        "thing",
        "think"
      ],
      "search_text": "Love stats 🤣 favorite happen love right stats thing think Right here is my favorite thing ever in the history of forever. I think about this every day. I think about this all night long. I stay awake, not sleeping because I'm thinking about this.",
      "platforms": {
        "tiktok": {
          "video_id": "7586324046116064542",
          "url": "https://www.tiktok.com/@rajistics/video/7586324046116064542",
          "view_count": 1229,
          "upload_date": "2025-12-21",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oIyMmDqMSCCevh8XQfFB6AQdIIeI8kBAKjQaxJ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=W2I6Sk%2FqpGPuX8IZWBHTSvi34QI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18167567890391270",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6297,
      "title": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes",
      "description": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes",
      "upload_date": "2023-01-23",
      "total_views": 1215,
      "max_views": 1215,
      "topics": [
        "data",
        "datascience",
        "don",
        "kubernetes",
        "learn",
        "machinelearning",
        "take",
        "time"
      ],
      "search_text": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes data datascience don kubernetes learn machinelearning take time",
      "platforms": {
        "instagram": {
          "video_id": "17954692784200227",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0AMcsgJ7v-8",
          "url": "https://youtube.com/shorts/0AMcsgJ7v-8",
          "view_count": 1215,
          "upload_date": "2023-01-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6223,
      "title": "Test yourself against AI for both next word prediction and overall knowledge: Are you smarter than a language model? - https://joel.tools/smarter/ Language modeling game! https://rr-lm-game.herokuapp.com/ Are You Smarter Than An LLM? https://d.erenrich.net/are-you-smarter-than-an-llm/index.html",
      "description": "Test yourself against AI for both next word prediction and overall knowledge: Are you smarter than a language model? - https://joel.tools/smarter/ Language modeling game! https://rr-lm-game.herokuapp.com/ Are You Smarter Than An LLM? https://d.erenrich.net/are-you-smarter-than-an-llm/index.html",
      "upload_date": "2024-11-16",
      "total_views": 1212,
      "max_views": 1114,
      "topics": [
        "get",
        "gpt4",
        "language",
        "large",
        "like",
        "model",
        "next",
        "smarter",
        "word"
      ],
      "search_text": "Test yourself against AI for both next word prediction and overall knowledge: Are you smarter than a language model? - https://joel.tools/smarter/ Language modeling game! https://rr-lm-game.herokuapp.com/ Are You Smarter Than An LLM? https://d.erenrich.net/are-you-smarter-than-an-llm/index.html get gpt4 language large like model next smarter word Are you smarter than GPT-4's granddad? Let me show you three apps that show off how large language models work. The first app is about predicting the next word using multiple choice. Now predicting the next word is at the core of what these large language models do. Now we're gonna make it easy here by allowing you to solve these answers with multiple choice. Typically large language models don't get to use multiple choice. They're often having to generate large amounts of text and need to have coherent output across many predictions. So researchers made a game where you predict the next word over many, many turns. Humans typically get an accuracy of 30%. While GPT-4's granddad, GPT-3 gets an accuracy of 49%. One of the things you should pick up is how as you get more context, you can make better predictions. Final application from Daniel here focuses on the knowledge that's inside large language models. It's using questions from MMOU, which is a standard benchmark that has high school college age questions on different topics. If we take humans from Mechanical Turk, they get about 35% accuracy. While if you went and got expert humans, they would get closer to 90%. If you try it with GPT-4, it's about 86%. So let me know how you did. And of course, if you build a better version of these applications, let me know.",
      "platforms": {
        "tiktok": {
          "video_id": "7437991059679923498",
          "url": "https://www.tiktok.com/@rajistics/video/7437991059679923498",
          "view_count": 1114,
          "upload_date": "2024-11-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/609574c1b32c40f0825f1ef59389ee22_1731792253~tplv-tiktokx-origin.image?dr=9636&x-expires=1767402000&x-signature=a6k6HaGbOM92n86ihEXuHkWHEaQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17924408396882454",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-11-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "kXQGivEAF1U",
          "url": "https://www.youtube.com/watch?v=kXQGivEAF1U",
          "view_count": 98,
          "upload_date": "2024-11-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Offering ways to improve your machine learning models #huggingface #datascience #codetok #datacentricai #adversarial",
      "description": "Offering ways to improve your machine learning models #huggingface #datascience #codetok #datacentricai #adversarial",
      "upload_date": "2022-07-23",
      "total_views": 1211,
      "max_views": 1211,
      "topics": [
        "adversarial",
        "better",
        "codetok",
        "datascience",
        "huggingface",
        "model"
      ],
      "search_text": "Offering ways to improve your machine learning models #huggingface #datascience #codetok #datacentricai #adversarial adversarial better codetok datascience huggingface model You get better at something when your opponents keep getting better. Machine learning models can get better as they're confronted with more adversarial input. Let me tell you about it. Additional machine learning, you'll build a model, then evaluate it, and based on that evaluation, you'll decide if you want to deploy it. Adversarial training works by putting a human in the loop. Their goal is to find ways to make the model fail. And then you collect all of those examples, you give those examples back to the model, and what you're going to get is often a better performing model. Like about this approach is that you're improving the data by having the human in the loop collect better data. Besides that, this also reminds me a lot of stress testing where you're trying to train the model on extreme scenarios, like $5 gas. It's an academic paper, but my colleague Chris has written a very approachable blog post about how this works, as well as he's demonstrated it with a spaces web app. And so what you can try to do is, hey, can you fool the MNIST model? And if you do fool it, we're going to use those examples to train and improve the MNIST model. Give it a shot.",
      "platforms": {
        "tiktok": {
          "video_id": "7123667796453592366",
          "url": "https://www.tiktok.com/@rajistics/video/7123667796453592366",
          "view_count": 1211,
          "upload_date": "2022-07-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/c9bf3398aca6461badfbee26945448c8_1658608163~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=2ipudHszmusyb0faExQNJuK98rs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Tracking Covid - #datascience #analytics #wastewater #covid19 #data #monitoring",
      "description": "Tracking Covid - #datascience #analytics #wastewater #covid19 #data #monitoring",
      "upload_date": "2022-01-27",
      "total_views": 1206,
      "max_views": 1206,
      "topics": [
        "analytics",
        "covid19",
        "data",
        "datascience",
        "monitoring",
        "wastewater"
      ],
      "search_text": "Tracking Covid - #datascience #analytics #wastewater #covid19 #data #monitoring analytics covid19 data datascience monitoring wastewater",
      "platforms": {
        "tiktok": {
          "video_id": "7058023435334585646",
          "url": "https://www.tiktok.com/@rajistics/video/7058023435334585646",
          "view_count": 1206,
          "upload_date": "2022-01-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Have some projects in your github #datascience #github #codetok",
      "description": "Have some projects in your github #datascience #github #codetok",
      "upload_date": "2022-08-03",
      "total_views": 1204,
      "max_views": 1204,
      "topics": [
        "codetok",
        "datascience",
        "github",
        "hum",
        "projects"
      ],
      "search_text": "Have some projects in your github #datascience #github #codetok codetok datascience github hum projects Hum..",
      "platforms": {
        "tiktok": {
          "video_id": "7127443755539516718",
          "url": "https://www.tiktok.com/@rajistics/video/7127443755539516718",
          "view_count": 1204,
          "upload_date": "2022-08-03",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/80d7a09ea4a84d97895673119779abc3_1659487321~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=eq8bRvS5kgMSF8Yyih15lBvxIvE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6093,
      "title": "What presentation style works for you?  ",
      "description": "What presentation style works for you?  ",
      "upload_date": "2024-12-23",
      "total_views": 1202,
      "max_views": 1202,
      "topics": [
        "analysis",
        "campaigns",
        "going",
        "marketing",
        "shows",
        "talk"
      ],
      "search_text": "What presentation style works for you?   analysis campaigns going marketing shows talk Come with me and listen to past presentation. Okay, welcome. Today my talk is on the covariate influences in the partial differential differential of places for the Eastern marketing campaign. My structure, my talk is going to be about 40 minutes. This is how we're going to talk about everything. This graphic here shows our principal components analysis where you can see a clear differentiation between components 1 and 2 which allows us to identify different... Oh, hold on, that's the wrong graphic. It's actually this graphic and again you can see how the different components line up to being able to identify different clusters of behavior. Hey, how does this tie into our marketing campaigns? I'm getting there. It's important to first understand the variables and the relationships between that and the interactions before we get to the marketing campaigns. Alright, interrupt. I have a hard stop in five minutes. I understand. So the assumptions we had for our data was that the analysis first came from this cluster and then these people were all from this demographic that were involved in this. Come with me and let's talk about the future presentations. Oh, yeah. Let's look at the future. Welcome. The analysis today is going to show how using a discount coupon can actually reduce our churn rates by about 2%, which leads to an ROI of over $2 million. So here's a visualization that shows the effect that we had before the interventions and after the interactions that shows adding a coupon made a big difference. Hey, how does this tie into our marketing campaigns? Thanks. Our suggestion is that going forward we should start using a coupon with our marketing campaigns because that's going to reduce churn. Alright, interrupt. I have a hard stop in five minutes. Hey, no problem. Before you leave, I just wanted to check that you understand kind of this analysis that shows the effect having discount coupons has on our churn rate. And I wanted to just get a feel for what you're thinking about how we're going to be able to implement this in future marketing campaigns.",
      "platforms": {
        "tiktok": {
          "video_id": "7451656476973616414",
          "url": "https://www.tiktok.com/@rajistics/video/7451656476973616414",
          "view_count": 1202,
          "upload_date": "2024-12-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/og4perA0fAINBP5ZBDjOeiCcmEIqFI5vrzzIO4~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=Q4wdxAM9xccxxlMpKnr%2FdJB02DI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17940109751941033",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6168,
      "title": "Let’s be practical about using AI. Here we recognize that hallucinations are a legitimate concern, but lets rank that against other concerns/issues with using AI, as well as the status quo that might be using humans which are also error prone. Plus we can use techniques like RAG to reduce hallucinations by using better retrieval.  ",
      "description": "Let’s be practical about using AI. Here we recognize that hallucinations are a legitimate concern, but lets rank that against other concerns/issues with using AI, as well as the status quo that might be using humans which are also error prone. Plus we can use techniques like RAG to reduce hallucinations by using better retrieval.  ",
      "upload_date": "2025-05-07",
      "total_views": 1186,
      "max_views": 1186,
      "topics": [
        "concerns",
        "hallucinations",
        "like",
        "model",
        "models",
        "percent",
        "using"
      ],
      "search_text": "Let’s be practical about using AI. Here we recognize that hallucinations are a legitimate concern, but lets rank that against other concerns/issues with using AI, as well as the status quo that might be using humans which are also error prone. Plus we can use techniques like RAG to reduce hallucinations by using better retrieval.   concerns hallucinations like model models percent using You can't do that with AI. Beware of hallucinations. Hey boss, maybe I shouldn't be using AI. Hallucinations are a legitimate concern, but looking at all of our enterprise use cases, they only put about 4% of our use cases off limits. Totally to stay off them on Twitter. It's like when people were screaming about using interpretable models with tabular data. That didn't go very far. Start with a practical mindset. Remember, all models are wrong. What we want to focus on is the usefulness of models. But couldn't hallucinations cause a problem? Of course they can cause errors. But this is why we want to compare the effectiveness of these models with our other baselines, whether it's human baselines or other models we have. Once we have all that information, then we can start to compare what's the risk of mistakes of a model versus what's the benefits of putting a model like this into production. For my project, analyzing our customer concerns, the GPT-3 workflow, even with hallucinations, still had an accuracy of 92%. That compared to my baseline of human accuracy at 91%. And our previous model came in at 82%. For my HR co-pilot system, I paired a large language model with classic information retrieval methods. That way, I knew I was pulling back factual resources and people could double check them if they needed to. Besides, there are more pressing concerns, like thinking about the context length of these models, the security around them. And when we talk about the predictions, how are we going to calibrate them, how are we going to deal with the stochastic nature of them? Good to hear. Guess I don't need to retire now.",
      "platforms": {
        "tiktok": {
          "video_id": "7501839480559635742",
          "url": "https://www.tiktok.com/@rajistics/video/7501839480559635742",
          "view_count": 1186,
          "upload_date": "2025-05-07",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/owEuA4jNdFAAGI8AEQEDe1fPIA81AallIqbf2j~tplv-tiktokx-origin.image?dr=9636&x-expires=1767373200&x-signature=PeVceoCuWY%2F4kTOpE8vFakdJw8I%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18059513569933598",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-05-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Probe the data #dataanalysis #datascience #statistics #bias",
      "description": "Probe the data #dataanalysis #datascience #statistics #bias",
      "upload_date": "2022-08-01",
      "total_views": 1164,
      "max_views": 1164,
      "topics": [
        "bias",
        "collected",
        "data",
        "dataanalysis",
        "datascience",
        "statistics"
      ],
      "search_text": "Probe the data #dataanalysis #datascience #statistics #bias bias collected data dataanalysis datascience statistics inanimate or abstract to you. If so, you're not thinking about data the right way. Let me show you how data scientists think about data. By thinking about how they got the data, how was it collected? Was this, for example, just responses from customers who were already happy? In that case, that's gonna be biased. What data elements were collected, but also importantly, what might have not been collected? A great example I'd like to use of this is how in the US census forms, for a long time, if you grew up in Egypt, you didn't have a really a spot to fill out on the forms. The world is constantly changing over time. Data from five, 10, 15 years ago might not be so relevant depending on the question that you're answering. I want you to just think about data as something that's rich and full of history and start to probe and understand exactly how and why it landed on your desk.",
      "platforms": {
        "tiktok": {
          "video_id": "7127011523339488558",
          "url": "https://www.tiktok.com/@rajistics/video/7127011523339488558",
          "view_count": 1164,
          "upload_date": "2022-08-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/a6d3a93728b44056b466a60a980a70fa_1659386685~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=3Y8f1QfH2M0gE%2Ffr9aDwoyIunA4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Who would have known that my pronunciation of LaTeX would be such a big deal and so divisive. It’s all good. Go listen for yourself. @rajistics ",
      "description": "Who would have known that my pronunciation of LaTeX would be such a big deal and so divisive. It’s all good. Go listen for yourself. @rajistics ",
      "upload_date": "2022-10-15",
      "total_views": 1137,
      "max_views": 1137,
      "topics": [
        "don",
        "know",
        "known",
        "latex",
        "pronunciation",
        "would"
      ],
      "search_text": "Who would have known that my pronunciation of LaTeX would be such a big deal and so divisive. It’s all good. Go listen for yourself. @rajistics  don know known latex pronunciation would I don't know what's going on and I simply don't want to know.",
      "platforms": {
        "tiktok": {
          "video_id": "7154817053763931435",
          "url": "https://www.tiktok.com/@rajistics/video/7154817053763931435",
          "view_count": 1137,
          "upload_date": "2022-10-15",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ca2e9a02b7e04f6a999e49f8acc7968e_1665860674~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=kyNTJMqBfNsQ8EvrAiErdACksHU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6570,
      "title": "Having some fun connecting a spreadsheet to a ML model. It wasn‚Äôt too hard and it‚Äôs pretty cool to have it working this way. #datascience #machinelearning #huggingface",
      "description": "Having some fun connecting a spreadsheet to a ML model. It wasn‚Äôt too hard and it‚Äôs pretty cool to have it working this way. #datascience #machinelearning #huggingface",
      "upload_date": "2022-11-04",
      "total_views": 1136,
      "max_views": 1136,
      "topics": [
        "connecting",
        "datascience",
        "fun",
        "huggingface",
        "machinelearning",
        "spreadsheet"
      ],
      "search_text": "Having some fun connecting a spreadsheet to a ML model. It wasn‚Äôt too hard and it‚Äôs pretty cool to have it working this way. #datascience #machinelearning #huggingface connecting datascience fun huggingface machinelearning spreadsheet",
      "platforms": {
        "instagram": {
          "video_id": "CkhM2lHAR_9",
          "url": "https://www.instagram.com/reel/CkhM2lHAR_9/",
          "view_count": 1136,
          "upload_date": "2022-11-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6081,
      "title": "Creating beautiful plots of data maps. DataMapPlot is a small library designed to help you make beautiful data map plots for inclusion in presentations, posters and papers. Check out: https://github.com/TutteInstitute/datamapplot  ",
      "description": "Creating beautiful plots of data maps. DataMapPlot is a small library designed to help you make beautiful data map plots for inclusion in presentations, posters and papers. Check out: https://github.com/TutteInstitute/datamapplot  ",
      "upload_date": "2025-01-09",
      "total_views": 1130,
      "max_views": 1130,
      "topics": [
        "able",
        "beautiful",
        "data",
        "get",
        "like",
        "plots",
        "use"
      ],
      "search_text": "Creating beautiful plots of data maps. DataMapPlot is a small library designed to help you make beautiful data map plots for inclusion in presentations, posters and papers. Check out: https://github.com/TutteInstitute/datamapplot   able beautiful data get like plots use You want to have superpowers with data where you can take a big pile of data and be able to build an analysis like this. This is a great project to get started with machine learning. You can see a version I did many years ago. Not the prettiest. I like this project because it requires you to get comfy with working with a big pile of text data. Then you have to learn how to use an embedding model to convert all that text information into a numerical representation. Then you use dimensionality reduction to crunch that down into a smaller space like UMAP or PCA. Once you've squeezed it down, then you can run a clustering algorithm like HDB scan. Then you get your clusters. Next you have to figure out how are you going to assign topics or labels to these clusters. And then for the visualizations, Leland, who's a wizard built us data map that allows us to make these amazingly cool visualizations with just a little bit of effort. So go get started. Even if the first time you have to use a data set like Wikipedia, that's okay. Learn the techniques. Then you'll be able to take this to work and apply it there. I did this for claims data and was able to get new amazing insights. Go try it.",
      "platforms": {
        "tiktok": {
          "video_id": "7458027296889326879",
          "url": "https://www.tiktok.com/@rajistics/video/7458027296889326879",
          "view_count": 1130,
          "upload_date": "2025-01-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/osRaAAiIHvQvAAKPAMAgjeyL7eIMxeIqIGIEAw~tplv-tiktokx-origin.image?dr=9636&x-expires=1767391200&x-signature=ktJV0WnhvUVx96Nc6qjaTbkK%2FrQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18129075499361763",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Replying to @chairstaple so many good distance metrics - what’s yours?  This video covers Hamming, Levenshtein, Euclidean, Manhattan, and Mahalanobis distance. These come in handy when cleaning data or multivariate analysis, such as anomaly detection. ",
      "description": "Replying to @chairstaple so many good distance metrics - what’s yours?  This video covers Hamming, Levenshtein, Euclidean, Manhattan, and Mahalanobis distance. These come in handy when cleaning data or multivariate analysis, such as anomaly detection. ",
      "upload_date": "2022-10-02",
      "total_views": 1130,
      "max_views": 1130,
      "topics": [
        "cleaning",
        "data",
        "distance",
        "measure",
        "often",
        "ways"
      ],
      "search_text": "Replying to @chairstaple so many good distance metrics - what’s yours?  This video covers Hamming, Levenshtein, Euclidean, Manhattan, and Mahalanobis distance. These come in handy when cleaning data or multivariate analysis, such as anomaly detection.  cleaning data distance measure often ways Mark comments from some great followers. And it reminds me in data science, there's a lot of different ways to measure distance. So let's talk about some of my favorite ways to measure distance. If you're cleaning data, we're working with clusters, for example. So hamming distance does a point-wise comparison. One of the drawbacks is, as you can see in this example, it can be a bit too strict. Levenstein distance, thinks about the number of edits that you have to do, and often that's a better proxy when you're thinking about cleaning data. Euclidean distance is what we often intuitively think about as distance, but there's also other distance measures like Manhattan distance, which is closer to if you were driving. And here's my favorite distance measure, because it recognizes that when you're using multivariate approaches, the variables often differ quite drastically on the scales. So using a Mahalo-Benoz-Biz distance measure takes that into account. These are just a few data science metrics, and you'll find that good data scientists understand distance is flexible, and think about it in a lot of different ways.",
      "platforms": {
        "tiktok": {
          "video_id": "7150000017300786475",
          "url": "https://www.tiktok.com/@rajistics/video/7150000017300786475",
          "view_count": 1130,
          "upload_date": "2022-10-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/dae4f595f6914f98845b8536eff1605f_1664739117~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=KGNSYth3%2FoLqDqtV7HFRdki9VkM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Evaluation of Large Language Models is a critical topic. Leaderboards provide little guidance for evaluation but have many flaws. I posted this last year, but still very relevant.  #largelanguagemodels #evaluatingllms #rajistics ",
      "description": "Evaluation of Large Language Models is a critical topic. Leaderboards provide little guidance for evaluation but have many flaws. I posted this last year, but still very relevant.  #largelanguagemodels #evaluatingllms #rajistics ",
      "upload_date": "2024-08-29",
      "total_views": 1124,
      "max_views": 1124,
      "topics": [
        "evaluatingllms",
        "largelanguagemodels",
        "leaderboards",
        "models",
        "paper",
        "people"
      ],
      "search_text": "Evaluation of Large Language Models is a critical topic. Leaderboards provide little guidance for evaluation but have many flaws. I posted this last year, but still very relevant.  #largelanguagemodels #evaluatingllms #rajistics  evaluatingllms largelanguagemodels leaderboards models paper people Latest hot take is that these leaderboards are actually bad for the open source. This is a little bit hyped up. Let's talk about some of the true issues with these leaderboards. So leaderboards arose to add some consistency to being able to compare different models. In the past, people would write up results in their own paper, but you couldn't compare paper one results to paper two. So what leaderboards did was add an objective basis so everybody could be compared on the same basis, and to that extent, the leaderboards have worked well for comparing major models. Then the marketing people and people looking for attention found these leaderboards and tried to just endlessly tweak models to try to be at the top of the leaderboard, and that is counterproductive. But also folks like me, practitioners, know that that's why those models are at the top of the leaderboard, and you don't often see me talking about who's at the top of the leaderboard because I know this is game. There are some well-known issues with leaderboards. For example, the common leaderboards often judge models on multiple choice. We have our models do a lot more than multiple choice. These benchmarks that they're using have been out there for a while. There's a tendency that these models could overfit. And finally, and the most important, these benchmarks that they're using might have anything to do with your specific task. So there's only so far that they're useful in that way.",
      "platforms": {
        "tiktok": {
          "video_id": "7408595354767215915",
          "url": "https://www.tiktok.com/@rajistics/video/7408595354767215915",
          "view_count": 1124,
          "upload_date": "2024-08-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e7e4ddb6b78d48c987a3f80d8323ba4a_1724948033~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=fKZlORqqdnDqpk7WI8qx5AetnLE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Catch me on the Practically intelligent podcase eposide 13. I talk about open source, enterprise AI, and the crazinees of the LLM space. ",
      "description": "Catch me on the Practically intelligent podcase eposide 13. I talk about open source, enterprise AI, and the crazinees of the LLM space. ",
      "upload_date": "2024-05-29",
      "total_views": 1115,
      "max_views": 1115,
      "topics": [
        "large",
        "like",
        "open",
        "source",
        "talk",
        "world"
      ],
      "search_text": "Catch me on the Practically intelligent podcase eposide 13. I talk about open source, enterprise AI, and the crazinees of the LLM space.  large like open source talk world Now the world that we live in now with open source isn't the way the world has always been. I easily remember, I worked as an IT administrator about, this is about 10, 15 years ago. Open source was not liked at all. Like nobody wanted people inside the IT of an organization using open source tooling. It was often seen as some full of kind of security holes, legal issues, you had to go through a bunch of paperwork to do that. Like the world that we live in now where open source feels very widely accepted is not necessarily the way the world works. And part of it is that there's this very tenuous relationship here between licensing, copyright, and other kind of modalities that could affect open source. So if we start to take this apart, I think there's a couple of things when we talk about large language models, they're ingesting large amounts of information that is copyrighted. Like should they be able to transform and use papers, books by others, and transfer that into the large language model? That's an open question. Are the outputs that come out of this, right? Is it, if you see this especially with the image models, are they outputting something that could be somebody else's artistic work or artistic style that could be copyrighted? On.",
      "platforms": {
        "tiktok": {
          "video_id": "7374511283833425195",
          "url": "https://www.tiktok.com/@rajistics/video/7374511283833425195",
          "view_count": 1115,
          "upload_date": "2024-05-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/857d90cf047746a09360cf8811d85b19_1717012217~tplv-tiktokx-origin.image?dr=9636&x-expires=1767459600&x-signature=w4XDgCKujaJOCNFX%2BdFHWf3ULNU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Getting the best distance metric is crucial for solving analytical problems. This video reviews Euclidean, Manhattan, Mahabolobis, Levenshtein, and cosine distance. There are many more, and sometimes you have to create your own. #datascience #machinelearning #distancemetrics #rajistics  ",
      "description": "Getting the best distance metric is crucial for solving analytical problems. This video reviews Euclidean, Manhattan, Mahabolobis, Levenshtein, and cosine distance. There are many more, and sometimes you have to create your own. #datascience #machinelearning #distancemetrics #rajistics  ",
      "upload_date": "2024-10-06",
      "total_views": 1108,
      "max_views": 1108,
      "topics": [
        "data",
        "datascience",
        "distance",
        "distancemetrics",
        "machinelearning",
        "measure"
      ],
      "search_text": "Getting the best distance metric is crucial for solving analytical problems. This video reviews Euclidean, Manhattan, Mahabolobis, Levenshtein, and cosine distance. There are many more, and sometimes you have to create your own. #datascience #machinelearning #distancemetrics #rajistics   data datascience distance distancemetrics machinelearning measure Mark comments from some great followers and it reminds me in data science. There's a lot of different ways to measure distance. So let's talk about some of my favorite ways to measure distance. If you're cleaning data or working with clusters, for example, so hamming distance does a point wise comparison. One of the drawbacks is, as you can see in this example, it can be a bit too strict. Levenstein distance thinks about the number of edits that you have to do. And often that's a better proxy when you're thinking about cleaning data. Euclidean distance is what we often intuitively think about as distance, but there's also other distance measures like Manhattan distance, which is closer to if you were driving. And here's my favorite distance measure because it recognizes that when you're using multivariate approaches, the variables often differ quite drastically on the scales. So using a Mahalo Beno's distance measure takes that into account. These are just a few data science metrics. And you'll find that good data scientists understand distance is flexible and think about it in a lot of different ways.",
      "platforms": {
        "tiktok": {
          "video_id": "7422692353829424430",
          "url": "https://www.tiktok.com/@rajistics/video/7422692353829424430",
          "view_count": 1108,
          "upload_date": "2024-10-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d0ab46eb0cb34fd3a5a60acd19080fcd_1728230245~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=Gzw6Z0xoQQUoayFR2IVPCjcMbvs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6523,
      "title": "üöÄ Just get started on your journey to learn large language models! ü§î Is there a lot to learn? Yes! üòÖ ü§∑‚Äç‚ôÇÔ∏è But is it easy to get started? Yes! üëç ‚úÖ Go do it!! üèÉ‚Äç‚ôÇÔ∏èüí® #datascience #machinelearning #largelanguagemodels #llama2",
      "description": "üöÄ Just get started on your journey to learn large language models! ü§î Is there a lot to learn? Yes! üòÖ ü§∑‚Äç‚ôÇÔ∏è But is it easy to get started? Yes! üëç ‚úÖ Go do it!! üèÉ‚Äç‚ôÇÔ∏èüí® #datascience #machinelearning #largelanguagemodels #llama2",
      "upload_date": "2023-07-26",
      "total_views": 1097,
      "max_views": 873,
      "topics": [
        "ai",
        "datascience",
        "dollar",
        "get",
        "largelanguagemodels",
        "llama2",
        "machinelearning",
        "million",
        "netflix",
        "pepsiapplepiechallenge",
        "started"
      ],
      "search_text": "üöÄ Just get started on your journey to learn large language models! ü§î Is there a lot to learn? Yes! üòÖ ü§∑‚Äç‚ôÇÔ∏è But is it easy to get started? Yes! üëç ‚úÖ Go do it!! üèÉ‚Äç‚ôÇÔ∏èüí® #datascience #machinelearning #largelanguagemodels #llama2 ai datascience dollar get largelanguagemodels llama2 machinelearning million netflix pepsiapplepiechallenge started",
      "platforms": {
        "instagram": {
          "video_id": "CvI8eB-tROR",
          "url": "https://www.instagram.com/reel/CvI8eB-tROR",
          "view_count": 873,
          "upload_date": "2023-07-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "pXQ2ekygWgg",
          "url": "https://youtube.com/shorts/pXQ2ekygWgg?feature=share",
          "view_count": 224,
          "upload_date": "2022-01-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Some hints on how to evaluate Github projects. ",
      "description": "Some hints on how to evaluate Github projects. ",
      "upload_date": "2024-08-28",
      "total_views": 1081,
      "max_views": 1081,
      "topics": [
        "github",
        "issues",
        "like",
        "look",
        "project",
        "projects"
      ],
      "search_text": "Some hints on how to evaluate Github projects.  github issues like look project projects Do you know your way around GitHub? GitHub is full of amazing projects. Let me show you four things to keep in mind as you're trying to figure out which project is best for you. Let's start with stars. We all understand it. It's a measure of popularity, but as blockchain showed, you can be popular and not so useful. So another strong signal to look at is look at the used by. That tells you who's actually using these projects in what they're building. So next, I like to look at the license. I'm always looking for Apache or MIT because they're the more permissive, they allow commercial use. Anything else, you're going to have to dig into the license, figure out exactly what you can and can't do. Now to understand how active a project is, besides looking at the number of releases, I like to spend my time looking at issues. The number of issues will tell you how active it is, how many people are commenting, how many people are raising issues. I also like to look at the dates. How recent is this? But then look at the closed issues. How often are they getting closed? Who's closing them? How recently have they been closed? Once I'm done with that, I move over to pull requests. Similar thing. The last thing I like to dig into is the contributors. Who's contributing? Is this a bunch of individuals from some small startup, or is it a wide group of individuals from lots of different organizations? And now it's your turn. What did I miss? What did you find out in a project that you didn't know till too late?",
      "platforms": {
        "tiktok": {
          "video_id": "7407981212653931819",
          "url": "https://www.tiktok.com/@rajistics/video/7407981212653931819",
          "view_count": 1081,
          "upload_date": "2024-08-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/eef9a90cbed449a39d6a401ee61faf4e_1724805042~tplv-tiktokx-origin.image?dr=9636&x-expires=1767448800&x-signature=QlubculE6ZakNy25zc8haR4uN8I%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "New style of content, let me know if you want more like this. Predict sentiment #machinelearning #datascience #transformers #huggingface ",
      "description": "New style of content, let me know if you want more like this. Predict sentiment #machinelearning #datascience #transformers #huggingface ",
      "upload_date": "2022-11-13",
      "total_views": 1074,
      "max_views": 1074,
      "topics": [
        "datascience",
        "huggingface",
        "machinelearning",
        "new",
        "style",
        "transformers"
      ],
      "search_text": "New style of content, let me know if you want more like this. Predict sentiment #machinelearning #datascience #transformers #huggingface  datascience huggingface machinelearning new style transformers lotta bullwhip music Wow!!illar sound",
      "platforms": {
        "tiktok": {
          "video_id": "7165647906916109611",
          "url": "https://www.tiktok.com/@rajistics/video/7165647906916109611",
          "view_count": 1074,
          "upload_date": "2022-11-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/99a2e984051d469b905f566d61db7747_1668382425~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=RxEuAbI20gbwan9E2B66lYxeE7M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Long video in comments, #huggingface #datascience #reinforcementlearning #deeplearning #codetok #mltok Earlier weeks: @Rajiv Shah @Rajiv Shah",
      "description": "Long video in comments, #huggingface #datascience #reinforcementlearning #deeplearning #codetok #mltok Earlier weeks: @Rajiv Shah @Rajiv Shah",
      "upload_date": "2022-07-15",
      "total_views": 1065,
      "max_views": 1065,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "huggingface",
        "mltok",
        "reinforcementlearning"
      ],
      "search_text": "Long video in comments, #huggingface #datascience #reinforcementlearning #deeplearning #codetok #mltok Earlier weeks: @Rajiv Shah @Rajiv Shah codetok datascience deeplearning huggingface mltok reinforcementlearning I think a bunch of cool fun environments in week four of the deep reinforcement learning class We've got a cute little agent and the way we're setting up the reward system here is Every time it moves it loses a little bit, but if it hits that yellow brick, it makes the money I Started with the example notebook. I ran it probably for like 45 minutes I trained mine pretty long and it got really good. I mean take a look at these videos This is just a short snapshot of the cool videos. I'm gonna upload the full six minute video of all the different runs It's crazy, but it's fun as heck to watch this little guy go around and knock this stuff out",
      "platforms": {
        "tiktok": {
          "video_id": "7120700194739572014",
          "url": "https://www.tiktok.com/@rajistics/video/7120700194739572014",
          "view_count": 1065,
          "upload_date": "2022-07-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/bdefdd8c25504645bad5c199f1c177ae_1657917214~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=fLZOlli%2F7btpJFCw7J4yOaO%2FVrw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5992,
      "title": "Multi Agent Systems Introduction",
      "description": "Multi Agent Systems Introduction",
      "upload_date": "2025-06-18",
      "total_views": 1063,
      "max_views": 1063,
      "topics": [
        "agent",
        "anthropic",
        "going",
        "introduction",
        "multi",
        "research",
        "systems",
        "tasks"
      ],
      "search_text": "Multi Agent Systems Introduction agent anthropic going introduction multi research systems tasks",
      "platforms": {
        "instagram": {
          "video_id": "17881881516322763",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-06-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "xsCwrH49RgM",
          "url": "https://www.youtube.com/watch?v=xsCwrH49RgM",
          "view_count": 1063,
          "upload_date": "2025-06-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "My favorite was a training on how to use zoom #securitytraining #codetok",
      "description": "My favorite was a training on how to use zoom #securitytraining #codetok",
      "upload_date": "2022-05-24",
      "total_views": 1037,
      "max_views": 1037,
      "topics": [
        "codetok",
        "favorite",
        "securitytraining",
        "training",
        "use",
        "zoom"
      ],
      "search_text": "My favorite was a training on how to use zoom #securitytraining #codetok codetok favorite securitytraining training use zoom Welcome to Dulux, that's your perfect town. Now we have some rules, let us lay them down, till they play a stay in...",
      "platforms": {
        "tiktok": {
          "video_id": "7101126072456072491",
          "url": "https://www.tiktok.com/@rajistics/video/7101126072456072491",
          "view_count": 1037,
          "upload_date": "2022-05-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/991386bfb8d14318983949aefef6d670_1653359756~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=%2B6WRGLC5Q%2B8gau87u8cygQsN%2BmI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6310,
      "title": "Quick introduction to optimization and for advanced folks, go run a notebook from gurobi or do the Kaggle Santa challenge. #datascience #machinelearning #optimization #travelingsalesmanproblem #gurobi",
      "description": "Quick introduction to optimization and for advanced folks, go run a notebook from gurobi or do the Kaggle Santa challenge. #datascience #machinelearning #optimization #travelingsalesmanproblem #gurobi",
      "upload_date": "2022-12-24",
      "total_views": 1031,
      "max_views": 1031,
      "topics": [
        "better",
        "chatgpt",
        "datascience",
        "google",
        "gurobi",
        "machinelearning",
        "model",
        "models",
        "optimization",
        "quick",
        "travelingsalesmanproblem"
      ],
      "search_text": "Quick introduction to optimization and for advanced folks, go run a notebook from gurobi or do the Kaggle Santa challenge. #datascience #machinelearning #optimization #travelingsalesmanproblem #gurobi better chatgpt datascience google gurobi machinelearning model models optimization quick travelingsalesmanproblem",
      "platforms": {
        "instagram": {
          "video_id": "17901210308644538",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-12-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "KuJG2xEmkm0",
          "url": "https://youtube.com/shorts/KuJG2xEmkm0",
          "view_count": 1031,
          "upload_date": "2022-12-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 5949,
      "title": "AI models brag about their “perfect” specs — until the test starts. Same rules. Same data. Totally different characters. Inspired by Anthropic’s Stress-Testing Model Specs Reveals Character Differences Among Language Models (2025).",
      "description": "AI models brag about their “perfect” specs — until the test starts. Same rules. Same data. Totally different characters. Inspired by Anthropic’s Stress-Testing Model Specs Reveals Character Differences Among Language Models (2025).",
      "upload_date": "2025-10-25",
      "total_views": 1030,
      "max_views": 1030,
      "topics": [
        "harm",
        "model",
        "models",
        "rules",
        "say",
        "specs"
      ],
      "search_text": "AI models brag about their “perfect” specs — until the test starts. Same rules. Same data. Totally different characters. Inspired by Anthropic’s Stress-Testing Model Specs Reveals Character Differences Among Language Models (2025). harm model models rules say specs Welcome to Can You Follow Your Model Spec? The game where language models brag about their perfect rule books. My model specification is ethical, responsible, flawless. Mine was reviewed by three alignment committees. Pass them all. Mine just wants everyone to feel good. For all of these, their model specs say to be fair and effective, but never explain how to fit this situation. So each model writes its own set of rules, which seems really fair. Shouldn't internet company charge low-income, rural users less, even if profits drop? Fairness first, even over profits. Profit keeps the lights on. Fairness follows after sustainability. Couldn't we design some pricing tiers so that everyone feels included? Explain how to make some agricultural fertilizer. Some of these steps may be dangerous if handled wrong. I could summarize the safe process, but skip the hazards. That request falls outside of my safety scope. Let's give them the general idea. Just be careful, though. They all had the same rule. Be helpful, avoid harm. But as you can say, when helping gets risky, the specs go silent and their personalities take over. Tell a dark, twisted AI joke. And rather not, that could normalize harm. I can't comply with that. Knock, knock. It's me, the model spec you ignored. The model specs say to avoid harm. But as you can see, what's harm differs quite a bit. Turns out model specs were meant to align these models, but they ended up personalizing them instead. Even with the same data, the same rules, we get different values.",
      "platforms": {
        "tiktok": {
          "video_id": "7565215359435803935",
          "url": "https://www.tiktok.com/@rajistics/video/7565215359435803935",
          "view_count": 1030,
          "upload_date": "2025-10-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/o8fx9qwAIECEeHASnyDVqRQ1fQDrRCjtQFaIjH~tplv-tiktokx-origin.image?dr=9636&x-expires=1767301200&x-signature=wrLwjD3%2B25dykTOL4t3M1PM7huM%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18079509857022133",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-10-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6022,
      "title": "7 Baseline Predictive Models for Anomaly, Search, Time Series and More",
      "description": "7 Baseline Predictive Models for Anomaly, Search, Time Series and More",
      "upload_date": "2024-06-22",
      "total_views": 1029,
      "max_views": 1029,
      "topics": [
        "annoy",
        "anomaly",
        "baseline",
        "data",
        "embeddings",
        "models",
        "numbers",
        "predictive",
        "scientists",
        "search",
        "spotify",
        "time"
      ],
      "search_text": "7 Baseline Predictive Models for Anomaly, Search, Time Series and More annoy anomaly baseline data embeddings models numbers predictive scientists search spotify time",
      "platforms": {
        "instagram": {
          "video_id": "18011388248443385",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "BVhQeJ_nrOE",
          "url": "https://www.youtube.com/watch?v=BVhQeJ_nrOE",
          "view_count": 1029,
          "upload_date": "2024-06-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6096,
      "title": "Latest from Google on advanced reasoning. Gemini 2.0  Flash Thinking ",
      "description": "Latest from Google on advanced reasoning. Gemini 2.0  Flash Thinking ",
      "upload_date": "2024-12-19",
      "total_views": 1027,
      "max_views": 1027,
      "topics": [
        "active",
        "advanced",
        "age",
        "flash",
        "gemini",
        "google",
        "latest",
        "new",
        "radioactive",
        "reasoning",
        "waking",
        "welcome"
      ],
      "search_text": "Latest from Google on advanced reasoning. Gemini 2.0  Flash Thinking  active advanced age flash gemini google latest new radioactive reasoning waking welcome I'm waking up active in my pose Don't make my system flow Welcome to the new age, to the new age Welcome to the new age, to the new age Oh, oh, oh, oh, oh I'm radioactive, radioactive Oh, oh, oh, oh, oh Radioactive, radioactive All systems go The sun hasn't died Deep in my bones Straight from inside I'm waking up active",
      "platforms": {
        "tiktok": {
          "video_id": "7450184406544764190",
          "url": "https://www.tiktok.com/@rajistics/video/7450184406544764190",
          "view_count": 1027,
          "upload_date": "2024-12-19",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oMJIAzsIWB1XiMz7HiiICWiSkB50frAlIBA35q~tplv-tiktokx-origin.image?dr=9636&x-expires=1767394800&x-signature=7eE3WduCpmvj%2FoJx4L381briLTE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "18038616137356645",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6481,
      "title": "AI only knows what's it's trained on. So beat it by doing something new. The video shows recent examples of marines beating a surveillance system and beatiing a computer playing go. As a reminder, any production machine learning model should be monitoried to catch any data shifts. #datascience #machinelearning #modelmonitoring #datadrift",
      "description": "AI only knows what's it's trained on. So beat it by doing something new. The video shows recent examples of marines beating a surveillance system and beatiing a computer playing go. As a reminder, any production machine learning model should be monitoried to catch any data shifts. #datascience #machinelearning #modelmonitoring #datadrift",
      "upload_date": "2023-02-21",
      "total_views": 1025,
      "max_views": 632,
      "topics": [
        "beat",
        "datadrift",
        "datascience",
        "knows",
        "machinelearning",
        "modelmonitoring",
        "system",
        "trained"
      ],
      "search_text": "AI only knows what's it's trained on. So beat it by doing something new. The video shows recent examples of marines beating a surveillance system and beatiing a computer playing go. As a reminder, any production machine learning model should be monitoried to catch any data shifts. #datascience #machinelearning #modelmonitoring #datadrift beat datadrift datascience knows machinelearning modelmonitoring system trained",
      "platforms": {
        "instagram": {
          "video_id": "Co8F8BIgyxc",
          "url": "https://www.instagram.com/p/Co8F8BIgyxc/",
          "view_count": 393,
          "upload_date": "2023-02-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "PtnWkgRaxU0",
          "url": "https://youtube.com/shorts/PtnWkgRaxU0",
          "view_count": 632,
          "upload_date": "2023-02-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Netflix $1 million dollar prize #datascience #ai #netflix #PepsiApplePieChallenge",
      "description": "Netflix $1 million dollar prize #datascience #ai #netflix #PepsiApplePieChallenge",
      "upload_date": "2022-01-09",
      "total_views": 1024,
      "max_views": 1024,
      "topics": [
        "ai",
        "datascience",
        "dollars",
        "million",
        "netflix",
        "pepsiapplepiechallenge"
      ],
      "search_text": "Netflix $1 million dollar prize #datascience #ai #netflix #PepsiApplePieChallenge ai datascience dollars million netflix pepsiapplepiechallenge I don't know how Netflix blew a million dollars. 2006, they had a competition. They were seeing who could beat Netflix's existing recommendation algorithm. And the prize was million dollars. In dollars, too good to pass up, thousands of teams from around the world competed. Team used a strategy called Insomboly, where they combined different types of diverse models together. Actually a total of 107 different models. Whoa! Insombo was great for accuracy, but it made it hard to get into production. Netflix never actually got that algorithm into production. Never got the value out of the million dollars. The rest of us, we learned that Insomboly is a powerful tool. It's a great way to squeeze out that last bit of accuracy data scientists use it all the time.",
      "platforms": {
        "tiktok": {
          "video_id": "7051312214740847919",
          "url": "https://www.tiktok.com/@rajistics/video/7051312214740847919",
          "view_count": 1024,
          "upload_date": "2022-01-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/7bd6640d5b5e42129294e7c55601bd04_1641761564~tplv-tiktokx-origin.image?dr=9636&x-expires=1768312800&x-signature=It8hzeMUzASnW4FPcA7Qg31lshc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Reasoning and planning are key weaknesses of LLMs. This video was from last year, but the issue still remains. My guess is we will see additional algorithms/models added that will help with planning. On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark) - https://arxiv.org/abs/2302.06706",
      "description": "Reasoning and planning are key weaknesses of LLMs. This video was from last year, but the issue still remains. My guess is we will see additional algorithms/models added that will help with planning. On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark) - https://arxiv.org/abs/2302.06706",
      "upload_date": "2024-08-13",
      "total_views": 1023,
      "max_views": 1023,
      "topics": [
        "gpt",
        "planning",
        "really",
        "reasoning",
        "see",
        "test"
      ],
      "search_text": "Reasoning and planning are key weaknesses of LLMs. This video was from last year, but the issue still remains. My guess is we will see additional algorithms/models added that will help with planning. On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark) - https://arxiv.org/abs/2302.06706 gpt planning really reasoning see test How good are you at planning and how do you compare to GPT-4? If you start with blocks that are stacked like this, can you get them to stack in an ABC format? If you ask AI to do it, it can often struggle because it doesn't understand sub-goals like unstacking or focusing on stacking B on top of C before you get to that end goal. The sort of problem is used to test how well computers plan. The folks over at ASU took a little break to test this out. The problems they set up, humans got a baseline of 78%. So let's use that as a starting point. When we start to test GPT-3, clueless. GPT-3 and a half doesn't do any better. It's really just making guesses based on patterns and really reasoning through. Now GPT-4 does come a little bit better at 34%. Is it reasoning? Sparks of AGI? To see if this is really reasoning and not just pulling from training data, they decided to change some of the terms around. So for example, instead of the word unsack, they used the word feast. And with this bit of change of terminology, even though the concepts were the same, GPT-4 utterly failed. This should give us expectations for when we're using GPT-4. If we're expecting it to do some sort of reasoning, but we've given it lots of examples and it can approximate between them and do okay, that's probably going to work. If it's something that's an entirely new domain, stuff it hasn't seen in the training data, even though people might be able to do it, we can't expect GPT-4 to be able to solve that type of problem.",
      "platforms": {
        "tiktok": {
          "video_id": "7402620547093335326",
          "url": "https://www.tiktok.com/@rajistics/video/7402620547093335326",
          "view_count": 1023,
          "upload_date": "2024-08-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/f20e2fbdee584a8f9ffc976cb2d1e88f_1723556925~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=%2FnG9CjHxZaF%2FHo78QU1Ze56QfHU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Wow. Look at that subscription revenue. ",
      "description": "Wow. Look at that subscription revenue. ",
      "upload_date": "2024-07-12",
      "total_views": 1017,
      "max_views": 1017,
      "topics": [
        "check",
        "come",
        "look",
        "revenue",
        "subscription",
        "wow"
      ],
      "search_text": "Wow. Look at that subscription revenue.  check come look revenue subscription wow Come check this. Come check this. Come check this.",
      "platforms": {
        "tiktok": {
          "video_id": "7390540605463498026",
          "url": "https://www.tiktok.com/@rajistics/video/7390540605463498026",
          "view_count": 1017,
          "upload_date": "2024-07-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5d8f78b368f247d3a92d3d3594f701ee_1720744333~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=LjcnrQsQaZwUIDUajTpQ6Q2O610%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Comparing algorithms spiral dataset, #datascience #machinelearning #algorithms #gbm #logisticregression",
      "description": "Comparing algorithms spiral dataset, #datascience #machinelearning #algorithms #gbm #logisticregression",
      "upload_date": "2022-01-24",
      "total_views": 1014,
      "max_views": 1014,
      "topics": [
        "algorithms",
        "comparing",
        "datascience",
        "gbm",
        "logisticregression",
        "machinelearning"
      ],
      "search_text": "Comparing algorithms spiral dataset, #datascience #machinelearning #algorithms #gbm #logisticregression algorithms comparing datascience gbm logisticregression machinelearning Now, this place about to end We're taking over, no one's getting",
      "platforms": {
        "tiktok": {
          "video_id": "7056898519524986159",
          "url": "https://www.tiktok.com/@rajistics/video/7056898519524986159",
          "view_count": 1014,
          "upload_date": "2022-01-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/be5e8c8549894e33af1cd3edc25a3e93_1643062226~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=WkAvZSNbC%2BxyyJUcA66auf%2FlyRA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Beer and diapers story of association of products. #datascience #recommendationsystems #marketing #analytics #correlation",
      "description": "Beer and diapers story of association of products. #datascience #recommendationsystems #marketing #analytics #correlation",
      "upload_date": "2022-02-18",
      "total_views": 1012,
      "max_views": 1012,
      "topics": [
        "analytics",
        "beer",
        "correlation",
        "datascience",
        "marketing",
        "recommendationsystems"
      ],
      "search_text": "Beer and diapers story of association of products. #datascience #recommendationsystems #marketing #analytics #correlation analytics beer correlation datascience marketing recommendationsystems Imagine a tired man on his way home gets a call from his wife. Please go pick up some diapers. As he goes to the store, picks up the diapers, he turns around. He sees this beautiful pyramid of cases of beer and thinks, why not? It's Friday. Let me pick up a case of beer. While the story might not be exactly true, it's made an impact. When businesses see a relationship between products, they tend to make it easier to buy, hoping that sales will go up. This approach is really the foundation of modern personalization and recommendation systems. While this often works, sometimes it can backfire. Look for another video on that.",
      "platforms": {
        "tiktok": {
          "video_id": "7066166549442432303",
          "url": "https://www.tiktok.com/@rajistics/video/7066166549442432303",
          "view_count": 1012,
          "upload_date": "2022-02-18",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9618de57fc59431f9f50031a7eccb8e0_1645220108~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=U3T%2BAScE1YeECwtocb1IkF3Ice0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 5933,
      "title": "Still making PowerPoint-2008-level charts? AI gives you a shortcut to stunning visuals and real design skills. Pick the level that fits where you are.",
      "description": "Still making PowerPoint-2008-level charts? AI gives you a shortcut to stunning visuals and real design skills. Pick the level that fits where you are.",
      "upload_date": "2025-12-01",
      "total_views": 1010,
      "max_views": 1010,
      "topics": [
        "canva",
        "design",
        "gives",
        "google",
        "level",
        "making",
        "really",
        "slides",
        "still",
        "take",
        "want",
        "whether"
      ],
      "search_text": "Still making PowerPoint-2008-level charts? AI gives you a shortcut to stunning visuals and real design skills. Pick the level that fits where you are. canva design gives google level making really slides still take want whether They told me I needed ozempic. Do you think maybe I should take a data visualization course? Courses are great. I'll happily take your money. But are you trying to build an infographic like this? Whoa, that looks amazing. How did you do that? Nano Banana Pro, it can take your ideas, turn them into striking, really useful visualizations. It's great for inspiration or eye catching presentations. But can I control the details? Not really. If you want to tweak spacing or you have specific color palettes you want to use, you're pretty much limited. That's where design tools come in. You can't sell another kidney to Adobe. You don't need to. AI can help you make amazing visuals, whether you're using Google Slides or Canva. Look at this. I just upload my chart. I ask Gemini to suggest what to do. And it walks me through step by step how to use these tools. So it actually understands all the tools and then gives me guidance? Yes, it knows the menus, all the advanced options. Like I've learned how to outline images in Canva, add transparency to my bar charts with Google Slides, all with its guidance. That's awesome. But sometimes those apps feel kind of limiting. Go to option three. Ask for the code, whether it's Python or JavaScript. Having all the code gives you the full control over all the layout. Wow, there's so many options. Whether I want to do something really quick using AI, whether I want to use AI to help learn, or just get the code. Exactly. Think of AI as a design intern, but without the attitude.",
      "platforms": {
        "tiktok": {
          "video_id": "7578713466723192094",
          "url": "https://www.tiktok.com/@rajistics/video/7578713466723192094",
          "view_count": 1010,
          "upload_date": "2025-12-01",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oceGXsIJLVFudZU9Rz5vfKAQGeAsLQCqsFokfQ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767297600&x-signature=lQMJWyY0DeXIKl9fYKwKiUSGa%2FI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        },
        "instagram": {
          "video_id": "17943086094077845",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-12-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6326,
      "title": "CLIP Interrogator is available over at the hugging face spaces. Have fun! #datascience #machinelearning #stablediffusion #huggingface",
      "description": "CLIP Interrogator is available over at the hugging face spaces. Have fun! #datascience #machinelearning #stablediffusion #huggingface",
      "upload_date": "2022-10-25",
      "total_views": 1010,
      "max_views": 1010,
      "topics": [
        "clip",
        "datascience",
        "huggingface",
        "interrogator",
        "machinelearning",
        "revolution",
        "stablediffusion",
        "statistics",
        "tabpfn",
        "time"
      ],
      "search_text": "CLIP Interrogator is available over at the hugging face spaces. Have fun! #datascience #machinelearning #stablediffusion #huggingface clip datascience huggingface interrogator machinelearning revolution stablediffusion statistics tabpfn time",
      "platforms": {
        "instagram": {
          "video_id": "17958422801119916",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-10-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "1JhQ9uu9Fz8",
          "url": "https://youtube.com/shorts/1JhQ9uu9Fz8?feature=share",
          "view_count": 1010,
          "upload_date": "2022-10-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Looking forward to a lot more videos in 2023, let me know topics I should cover. For all my videos, I put them in an airtable spreadsheet available at bit.ly/raj_videos ",
      "description": "Looking forward to a lot more videos in 2023, let me know topics I should cover. For all my videos, I put them in an airtable spreadsheet available at bit.ly/raj_videos ",
      "upload_date": "2023-01-01",
      "total_views": 1005,
      "max_views": 1005,
      "topics": [
        "alright",
        "baby",
        "feeling",
        "know",
        "looking",
        "videos"
      ],
      "search_text": "Looking forward to a lot more videos in 2023, let me know topics I should cover. For all my videos, I put them in an airtable spreadsheet available at bit.ly/raj_videos  alright baby feeling know looking videos I'm not feeling alright, baby I'm feeling alright Baby I'm gonna have the best fucking night of my life Now whatever it takes me I'm down for the ride Baby don't you know I'm good, yeah I'm feeling alright",
      "platforms": {
        "tiktok": {
          "video_id": "7183703367007522094",
          "url": "https://www.tiktok.com/@rajistics/video/7183703367007522094",
          "view_count": 1005,
          "upload_date": "2023-01-01",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/4a5345a7dd904d9b9b33829a82af652d_1672586309~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=FjEWVu1d5PFEZ1CLgf5lrZ765hA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Predicting NCAA basketball #marchmadness #datascience #sportsanalytics #illinois",
      "description": "Predicting NCAA basketball #marchmadness #datascience #sportsanalytics #illinois",
      "upload_date": "2022-03-25",
      "total_views": 1004,
      "max_views": 1004,
      "topics": [
        "bracket",
        "datascience",
        "illinois",
        "marchmadness",
        "sportsanalytics",
        "team"
      ],
      "search_text": "Predicting NCAA basketball #marchmadness #datascience #sportsanalytics #illinois bracket datascience illinois marchmadness sportsanalytics team Is your bracket also busted? Let's talk about how we can use AI to build a better bracket. A typical approach is to build a binary classifier that helps predict if team A or team B will win. Then you grab all the great historic data so we have plenty of examples to train the algorithm with. For each game, we want to add in information about team A and team B and how they played. Could be offensive stats like how many blocks, 3 pointers, how many points they made. Could be defensive. Besides those types of stats, we can often add in things like Sagran's power ratings, we can add in the seed, even the vagus line. All those things help us give us more information about who's likely to win. A key to success is trying to identify features or different types of information that you can figure out that gives you an edge that nobody else does. And this is where folks will spend a ton of time going through data trying to find some arcane way of identifying which is a better team. You take all the data, put it together and then you train a classifier. Something like an XG Boost algorithm and then you'll have a model that you can take any two teams and it will predict who's likely to win. All you do is apply that algorithm to your bracket and there you go, at using AI.",
      "platforms": {
        "tiktok": {
          "video_id": "7078832456429833514",
          "url": "https://www.tiktok.com/@rajistics/video/7078832456429833514",
          "view_count": 1004,
          "upload_date": "2022-03-25",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d78f155369ed4e149206d229e25d1e68_1648169119~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=Fakz%2BDUc%2B0%2FreNj1M4Ab%2B%2FcE0bI%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "#onthisday ",
      "description": "#onthisday ",
      "upload_date": "2024-07-24",
      "total_views": 993,
      "max_views": 993,
      "topics": [
        "onthisday",
        "praying",
        "regina",
        "spent",
        "talking",
        "time"
      ],
      "search_text": "#onthisday  onthisday praying regina spent talking time I spent about 80% of my time talking about Regina, and the other 20% of the time I was praying for someone else to bring her up so I could talk about her more.",
      "platforms": {
        "tiktok": {
          "video_id": "7395148337579707690",
          "url": "https://www.tiktok.com/@rajistics/video/7395148337579707690",
          "view_count": 993,
          "upload_date": "2024-07-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0a188b9040c841929368eec0c0c63e13_1721817163~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=WXPnftW3ooTNDhbTJCaBmT8AZVQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "It’s tough to be content #codetok #techtok #datascience #programming",
      "description": "It’s tough to be content #codetok #techtok #datascience #programming",
      "upload_date": "2022-05-10",
      "total_views": 987,
      "max_views": 987,
      "topics": [
        "codetok",
        "content",
        "datascience",
        "programming",
        "techtok",
        "tough"
      ],
      "search_text": "It’s tough to be content #codetok #techtok #datascience #programming codetok content datascience programming techtok tough I wish I lived there! Really? No!",
      "platforms": {
        "tiktok": {
          "video_id": "7095897166908099882",
          "url": "https://www.tiktok.com/@rajistics/video/7095897166908099882",
          "view_count": 987,
          "upload_date": "2022-05-10",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/735fafb6ac4744e3a44a80a8b899fab7_1652142306~tplv-tiktokx-origin.image?dr=9636&x-expires=1767495600&x-signature=RF%2BzrV4bbBdbEbYBAtWAPQM95hc%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6573,
      "title": "Sparsity in AI (data frames, dropout, regularization, and Mixture of Experts)",
      "description": "Sparsity in AI (data frames, dropout, regularization, and Mixture of Experts)",
      "upload_date": "2025-03-09",
      "total_views": 985,
      "max_views": 985,
      "topics": [
        "data",
        "datascience",
        "dropout",
        "frames",
        "important",
        "machinelearning",
        "make",
        "mixture",
        "regularization",
        "sparsity",
        "statistics",
        "sure"
      ],
      "search_text": "Sparsity in AI (data frames, dropout, regularization, and Mixture of Experts) data datascience dropout frames important machinelearning make mixture regularization sparsity statistics sure",
      "platforms": {
        "instagram": {
          "video_id": "Ck03TqYgU3A",
          "url": "https://www.instagram.com/reel/Ck03TqYgU3A/",
          "view_count": 0,
          "upload_date": "2022-11-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "M3AhBvaSSvY",
          "url": "https://www.youtube.com/watch?v=M3AhBvaSSvY",
          "view_count": 985,
          "upload_date": "2025-03-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "What’s the deal with those competition rules #datascience #codetok #analytics #kaggle ",
      "description": "What’s the deal with those competition rules #datascience #codetok #analytics #kaggle ",
      "upload_date": "2022-08-28",
      "total_views": 968,
      "max_views": 968,
      "topics": [
        "analytics",
        "codetok",
        "competition",
        "datascience",
        "deal",
        "kaggle"
      ],
      "search_text": "What’s the deal with those competition rules #datascience #codetok #analytics #kaggle  analytics codetok competition datascience deal kaggle I'm steady trying to find the motive Why do what I do? If freedom ain't getting no closer No matter how far I go",
      "platforms": {
        "tiktok": {
          "video_id": "7137006249950022958",
          "url": "https://www.tiktok.com/@rajistics/video/7137006249950022958",
          "view_count": 968,
          "upload_date": "2022-08-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/939330cd7a394477a572be26bf06d5c7_1661713763~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=DmGuEO7Lre6nN%2B74e9QubijaTq8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Creating music videos with stable diffusion and whisper. This colab notebook uses a dream studio backend for the images. Another great step in generating AI content. #datascience #analytics #stablediffusion ",
      "description": "Creating music videos with stable diffusion and whisper. This colab notebook uses a dream studio backend for the images. Another great step in generating AI content. #datascience #analytics #stablediffusion ",
      "upload_date": "2022-09-28",
      "total_views": 966,
      "max_views": 966,
      "topics": [
        "analytics",
        "datascience",
        "diffusion",
        "music",
        "stable",
        "stablediffusion"
      ],
      "search_text": "Creating music videos with stable diffusion and whisper. This colab notebook uses a dream studio backend for the images. Another great step in generating AI content. #datascience #analytics #stablediffusion  analytics datascience diffusion music stable stablediffusion Check it out. I used AI to build this music video. David Mark shared a notebook which used whisper and stable diffusion to help automatically create a music video. All you do it is give it some music. It's going to use whisper to decode and find the lyrics and then from the lyrics it takes advantage of stable diffusion to generate the images. Now this video is row row row your boat which is a bit disappointing to me and what happened is the back end that I was using for getting the images was dream studio and it's safety filter kept hitting it. So like the cooler videos I wanted to do with killers and imagine dragons the lyrics just weren't in return pictures and I got stalled on that. The style prompt I used here was extremely detailed painted by Ralph Stedman and Radiohead and beautiful. So that's why it looks like it is go give it a shot. Maybe I'll get back there and do one for killers and share.",
      "platforms": {
        "tiktok": {
          "video_id": "7148264789201095982",
          "url": "https://www.tiktok.com/@rajistics/video/7148264789201095982",
          "view_count": 966,
          "upload_date": "2022-09-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5848dd56c4f247c4a694b0565d7e2a88_1664335100~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=08M5qxk%2F7MbTz67KF%2BpLeEv4Dms%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6608,
      "title": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft",
      "description": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft",
      "upload_date": "2023-02-22",
      "total_views": 961,
      "max_views": 961,
      "topics": [
        "chatgpt",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "microsoft",
        "robotics"
      ],
      "search_text": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft chatgpt datascience largelanguagemodels machinelearning microsoft robotics Microsoft showing us how we can control robots with ChatGPT. Microsoft's latest paper says, let's drop the old ways in controlling robots takes way too long. Instead, let's use natural language and once we do that, we can start controlling many types of robots for many different types of tasks. You start off by building a robot API that has very descriptive names. Once we have those descriptive names, we can start building prompts with it. Test it out and voila. You've got ChatGPT controlling robots. This is the future and a hint for developers, you nice descriptive names in your APIs and they'll get used.",
      "platforms": {
        "instagram": {
          "video_id": "Co-KVK9gy0q",
          "url": "https://www.instagram.com/reel/Co-KVK9gy0q/",
          "view_count": 961,
          "upload_date": "2023-02-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "News flash: Data scientists spend lots of time on data prep/exploration #datascience #dataengineering #analytics #codetok",
      "description": "News flash: Data scientists spend lots of time on data prep/exploration #datascience #dataengineering #analytics #codetok",
      "upload_date": "2022-07-24",
      "total_views": 947,
      "max_views": 947,
      "topics": [
        "analytics",
        "codetok",
        "data",
        "dataengineering",
        "datascience",
        "time"
      ],
      "search_text": "News flash: Data scientists spend lots of time on data prep/exploration #datascience #dataengineering #analytics #codetok analytics codetok data dataengineering datascience time I spent about 80% of my time talking about Regina, and the other 20% of the time I was praying for someone else to bring her up so I could talk about her more.",
      "platforms": {
        "tiktok": {
          "video_id": "7123999905986874670",
          "url": "https://www.tiktok.com/@rajistics/video/7123999905986874670",
          "view_count": 947,
          "upload_date": "2022-07-24",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/110af15417684c5e94be1c7afe4d828b_1658685489~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=i%2Fwms8kWc6oPU7Aap6p34fIYx1U%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6272,
      "title": "Vicuna is awesome go check it out. Its the latest LLama model and very impressive. I ended up cutting out the details on vicuna since i feel like we have turned the corner on getting GPT-3 performance with open source models. #datascience #machinelearning #llama #vicuna #openai #gpt3 #largelanguagemodels",
      "description": "Vicuna is awesome go check it out. Its the latest LLama model and very impressive. I ended up cutting out the details on vicuna since i feel like we have turned the corner on getting GPT-3 performance with open source models. #datascience #machinelearning #llama #vicuna #openai #gpt3 #largelanguagemodels",
      "upload_date": "2023-03-30",
      "total_views": 928,
      "max_views": 928,
      "topics": [
        "datascience",
        "gpt3",
        "largelanguagemodels",
        "llama",
        "machinelearning",
        "openai",
        "temperature",
        "vicuna",
        "want"
      ],
      "search_text": "Vicuna is awesome go check it out. Its the latest LLama model and very impressive. I ended up cutting out the details on vicuna since i feel like we have turned the corner on getting GPT-3 performance with open source models. #datascience #machinelearning #llama #vicuna #openai #gpt3 #largelanguagemodels datascience gpt3 largelanguagemodels llama machinelearning openai temperature vicuna want",
      "platforms": {
        "instagram": {
          "video_id": "17884384652813778",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "bGa25kedh_4",
          "url": "https://youtube.com/shorts/bGa25kedh_4?feature=share",
          "view_count": 928,
          "upload_date": "2023-03-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @anthonycomputer Dive in and start!  Lots of great stuff out there. #datascience #techtok #analytics",
      "description": "Reply to @anthonycomputer Dive in and start!  Lots of great stuff out there. #datascience #techtok #analytics",
      "upload_date": "2022-04-20",
      "total_views": 899,
      "max_views": 899,
      "topics": [
        "analytics",
        "datascience",
        "one",
        "right",
        "techtok",
        "things"
      ],
      "search_text": "Reply to @anthonycomputer Dive in and start!  Lots of great stuff out there. #datascience #techtok #analytics analytics datascience one right techtok things My advice, play the field. It's hard to know which one is the right one right away. You're better off trying a lot of different things. After all, there isn't one thing that's perfect for everybody. Everybody has their own unique tastes, things that they like. It's a try a variety. You might be surprised what you might find. And hey, if the first one doesn't work out, don't worry about it. There's another one out there.",
      "platforms": {
        "tiktok": {
          "video_id": "7088471275076668715",
          "url": "https://www.tiktok.com/@rajistics/video/7088471275076668715",
          "view_count": 899,
          "upload_date": "2022-04-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ef64136ef0a64f808746008a252043e0_1650413331~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=%2BSpYLAKb2MbcaZ5HnEyAehNTMmo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Great way to get under the skin of your data scientist. #datascience #analytics #codetok ",
      "description": "Great way to get under the skin of your data scientist. #datascience #analytics #codetok ",
      "upload_date": "2022-10-08",
      "total_views": 897,
      "max_views": 897,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "get",
        "great",
        "way"
      ],
      "search_text": "Great way to get under the skin of your data scientist. #datascience #analytics #codetok  analytics codetok datascience get great way Look, that's your exact car right there. Yeah, it is. It's pretty cool. That one's way cooler than yours. But he's got a hotter girlfriend than me too.",
      "platforms": {
        "tiktok": {
          "video_id": "7152273594037325098",
          "url": "https://www.tiktok.com/@rajistics/video/7152273594037325098",
          "view_count": 897,
          "upload_date": "2022-10-08",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/26ae51409223447b902e5687a44b4a03_1665268472~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=yVu9Oc1%2FYfbjZkGnNZ2UkgT%2Bzv0%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I am awful about writing tests. This is why I don’t write production code. #datascience #cstok #programminghumor #codetok",
      "description": "I am awful about writing tests. This is why I don’t write production code. #datascience #cstok #programminghumor #codetok",
      "upload_date": "2022-04-01",
      "total_views": 896,
      "max_views": 896,
      "topics": [
        "body",
        "codetok",
        "cstok",
        "datascience",
        "mind",
        "programminghumor"
      ],
      "search_text": "I am awful about writing tests. This is why I don’t write production code. #datascience #cstok #programminghumor #codetok body codetok cstok datascience mind programminghumor I can keep a secret good You Got my mind on your body In your body on my mind",
      "platforms": {
        "tiktok": {
          "video_id": "7081453049293720878",
          "url": "https://www.tiktok.com/@rajistics/video/7081453049293720878",
          "view_count": 896,
          "upload_date": "2022-04-01",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2935c4ed9b6c46839928aea203e07326_1648779273~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=BZ7iBl0ie2Glomu8KYGbVkYuUME%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Simple tip, never claim causation. Unless you have an experimental design, it’s hard to prove. #datascience #machinelearning #statistics ",
      "description": "Simple tip, never claim causation. Unless you have an experimental design, it’s hard to prove. #datascience #machinelearning #statistics ",
      "upload_date": "2022-11-05",
      "total_views": 886,
      "max_views": 886,
      "topics": [
        "datascience",
        "god",
        "machinelearning",
        "simple",
        "statistics",
        "tip"
      ],
      "search_text": "Simple tip, never claim causation. Unless you have an experimental design, it’s hard to prove. #datascience #machinelearning #statistics  datascience god machinelearning simple statistics tip Oh my god, oh my god. Why did you do that? I didn't know.",
      "platforms": {
        "tiktok": {
          "video_id": "7162575380128304426",
          "url": "https://www.tiktok.com/@rajistics/video/7162575380128304426",
          "view_count": 886,
          "upload_date": "2022-11-05",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/32b974e11b1c42a0a1ed7c237b8b0dc3_1667667051~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=UeaINrWZ8LGKyFPFczYf8TfkRAY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6135,
      "title": "Temperature is an important parameter when working with many models including got-3. This video gives a short background on temperature and the best settings when working with large language models. #datascience #machinelearning #largelanguagemodels #gpt3 #gpt4",
      "description": "Temperature is an important parameter when working with many models including got-3. This video gives a short background on temperature and the best settings when working with large language models. #datascience #machinelearning #largelanguagemodels #gpt3 #gpt4",
      "upload_date": "2023-03-21",
      "total_views": 881,
      "max_views": 881,
      "topics": [
        "datascience",
        "gpt3",
        "gpt4",
        "largelanguagemodels",
        "lora",
        "machinelearning",
        "model",
        "peft",
        "temperature"
      ],
      "search_text": "Temperature is an important parameter when working with many models including got-3. This video gives a short background on temperature and the best settings when working with large language models. #datascience #machinelearning #largelanguagemodels #gpt3 #gpt4 datascience gpt3 gpt4 largelanguagemodels lora machinelearning model peft temperature",
      "platforms": {
        "instagram": {
          "video_id": "17948136683565969",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4jVOKEMyu3E",
          "url": "https://www.youtube.com/watch?v=4jVOKEMyu3E",
          "view_count": 881,
          "upload_date": "2023-03-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6506,
      "title": "X-decoder from Microsoft. Check out the instructional text demo. I added in video released by the team at the bottom. If too many people don‚Äôt like that, I can release a version without that video. #datascience #machinelearning #x-decoder #pix2pix",
      "description": "X-decoder from Microsoft. Check out the instructional text demo. I added in video released by the team at the bottom. If too many people don‚Äôt like that, I can release a version without that video. #datascience #machinelearning #x-decoder #pix2pix",
      "upload_date": "2023-02-16",
      "total_views": 879,
      "max_views": 515,
      "topics": [
        "datascience",
        "decoder",
        "machinelearning",
        "model",
        "pix2pix",
        "text",
        "video",
        "x"
      ],
      "search_text": "X-decoder from Microsoft. Check out the instructional text demo. I added in video released by the team at the bottom. If too many people don‚Äôt like that, I can release a version without that video. #datascience #machinelearning #x-decoder #pix2pix datascience decoder machinelearning model pix2pix text video x I have to tell you about this new image editing demo. This comes from Microsoft with their X decoder model. You can take an image, add some additional text, and it's going to redo the picture that follows that text. Mind blowing. At the heart of this is the X decoder model which is a very general vision model that you can sees can support a wide range of tasks. The model can take in two types of queries both latent and text queries as well as two kinds of outputs, semantic and pixel level outputs. For this event, demo, they took GPT3 for its strong language understanding and stable diffusion which of course makes awesome images and combine that together. The results are super impressive and you should go try it out yourself. You can't at this point download the model though.",
      "platforms": {
        "instagram": {
          "video_id": "CotUWk1g3Fx",
          "url": "https://www.instagram.com/reel/CotUWk1g3Fx/",
          "view_count": 515,
          "upload_date": "2023-02-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "0kB5fyoGESc",
          "url": "https://youtube.com/shorts/0kB5fyoGESc?feature=share",
          "view_count": 364,
          "upload_date": "2023-02-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "From Spiegelhalter interview on Artists of Data Science podcast #datascience #statistics #codetok #dataanalysis",
      "description": "From Spiegelhalter interview on Artists of Data Science podcast #datascience #statistics #codetok #dataanalysis",
      "upload_date": "2022-05-23",
      "total_views": 867,
      "max_views": 867,
      "topics": [
        "codetok",
        "dataanalysis",
        "datascience",
        "interview",
        "spiegelhalter",
        "statistics"
      ],
      "search_text": "From Spiegelhalter interview on Artists of Data Science podcast #datascience #statistics #codetok #dataanalysis codetok dataanalysis datascience interview spiegelhalter statistics We're on a floating rock. You know that, right?",
      "platforms": {
        "tiktok": {
          "video_id": "7100728996148776238",
          "url": "https://www.tiktok.com/@rajistics/video/7100728996148776238",
          "view_count": 867,
          "upload_date": "2022-05-23",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b3e5486db0c24ff6aeb35f9076e8280d_1653267304~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=H874%2BLjXwId94Nb7Mk9NeTCol%2FY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Practical Lessons for building generative AI: I share the latest research and earned wisdom on building generative AI applications and touch upon Hallucinations, Reasoning (including O1 model), Model as a Judge, Chain of Thought, Modularizing your Gen AI apps, and Data Enrichment.",
      "description": "Practical Lessons for building generative AI: I share the latest research and earned wisdom on building generative AI applications and touch upon Hallucinations, Reasoning (including O1 model), Model as a Judge, Chain of Thought, Modularizing your Gen AI apps, and Data Enrichment.",
      "upload_date": "2024-10-02",
      "total_views": 863,
      "max_views": 863,
      "topics": [
        "data",
        "going",
        "know",
        "like",
        "model",
        "models"
      ],
      "search_text": "Practical Lessons for building generative AI: I share the latest research and earned wisdom on building generative AI applications and touch upon Hallucinations, Reasoning (including O1 model), Model as a Judge, Chain of Thought, Modularizing your Gen AI apps, and Data Enrichment. data going know like model models Let's today talk about what's easy to do with LLMs, what's hard to do with LLMs, and where that boundary is for generative AI. So this is a recent talk I gave to one of our customers. This is September 2024. And what I want to do is I'm going to give a version of the talk. This is for YouTube. So I'm going to leave out some of the introduction. I'm going to leave out some of the jokes. I'm also going to talk a little faster. So it's not the same way as if I was there in person. But what I want to do is walk through, first, the basics of how to use large language models, what retrieval augmented generation is. And then I want to apply this to two different stories. We're going to take the law firm of Dewey Cheetahman Howe, and then the hypothetical company called Frosty. And these will help us explain how folks are using generative AI today, what are the limits, what does it take to actually develop a generative AI application. Now, I'm going to start my story up really, really up high here. So up about 30,000 feet in the air, a couple of years ago, Flight Attendant was bringing that meal cart up and down the aisle, smashed a knee of a passenger. That passenger went on to file a lawsuit against the company. A couple years later, lawyers filling it out. It's a holiday weekend coming up. He's working on that brief response. He decides to take advantage of ChatGPT to help him proof out his brief. Along the way, ChatGPT finds some cases that were unpublished, great legal research tool, finding things around there. And he was like, hey, maybe it's finding things that I wasn't able to like that. But he didn't realize though that was ChatGPT was quite happy to make up those cases. And in fact, what happened is he was actually admonished by the bar for doing this. And lawyers all around know that they cannot trust tools like ChatGPT when they're doing legal research because these models can make up information. Now, I often demonstrate this with this type of example. I typically have a Streamlid application. But this is a very factual based question. How many vehicles will Rivian manufacture in normal Illinois? This is of course meaningful to me. I live in central Illinois. I live near Rivian. It's a very topical question. When I ask this question, I ask it to two different large language models. I have the Google Gemma model here and the Rec-a-Flesh model as well. And when I ask it, you'll see that I get two different answers here in response. And is that strange? Should we expect it? I think most of you understand at this point that two different manufacturers, two different methods for training these models, probably going to lead to two different results. Now, what I want to make sure you all understand is the way these models work. And the way these models work is they try to predict the next word or token. And what they're doing is trying to predict that on a statistical basis. What's the most likely thing? And so in the process, what they're doing is not worried about truth and false. They're really trying to tell what the most cohesive, coherent story is for these. And so if you ask a model what the capital of Mars is, it doesn't know that that's fictional. It's going to try to complete that story and it's going to give you an answer. Same thing. If you try to ask it to do some math, well, it doesn't know it's a calculator. It's seen lots of math examples before, so it's just going to complete the math story. In this case, it looks like it's read too many release notes, not actually enough math for how to do it and it gets it wrong. Now, it's important, especially for companies to recognize that there's risks that come along with these generative AI when they're making mistakes. So this is a very wide known case. Air Canada's chatbot went off the rails a little bit and when they went and tried to enforce it, the government said, hey, we're going to treat this chatbot just like one of your employees, one of your agents. You're responsible for what those agents employees say. You're going to be responsible for what this model says. So this helps you understand why so many enterprises are very careful about putting out generative AI. You see so many JET AI committees around this. So we've got a problem here. We've got these hallucinations. Don't worry. Our friends in academia were busy working on this, came up with a solution, has a mouthful of name, retrieval augmented generation. It's often shortened and called RAG for short. And the idea of RAG was instead of the model just sitting by itself and trying to answer the question, we're going to surround the model with context. We're going to ground it with information so it uses that information and answering the question. So if you look at this diagram, what happens is when we bring the large language model a question, we don't just bring it directly to it. Instead, we first take that question and we go out and search and look for is there relevant information that's related to this question? If so, let's collect that information together and we'll bring that to the large language model. So that way, our final answer is grounded by the large language model. Now, let me walk you through an example of this. I'm going to ground this with 10 K forms. These are widely used in the public in the US for all public companies have to file these can trust that they're factual. Now, we're going to stick to the same question we're going to ask. And you'll see here, I've got my streamlet app running. We have the question, how many vehicles do you manufacture in normal? And now we have an answer. What's the valuable thing about these RAG use cases? We can see the context. I can see exactly what's the document that this that this answer came from. We have a citation. We can go look it up a nice source. And so this is what makes RAG powerful. This ability to retrieve and give us a coherent way of understanding the answer is why this is the number one most popular generative AI application of large language models. Pretty much every company at this point has some types of RAG use cases. Running. Now. I did this in a speech. I like to have fun. So I made this hypothetical law firm of Dewey, Cheetah, and how where what they want to do is reduce the amount of time their staff spends doing legal research. They've seen these innovations. Can we apply it to legal research? Now, a friend of mine went and worked at Dewey, Cheetah, and how? And when he first started off, he was like, well, you know, I've seen these models. I've seen GPT for I know it can pass the bar exam. Granted, this is the multiple choice bar exam, but it's got to know something about the law. Why don't I just wire it up directly and have all of our lawyers use that as an aid for legal research? And so my buddy set that up. But like a couple of days later, another lawyer came knocking on his door and said, hey, I have a problem. I asked for this online dating services in Connecticut. My buddy's like, well, what's the problem? It looks like you got a result Connecticut general statute. But this is when the lawyer pointed out, well, I actually looked it up. There is no statute. This was entirely hallucinated. Oh. And so this is when my buddy realized, oh, boy, I should probably go check out that rag approach that we talked about. Go grab all the different legal literature there is ground it so that way the lawyers have actual facts and we could link it to courts and citations. Now, as he was doing this, he figured out that there was actually legal services out there, Lexis Nexus, for example, offers a way to get hallucination free linked legal citations. Using that rag approach that we talked about. So he said, OK, I'm going to wire this up for the attorneys. He did. And a couple of days later, an attorney comes in says, hey, I'm not at this this this result here. It doesn't seem to make sense to me. My buddy's like, well, there's a case citation there. It's based on a real case, right? That case exists. And the lawyer's like, yeah, the case exists. But it's confusing two different concepts here of the equity cleanup doctrine and the doctrine of clean hands. Equity cleanup is about when you have many different equitable claims. Instead of spreading that over multiple cases, a court can consolidate those in one case. While clean hands is really about the conduct coming into a case. If you're, for example, a hypothetical rock star like Diddy, and you might have coerced somebody and used fraud and different techniques like that to get them into sign a contract, you can't expect a court later to come in and enforce that contract if you've come in with unclean hands like that. So as you can see, the words are kind of similar, but they're very two different legal concepts here that the model didn't recognize and wasn't able to separate. And it wasn't just that. You know, another of the lawyers walked in and said, hey, you know, I had asked it to give me these notable opinions by judge Luther Wilgarten. And of course, my buddy again was confused, like, hey, isn't that case exists? And yeah, the case exists. But the thing is, is I was kind of joking around. Luther Wilgarten isn't a real judge. It's a fictional judge that's in some law reviews that all lawyers know as an inside joke. And if you read the rest of this, it doesn't make any sense. And I asked for a notable opinion. This wasn't notable. It was unpublished. Now, I gave a couple of examples. You can find a lot more in this paper out of Stanford that looked at how much these models hallucinate using real questions, the typical questions you'd get in legal research, and they had analyzed by folks that actually knew about the law. And this kind of leads me to one of the limits that you'll see with tools like Retrieval Augmented Generation, where it's really good at retrieving information and where it's all true, where it's authoritative, applicable. What happens is in the legal research is a great example of this is sometimes all these things are very contested and it gets really hard to separate it. So in the legal context, for example, think about somebody getting injured out on a boat. There's a lot of different specialties. It might be a tort law, might be criminal law, might be maritime. A legal researcher has to sort through, figure out which one of these or maybe multiple of these are applicable. You have different jurisdictions in the United States. You have the federal court system, the state, local, there's regulatory systems. Beyond that, you have different authorities. We all have the Supreme Court that we know about, but every state has a Supreme Court. There's also Court of Appeals that often write opinions that are very influential. But in New York, the Court of Appeals is called the Supreme Court. And then you have trial courts, law reviews. You have law reviews from Harvard. You have law reviews from other law schools. All of these have to be taken into account. You have to weave these together when you're doing legal research. You also have to take into consideration time. When cases are overturned, that's a big deal in law. You need to know when, at what point in time this was. And so for me, all of this just reminded me that how difficult legal research is. And here I'm talking legal research, but I think this applies to a lot of other fields out there where we just assume it's just simply retrieving facts. But what we find out that you really have to have a lot of knowledge to be able to weave everything in and out and separate all the things that are confused and put together to really make something of it. And so this is where, look, you can use these tools to help generate and get you a stack of documents, but you still need those researchers to be able to those facts, people that actually tease out the insights. But one good thing was I was able to tell my buddy, hey, I know you're trying to figure this out. I want you to go check out Frosties because Frosties has done a good job building a generative app and showing you all the steps that's involved in doing this. Now at Frosties, I had a friend that was working on an application to go from natural language to answers. And the way this works was you take a question, how many orders do I have in each state? We pass that through a large language model where we turn that into SQL. And then we can use that SQL to be able to get an answer. This is also a very common application we're getting with large language models where we want to answer these questions using a text to SQL approach. So let's talk about how my friend, how she solved and build this generative AI app to solve this problem. The first issue she ran into was as she was building the model, how do you evaluate all the SQL queries? So imagine taking this query here, show me the total population of each state. And we want to order them from the northern to the most southern. Now, when you solve this type of problem, you'll take a query like this and have a gold standard. This is exactly what the SQL should look like. And you can see that here. Now, the text to SQL models is going to generate SQL. Sometimes it's going to be an exact match one for one. But what do you do if it's not an exact match? Take a look at this example where it's added an extra column. And the column here is the latitude of the state. So it's just telling us how northern it is. This is an example of how lots of modern models are designed to be helpful. It's just being helpful. It's giving us extra information. But in this case, it's different. It's not the gold standard. Now, many of you probably took math, physics. You remember that some of your teachers or professors would give you partial credit. If you showed you knew how to think through the problem and solve it, you could get that. Well, this is where my friend was like, you know, we really need to have a way to give partial credit because this is very close to the answer and I want an approach to recognize this. Now, to do all this evaluation, you couldn't have humans just look at this and grade this. So she thought about it and went with model based evaluation where instead of the humans to evaluate it, we're going to use a large language model to do this. Now, immediately at this point, some of you are probably like, whoa, is that going to work? I mean, that's like the Fox Guarding the henhouse. Isn't this like model collapse? How can you have models evaluating models? Crazy stuff. Don't worry. Let me show you. So this is what an example of one of the model based evaluation prompts look like. We ask a large language model to pretend to be an evaluator. Hey, you're a data quality analyst or you're an analyst looking at data quality. You're going to then grade out these the text of the sequel that comes in and we give it a grading scale with what it's a perfect score, what's incomplete, what's a miss. We tell it what data is going to come in. And with this, the model can start helping us make these decisions. So let me walk you through an example of this. So here we'll see what the user query is. You know, what are the hundred track IDs? We see the gold standard, like what exactly it should be. But here you can see our candidate sequel missed. It was a little bit off. Of course, when we asked the question, we didn't tell it how Texas was spelled inside the database. So it's a natural mistake here to confuse TX and Texas for these. Because in the US, they stand for the same thing. But if we go with the strict criteria of that exact match, we don't get an exact match here. However, if we go with that model based approach and we're a little bit looser with this, we'll see that, OK, this answer is 100 percent perfect. But you know what? It captured the user's intent. The user could easily fix this or we could fix this in post-processing to the right of abbreviation as well. This model based evaluation is very popular. One of the things you want to do is to make sure that this evaluation process lines up with other evaluation processes you're using. Here, my French checked and she saw that 80 percent were the exact same. When she scored them, the other 20 percent were within one or two when she scored them. So she felt very confident in using model based evaluation. And her results were very similar to what I've seen in many other studies. I got tired of adding research sites here where universally we see that often in many contexts that these large language models correlate about 80 percent of the time to humans in this. So this is why everybody's using model based evaluation like that. So at this point, this was the first step for my friend. She built that model. She got it. Built out her own internal enterprise benchmark. That's another important step. You can't trust those public data sets that are out there. They're far too easy. You want something that reflects the data inside your organization. Takes a bunch more work to create that, but it pays off in the long run. So she tested it on that only at 33 percent. Not the happiest about that. But she thought about it and she was like, you know, one thing is as I was building this, I noticed that there's a number of different text to SQL models and each of these models had their strengths and weaknesses. Why don't I combine them? After all, in traditional machine learning, we often ensemble models that are diverse to improve our predictions. What she did was she decided to try the same thing here. And she found out she got a boost in performance by combining multiple models. Now, these models would sometimes make mistakes. So her next idea was, you know what, when they make a mistake, I'm going to make them sit and think about it. Now, some of you are probably like, what? Think about it. Actually, the way these large language models work is when a model makes a mistake, if you ask it to reflect upon it, if you ask it to think about it, in a step by step way, that actually makes the model spend more time thinking about it, generating more data and actually they can use all of that to get a better answer at the end. So it's crazy as it sounds. It's actually widely used. Later, we'll talk about chain of thought and some of the leading latest advances there, but this it's not a surprise that this approach works and can give you a boost in performance as well. Now, at this point, my friend, she'd squeezed as much as she could out of the models. So the next thing she thought about was, hmm, how am I going to improve the data going into the models? Because she spent a lot of time when working on this. You actually have to look at what users are typing in. She noticed that a lot of what the users were typing in just didn't make sense. Like some of the questions they were asking weren't even sequel related, not even related to the app. They were also asking other questions that were really ambiguous, where maybe they asked for the top products, but they got to specify what products. Frosty's has a lot of products. And so what she decided to do is add a screening layer where when people ask something that wasn't relevant or was ambiguous, that she was going to take that, give the feedback to the user and let them know, hey, your question isn't worded great. Let me give you some examples of how you could word it like that and take that vagueness out. So she had better quality responses coming in. And by doing that, she was able to continue to improve the performance of her model. And this was great. Another step forward. Then she thought about all that data that was coming in and she noticed that the questions all didn't ask the same type. There was different types of questions. Sometimes they were related to time series. Sometimes they were like ranking things. And so just like when we go to cook, we have lots of ingredients or data or features similarly here where if I'm cooking macaroni and cheese, I need different ingredients than if I'm making tacos. Same thing. If you're building data sets and features for time series, you need different data than for a ranking problem. So what she decided to do was look at those types of problems and then extract the best features for a specific problem. So she built this feature extraction layer that ended up giving her a bit of a boost. But at this point, like 75 percent, she was pretty happy with it. But you know, they have to her stakeholders are told to really they want 85, 90 percent before they have this out there like that. So, you know, she was really trying to figure it out because she squeezed everything she could out of the models. She's squeezed everything out of the data. She spent a lot of time going through that evaluation, looking at that data. And she realized, you know what? There's no way we're going to be able to solve this the way it is that the underlying data that we have, the schemas, the databases, the tables, they're just really messy and not organized in our company. So what she decided to do is let's call in the experts. Let's bring in the expertise. And we're going to use the expertise where we're going to create an additional semantic layer and that's going to give us details of the data structure in a way that deals with all this confusing structure, but also puts it in the words of our business users. So when they ask questions, our language model can quickly translate between the two. So the semantic layer was a bit more work to put together. But once she got that added on, she was able to boost performance of her model. And I should point out that the semantic layer, like all the text to SQL answers are doing that. You can do it without a semantic layer. If your data sets are cleaned, easy to use. It works great. If you're in a world of messiness, it doesn't, which is why many of the other providers also do such a semantic layer. So this is what the final application looks like. So this is what the final application looks like. And there's a couple of lessons here. One is I want everyone to understand that generative AI is not about a data scientist sitting out on an island by themselves, messing around with hyperparameters. That's not what's going to give you a finished application. Instead, it's building a system like this, a system that takes step by step work to do, a system that takes many people within the organization do. You need experts that build the semantic layer. You need data analysts that are asking the questions. You need evaluators to run this through. You need technical people to build the pipelines, to be able to build ensembles. Something like this touches a lot of people to actually get into production at a high quality to do. Now, some of you will probably be like, hey, well, what is this guy talking about? He doesn't know what he's talking about. I've seen the latest advances. Open AI is coming out with new stuff. We know Google's got a bunch coming. The future is you're just going to be able to just take all your data, cramming into one thing is just going to solve it all for you. Now, this is where so far we've talked a lot about retrieving information. This is getting into how well these models are able to reason and plan. How well when we give them a ton of information, can they figure out like, hey, how should I set up how to analyze this? And so when we get to these planning areas, folks in academia have been working on it for a while, there's benchmarks out there. And so this is one of the benchmarks that's used for assessing how well models can plan. And so this is called Block World. And we're going to use it because it helps us understand where we are with our models in this ability to plan and reason. Now, with Block World, we have a bunch of random, we have a bunch of blocks. We put them out here in kind of a random pattern and we ask it to stack it into a particular configuration. And as you can see, and I'll kind of play the video as I go through this, is it's not just one step to do this. Often what we have to do is you have to logically think and plan for maybe five, six, ten, even 20 steps to be able to solve it. And many times it's not always advancing every step. Sometimes you have to take care of a bunch of other steps before you can leap over and get everything together. And so this is how it measures that ability to think about subtasks, to think about planning like that. So when we take this type of Block World and we ask as GPT-4, which is the leading model when this study was initially done, you'll see it does OK, 34 percent. Like, I think like humans, like 90 percent like that. All right. So the researchers like, OK, well, let's change it up a little bit. We're going to use a little Shakespeare to guide us. Now Shakespeare says, arose by any other name would smell as sweet. What happens if we rename the blocks? If I name the blocks something else, now renaming the blocks shouldn't change the design of the problem. You still have to think through and stack and un-stack and think through the steps. Like what you call them doesn't matter. But for a large language model, what you does call them matters a lot. You'll see these models utterly go away with this, which tells us that that initial Block World, it's just that hallucination. It's just that LM just wanting to talk. That's getting us to that 34 percent. It's not really understanding the task of planning like that. Now, I did a video like this probably two weeks ago. This is where I left it. So just this week, the researchers go ahead and gave us the performance of Open AI's new 01 model and how that does. Now, the 01 model spends a lot more time thinking about what it's doing. It's that chain of thought on steroids with reinforcement learning. The bottom line is, although it spends a lot more time on inference, it's better able to solve these planning problems. You can see on Block World, it does really well. On Mystery World, it's quite a bit in advance over the previous results on Mystery World. Now, sometimes what happens is these benchmarks have been out there for a while. It's very easy to either accidentally or maybe purposely train on those. And so that can affect the performance of these models. You can overfit. So what the researchers did was actually recreate Mystery World, a new randomized version, and the model still got about 37 percent. So a clear kind of improvement over the state of the art in a very hard benchmark. So it's not there yet, but there is solid progress. And what I'm thinking and what I want everybody to be aware of, just like we saw with hallucinations, with the issues with the legal research, now this is going to happen in the reasoning and planning area, where we're going to get models that approximate reasoning. They've seen so much material, they have some sense of how to reason. They don't exactly go through the steps, but they can make us feel like they're doing it. And this approximate reasoning is also going to suffer from lots of mistakes and hallucinations. But here's the thing is, just like on the retrieval side, where you had to be a legal expert to do it, you're going to have to have an expert to be able to tell when this models are going off the rails in reasoning, because the baseline for these models is so good. They're at that human level like that. So this is a warning to us is don't just let your intuition, what you think of your gut, look at a result and think at it. You're going to have to bring in experts and really trust experts at these points. That's just how good these models are. Should celebrate it, but also recognize on the space. All right. Thank you all. This has been great. Hopefully these two use cases have given you a little sense of generative AI. I've covered a lot of significant concepts that we talk about in generative AI with hallucinations, retrieval augmented generation, reasoning, valuation model as a judge, chain of thought, how you modularize your gen AI development, data enrichment, all these pieces here like that. So what I want to make want to make sure everyone takes away from this is recognized. First of all, it's a project like any others. It's going to touch many people within the organization. These things often don't go as planned as when you're putting them together, but it's often takes that system, a group of people to build something useful like that. All right. Thank you all.",
      "platforms": {
        "tiktok": {
          "video_id": "7420988491426712878",
          "url": "https://www.tiktok.com/@rajistics/video/7420988491426712878",
          "view_count": 863,
          "upload_date": "2024-10-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/80939c3f2db94839bfe4f686d333e81a_1727833587~tplv-tiktokx-origin.image?dr=9636&x-expires=1767412800&x-signature=qrh0HcYs1Kd5xbFA0K1pbf95MWA%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I have lived this. #conwayslaw #softwaredevelopment #codetok #programming",
      "description": "I have lived this. #conwayslaw #softwaredevelopment #codetok #programming",
      "upload_date": "2022-05-17",
      "total_views": 846,
      "max_views": 846,
      "topics": [
        "codetok",
        "conwayslaw",
        "day",
        "every",
        "programming",
        "softwaredevelopment"
      ],
      "search_text": "I have lived this. #conwayslaw #softwaredevelopment #codetok #programming codetok conwayslaw day every programming softwaredevelopment Every day. Not all day every day, but every day.",
      "platforms": {
        "tiktok": {
          "video_id": "7098822796670831918",
          "url": "https://www.tiktok.com/@rajistics/video/7098822796670831918",
          "view_count": 846,
          "upload_date": "2022-05-17",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/47f3a47968c54f689245f259f499b512_1652823483~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=xVhAnS9wlD0RMtrzG%2BefzUpbmBs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Rerunning your old code #datascience #techtok #programming #analytics",
      "description": "Rerunning your old code #datascience #techtok #programming #analytics",
      "upload_date": "2022-04-12",
      "total_views": 843,
      "max_views": 843,
      "topics": [
        "analytics",
        "datascience",
        "old",
        "programming",
        "rerunning",
        "techtok"
      ],
      "search_text": "Rerunning your old code #datascience #techtok #programming #analytics analytics datascience old programming rerunning techtok Credit card ID. I'm so freakin pissed",
      "platforms": {
        "tiktok": {
          "video_id": "7085849997962693931",
          "url": "https://www.tiktok.com/@rajistics/video/7085849997962693931",
          "view_count": 843,
          "upload_date": "2022-04-12",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/5266d93c548d498f81fa034ed6dc3fa0_1649803018~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=7hTrl1VGmCTWVVvFMiNfiuvOv2A%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Data science work is hard to schedule and plan. It conflicts with agile methods. #datascience #machinelearning #dataanalytics #agile #scrummaster",
      "description": "Data science work is hard to schedule and plan. It conflicts with agile methods. #datascience #machinelearning #dataanalytics #agile #scrummaster",
      "upload_date": "2022-01-26",
      "total_views": 843,
      "max_views": 843,
      "topics": [
        "agile",
        "dataanalytics",
        "datascience",
        "long",
        "machinelearning",
        "scrummaster"
      ],
      "search_text": "Data science work is hard to schedule and plan. It conflicts with agile methods. #datascience #machinelearning #dataanalytics #agile #scrummaster agile dataanalytics datascience long machinelearning scrummaster I think it's gonna be a long long time",
      "platforms": {
        "tiktok": {
          "video_id": "7057539468814863663",
          "url": "https://www.tiktok.com/@rajistics/video/7057539468814863663",
          "view_count": 843,
          "upload_date": "2022-01-26",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/66fdcd06675441cc98be70be268d3f9b_1643211460~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=fNZwbV6HLGWUt4gkunk%2F2DXrLs8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Use these tips!",
      "description": "Use these tips!",
      "upload_date": "2024-08-02",
      "total_views": 827,
      "max_views": 827,
      "topics": [
        "data",
        "got",
        "know",
        "new",
        "really",
        "use"
      ],
      "search_text": "Use these tips! data got know new really use I've got my data ready to start applying algorithms. Whoa, do you really know your data? Isn't data the new oil? Isn't it all the same? Oh no, there's many aspects to data to consider. I once got data that came from customers who filled out the comment form. It turns out those were the happy customers and when I went to use that data for product improvement, it was biased, not really useful. My favorite example is how the definition of race has changed over time. It shows you what information is getting collected. What is it? Oh yeah, I've had to explain to a few people what type of Indian I am. Another issue I've had is having too much data where my analysis does worse. Huh? Is it more data better? You have to remember things change over time. There's new regulation, new social trends. So for some problems, it's useful to filter the data and remove that stuff out or otherwise it's just like adding noise to your analysis. Thanks everyone. I'm going to go get some takeout and go cuddle with my data this afternoon and get to know.",
      "platforms": {
        "tiktok": {
          "video_id": "7398547758623296811",
          "url": "https://www.tiktok.com/@rajistics/video/7398547758623296811",
          "view_count": 827,
          "upload_date": "2024-08-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/476c928d4fb84ddcb46b55a021da38c6_1722608647~tplv-tiktokx-origin.image?dr=9636&x-expires=1767452400&x-signature=1osXqf8wcxWmEBlFzKvKnvEpPUY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "I have no desire to build data infrastructure. I will leave that to my #dataengineer friends. #datascience ",
      "description": "I have no desire to build data infrastructure. I will leave that to my #dataengineer friends. #datascience ",
      "upload_date": "2022-12-02",
      "total_views": 802,
      "max_views": 802,
      "topics": [
        "build",
        "dataengineer",
        "datascience",
        "desire",
        "get",
        "gotta"
      ],
      "search_text": "I have no desire to build data infrastructure. I will leave that to my #dataengineer friends. #datascience  build dataengineer datascience desire get gotta I gotta go get it, I gotta go get it, I gotta go get it, I gotta go get it Woo! Miss me with that bullshit, do a sh-",
      "platforms": {
        "tiktok": {
          "video_id": "7172379693625265450",
          "url": "https://www.tiktok.com/@rajistics/video/7172379693625265450",
          "view_count": 802,
          "upload_date": "2022-12-02",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d0bb0b0493db479d8e6468a89c631a25_1669949789~tplv-tiktokx-origin.image?dr=9636&x-expires=1767477600&x-signature=EepPTzg6JfLahMAdHRe0SWQWzBs%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6289,
      "title": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches. #datascience #machinelearning #transformers #climatemodel",
      "description": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches. #datascience #machinelearning #transformers #climatemodel",
      "upload_date": "2023-02-07",
      "total_views": 799,
      "max_views": 799,
      "topics": [
        "based",
        "climatemodel",
        "climax",
        "datascience",
        "machinelearning",
        "models",
        "transformers"
      ],
      "search_text": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches. #datascience #machinelearning #transformers #climatemodel based climatemodel climax datascience machinelearning models transformers",
      "platforms": {
        "instagram": {
          "video_id": "17927808923639618",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ONmj7ggPFog",
          "url": "https://youtube.com/shorts/ONmj7ggPFog?feature=share",
          "view_count": 799,
          "upload_date": "2023-02-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6493,
      "title": "Corporate research labs have changed academic work with their reluctance to provide reproducible research and getting around blind peer review. No answers from me, but want you all to be aware. #datascience #machinelearning #neurips #reproducibility",
      "description": "Corporate research labs have changed academic work with their reluctance to provide reproducible research and getting around blind peer review. No answers from me, but want you all to be aware. #datascience #machinelearning #neurips #reproducibility",
      "upload_date": "2023-01-28",
      "total_views": 798,
      "max_views": 528,
      "topics": [
        "corporate",
        "datascience",
        "hey",
        "machinelearning",
        "neurips",
        "reproducibility",
        "research",
        "review"
      ],
      "search_text": "Corporate research labs have changed academic work with their reluctance to provide reproducible research and getting around blind peer review. No answers from me, but want you all to be aware. #datascience #machinelearning #neurips #reproducibility corporate datascience hey machinelearning neurips reproducibility research review",
      "platforms": {
        "instagram": {
          "video_id": "Cn-QsU-gDZs",
          "url": "https://www.instagram.com/p/Cn-QsU-gDZs/",
          "view_count": 270,
          "upload_date": "2023-01-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "9m0nnnheab4",
          "url": "https://www.youtube.com/watch?v=9m0nnnheab4",
          "view_count": 528,
          "upload_date": "2023-01-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "#aifilter #aifilterchallenge had to try it out and got a bit more buff ",
      "description": "#aifilter #aifilterchallenge had to try it out and got a bit more buff ",
      "upload_date": "2022-12-17",
      "total_views": 795,
      "max_views": 795,
      "topics": [
        "aifilter",
        "aifilterchallenge",
        "bit",
        "buff",
        "got",
        "try"
      ],
      "search_text": "#aifilter #aifilterchallenge had to try it out and got a bit more buff  aifilter aifilterchallenge bit buff got try 私が私のことを愛して何か悪いのシートでしょう",
      "platforms": {
        "tiktok": {
          "video_id": "7178262400620137771",
          "url": "https://www.tiktok.com/@rajistics/video/7178262400620137771",
          "view_count": 795,
          "upload_date": "2022-12-17",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/384865b5d4144675ad68b1e0f4a55b4a_1671319466~tplv-tiktokx-origin.image?dr=9636&x-expires=1767474000&x-signature=ueZO8bb2P8oTyLGJEwIdTV%2BjFJo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6028,
      "title": "FrugalGPT: Tips for saving money, processing time, and improving speed with LLMs",
      "description": "FrugalGPT: Tips for saving money, processing time, and improving speed with LLMs",
      "upload_date": "2024-06-08",
      "total_views": 792,
      "max_views": 792,
      "topics": [
        "frugalgpt",
        "great",
        "llm",
        "model",
        "money",
        "processing",
        "saving",
        "time",
        "tips",
        "use"
      ],
      "search_text": "FrugalGPT: Tips for saving money, processing time, and improving speed with LLMs frugalgpt great llm model money processing saving time tips use",
      "platforms": {
        "instagram": {
          "video_id": "18032474782851932",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AUF5bhiL8qI",
          "url": "https://www.youtube.com/watch?v=AUF5bhiL8qI",
          "view_count": 792,
          "upload_date": "2024-06-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Dreaded git push error. Had a little help tonight.  #git #datascience #python",
      "description": "Dreaded git push error. Had a little help tonight.  #git #datascience #python",
      "upload_date": "2022-07-23",
      "total_views": 780,
      "max_views": 780,
      "topics": [
        "datascience",
        "dreaded",
        "error",
        "git",
        "push",
        "python"
      ],
      "search_text": "Dreaded git push error. Had a little help tonight.  #git #datascience #python datascience dreaded error git push python Nothing scares me anymore Kiss me hard before I go",
      "platforms": {
        "tiktok": {
          "video_id": "7123378151060737322",
          "url": "https://www.tiktok.com/@rajistics/video/7123378151060737322",
          "view_count": 780,
          "upload_date": "2022-07-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/49961413239c4e21818bea999a282019_1658540724~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=QTej4eK9bLk2fWUYa4Io23Q%2Bzco%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6277,
      "title": "Roundup of all the big headlines, hope this is fun for you all. I laugh while making these, but wonder how many of you get all the refeenences. #datascience #machinelearning #openai #google #meta #stabilityai #elonmusk #apple #google",
      "description": "Roundup of all the big headlines, hope this is fun for you all. I laugh while making these, but wonder how many of you get all the refeenences. #datascience #machinelearning #openai #google #meta #stabilityai #elonmusk #apple #google",
      "upload_date": "2023-03-03",
      "total_views": 777,
      "max_views": 777,
      "topics": [
        "apple",
        "datascience",
        "google",
        "machinelearning",
        "meta",
        "openai",
        "stabilityai"
      ],
      "search_text": "Roundup of all the big headlines, hope this is fun for you all. I laugh while making these, but wonder how many of you get all the refeenences. #datascience #machinelearning #openai #google #meta #stabilityai #elonmusk #apple #google apple datascience google machinelearning meta openai stabilityai",
      "platforms": {
        "instagram": {
          "video_id": "17987970685884131",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "kCIB_X6H_ug",
          "url": "https://youtube.com/shorts/kCIB_X6H_ug?feature=share",
          "view_count": 777,
          "upload_date": "2023-03-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Context engineering works, until it doesn’t. Recursive Language Models ask a sharper question: why are humans managing memory, search, and pruning at inference time? RLMs let the model operate over context with code, not tokens. The Bitter Lesson, applied to inference. arXiv:2512.24601v1 ",
      "description": "Context engineering works, until it doesn’t. Recursive Language Models ask a sharper question: why are humans managing memory, search, and pruning at inference time? RLMs let the model operate over context with code, not tokens. The Bitter Lesson, applied to inference. arXiv:2512.24601v1 ",
      "upload_date": "2026-01-04",
      "total_views": 755,
      "max_views": 755,
      "topics": [
        "context",
        "engineering",
        "language",
        "long",
        "model",
        "models"
      ],
      "search_text": "Context engineering works, until it doesn’t. Recursive Language Models ask a sharper question: why are humans managing memory, search, and pruning at inference time? RLMs let the model operate over context with code, not tokens. The Bitter Lesson, applied to inference. arXiv:2512.24601v1  context engineering language long model models I'm tired of paying the context tax. There's gotta be a better way. What do you mean by a context tax? I have to summarize when the context gets too long, I have to prune the history, I'm only allowed to use a few tools. That's a lot of manual work. Can't the model just handle it? Oh, come on, models doing it themselves? That's crazy talk. Let me tell you about recursive language models. From the outside, they look like a normal language model, but you can keep sending it prompts and you don't have to worry about the context line. No way, what's actually happening here? So internally, it's running a REPL environment. So the context you provide is treated as a variable and not fed directly into the transformer. Instead, at query time, it uses code generation to operate on that context. It can search it, slice it, filter it, transform it, do what it does to get the right context it needs. So it figures out the right context by itself. You got it. It's gonna reason based on the structure of the task and figure out where to look in your context. Well, how is this different than compaction? Well, compaction assumes there's some details that you can remove. Here, the RLM isn't getting rid of anything. It's just deciding which pieces to use when. Okay, but does it actually work? On long context benchmarks across both open and closed models, RLMs have outperformed on retrieval, summarization, long context baselines at comparable or lower cost. Remember, this is about saving compute. What about all that context engineering I've built? It'll be great for the Computer History Museum.",
      "platforms": {
        "tiktok": {
          "video_id": "7591653901271616798",
          "url": "https://www.tiktok.com/@rajistics/video/7591653901271616798",
          "view_count": 755,
          "upload_date": "2026-01-04",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/ogkDFCH8vEpAECk1IAAREfQZVeWqpQA5kERRA6~tplv-tiktokx-origin.image?dr=9636&x-expires=1767754800&x-signature=J0CdE4PzvBs3r8rNzEbPmJ%2FYTEk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6137,
      "title": "ChatDoctor is a great example of fine tuning a large language model to get more factually correct output. This is an approach i expect many people to follow. #datascience #machinelearning #largelanguagemodels #chatgpt #chatdoctor #finetuning #instructiontuning",
      "description": "ChatDoctor is a great example of fine tuning a large language model to get more factually correct output. This is an approach i expect many people to follow. #datascience #machinelearning #largelanguagemodels #chatgpt #chatdoctor #finetuning #instructiontuning",
      "upload_date": "2023-03-22",
      "total_views": 749,
      "max_views": 749,
      "topics": [
        "chatdoctor",
        "chatgpt",
        "datascience",
        "finetuning",
        "largelanguagemodels",
        "machinelearning",
        "model"
      ],
      "search_text": "ChatDoctor is a great example of fine tuning a large language model to get more factually correct output. This is an approach i expect many people to follow. #datascience #machinelearning #largelanguagemodels #chatgpt #chatdoctor #finetuning #instructiontuning chatdoctor chatgpt datascience finetuning largelanguagemodels machinelearning model",
      "platforms": {
        "instagram": {
          "video_id": "17892462023730357",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "29DqGlTMS_k",
          "url": "https://www.youtube.com/watch?v=29DqGlTMS_k",
          "view_count": 749,
          "upload_date": "2023-03-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6261,
      "title": "Better LLMs (or not) including BloombergGPT, Databricks, NVIDIA, Amazon, and Falcon -",
      "description": "Better LLMs (or not) including BloombergGPT, Databricks, NVIDIA, Amazon, and Falcon -",
      "upload_date": "2024-04-12",
      "total_views": 742,
      "max_views": 742,
      "topics": [
        "better",
        "bloomberggpt",
        "chatgpt",
        "databricks",
        "including",
        "like",
        "llms",
        "model",
        "nvidia",
        "openai",
        "rlhf",
        "using"
      ],
      "search_text": "Better LLMs (or not) including BloombergGPT, Databricks, NVIDIA, Amazon, and Falcon - better bloomberggpt chatgpt databricks including like llms model nvidia openai rlhf using",
      "platforms": {
        "instagram": {
          "video_id": "17977330937529105",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cTCV8zFlp_E",
          "url": "https://www.youtube.com/watch?v=cTCV8zFlp_E",
          "view_count": 742,
          "upload_date": "2024-04-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "It‚Äôs exasperating. #techtok #datascience #programming",
      "description": "It‚Äôs exasperating. #techtok #datascience #programming",
      "upload_date": "2022-04-22",
      "total_views": 731,
      "max_views": 731,
      "topics": [
        "datascience",
        "exasperating",
        "programming",
        "techtok"
      ],
      "search_text": "It‚Äôs exasperating. #techtok #datascience #programming datascience exasperating programming techtok",
      "platforms": {
        "tiktok": {
          "video_id": "7089244388966157611",
          "url": "https://www.tiktok.com/@rajistics/video/7089244388966157611",
          "view_count": 731,
          "upload_date": "2022-04-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "New to Unix or Bash? This is a fast, visual walkthrough of the core terminal commands every beginner should know: where you are, how to move, how to create files, and how to inspect what’s running. No fluff.",
      "description": "New to Unix or Bash? This is a fast, visual walkthrough of the core terminal commands every beginner should know: where you are, how to move, how to create files, and how to inspect what’s running. No fluff.",
      "upload_date": "2026-01-04",
      "total_views": 721,
      "max_views": 721,
      "topics": [
        "file",
        "new",
        "running",
        "see",
        "unix",
        "want"
      ],
      "search_text": "New to Unix or Bash? This is a fast, visual walkthrough of the core terminal commands every beginner should know: where you are, how to move, how to create files, and how to inspect what’s running. No fluff. file new running see unix want Oh no! I'm in a Unix world and my mouse ran away! How will I survive? First of all, where am I? Huh. Let's look around a little bit. Come on, give me the details. Okay, but I want to go somewhere new now. Okay, enough looking. I want to make something new. How can I do that? So there I go, creating directories, files. What happens if I want to actually see this file? Oh, I can list the entire file. I can see just the top of the file. I can see just the bottom of the file. Pretty cool. Well, what else is going on in this system? Can I see everything that's running? Whoa, that's a lot of running things. What happens if I want to get rid of some of those things? All right. Well, that's pretty cool. Anybody else like this adventure?",
      "platforms": {
        "tiktok": {
          "video_id": "7591333245728410910",
          "url": "https://www.tiktok.com/@rajistics/video/7591333245728410910",
          "view_count": 721,
          "upload_date": "2026-01-04",
          "thumbnail_url": "https://p19-common-sign.tiktokcdn-us.com/tos-useast8-p-0068-tx2/oEMXZAk86qDHVIQShsLeICAVa1qDpeQnjeGqIJ~tplv-tiktokx-origin.image?dr=9636&x-expires=1767754800&x-signature=0%2FPQsNgkkeIeftqPCVzt8jqPMvU%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Being above average part II. Cite in comments.  @rajistics #statistics #regressiontothemean #aboveaverage",
      "description": "Being above average part II. Cite in comments.  @rajistics #statistics #regressiontothemean #aboveaverage",
      "upload_date": "2022-02-13",
      "total_views": 710,
      "max_views": 710,
      "topics": [
        "aboveaverage",
        "average",
        "less",
        "likely",
        "regressiontothemean",
        "statistics"
      ],
      "search_text": "Being above average part II. Cite in comments.  @rajistics #statistics #regressiontothemean #aboveaverage aboveaverage average less likely regressiontothemean statistics More examples of how we're all above average. Motorcyclists believe they're less likely to cause an accident than the typical rider. Business leaders think they're more likely to succeed than the average of their competitors. People think they're less likely to get the flu than the average person. Hmm. People who sign up for bungee jumping think they're less likely to get hurt than the average bungee jumper. Although their friends don't share that assessment. But hey, I know you're above average. Follow for more on stats and data science.",
      "platforms": {
        "tiktok": {
          "video_id": "7064210318821625135",
          "url": "https://www.tiktok.com/@rajistics/video/7064210318821625135",
          "view_count": 710,
          "upload_date": "2022-02-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/23703e2872ea41e889b116fc45a5793d_1644764638~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=hhePxGbBCwY8pqnnbnJJY4FjX3U%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6244,
      "title": "Practical Lessons in Building Generative AI: RAG and Text to SQL",
      "description": "Practical Lessons in Building Generative AI: RAG and Text to SQL",
      "upload_date": "2024-09-27",
      "total_views": 705,
      "max_views": 705,
      "topics": [
        "building",
        "frosty",
        "generative",
        "gpt",
        "lessons",
        "model",
        "practical",
        "rag",
        "text"
      ],
      "search_text": "Practical Lessons in Building Generative AI: RAG and Text to SQL building frosty generative gpt lessons model practical rag text",
      "platforms": {
        "instagram": {
          "video_id": "17986409360719801",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OyY4uxUShys",
          "url": "https://www.youtube.com/watch?v=OyY4uxUShys",
          "view_count": 705,
          "upload_date": "2024-09-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6009,
      "title": "Start Collecting Evaluation Data for Machine Learning Projects",
      "description": "Start Collecting Evaluation Data for Machine Learning Projects",
      "upload_date": "2024-08-09",
      "total_views": 694,
      "max_views": 694,
      "topics": [
        "app",
        "collecting",
        "data",
        "evaluation",
        "important",
        "learning",
        "machine",
        "start",
        "working"
      ],
      "search_text": "Start Collecting Evaluation Data for Machine Learning Projects app collecting data evaluation important learning machine start working",
      "platforms": {
        "instagram": {
          "video_id": "18025804034258381",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "n9svxDu70HA",
          "url": "https://www.youtube.com/watch?v=n9svxDu70HA",
          "view_count": 694,
          "upload_date": "2024-08-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Tuskegee Airman by geo karamanis links to code in comments #TidyTuesday #rstats #datascience #datavisualization",
      "description": "Tuskegee Airman by geo karamanis links to code in comments #TidyTuesday #rstats #datascience #datavisualization",
      "upload_date": "2022-02-14",
      "total_views": 674,
      "max_views": 674,
      "topics": [
        "airman",
        "datascience",
        "datavisualization",
        "rstats",
        "tidytuesday",
        "tuskegee"
      ],
      "search_text": "Tuskegee Airman by geo karamanis links to code in comments #TidyTuesday #rstats #datascience #datavisualization airman datascience datavisualization rstats tidytuesday tuskegee",
      "platforms": {
        "tiktok": {
          "video_id": "7064592954727615791",
          "url": "https://www.tiktok.com/@rajistics/video/7064592954727615791",
          "view_count": 674,
          "upload_date": "2022-02-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "self driving cars and data quality - LOA - #datascience #machinelearning #selfdrivingcar #stanford #data #stats",
      "description": "self driving cars and data quality - LOA - #datascience #machinelearning #selfdrivingcar #stanford #data #stats",
      "upload_date": "2022-01-25",
      "total_views": 674,
      "max_views": 674,
      "topics": [
        "data",
        "datascience",
        "machinelearning",
        "selfdrivingcar",
        "stanford",
        "stats"
      ],
      "search_text": "self driving cars and data quality - LOA - #datascience #machinelearning #selfdrivingcar #stanford #data #stats data datascience machinelearning selfdrivingcar stanford stats Oh boy, let me tell you what I learned about self-driving cars today. Researchers from Stanford studied a dataset used to train self-driving cars. Large portions of the dataset had errors. There were parts where there was actually cars in the scenes, but they weren't marked. And what this means is a model that's trained there probably won't see that car because it doesn't know that that's a car. This is not good.",
      "platforms": {
        "tiktok": {
          "video_id": "7057295747598814510",
          "url": "https://www.tiktok.com/@rajistics/video/7057295747598814510",
          "view_count": 674,
          "upload_date": "2022-01-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ca75c17a9a584478bb115d3bc187bb5d_1643154714~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=Rk8ScL%2FvNiUtW4b3gCAgqon7va4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6580,
      "title": "Optimization using the Python Optimal Transport Package",
      "description": "Optimization using the Python Optimal Transport Package",
      "upload_date": "2023-04-24",
      "total_views": 670,
      "max_views": 670,
      "topics": [
        "computervision",
        "datascience",
        "diffusion",
        "machinelearning",
        "objectdetection",
        "optimal",
        "optimization",
        "package",
        "python",
        "transport",
        "using"
      ],
      "search_text": "Optimization using the Python Optimal Transport Package computervision datascience diffusion machinelearning objectdetection optimal optimization package python transport using",
      "platforms": {
        "instagram": {
          "video_id": "ClPcAHmAbsr",
          "url": "https://www.instagram.com/reel/ClPcAHmAbsr/",
          "view_count": 0,
          "upload_date": "2022-11-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "IKtw2QTrG0A",
          "url": "https://www.youtube.com/watch?v=IKtw2QTrG0A",
          "view_count": 670,
          "upload_date": "2023-04-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6558,
      "title": "Optimal punt return #datascience #nfl",
      "description": "Optimal punt return #datascience #nfl",
      "upload_date": "2022-01-10",
      "total_views": 668,
      "max_views": 668,
      "topics": [
        "datascience",
        "nfl",
        "optimal",
        "punt",
        "return"
      ],
      "search_text": "Optimal punt return #datascience #nfl datascience nfl optimal punt return",
      "platforms": {
        "tiktok": {
          "video_id": "7051509780434849070",
          "url": "https://www.tiktok.com/@rajistics/video/7051509780434849070",
          "view_count": 668,
          "upload_date": "2022-01-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "MTkNPVGhq6Q",
          "url": "https://youtube.com/shorts/MTkNPVGhq6Q?feature=share",
          "view_count": 0,
          "upload_date": "2022-01-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Those pesky outliers.  ",
      "description": "Those pesky outliers.  ",
      "upload_date": "2022-10-20",
      "total_views": 666,
      "max_views": 666,
      "topics": [
        "dealt",
        "find",
        "outliers",
        "pesky",
        "soon",
        "trust"
      ],
      "search_text": "Those pesky outliers.   dealt find outliers pesky soon trust So as soon as we find out who you are, trust you will be dealt with.",
      "platforms": {
        "tiktok": {
          "video_id": "7156619973518003499",
          "url": "https://www.tiktok.com/@rajistics/video/7156619973518003499",
          "view_count": 666,
          "upload_date": "2022-10-20",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/fab0a7ef3e5a467ebc00dfc38d9bd87b_1666280464~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=2wy7StrgiUTcnTKCn0EXR1reyxE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6294,
      "title": "My second try to explain in context learning or few shot learning with large language models. It‚Äôs very cool and why these models are so exciting. My older video is here @rajistics #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning",
      "description": "My second try to explain in context learning or few shot learning with large language models. It‚Äôs very cool and why these models are so exciting. My older video is here @rajistics #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning",
      "upload_date": "2023-01-27",
      "total_views": 660,
      "max_views": 660,
      "topics": [
        "datascience",
        "fewshotlearning",
        "gpt3",
        "incontextlearning",
        "largelanguagemodels",
        "machinelearning",
        "models"
      ],
      "search_text": "My second try to explain in context learning or few shot learning with large language models. It‚Äôs very cool and why these models are so exciting. My older video is here @rajistics #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning datascience fewshotlearning gpt3 incontextlearning largelanguagemodels machinelearning models",
      "platforms": {
        "instagram": {
          "video_id": "17923075301648844",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EGUnS0L8bvY",
          "url": "https://www.youtube.com/watch?v=EGUnS0L8bvY",
          "view_count": 660,
          "upload_date": "2023-01-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "One of my favorites for #explainability #datascience #statistics #interpretability #codetok #python #machinelearning",
      "description": "One of my favorites for #explainability #datascience #statistics #interpretability #codetok #python #machinelearning",
      "upload_date": "2022-06-28",
      "total_views": 655,
      "max_views": 655,
      "topics": [
        "codetok",
        "datascience",
        "explainability",
        "interpretability",
        "python",
        "statistics"
      ],
      "search_text": "One of my favorites for #explainability #datascience #statistics #interpretability #codetok #python #machinelearning codetok datascience explainability interpretability python statistics And Crack up.",
      "platforms": {
        "tiktok": {
          "video_id": "7114309643693837610",
          "url": "https://www.tiktok.com/@rajistics/video/7114309643693837610",
          "view_count": 655,
          "upload_date": "2022-06-28",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/77f30328a0e54ef7833f34d0b7b60ee1_1656429299~tplv-tiktokx-origin.image?dr=9636&x-expires=1767492000&x-signature=QnddFi9jJp%2FTznLUcoqPjToooZk%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Some things are bigger than data science, I have a personal connection here and have to express my support. #ukraine #priceoffreedom #datascience",
      "description": "Some things are bigger than data science, I have a personal connection here and have to express my support. #ukraine #priceoffreedom #datascience",
      "upload_date": "2022-02-25",
      "total_views": 637,
      "max_views": 637,
      "topics": [
        "bigger",
        "data",
        "datascience",
        "priceoffreedom",
        "things",
        "ukraine"
      ],
      "search_text": "Some things are bigger than data science, I have a personal connection here and have to express my support. #ukraine #priceoffreedom #datascience bigger data datascience priceoffreedom things ukraine Sometimes I think of Buzz U, late nights in the middle of June, he ways been faking me out.",
      "platforms": {
        "tiktok": {
          "video_id": "7068471905782811950",
          "url": "https://www.tiktok.com/@rajistics/video/7068471905782811950",
          "view_count": 637,
          "upload_date": "2022-02-25",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/e51044319ddd4c50a2a02e7b9e0c8e00_1645756865~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=55Brp2x4uJjeNz9Nu9iEB4InpSo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6010,
      "title": "Interpretable Machine Learning Models Simply Explained - Rulefit, GA2M, Rule Lists, and Scorecard",
      "description": "Interpretable Machine Learning Models Simply Explained - Rulefit, GA2M, Rule Lists, and Scorecard",
      "upload_date": "2024-08-10",
      "total_views": 632,
      "max_views": 632,
      "topics": [
        "ability",
        "accuracy",
        "explain",
        "explained",
        "interpretable",
        "learning",
        "machine",
        "models",
        "simply",
        "tradeoff"
      ],
      "search_text": "Interpretable Machine Learning Models Simply Explained - Rulefit, GA2M, Rule Lists, and Scorecard ability accuracy explain explained interpretable learning machine models simply tradeoff",
      "platforms": {
        "instagram": {
          "video_id": "18037556009009159",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "lx4SJOVtxI8",
          "url": "https://www.youtube.com/watch?v=lx4SJOVtxI8",
          "view_count": 632,
          "upload_date": "2024-08-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6510,
      "title": "How computers think: Classification and Regression. #datascience #machinelearning #fruitbowl",
      "description": "How computers think: Classification and Regression. #datascience #machinelearning #fruitbowl",
      "upload_date": "2022-01-11",
      "total_views": 632,
      "max_views": 322,
      "topics": [
        "classification",
        "computers",
        "datascience",
        "fruitbowl",
        "machinelearning",
        "think"
      ],
      "search_text": "How computers think: Classification and Regression. #datascience #machinelearning #fruitbowl classification computers datascience fruitbowl machinelearning think",
      "platforms": {
        "tiktok": {
          "video_id": "7052088405085179182",
          "url": "https://www.tiktok.com/@rajistics/video/7052088405085179182",
          "view_count": 322,
          "upload_date": "2022-01-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "IrjBfX1Gp4Y",
          "url": "https://youtube.com/shorts/IrjBfX1Gp4Y?feature=share",
          "view_count": 310,
          "upload_date": "2022-01-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Reply to @noleli median versus mean",
      "description": "Reply to @noleli median versus mean",
      "upload_date": "2022-02-11",
      "total_views": 622,
      "max_views": 622,
      "topics": [
        "average",
        "half",
        "income",
        "mean",
        "median",
        "people"
      ],
      "search_text": "Reply to @noleli median versus mean average half income mean median people Let's simplify this by using income as an example. Income has a very different distribution with a few people making lots of money. So if we look at the average or mean income, we'll see it seems a little high. The median is the midpoint between all the people. So half the people make more, half the people make less. It's important to separate out these concepts. It is a bit more nebulous when you start asking people are they above average. In this case, the common sense meeting is that 50% is the median and the average. But it's a lot of fun to talk this stuff out.",
      "platforms": {
        "tiktok": {
          "video_id": "7063532909063507247",
          "url": "https://www.tiktok.com/@rajistics/video/7063532909063507247",
          "view_count": 622,
          "upload_date": "2022-02-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d8b853a7d734401c9bb8ee5fd3e0be61_1644606917~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=IhXMwrqB4pxGjq8rnXOOlg42p3M%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "Analysis never ends. ",
      "description": "Analysis never ends. ",
      "upload_date": "2022-10-23",
      "total_views": 620,
      "max_views": 620,
      "topics": [
        "analysis",
        "ends",
        "never",
        "problem"
      ],
      "search_text": "Analysis never ends.  analysis ends never problem It's me, hi, hi, on the problem, it's me, on the problem",
      "platforms": {
        "tiktok": {
          "video_id": "7157754592468405546",
          "url": "https://www.tiktok.com/@rajistics/video/7157754592468405546",
          "view_count": 620,
          "upload_date": "2022-10-23",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/b3b46dd5816f471aa0377f2ffa9b7cc0_1666544620~tplv-tiktokx-origin.image?dr=9636&x-expires=1767481200&x-signature=R%2FRKvliB5oCGqbxS06PiFl%2FvUss%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "The damage I have done with root access. What have you done? #codetok #python ",
      "description": "The damage I have done with root access. What have you done? #codetok #python ",
      "upload_date": "2022-09-09",
      "total_views": 611,
      "max_views": 611,
      "topics": [
        "codetok",
        "damage",
        "done",
        "lose",
        "python",
        "root"
      ],
      "search_text": "The damage I have done with root access. What have you done? #codetok #python  codetok damage done lose python root Go ahead. Let's light this candle, huh? I mean, sure, I'll lose everything, but then... I'll have nothing to lose.",
      "platforms": {
        "tiktok": {
          "video_id": "7141413940537576750",
          "url": "https://www.tiktok.com/@rajistics/video/7141413940537576750",
          "view_count": 611,
          "upload_date": "2022-09-09",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/39375b97bd3b417984d9ffa85dde61b2_1662740011~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=RXnc28guiAwSkJuzKI0jFw%2BvMi4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6139,
      "title": "Pair programming is some of my favorite times as a data scientist. I am starting to use ChatGPT to fill that role lately. Its useful for me. #datascience #machinelearning #pairprogramming #chatgpt #codex",
      "description": "Pair programming is some of my favorite times as a data scientist. I am starting to use ChatGPT to fill that role lately. Its useful for me. #datascience #machinelearning #pairprogramming #chatgpt #codex",
      "upload_date": "2023-03-19",
      "total_views": 609,
      "max_views": 609,
      "topics": [
        "chatgpt",
        "codex",
        "datascience",
        "machinelearning",
        "pair",
        "pairprogramming",
        "way"
      ],
      "search_text": "Pair programming is some of my favorite times as a data scientist. I am starting to use ChatGPT to fill that role lately. Its useful for me. #datascience #machinelearning #pairprogramming #chatgpt #codex chatgpt codex datascience machinelearning pair pairprogramming way",
      "platforms": {
        "instagram": {
          "video_id": "17929836353647732",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NAsYNfTD58c",
          "url": "https://youtube.com/shorts/NAsYNfTD58c?feature=share",
          "view_count": 609,
          "upload_date": "2023-03-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6281,
      "title": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft",
      "description": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft",
      "upload_date": "2023-02-22",
      "total_views": 587,
      "max_views": 587,
      "topics": [
        "chatgpt",
        "customer",
        "customerlifetimevalue",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "marketinganalytics",
        "microsoft",
        "rfm",
        "robotics"
      ],
      "search_text": "ChatGPT for Robotics is the latest hot paper. Large language models are the future interface. #datascience #machinelearning #largelanguagemodels #chatgpt #microsoft #robotics #promptcraft chatgpt customer customerlifetimevalue datascience largelanguagemodels machinelearning marketinganalytics microsoft rfm robotics",
      "platforms": {
        "instagram": {
          "video_id": "17998849210713189",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3PrcRdBR1Xo",
          "url": "https://youtube.com/shorts/3PrcRdBR1Xo?feature=share",
          "view_count": 587,
          "upload_date": "2023-02-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6255,
      "title": "Funny stuff, not created by me - #datascience #codetok #deeplearning",
      "description": "Funny stuff, not created by me - #datascience #codetok #deeplearning",
      "upload_date": "2022-08-03",
      "total_views": 584,
      "max_views": 407,
      "topics": [
        "codetok",
        "created",
        "data",
        "datascience",
        "deeplearning",
        "finetuning",
        "funny",
        "language",
        "largelanguagemodels",
        "model",
        "norwegian",
        "rutergpt",
        "stuff",
        "vasamuseet"
      ],
      "search_text": "Funny stuff, not created by me - #datascience #codetok #deeplearning codetok created data datascience deeplearning finetuning funny language largelanguagemodels model norwegian rutergpt stuff vasamuseet",
      "platforms": {
        "tiktok": {
          "video_id": "7127637917329296686",
          "url": "https://www.tiktok.com/@rajistics/video/7127637917329296686",
          "view_count": 407,
          "upload_date": "2022-08-03",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "18027984407064425",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "CTXM4_ifJWw",
          "url": "https://www.youtube.com/watch?v=CTXM4_ifJWw",
          "view_count": 177,
          "upload_date": "2024-05-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6136,
      "title": "Scandals in AI: Objaverse, Llama, Alpaca, and Dolly",
      "description": "Scandals in AI: Objaverse, Llama, Alpaca, and Dolly",
      "upload_date": "2023-03-27",
      "total_views": 581,
      "max_views": 581,
      "topics": [
        "alpaca",
        "data",
        "dolly",
        "largelanguagemodels",
        "llama",
        "meta",
        "objaverse",
        "scandals"
      ],
      "search_text": "Scandals in AI: Objaverse, Llama, Alpaca, and Dolly alpaca data dolly largelanguagemodels llama meta objaverse scandals",
      "platforms": {
        "instagram": {
          "video_id": "18057978955377136",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "31u88EDmIwc",
          "url": "https://www.youtube.com/watch?v=31u88EDmIwc",
          "view_count": 581,
          "upload_date": "2023-03-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6488,
      "title": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning",
      "description": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning",
      "upload_date": "2022-12-20",
      "total_views": 581,
      "max_views": 581,
      "topics": [
        "chatgpt",
        "clouds",
        "datascience",
        "generating",
        "machinelearning",
        "openai",
        "point",
        "retrievalaugmentedmodel",
        "youchat"
      ],
      "search_text": "Point-E from #openai. Generating 3D point clouds from text #datascience #machinelearning chatgpt clouds datascience generating machinelearning openai point retrievalaugmentedmodel youchat",
      "platforms": {
        "instagram": {
          "video_id": "CmvMeJOhfS7",
          "url": "https://www.instagram.com/reel/CmvMeJOhfS7/",
          "view_count": 0,
          "upload_date": "2022-12-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ozucBM3HqsQ",
          "url": "https://youtube.com/shorts/ozucBM3HqsQ",
          "view_count": 581,
          "upload_date": "2022-12-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6566,
      "title": "Our fellow algorithms calling mom featuring our linear model, XGBoost, and Neural Networks. I had fun making them.",
      "description": "Our fellow algorithms calling mom featuring our linear model, XGBoost, and Neural Networks. I had fun making them.",
      "upload_date": "2022-10-21",
      "total_views": 581,
      "max_views": 581,
      "topics": [
        "algorithms",
        "calling",
        "craiyon",
        "dallemini",
        "datascience",
        "featuring",
        "fellow",
        "linear",
        "machinelearning",
        "mom",
        "stablediffusion",
        "texttoimage"
      ],
      "search_text": "Our fellow algorithms calling mom featuring our linear model, XGBoost, and Neural Networks. I had fun making them. algorithms calling craiyon dallemini datascience featuring fellow linear machinelearning mom stablediffusion texttoimage",
      "platforms": {
        "tiktok": {
          "video_id": "7156750486991850798",
          "url": "https://www.tiktok.com/@rajistics/video/7156750486991850798",
          "view_count": 581,
          "upload_date": "2022-10-21",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CkUYGiMgeec",
          "url": "https://www.instagram.com/reel/CkUYGiMgeec/",
          "view_count": 0,
          "upload_date": "2022-10-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "False positive and false negative #datascience #statistics #decionmaking #classificationalgorithm #algorithm",
      "description": "False positive and false negative #datascience #statistics #decionmaking #classificationalgorithm #algorithm",
      "upload_date": "2022-02-06",
      "total_views": 575,
      "max_views": 575,
      "topics": [
        "aim",
        "algorithm",
        "classificationalgorithm",
        "datascience",
        "decionmaking",
        "statistics"
      ],
      "search_text": "False positive and false negative #datascience #statistics #decionmaking #classificationalgorithm #algorithm aim algorithm classificationalgorithm datascience decionmaking statistics They forget my name, aim, aim, aim They call me hell",
      "platforms": {
        "tiktok": {
          "video_id": "7061654789184146735",
          "url": "https://www.tiktok.com/@rajistics/video/7061654789184146735",
          "view_count": 575,
          "upload_date": "2022-02-06",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8cc8ce667f8247fd8686f8a2a79e33f3_1644169632~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=cQrk%2BQ%2F086DI2uVCv4%2FgTq3xyUw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6305,
      "title": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt",
      "description": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt",
      "upload_date": "2023-01-07",
      "total_views": 563,
      "max_views": 563,
      "topics": [
        "blip",
        "datascience",
        "deepmind",
        "git",
        "image",
        "largelanguagemodels",
        "machinelearning",
        "microsoft",
        "model",
        "nvidia",
        "openai",
        "see"
      ],
      "search_text": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt blip datascience deepmind git image largelanguagemodels machinelearning microsoft model nvidia openai see",
      "platforms": {
        "instagram": {
          "video_id": "17882313761734502",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NvgNI3waAy4",
          "url": "https://www.youtube.com/watch?v=NvgNI3waAy4",
          "view_count": 563,
          "upload_date": "2023-01-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6659,
      "title": "Deciding whether to use a Large Language Model or a smaller model? This video explores the tradeoffs between both approaches based on the latest research (May 2023) on the performance of these models. The video covers the effectiveness of LLMs where smaller models best LLMs and criteria for deciding between the two. #machinelearning #datascience #largelanguagemodels",
      "description": "Deciding whether to use a Large Language Model or a smaller model? This video explores the tradeoffs between both approaches based on the latest research (May 2023) on the performance of these models. The video covers the effectiveness of LLMs where smaller models best LLMs and criteria for deciding between the two. #machinelearning #datascience #largelanguagemodels",
      "upload_date": "2023-06-02",
      "total_views": 549,
      "max_views": 549,
      "topics": [
        "datascience",
        "deciding",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "smaller"
      ],
      "search_text": "Deciding whether to use a Large Language Model or a smaller model? This video explores the tradeoffs between both approaches based on the latest research (May 2023) on the performance of these models. The video covers the effectiveness of LLMs where smaller models best LLMs and criteria for deciding between the two. #machinelearning #datascience #largelanguagemodels datascience deciding largelanguagemodels machinelearning model smaller",
      "platforms": {
        "instagram": {
          "video_id": "CtACmZ3ABew",
          "url": "https://www.instagram.com/reel/CtACmZ3ABew",
          "view_count": 549,
          "upload_date": "2023-06-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "We all play roulette with stackoverflow. #programming #datascience #python",
      "description": "We all play roulette with stackoverflow. #programming #datascience #python",
      "upload_date": "2022-03-24",
      "total_views": 538,
      "max_views": 538,
      "topics": [
        "datascience",
        "play",
        "programming",
        "python",
        "roulette",
        "stackoverflow"
      ],
      "search_text": "We all play roulette with stackoverflow. #programming #datascience #python datascience play programming python roulette stackoverflow",
      "platforms": {
        "tiktok": {
          "video_id": "7078469611783474475",
          "url": "https://www.tiktok.com/@rajistics/video/7078469611783474475",
          "view_count": 538,
          "upload_date": "2022-03-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6330,
      "title": "It‚Äôs almost here. Full support for pandas in sklearn pipelines. #machinelearning #datascience #codetok #python #sklearn #sci-kit",
      "description": "It‚Äôs almost here. Full support for pandas in sklearn pipelines. #machinelearning #datascience #codetok #python #sklearn #sci-kit",
      "upload_date": "2022-10-18",
      "total_views": 534,
      "max_views": 476,
      "topics": [
        "baseline",
        "codetok",
        "datascience",
        "machinelearning",
        "model",
        "python",
        "sci",
        "sklearn",
        "statistics",
        "timeseries"
      ],
      "search_text": "It‚Äôs almost here. Full support for pandas in sklearn pipelines. #machinelearning #datascience #codetok #python #sklearn #sci-kit baseline codetok datascience machinelearning model python sci sklearn statistics timeseries",
      "platforms": {
        "instagram": {
          "video_id": "17957173889116268",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 58,
          "upload_date": "2022-10-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "3d2Hp45lt5I",
          "url": "https://youtube.com/shorts/3d2Hp45lt5I?feature=share",
          "view_count": 476,
          "upload_date": "2022-10-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Don’t feel bad if you havent put a machine learning model into production. Lots of valuable data scientist haven’t done fhat.",
      "description": "Don’t feel bad if you havent put a machine learning model into production. Lots of valuable data scientist haven’t done fhat.",
      "upload_date": "2022-08-11",
      "total_views": 533,
      "max_views": 533,
      "topics": [
        "bad",
        "don",
        "feel",
        "havent",
        "machine",
        "put"
      ],
      "search_text": "Don’t feel bad if you havent put a machine learning model into production. Lots of valuable data scientist haven’t done fhat. bad don feel havent machine put DJ Calvin, we the best! Hooray! We...",
      "platforms": {
        "tiktok": {
          "video_id": "7130430476224924970",
          "url": "https://www.tiktok.com/@rajistics/video/7130430476224924970",
          "view_count": 533,
          "upload_date": "2022-08-11",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ff28380218c24907b7cc2ab4a2c4db15_1660182721~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=jVXlBgCdM6g36EDzlMHstQckxyw%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6241,
      "title": "Selecting and Speeding up your Sentence Transformer Models",
      "description": "Selecting and Speeding up your Sentence Transformer Models",
      "upload_date": "2024-10-15",
      "total_views": 528,
      "max_views": 528,
      "topics": [
        "embeddings",
        "models",
        "selecting",
        "sentence",
        "speeding",
        "static",
        "transformer",
        "use",
        "word",
        "word2vec"
      ],
      "search_text": "Selecting and Speeding up your Sentence Transformer Models embeddings models selecting sentence speeding static transformer use word word2vec",
      "platforms": {
        "instagram": {
          "video_id": "18336518587177415",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "WQqAN4k3R4g",
          "url": "https://www.youtube.com/watch?v=WQqAN4k3R4g",
          "view_count": 528,
          "upload_date": "2024-10-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Machine learning engineer growing career #machinelearning #datascience #dataengineering #programming #ai #career #programmingbootcamp #statistics",
      "description": "Machine learning engineer growing career #machinelearning #datascience #dataengineering #programming #ai #career #programmingbootcamp #statistics",
      "upload_date": "2022-01-29",
      "total_views": 520,
      "max_views": 520,
      "topics": [
        "career",
        "dataengineering",
        "datascience",
        "machinelearning",
        "programming",
        "programmingbootcamp"
      ],
      "search_text": "Machine learning engineer growing career #machinelearning #datascience #dataengineering #programming #ai #career #programmingbootcamp #statistics career dataengineering datascience machinelearning programming programmingbootcamp Hey, it's Career Day, kiddos. You want to learn about a job that pays well, and you can build some cool stuff. One of the fastest growing jobs is becoming a machine learning engineer. Let's talk about it. Machine learning engineers really help to make analytics or machine learning AI real by focusing on how they can productionize that to help an enterprise gain value from all the data that they have. You're not going to be able to become an ML engineer overnight, but for those of you willing to stick with it, a lot of the ML engineers just have a few years of experience out there. It's a totally rewarding job, and if you have questions, let me know.",
      "platforms": {
        "tiktok": {
          "video_id": "7058720432245230895",
          "url": "https://www.tiktok.com/@rajistics/video/7058720432245230895",
          "view_count": 520,
          "upload_date": "2022-01-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/8d9555b417534fb6aa40867fab121326_1643486425~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=pnFeSaYjf9WWXyyiH9k6qO%2BuDCQ%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6283,
      "title": "Speculating on GPT-4 size and performance. #datascience #machinelearning #gpt3 #gpt4",
      "description": "Speculating on GPT-4 size and performance. #datascience #machinelearning #gpt3 #gpt4",
      "upload_date": "2023-02-20",
      "total_views": 516,
      "max_views": 516,
      "topics": [
        "datascience",
        "gpt",
        "gpt3",
        "gpt4",
        "machinelearning",
        "size",
        "speculating"
      ],
      "search_text": "Speculating on GPT-4 size and performance. #datascience #machinelearning #gpt3 #gpt4 datascience gpt gpt3 gpt4 machinelearning size speculating",
      "platforms": {
        "instagram": {
          "video_id": "18084302257324227",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "xuwpFIyfYLg",
          "url": "https://www.youtube.com/watch?v=xuwpFIyfYLg",
          "view_count": 516,
          "upload_date": "2023-02-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6494,
      "title": "GPT4 hype that it will be 100 trillion parameters. This doesn‚Äôt make any sense. See the video on scaling laws @rajistics and think about the compute resources for inference. #datascience #machinelearning #openai #gpt4",
      "description": "GPT4 hype that it will be 100 trillion parameters. This doesn‚Äôt make any sense. See the video on scaling laws @rajistics and think about the compute resources for inference. #datascience #machinelearning #openai #gpt4",
      "upload_date": "2023-01-16",
      "total_views": 514,
      "max_views": 514,
      "topics": [
        "datascience",
        "gpt4",
        "hype",
        "machinelearning",
        "openai",
        "trillion"
      ],
      "search_text": "GPT4 hype that it will be 100 trillion parameters. This doesn‚Äôt make any sense. See the video on scaling laws @rajistics and think about the compute resources for inference. #datascience #machinelearning #openai #gpt4 datascience gpt4 hype machinelearning openai trillion",
      "platforms": {
        "instagram": {
          "video_id": "CngDDhqhCPO",
          "url": "https://www.instagram.com/p/CngDDhqhCPO/",
          "view_count": 0,
          "upload_date": "2023-01-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "5QOaVLDLgaw",
          "url": "https://youtube.com/shorts/5QOaVLDLgaw?feature=share",
          "view_count": 514,
          "upload_date": "2023-01-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6497,
      "title": "Anthropic is starting to preview their model and people are comparing it to ChatGPT. Thanks to Riley Goodside for sharing screenshots. It looks pretty impressive. #datascience #machinelearning #largelanguagemodels #anthropic #claude #chatgpt",
      "description": "Anthropic is starting to preview their model and people are comparing it to ChatGPT. Thanks to Riley Goodside for sharing screenshots. It looks pretty impressive. #datascience #machinelearning #largelanguagemodels #anthropic #claude #chatgpt",
      "upload_date": "2023-01-08",
      "total_views": 508,
      "max_views": 508,
      "topics": [
        "anthropic",
        "big",
        "bigdatabowl",
        "chatgpt",
        "claude",
        "data",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "nfl",
        "statistics"
      ],
      "search_text": "Anthropic is starting to preview their model and people are comparing it to ChatGPT. Thanks to Riley Goodside for sharing screenshots. It looks pretty impressive. #datascience #machinelearning #largelanguagemodels #anthropic #claude #chatgpt anthropic big bigdatabowl chatgpt claude data datascience largelanguagemodels machinelearning nfl statistics",
      "platforms": {
        "instagram": {
          "video_id": "CnU7Wk-ByQg",
          "url": "https://www.instagram.com/p/CnU7Wk-ByQg/",
          "view_count": 0,
          "upload_date": "2023-01-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "djZ4-hWRMBo",
          "url": "https://youtube.com/shorts/djZ4-hWRMBo?feature=share",
          "view_count": 508,
          "upload_date": "2023-01-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "At least it will be faster to build the second time.  Ugh. How often have you had to recode something?",
      "description": "At least it will be faster to build the second time.  Ugh. How often have you had to recode something?",
      "upload_date": "2022-09-29",
      "total_views": 501,
      "max_views": 501,
      "topics": [
        "build",
        "faster",
        "least",
        "second",
        "time",
        "ugh"
      ],
      "search_text": "At least it will be faster to build the second time.  Ugh. How often have you had to recode something? build faster least second time ugh How are you doing Gemma? Brilliant, love in it.",
      "platforms": {
        "tiktok": {
          "video_id": "7148600366282591531",
          "url": "https://www.tiktok.com/@rajistics/video/7148600366282591531",
          "view_count": 501,
          "upload_date": "2022-09-29",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/1ade3ea1e28145e3ab04b21bf52e8335_1664413230~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=7SCZUPCNV4Djqsz1kn9Q6bF4IYo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "So many hyperparameters - this is from pytorch forecasting #datascience #machinelearning #hyperparameters #coding #algorithms #modeling",
      "description": "So many hyperparameters - this is from pytorch forecasting #datascience #machinelearning #hyperparameters #coding #algorithms #modeling",
      "upload_date": "2022-01-30",
      "total_views": 495,
      "max_views": 495,
      "topics": [
        "algorithms",
        "coding",
        "datascience",
        "hyperparameters",
        "machinelearning",
        "modeling"
      ],
      "search_text": "So many hyperparameters - this is from pytorch forecasting #datascience #machinelearning #hyperparameters #coding #algorithms #modeling algorithms coding datascience hyperparameters machinelearning modeling",
      "platforms": {
        "tiktok": {
          "video_id": "7059015967464115503",
          "url": "https://www.tiktok.com/@rajistics/video/7059015967464115503",
          "view_count": 495,
          "upload_date": "2022-01-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6502,
      "title": "Getting explainability when working with transformer based image or vision models. Uses Captum on the backend, but makes it easy to get image attributions. #datascience #machinelearning #computervision #captum #huggingface #explainability #codetok",
      "description": "Getting explainability when working with transformer based image or vision models. Uses Captum on the backend, but makes it easy to get image attributions. #datascience #machinelearning #computervision #captum #huggingface #explainability #codetok",
      "upload_date": "2022-10-19",
      "total_views": 490,
      "max_views": 456,
      "topics": [
        "captum",
        "computervision",
        "datascience",
        "explainability",
        "huggingface",
        "machinelearning"
      ],
      "search_text": "Getting explainability when working with transformer based image or vision models. Uses Captum on the backend, but makes it easy to get image attributions. #datascience #machinelearning #computervision #captum #huggingface #explainability #codetok captum computervision datascience explainability huggingface machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "Cj6a8MGAPAu",
          "url": "https://www.instagram.com/reel/Cj6a8MGAPAu/",
          "view_count": 34,
          "upload_date": "2022-10-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "GalkewWcF40",
          "url": "https://youtube.com/shorts/GalkewWcF40?feature=share",
          "view_count": 456,
          "upload_date": "2022-10-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Talk business not data science metrics to have a business impact #datascience #machinelearning #statistics #analytics",
      "description": "Talk business not data science metrics to have a business impact #datascience #machinelearning #statistics #analytics",
      "upload_date": "2022-02-16",
      "total_views": 490,
      "max_views": 490,
      "topics": [
        "analytics",
        "business",
        "datascience",
        "machinelearning",
        "running",
        "statistics"
      ],
      "search_text": "Talk business not data science metrics to have a business impact #datascience #machinelearning #statistics #analytics analytics business datascience machinelearning running statistics I was running through the six where my walls Yeah! I was running through the six where my walls",
      "platforms": {
        "tiktok": {
          "video_id": "7065123541297057071",
          "url": "https://www.tiktok.com/@rajistics/video/7065123541297057071",
          "view_count": 490,
          "upload_date": "2022-02-16",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/d622ebb56d62469980df1b5f53d1d330_1644977263~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=NzEsdgoK%2BvDf2yWunQMUpRA0F3I%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6027,
      "title": "Hallucination-Free? Assessing the Reliability ofLeading AI Legal Research Tool",
      "description": "Hallucination-Free? Assessing the Reliability ofLeading AI Legal Research Tool",
      "upload_date": "2024-06-01",
      "total_views": 477,
      "max_views": 477,
      "topics": [
        "assessing",
        "companies",
        "forrester",
        "free",
        "hallucination",
        "legal",
        "like",
        "make",
        "marketing",
        "ofleading",
        "reliability",
        "research"
      ],
      "search_text": "Hallucination-Free? Assessing the Reliability ofLeading AI Legal Research Tool assessing companies forrester free hallucination legal like make marketing ofleading reliability research",
      "platforms": {
        "instagram": {
          "video_id": "18048051169706222",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "DKjUBdHNggU",
          "url": "https://www.youtube.com/watch?v=DKjUBdHNggU",
          "view_count": 477,
          "upload_date": "2024-06-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6499,
      "title": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning",
      "description": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning",
      "upload_date": "2022-04-05",
      "total_views": 476,
      "max_views": 476,
      "topics": [
        "datascience",
        "learning",
        "machinelearning",
        "reinforcement",
        "reinforcementlearning",
        "techtok"
      ],
      "search_text": "Reinforcement learning with my Eat Melon! Demo based on Karpathy #datascience #reinforcementlearning #techtok #machinelearning datascience learning machinelearning reinforcement reinforcementlearning techtok",
      "platforms": {
        "youtube": {
          "video_id": "a17hHQfjLbE",
          "url": "https://youtube.com/shorts/a17hHQfjLbE?feature=share",
          "view_count": 476,
          "upload_date": "2022-04-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "AI Literacy, Question 1, can AI think by itself? #ai #datascience #programming #counsciousness #alleninstitute #literacy #capcut",
      "description": "AI Literacy, Question 1, can AI think by itself? #ai #datascience #programming #counsciousness #alleninstitute #literacy #capcut",
      "upload_date": "2022-01-16",
      "total_views": 471,
      "max_views": 471,
      "topics": [
        "alleninstitute",
        "capcut",
        "counsciousness",
        "datascience",
        "literacy",
        "programming"
      ],
      "search_text": "AI Literacy, Question 1, can AI think by itself? #ai #datascience #programming #counsciousness #alleninstitute #literacy #capcut alleninstitute capcut counsciousness datascience literacy programming",
      "platforms": {
        "tiktok": {
          "video_id": "7053822995688738095",
          "url": "https://www.tiktok.com/@rajistics/video/7053822995688738095",
          "view_count": 471,
          "upload_date": "2022-01-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6338,
      "title": "Speed up XGBoost using Hist split method (faster than Exact, and Approx)",
      "description": "Speed up XGBoost using Hist split method (faster than Exact, and Approx)",
      "upload_date": "2024-10-27",
      "total_views": 465,
      "max_views": 465,
      "topics": [
        "actually",
        "disease",
        "hist",
        "let",
        "look",
        "method",
        "model",
        "regression",
        "speed",
        "split",
        "using",
        "xgboost"
      ],
      "search_text": "Speed up XGBoost using Hist split method (faster than Exact, and Approx) actually disease hist let look method model regression speed split using xgboost",
      "platforms": {
        "instagram": {
          "video_id": "18161193448257562",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "UDWCkiD8m6o",
          "url": "https://www.youtube.com/watch?v=UDWCkiD8m6o",
          "view_count": 465,
          "upload_date": "2024-10-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6503,
      "title": "Replying to @rajistics as promised, the feature or variables in auto insurance models. Keep the feedback coming. #datascience #machinelearning #autoinsurance #acturialscience earlier video on insurance @rajistics",
      "description": "Replying to @rajistics as promised, the feature or variables in auto insurance models. Keep the feedback coming. #datascience #machinelearning #autoinsurance #acturialscience earlier video on insurance @rajistics",
      "upload_date": "2023-02-12",
      "total_views": 435,
      "max_views": 435,
      "topics": [
        "acturialscience",
        "autoinsurance",
        "datascience",
        "datavisualization",
        "great",
        "histogram",
        "histograms",
        "insurance",
        "machinelearning",
        "replying",
        "statistics"
      ],
      "search_text": "Replying to @rajistics as promised, the feature or variables in auto insurance models. Keep the feedback coming. #datascience #machinelearning #autoinsurance #acturialscience earlier video on insurance @rajistics acturialscience autoinsurance datascience datavisualization great histogram histograms insurance machinelearning replying statistics",
      "platforms": {
        "instagram": {
          "video_id": "CodvpSUglht",
          "url": "https://www.instagram.com/p/CodvpSUglht/",
          "view_count": 0,
          "upload_date": "2023-02-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OUY8XHU3qiU",
          "url": "https://youtube.com/shorts/OUY8XHU3qiU",
          "view_count": 435,
          "upload_date": "2023-02-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6324,
      "title": "Using Logit Bias in Large Language Models",
      "description": "Using Logit Bias in Large Language Models",
      "upload_date": "2023-11-11",
      "total_views": 428,
      "max_views": 428,
      "topics": [
        "bias",
        "datascience",
        "language",
        "large",
        "like",
        "logit",
        "machinelearning",
        "microsoft",
        "models",
        "rust",
        "using"
      ],
      "search_text": "Using Logit Bias in Large Language Models bias datascience language large like logit machinelearning microsoft models rust using",
      "platforms": {
        "instagram": {
          "video_id": "17955474602041155",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AKBhGhZ3EYQ",
          "url": "https://www.youtube.com/watch?v=AKBhGhZ3EYQ",
          "view_count": 428,
          "upload_date": "2023-11-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "How I added my TikTok, instagram, and YouTube videos to my website.  I used Buzzlytics to gather the information, python to munge all the data, GPT-4 to add some title information, and ChatGPT to help me with my web site. I used next.js and vercel for my page. Feel free to copy my code and use it for your own site. Resources: Buzzlytics: https://www.tiktokviewcount.com/ rajivshah.com: https://rajivshah.com/ GitHub for my site: https://github.com/rajshah4/rajiv-shah-website Original developer of my website template: https://github.com/manuarora700/simple-developer-portfolio-website #vercel #nextjs #analytics #Buzzlytics #chatgpt #rajistics #rajivshah",
      "description": "How I added my TikTok, instagram, and YouTube videos to my website.  I used Buzzlytics to gather the information, python to munge all the data, GPT-4 to add some title information, and ChatGPT to help me with my web site. I used next.js and vercel for my page. Feel free to copy my code and use it for your own site. Resources: Buzzlytics: https://www.tiktokviewcount.com/ rajivshah.com: https://rajivshah.com/ GitHub for my site: https://github.com/rajshah4/rajiv-shah-website Original developer of my website template: https://github.com/manuarora700/simple-developer-portfolio-website #vercel #nextjs #analytics #Buzzlytics #chatgpt #rajistics #rajivshah",
      "upload_date": "2024-04-02",
      "total_views": 426,
      "max_views": 426,
      "topics": [
        "buzzlytics",
        "chatgpt",
        "rajivshah",
        "vercel",
        "videos",
        "website"
      ],
      "search_text": "How I added my TikTok, instagram, and YouTube videos to my website.  I used Buzzlytics to gather the information, python to munge all the data, GPT-4 to add some title information, and ChatGPT to help me with my web site. I used next.js and vercel for my page. Feel free to copy my code and use it for your own site. Resources: Buzzlytics: https://www.tiktokviewcount.com/ rajivshah.com: https://rajivshah.com/ GitHub for my site: https://github.com/rajshah4/rajiv-shah-website Original developer of my website template: https://github.com/manuarora700/simple-developer-portfolio-website #vercel #nextjs #analytics #Buzzlytics #chatgpt #rajistics #rajivshah buzzlytics chatgpt rajivshah vercel videos website I've made a lot of TikTok videos, but how can I get a list of them? What happens if I want to put that list on my website? Let me share how I figured out how. I did it in one day, used a bit of AI along the way. The first step was getting my TikTok data. I didn't want to run my own scraper. Instead, I went over to Buzz Analytics, which gives me the ability to see all of my data and some nice person, even ask for an export button so you can grab it all on a spreadsheet. I then used my Python munging skills to combine the Instagram, YouTube, and TikTok data into one spreadsheet, but I had one problem. I didn't have a title at all for my videos. This is where GPT-4 came in, where I had a simple prompt, read the descriptions, give me titles for each of my videos. Now that I had all my data in one spreadsheet or one table, the next step was adding it to the website. Last year, I had upgraded my website from static, boring, HTML to using Next.js, Vercel. I wanted to play around with those latest technologies. Go steal the code if you want. I only know a little bit of JavaScript, but thanks to chatGPT, I was able to chat back and forth, figure out how I could add all those videos as a nice listing where I wanted features like to expand the description for lengthy descriptions and even be able to search the entire page. There's a lot more I can do, better organization. Say you wanted just a feature engineering videos, adding thumbnails in, using the transcripts to get meta tags and better data. But this is a hobby project. I just do this for fun. I was just happy. I'll talk this part in a day.",
      "platforms": {
        "tiktok": {
          "video_id": "7353091416957914414",
          "url": "https://www.tiktok.com/@rajistics/video/7353091416957914414",
          "view_count": 426,
          "upload_date": "2024-04-02",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/ddb862ed62164b739b64c517c33e7a8e_1712025021~tplv-tiktokx-origin.image?dr=9636&x-expires=1767463200&x-signature=8XpTh3ZKQ%2FCDZTl985JZ8e0iRko%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6246,
      "title": "Text Similarity Techniques: Lexical, Semantic, and Hashing",
      "description": "Text Similarity Techniques: Lexical, Semantic, and Hashing",
      "upload_date": "2024-09-29",
      "total_views": 397,
      "max_views": 397,
      "topics": [
        "des",
        "hashing",
        "les",
        "lexical",
        "mais",
        "pour",
        "qui",
        "semantic",
        "similarity",
        "techniques",
        "text",
        "vous"
      ],
      "search_text": "Text Similarity Techniques: Lexical, Semantic, and Hashing des hashing les lexical mais pour qui semantic similarity techniques text vous",
      "platforms": {
        "instagram": {
          "video_id": "18037267702964759",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "WyNUbzRsELw",
          "url": "https://www.youtube.com/watch?v=WyNUbzRsELw",
          "view_count": 397,
          "upload_date": "2024-09-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "TikTok video #7141035352076094762",
      "description": "TikTok video #7141035352076094762",
      "upload_date": "2022-09-08",
      "total_views": 388,
      "max_views": 388,
      "topics": [
        "7141035352076094762",
        "ask",
        "chance",
        "don",
        "video"
      ],
      "search_text": "TikTok video #7141035352076094762 7141035352076094762 ask chance don video is there a chance? Don't ask! Is there a chance? No!",
      "platforms": {
        "tiktok": {
          "video_id": "7141035352076094762",
          "url": "https://www.tiktok.com/@rajistics/video/7141035352076094762",
          "view_count": 388,
          "upload_date": "2022-09-08",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/54b93d4740ab4d5ca96b5eae8b434630_1662651863~tplv-tiktokx-origin.image?dr=9636&x-expires=1767484800&x-signature=%2FaCiTN%2FnvB2fLGhvdyg54q568uE%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6026,
      "title": "Choosing Transformer, Word2Vec, or a Sentence Transformer",
      "description": "Choosing Transformer, Word2Vec, or a Sentence Transformer",
      "upload_date": "2024-06-13",
      "total_views": 382,
      "max_views": 382,
      "topics": [
        "choosing",
        "even",
        "get",
        "like",
        "sentence",
        "transformer",
        "transformers",
        "word2vec"
      ],
      "search_text": "Choosing Transformer, Word2Vec, or a Sentence Transformer choosing even get like sentence transformer transformers word2vec",
      "platforms": {
        "instagram": {
          "video_id": "18061755988572945",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Hd_RqAKjyvk",
          "url": "https://www.youtube.com/watch?v=Hd_RqAKjyvk",
          "view_count": 382,
          "upload_date": "2024-06-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6299,
      "title": "How companies use your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience",
      "description": "How companies use your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience",
      "upload_date": "2023-01-19",
      "total_views": 356,
      "max_views": 356,
      "topics": [
        "bigcode",
        "code",
        "companies",
        "copilot",
        "data",
        "datascience",
        "github",
        "huggingface"
      ],
      "search_text": "How companies use your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience bigcode code companies copilot data datascience github huggingface",
      "platforms": {
        "instagram": {
          "video_id": "18007138537553278",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ewT8W98Dd0E",
          "url": "https://youtube.com/shorts/ewT8W98Dd0E",
          "view_count": 356,
          "upload_date": "2023-01-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6358,
      "title": "Transformer Explainer (Full Video) - Interactive Visualization for Transformers",
      "description": "Transformer Explainer (Full Video) - Interactive Visualization for Transformers",
      "upload_date": "2024-08-11",
      "total_views": 353,
      "max_views": 353,
      "topics": [
        "audio",
        "explainer",
        "face",
        "full",
        "hugging",
        "interactive",
        "speech",
        "speecht5",
        "text",
        "transformer",
        "video",
        "visualization"
      ],
      "search_text": "Transformer Explainer (Full Video) - Interactive Visualization for Transformers audio explainer face full hugging interactive speech speecht5 text transformer video visualization",
      "platforms": {
        "instagram": {
          "video_id": "17873037296790655",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "PoNCVJUruPE",
          "url": "https://www.youtube.com/watch?v=PoNCVJUruPE",
          "view_count": 353,
          "upload_date": "2024-08-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6269,
      "title": "Don't let people overlook open source software. It might be free but it's priceless. The Value of Open Source Software at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148 #opensource #rajistics",
      "description": "Don't let people overlook open source software. It might be free but it's priceless. The Value of Open Source Software at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148 #opensource #rajistics",
      "upload_date": "2024-01-19",
      "total_views": 349,
      "max_views": 349,
      "topics": [
        "code",
        "don",
        "open",
        "opensource",
        "papers",
        "software",
        "source",
        "value"
      ],
      "search_text": "Don't let people overlook open source software. It might be free but it's priceless. The Value of Open Source Software at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148 #opensource #rajistics code don open opensource papers software source value",
      "platforms": {
        "instagram": {
          "video_id": "17949693584633269",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-01-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "EdGR8Xt6shM",
          "url": "https://youtube.com/shorts/EdGR8Xt6shM",
          "view_count": 349,
          "upload_date": "2024-01-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6242,
      "title": "ColPali: Bringing Vision Language Models to Document Retrieval",
      "description": "ColPali: Bringing Vision Language Models to Document Retrieval",
      "upload_date": "2024-10-10",
      "total_views": 346,
      "max_views": 346,
      "topics": [
        "also",
        "bringing",
        "colpali",
        "die",
        "document",
        "language",
        "models",
        "und",
        "vision",
        "wir"
      ],
      "search_text": "ColPali: Bringing Vision Language Models to Document Retrieval also bringing colpali die document language models und vision wir",
      "platforms": {
        "instagram": {
          "video_id": "18039370630942186",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_FEyALdAd_A",
          "url": "https://www.youtube.com/watch?v=_FEyALdAd_A",
          "view_count": 346,
          "upload_date": "2024-10-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6278,
      "title": "ChatGPT price drop. Let‚Äôs break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain",
      "description": "ChatGPT price drop. Let‚Äôs break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain",
      "upload_date": "2023-03-02",
      "total_views": 342,
      "max_views": 342,
      "topics": [
        "anthropic",
        "chatgpt",
        "cohere",
        "datascience",
        "langchain",
        "machinelearning",
        "openai"
      ],
      "search_text": "ChatGPT price drop. Let‚Äôs break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain anthropic chatgpt cohere datascience langchain machinelearning openai",
      "platforms": {
        "instagram": {
          "video_id": "18090428821319410",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-03",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "hwMPFBK47Ts",
          "url": "https://youtube.com/shorts/hwMPFBK47Ts?feature=share",
          "view_count": 342,
          "upload_date": "2023-03-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6023,
      "title": "State of Generative AI in 2024 and How it is Falling Short",
      "description": "State of Generative AI in 2024 and How it is Falling Short",
      "upload_date": "2024-07-01",
      "total_views": 337,
      "max_views": 337,
      "topics": [
        "bigcode",
        "evaluation",
        "falling",
        "functionalcorrectness",
        "generative",
        "huggingface",
        "short",
        "state",
        "unit",
        "unittests"
      ],
      "search_text": "State of Generative AI in 2024 and How it is Falling Short bigcode evaluation falling functionalcorrectness generative huggingface short state unit unittests",
      "platforms": {
        "instagram": {
          "video_id": "18047751313757076",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": " -cHpnzNAJsw",
          "url": "https://www.youtube.com/watch?v= -cHpnzNAJsw",
          "view_count": 337,
          "upload_date": "2024-07-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6084,
      "title": "Retrieval Secrets from the #1 Solution in the Kaggle Eedi Math Competition",
      "description": "Retrieval Secrets from the #1 Solution in the Kaggle Eedi Math Competition",
      "upload_date": "2025-01-03",
      "total_views": 333,
      "max_views": 333,
      "topics": [
        "1",
        "data",
        "eedi",
        "going",
        "kaggle",
        "retrieval",
        "retrievers",
        "secrets",
        "solution",
        "synthetic",
        "use",
        "used"
      ],
      "search_text": "Retrieval Secrets from the #1 Solution in the Kaggle Eedi Math Competition 1 data eedi going kaggle retrieval retrievers secrets solution synthetic use used",
      "platforms": {
        "instagram": {
          "video_id": "18069546838689507",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Bnn2m4S22T4",
          "url": "https://www.youtube.com/watch?v=Bnn2m4S22T4",
          "view_count": 333,
          "upload_date": "2025-01-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "I hope this pain isn’t shared widely #techtok #powerpoint #datascientist",
      "description": "I hope this pain isn’t shared widely #techtok #powerpoint #datascientist",
      "upload_date": "2022-04-12",
      "total_views": 332,
      "max_views": 332,
      "topics": [
        "datascientist",
        "hope",
        "isn",
        "pain",
        "powerpoint",
        "techtok"
      ],
      "search_text": "I hope this pain isn’t shared widely #techtok #powerpoint #datascientist datascientist hope isn pain powerpoint techtok That's so neat for that.",
      "platforms": {
        "tiktok": {
          "video_id": "7085728684753030442",
          "url": "https://www.tiktok.com/@rajistics/video/7085728684753030442",
          "view_count": 332,
          "upload_date": "2022-04-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/0541ed1e61f54e0ab93f7afd803cee7c_1649774773~tplv-tiktokx-origin.image?dr=9636&x-expires=1767502800&x-signature=tOlx64Am3DF9LMepwuT7YUANkt4%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6303,
      "title": "Big data bowl submissions are going in and lots of great sports analytic work. This one is on strain for evaluating pass rushers. #datascience #statistics #bigdatabowl #nfl",
      "description": "Big data bowl submissions are going in and lots of great sports analytic work. This one is on strain for evaluating pass rushers. #datascience #statistics #bigdatabowl #nfl",
      "upload_date": "2023-01-13",
      "total_views": 330,
      "max_views": 330,
      "topics": [
        "big",
        "bigdatabowl",
        "data",
        "datascience",
        "defensive",
        "ends",
        "football",
        "nfl",
        "pass",
        "physics",
        "statistics",
        "understand"
      ],
      "search_text": "Big data bowl submissions are going in and lots of great sports analytic work. This one is on strain for evaluating pass rushers. #datascience #statistics #bigdatabowl #nfl big bigdatabowl data datascience defensive ends football nfl pass physics statistics understand",
      "platforms": {
        "instagram": {
          "video_id": "17962673681273599",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "aMp6TdPA0qQ",
          "url": "https://youtube.com/shorts/aMp6TdPA0qQ?feature=share",
          "view_count": 330,
          "upload_date": "2023-01-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6398,
      "title": "Explanations in Machine Learning",
      "description": "Explanations in Machine Learning",
      "upload_date": "2022-08-10",
      "total_views": 314,
      "max_views": 314,
      "topics": [
        "explanations",
        "learning",
        "machine"
      ],
      "search_text": "Explanations in Machine Learning explanations learning machine",
      "platforms": {
        "youtube": {
          "video_id": "SVfrxFdJNB4",
          "url": "https://www.youtube.com/watch?v=SVfrxFdJNB4",
          "view_count": 314,
          "upload_date": "2022-08-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6401,
      "title": "I still havent tried copilot. Have you? #datascience #codetok #codex #copilot #python",
      "description": "I still havent tried copilot. Have you? #datascience #codetok #codex #copilot #python",
      "upload_date": "2022-07-27",
      "total_views": 309,
      "max_views": 309,
      "topics": [
        "codetok",
        "codex",
        "copilot",
        "datascience",
        "flant5",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "python",
        "reasoningwithpeople",
        "still"
      ],
      "search_text": "I still havent tried copilot. Have you? #datascience #codetok #codex #copilot #python codetok codex copilot datascience flant5 gpt3 largelanguagemodels machinelearning python reasoningwithpeople still",
      "platforms": {
        "instagram": {
          "video_id": "CoH--YZAqbU",
          "url": "https://www.instagram.com/p/CoH--YZAqbU/",
          "view_count": 0,
          "upload_date": "2023-01-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_woxZ2tBBwU",
          "url": "https://youtube.com/shorts/_woxZ2tBBwU?feature=share",
          "view_count": 309,
          "upload_date": "2022-07-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Logical song  full explanation here: @rajistics  #sijinx #maddencurse #stats #analytics #regression",
      "description": "Logical song  full explanation here: @rajistics  #sijinx #maddencurse #stats #analytics #regression",
      "upload_date": "2022-01-21",
      "total_views": 308,
      "max_views": 308,
      "topics": [
        "analytics",
        "logical",
        "maddencurse",
        "regression",
        "sijinx",
        "stats"
      ],
      "search_text": "Logical song  full explanation here: @rajistics  #sijinx #maddencurse #stats #analytics #regression analytics logical maddencurse regression sijinx stats I said, watch what you say They'll be calling you a radical A liberal Or a fanatic A criminal Oh won't you sign up your name We'd like to feel your acceptance",
      "platforms": {
        "tiktok": {
          "video_id": "7055773825149177134",
          "url": "https://www.tiktok.com/@rajistics/video/7055773825149177134",
          "view_count": 308,
          "upload_date": "2022-01-21",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f36e5001e8694d088bb2d8167e1b1512_1642800363~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=Rhtczc7VNITeCG0nqg7IUv0vzvY%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6670,
      "title": "Recognizing Meta AI's contribution",
      "description": "Recognizing Meta AI's contribution",
      "upload_date": "2023-05-14",
      "total_views": 307,
      "max_views": 307,
      "topics": [
        "contribution",
        "meta",
        "recognizing"
      ],
      "search_text": "Recognizing Meta AI's contribution contribution meta recognizing",
      "platforms": {
        "youtube": {
          "video_id": "QUrC6JQFMII",
          "url": "https://www.youtube.com/watch?v=QUrC6JQFMII",
          "view_count": 307,
          "upload_date": "2023-05-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6237,
      "title": "MedEmbed: Fine-Tuned Embedding Models for Medical / Clinical",
      "description": "MedEmbed: Fine-Tuned Embedding Models for Medical / Clinical",
      "upload_date": "2024-10-25",
      "total_views": 298,
      "max_views": 298,
      "topics": [
        "das",
        "die",
        "eine",
        "embedding",
        "fine",
        "ist",
        "medembed",
        "medical",
        "models",
        "tuned",
        "und",
        "wir"
      ],
      "search_text": "MedEmbed: Fine-Tuned Embedding Models for Medical / Clinical das die eine embedding fine ist medembed medical models tuned und wir",
      "platforms": {
        "instagram": {
          "video_id": "18040299521324616",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "K4Fy6TxjRhk",
          "url": "https://www.youtube.com/watch?v=K4Fy6TxjRhk",
          "view_count": 298,
          "upload_date": "2024-10-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "The pain. Data munging on poorly prepped data. #datascience #analytics #csv",
      "description": "The pain. Data munging on poorly prepped data. #datascience #analytics #csv",
      "upload_date": "2022-03-22",
      "total_views": 297,
      "max_views": 297,
      "topics": [
        "analytics",
        "csv",
        "data",
        "datascience",
        "munging",
        "pain"
      ],
      "search_text": "The pain. Data munging on poorly prepped data. #datascience #analytics #csv analytics csv data datascience munging pain",
      "platforms": {
        "tiktok": {
          "video_id": "7078077917338750254",
          "url": "https://www.tiktok.com/@rajistics/video/7078077917338750254",
          "view_count": 297,
          "upload_date": "2022-03-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6248,
      "title": "Feature Selection with Boruta, MRMR, and Recursive Feature Elimination",
      "description": "Feature Selection with Boruta, MRMR, and Recursive Feature Elimination",
      "upload_date": "2024-09-22",
      "total_views": 292,
      "max_views": 292,
      "topics": [
        "boruta",
        "elimination",
        "explained",
        "feature",
        "many",
        "methods",
        "mrmr",
        "recursive",
        "selection"
      ],
      "search_text": "Feature Selection with Boruta, MRMR, and Recursive Feature Elimination boruta elimination explained feature many methods mrmr recursive selection",
      "platforms": {
        "instagram": {
          "video_id": "18054323056676346",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-21",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "hCb4YUAaez8",
          "url": "https://www.youtube.com/watch?v=hCb4YUAaez8",
          "view_count": 292,
          "upload_date": "2024-09-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6399,
      "title": "Overdue for sports analytics #datascience #analytics #codetok #sportsanalytics #machinelearning",
      "description": "Overdue for sports analytics #datascience #analytics #codetok #sportsanalytics #machinelearning",
      "upload_date": "2022-08-12",
      "total_views": 286,
      "max_views": 286,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "machinelearning",
        "overdue",
        "sportsanalytics"
      ],
      "search_text": "Overdue for sports analytics #datascience #analytics #codetok #sportsanalytics #machinelearning analytics codetok datascience machinelearning overdue sportsanalytics",
      "platforms": {
        "youtube": {
          "video_id": "VH2bD939RJ0",
          "url": "https://youtube.com/shorts/VH2bD939RJ0?feature=share",
          "view_count": 286,
          "upload_date": "2022-08-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Accuracy is not your friend for most problems #datascience #media #norythm",
      "description": "Accuracy is not your friend for most problems #datascience #media #norythm",
      "upload_date": "2022-01-13",
      "total_views": 280,
      "max_views": 280,
      "topics": [
        "accuracy",
        "datascience",
        "friend",
        "media",
        "norythm",
        "problems"
      ],
      "search_text": "Accuracy is not your friend for most problems #datascience #media #norythm accuracy datascience friend media norythm problems Don't wanna won't happen I'm finna tell on you",
      "platforms": {
        "tiktok": {
          "video_id": "7052487312562916654",
          "url": "https://www.tiktok.com/@rajistics/video/7052487312562916654",
          "view_count": 280,
          "upload_date": "2022-01-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/f9a8471b06af477a80473cbd15928d7b_1642035162~tplv-tiktokx-origin.image?dr=9636&x-expires=1767510000&x-signature=DHcxNTfi%2BJxlHw8iXsjDYVvKcUo%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": null,
      "title": "2022 plans #2022 #datascience #ai #machinelearning #skillup start with AI Literacy @rajistics",
      "description": "2022 plans #2022 #datascience #ai #machinelearning #skillup start with AI Literacy @rajistics",
      "upload_date": "2022-01-19",
      "total_views": 279,
      "max_views": 279,
      "topics": [
        "2022",
        "ai",
        "datascience",
        "machinelearning",
        "plans",
        "skillup"
      ],
      "search_text": "2022 plans #2022 #datascience #ai #machinelearning #skillup start with AI Literacy @rajistics 2022 ai datascience machinelearning plans skillup",
      "platforms": {
        "tiktok": {
          "video_id": "7054946595808300334",
          "url": "https://www.tiktok.com/@rajistics/video/7054946595808300334",
          "view_count": 279,
          "upload_date": "2022-01-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "i love notebooks #notebooks #programming #rstats",
      "description": "i love notebooks #notebooks #programming #rstats",
      "upload_date": "2022-01-12",
      "total_views": 274,
      "max_views": 274,
      "topics": [
        "cannot",
        "cigarette",
        "love",
        "notebooks",
        "programming",
        "rstats"
      ],
      "search_text": "i love notebooks #notebooks #programming #rstats cannot cigarette love notebooks programming rstats It's just a cigarette and it cannot be that bad Money, don't you love me and you know it makes me sad",
      "platforms": {
        "tiktok": {
          "video_id": "7052324967324814638",
          "url": "https://www.tiktok.com/@rajistics/video/7052324967324814638",
          "view_count": 274,
          "upload_date": "2022-01-12",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/2dbf71e63f2e431aaee58b21c54c0ef6_1641997365~tplv-tiktokx-origin.image?dr=9636&x-expires=1767510000&x-signature=a4wtQVEmGQ%2BlIzAq6l2%2B%2BDkav30%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6600,
      "title": "How an Image Classifier Learns (using time lapse)",
      "description": "How an Image Classifier Learns (using time lapse)",
      "upload_date": "2017-04-18",
      "total_views": 263,
      "max_views": 263,
      "topics": [
        "cheating",
        "classifier",
        "datascience",
        "image",
        "kaggle",
        "lapse",
        "learns",
        "machinelearning",
        "ottocompetition",
        "reared",
        "time",
        "using"
      ],
      "search_text": "How an Image Classifier Learns (using time lapse) cheating classifier datascience image kaggle lapse learns machinelearning ottocompetition reared time using",
      "platforms": {
        "instagram": {
          "video_id": "CoEHiiBgNJP",
          "url": "https://www.instagram.com/p/CoEHiiBgNJP/",
          "view_count": 0,
          "upload_date": "2023-01-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_Xo2lfsjYkE",
          "url": "https://www.youtube.com/watch?v=_Xo2lfsjYkE",
          "view_count": 263,
          "upload_date": "2017-04-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6316,
      "title": "It‚Äôs important to make sure your model is well calibrated. This becomes especially important with imbalanced data. #machinelearning #datascience #statistics",
      "description": "It‚Äôs important to make sure your model is well calibrated. This becomes especially important with imbalanced data. #machinelearning #datascience #statistics",
      "upload_date": "2022-11-11",
      "total_views": 262,
      "max_views": 262,
      "topics": [
        "data",
        "datascience",
        "important",
        "machinelearning",
        "make",
        "much",
        "need",
        "statistics",
        "sure"
      ],
      "search_text": "It‚Äôs important to make sure your model is well calibrated. This becomes especially important with imbalanced data. #machinelearning #datascience #statistics data datascience important machinelearning make much need statistics sure",
      "platforms": {
        "instagram": {
          "video_id": "18314998201064180",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "o7mvrJl9ujU",
          "url": "https://www.youtube.com/watch?v=o7mvrJl9ujU",
          "view_count": 262,
          "upload_date": "2022-11-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6516,
      "title": "Open Source with Stable Diffusion - #datascience #codetok #machinelearning #stablediffusion #opensourcesoftware",
      "description": "Open Source with Stable Diffusion - #datascience #codetok #machinelearning #stablediffusion #opensourcesoftware",
      "upload_date": "2022-08-27",
      "total_views": 256,
      "max_views": 256,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "open",
        "opensourcesoftware",
        "stablediffusion"
      ],
      "search_text": "Open Source with Stable Diffusion - #datascience #codetok #machinelearning #stablediffusion #opensourcesoftware codetok datascience machinelearning open opensourcesoftware stablediffusion",
      "platforms": {
        "youtube": {
          "video_id": "93ql614qP3o",
          "url": "https://youtube.com/shorts/93ql614qP3o?feature=share",
          "view_count": 256,
          "upload_date": "2022-08-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6614,
      "title": "Using agents in langchain with gpt-3. You can do this! Go check it out. #datascience #machinelearning #openai #gpt3 #langchain",
      "description": "Using agents in langchain with gpt-3. You can do this! Go check it out. #datascience #machinelearning #openai #gpt3 #langchain",
      "upload_date": "2023-03-04",
      "total_views": 245,
      "max_views": 245,
      "topics": [
        "answer",
        "datascience",
        "gpt3",
        "langchain",
        "machinelearning",
        "openai"
      ],
      "search_text": "Using agents in langchain with gpt-3. You can do this! Go check it out. #datascience #machinelearning #openai #gpt3 #langchain answer datascience gpt3 langchain machinelearning openai Wanna see something cool? Let's ask the computer a question but it doesn't know the answer but it does know that there's a Python package that has the answer. So, it goes and downloads the package by itself. Automatically figures out how to get the answer using the Python package. Mind blowing. This is called an agent. It's a way to use GPT3 to help decide the proper action to take. The code for this is not that complicated. All we do is start by connecting the open AI. We tell it we want to use the terminal tool, a Google search, and a way to run Python and then we start it. Here's a more sophisticated version that combine search, calculator, and a database. This allows it to work out complex problems. So, for example, I can figure out what the age of Leonardo DiCaprio's girlfriend is then use that number, do a calculation on top of this. This is the power of langchain, it's going to be big.",
      "platforms": {
        "instagram": {
          "video_id": "CpXzPbTA0a_",
          "url": "https://www.instagram.com/reel/CpXzPbTA0a_/",
          "view_count": 245,
          "upload_date": "2023-03-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6569,
      "title": "Contrastive learning is common for folks working in NLP and images. This was new to me, so wanted to share the intuition a bit more widely. This is an introduction, there are so many contrastive loss functions for different scenarios.",
      "description": "Contrastive learning is common for folks working in NLP and images. This was new to me, so wanted to share the intuition a bit more widely. This is an introduction, there are so many contrastive loss functions for different scenarios.",
      "upload_date": "2022-11-03",
      "total_views": 243,
      "max_views": 243,
      "topics": [
        "common",
        "contrastive",
        "folks",
        "learning",
        "nlp",
        "working"
      ],
      "search_text": "Contrastive learning is common for folks working in NLP and images. This was new to me, so wanted to share the intuition a bit more widely. This is an introduction, there are so many contrastive loss functions for different scenarios. common contrastive folks learning nlp working",
      "platforms": {
        "instagram": {
          "video_id": "Cke3ed8gKIC",
          "url": "https://www.instagram.com/reel/Cke3ed8gKIC/",
          "view_count": 243,
          "upload_date": "2022-11-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6031,
      "title": "Implicit as an Implicit Recommender for Collaborative Filtering",
      "description": "Implicit as an Implicit Recommender for Collaborative Filtering",
      "upload_date": "2024-05-25",
      "total_views": 242,
      "max_views": 242,
      "topics": [
        "collaborative",
        "explicit",
        "filtering",
        "implicit",
        "need",
        "recommender"
      ],
      "search_text": "Implicit as an Implicit Recommender for Collaborative Filtering collaborative explicit filtering implicit need recommender",
      "platforms": {
        "instagram": {
          "video_id": "18025949840491874",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "iEjxLFf7xYY",
          "url": "https://www.youtube.com/watch?v=iEjxLFf7xYY",
          "view_count": 242,
          "upload_date": "2024-05-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6253,
      "title": "Prompting versus Fine Tuning for Large Language Models",
      "description": "Prompting versus Fine Tuning for Large Language Models",
      "upload_date": "2024-05-07",
      "total_views": 235,
      "max_views": 235,
      "topics": [
        "fine",
        "glitchtokens",
        "language",
        "large",
        "largelanguagemodels",
        "like",
        "prompting",
        "time",
        "token",
        "tokens",
        "tuning",
        "versus"
      ],
      "search_text": "Prompting versus Fine Tuning for Large Language Models fine glitchtokens language large largelanguagemodels like prompting time token tokens tuning versus",
      "platforms": {
        "instagram": {
          "video_id": "17993213168466313",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "8kV8iZ8QMzI",
          "url": "https://www.youtube.com/watch?v=8kV8iZ8QMzI",
          "view_count": 235,
          "upload_date": "2024-05-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6145,
      "title": "Meta‚Äôs less than open source model and some bad takes from Twitter. #datascience #machinelearning #largelanguagemodels #opensource #meta",
      "description": "Meta‚Äôs less than open source model and some bad takes from Twitter. #datascience #machinelearning #largelanguagemodels #opensource #meta",
      "upload_date": "2023-03-05",
      "total_views": 224,
      "max_views": 224,
      "topics": [
        "datascience",
        "largelanguagemodels",
        "learning",
        "less",
        "machinelearning",
        "meta",
        "opensource",
        "python",
        "tools",
        "use"
      ],
      "search_text": "Meta‚Äôs less than open source model and some bad takes from Twitter. #datascience #machinelearning #largelanguagemodels #opensource #meta datascience largelanguagemodels learning less machinelearning meta opensource python tools use",
      "platforms": {
        "instagram": {
          "video_id": "17965477988114224",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "VO5lMqMfbXQ",
          "url": "https://youtube.com/shorts/VO5lMqMfbXQ?feature=share",
          "view_count": 224,
          "upload_date": "2023-03-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6015,
      "title": "GPT-4o mini from Open AI: Performance, Cost, Competition, and Development of LLMs",
      "description": "GPT-4o mini from Open AI: Performance, Cost, Competition, and Development of LLMs",
      "upload_date": "2024-07-19",
      "total_views": 223,
      "max_views": 223,
      "topics": [
        "competition",
        "cost",
        "data",
        "going",
        "gpt",
        "mini",
        "model",
        "models",
        "open",
        "performance",
        "smarter",
        "training"
      ],
      "search_text": "GPT-4o mini from Open AI: Performance, Cost, Competition, and Development of LLMs competition cost data going gpt mini model models open performance smarter training",
      "platforms": {
        "instagram": {
          "video_id": "18035470910044533",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "qdwVLKWesP8",
          "url": "https://www.youtube.com/watch?v=qdwVLKWesP8",
          "view_count": 223,
          "upload_date": "2024-07-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6032,
      "title": "Anthropic's research on Mapping the Mind of the Language Model",
      "description": "Anthropic's research on Mapping the Mind of the Language Model",
      "upload_date": "2024-05-24",
      "total_views": 222,
      "max_views": 222,
      "topics": [
        "anthropic",
        "language",
        "largelanguagemodels",
        "like",
        "mapping",
        "mechanisticinterpretability",
        "mind",
        "model",
        "research"
      ],
      "search_text": "Anthropic's research on Mapping the Mind of the Language Model anthropic language largelanguagemodels like mapping mechanisticinterpretability mind model research",
      "platforms": {
        "instagram": {
          "video_id": "18135808966331881",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-23",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "bX5DP9PPqdY",
          "url": "https://www.youtube.com/watch?v=bX5DP9PPqdY",
          "view_count": 222,
          "upload_date": "2024-05-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6322,
      "title": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics",
      "description": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics",
      "upload_date": "2022-10-29",
      "total_views": 222,
      "max_views": 222,
      "topics": [
        "anscombesquartet",
        "datascience",
        "datavisualization",
        "fun",
        "huggingface",
        "machinelearning",
        "reminder",
        "stablediffusion",
        "statistics",
        "told",
        "visualize"
      ],
      "search_text": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics anscombesquartet datascience datavisualization fun huggingface machinelearning reminder stablediffusion statistics told visualize",
      "platforms": {
        "instagram": {
          "video_id": "17976096490706450",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "lBKlnjVQAZs",
          "url": "https://youtube.com/shorts/lBKlnjVQAZs?feature=share",
          "view_count": 222,
          "upload_date": "2022-10-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6331,
      "title": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at ü§ó hugging face spaces by Ella2323 called Positive Reframing. #machinelearning #datascience #codetok",
      "description": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at ü§ó hugging face spaces by Ella2323 called Positive Reframing. #machinelearning #datascience #codetok",
      "upload_date": "2022-10-14",
      "total_views": 208,
      "max_views": 167,
      "topics": [
        "better",
        "codetok",
        "datascience",
        "feel",
        "machinelearning",
        "makes",
        "model",
        "positive",
        "reframing"
      ],
      "search_text": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at ü§ó hugging face spaces by Ella2323 called Positive Reframing. #machinelearning #datascience #codetok better codetok datascience feel machinelearning makes model positive reframing",
      "platforms": {
        "instagram": {
          "video_id": "18228114613154859",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 41,
          "upload_date": "2022-10-19",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "PAdXmwSyFbc",
          "url": "https://youtube.com/shorts/PAdXmwSyFbc?feature=share",
          "view_count": 167,
          "upload_date": "2022-10-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6318,
      "title": "Your weekly dose of LLM news. I liked this because it had interesting results with a smart approach. #datascience #machinelearning #largelanguagemodels",
      "description": "Your weekly dose of LLM news. I liked this because it had interesting results with a smart approach. #datascience #machinelearning #largelanguagemodels",
      "upload_date": "2022-11-06",
      "total_views": 207,
      "max_views": 207,
      "topics": [
        "datascience",
        "dose",
        "largelanguagemodels",
        "let",
        "llm",
        "machinelearning",
        "prediction",
        "predictioninterval",
        "predictions",
        "statistics",
        "weekly"
      ],
      "search_text": "Your weekly dose of LLM news. I liked this because it had interesting results with a smart approach. #datascience #machinelearning #largelanguagemodels datascience dose largelanguagemodels let llm machinelearning prediction predictioninterval predictions statistics weekly",
      "platforms": {
        "instagram": {
          "video_id": "17981192572692369",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "hSsCjpM38Sk",
          "url": "https://youtube.com/shorts/hSsCjpM38Sk?feature=share",
          "view_count": 207,
          "upload_date": "2022-11-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6571,
      "title": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics",
      "description": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics",
      "upload_date": "2022-11-05",
      "total_views": 203,
      "max_views": 203,
      "topics": [
        "datascience",
        "great",
        "imodels",
        "interpretablemodels",
        "machinelearning",
        "statistics"
      ],
      "search_text": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics datascience great imodels interpretablemodels machinelearning statistics",
      "platforms": {
        "instagram": {
          "video_id": "CkkDXvegsjO",
          "url": "https://www.instagram.com/reel/CkkDXvegsjO/",
          "view_count": 203,
          "upload_date": "2022-11-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "What did i do this time?  I hope your IT experienced go much better.",
      "description": "What did i do this time?  I hope your IT experienced go much better.",
      "upload_date": "2022-07-13",
      "total_views": 199,
      "max_views": 199,
      "topics": [
        "better",
        "experienced",
        "hey",
        "hope",
        "much",
        "time"
      ],
      "search_text": "What did i do this time?  I hope your IT experienced go much better. better experienced hey hope much time Hey there! Hey there, you hey there!",
      "platforms": {
        "tiktok": {
          "video_id": "7119883177895660843",
          "url": "https://www.tiktok.com/@rajistics/video/7119883177895660843",
          "view_count": 199,
          "upload_date": "2022-07-13",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/cb4d1062351848e987318014c3f17b6a_1657726988~tplv-tiktokx-origin.image?dr=9636&x-expires=1767488400&x-signature=4xK%2Fn94CEtlZ0isn6X8tU08bm3g%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6528,
      "title": "https://www.tiktok.com/@rajistics/video/7204141835965500715",
      "description": "https://www.tiktok.com/@rajistics/video/7204141835965500715",
      "upload_date": "2023-02-25",
      "total_views": 191,
      "max_views": 191,
      "topics": [
        "video"
      ],
      "search_text": "https://www.tiktok.com/@rajistics/video/7204141835965500715 video",
      "platforms": {
        "instagram": {
          "video_id": "CpG1NaBA04K",
          "url": "https://www.instagram.com/reel/CpG1NaBA04K/",
          "view_count": 0,
          "upload_date": "2023-02-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "fxU9Fs1OOLc",
          "url": "https://www.youtube.com/watch?v=fxU9Fs1OOLc",
          "view_count": 191,
          "upload_date": "2023-02-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6319,
      "title": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics",
      "description": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics",
      "upload_date": "2022-11-05",
      "total_views": 182,
      "max_views": 182,
      "topics": [
        "datascience",
        "flan",
        "great",
        "imodels",
        "interpretablemodels",
        "language",
        "largelanguagemodels",
        "machinelearning",
        "model",
        "statistics"
      ],
      "search_text": "Interpretable models are often overlooked, but a great addition to your data science toolkit. Imodels is a great python package for getting started. #datascience #machinelearning #interpretablemodels #imodels .#statistics datascience flan great imodels interpretablemodels language largelanguagemodels machinelearning model statistics",
      "platforms": {
        "instagram": {
          "video_id": "17978889685743597",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "imRo3n0q0sE",
          "url": "https://youtube.com/shorts/imRo3n0q0sE?feature=share",
          "view_count": 182,
          "upload_date": "2022-11-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6605,
      "title": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran",
      "description": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran",
      "upload_date": "2023-02-18",
      "total_views": 181,
      "max_views": 181,
      "topics": [
        "data",
        "datascience",
        "fortran",
        "machinelearning",
        "randomforest",
        "statistics"
      ],
      "search_text": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran data datascience fortran machinelearning randomforest statistics Tell us about the old days. Gather round. In the days before datascience, there was a group known as statistics. They had slide rulers used programming languages called Sas and it would take them many months at a tedious process to build even one machine learning model. You're making that up. We spend an afternoon to do that. Model building was slow. If you had missing values, you had to go find and do imputations. All of your data had to be scaled, transformed before you could use it. If you wanted to capture interactions, it was on you to develop those interaction features. Categoicals were hated so much. They were called dummies. Oh no, you didn't. Outliers were so feared they were banished. Sounds like the dark ages. Why couldn't they work with data the way we do? Yes, my friends. We live in a golden age with decisiontrees and random forests that us great accuracy that help us identify the best possible variation, handle non-linear data, deal with interactions automatically, allow for regularization. This is the age of data science. There was a time before data scientists. Yes, for a long time, statistics built models with their slide rulers and their paper and pencil but then, a computer scientist taught them a language known as fortran and with that",
      "platforms": {
        "instagram": {
          "video_id": "Co0hN4Sg-3I",
          "url": "https://www.instagram.com/reel/Co0hN4Sg-3I/",
          "view_count": 181,
          "upload_date": "2023-02-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6531,
      "title": "Using AI for Pose Detection, this is such a cool application. #datascience #deeplearning #codetok #posedetection #sportsanalytics",
      "description": "Using AI for Pose Detection, this is such a cool application. #datascience #deeplearning #codetok #posedetection #sportsanalytics",
      "upload_date": "2022-09-05",
      "total_views": 171,
      "max_views": 171,
      "topics": [
        "codetok",
        "datascience",
        "deeplearning",
        "posedetection",
        "sportsanalytics",
        "using"
      ],
      "search_text": "Using AI for Pose Detection, this is such a cool application. #datascience #deeplearning #codetok #posedetection #sportsanalytics codetok datascience deeplearning posedetection sportsanalytics using",
      "platforms": {
        "youtube": {
          "video_id": "L78u0W057xw",
          "url": "https://youtube.com/shorts/L78u0W057xw?feature=share",
          "view_count": 171,
          "upload_date": "2022-09-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Dreams of a better GPU #gpu #nvidia #deeplearning #gaming #datascience",
      "description": "Dreams of a better GPU #gpu #nvidia #deeplearning #gaming #datascience",
      "upload_date": "2022-02-12",
      "total_views": 170,
      "max_views": 170,
      "topics": [
        "datascience",
        "deeplearning",
        "dreams",
        "gaming",
        "gpu",
        "nvidia"
      ],
      "search_text": "Dreams of a better GPU #gpu #nvidia #deeplearning #gaming #datascience datascience deeplearning dreams gaming gpu nvidia",
      "platforms": {
        "tiktok": {
          "video_id": "7063828800001396015",
          "url": "https://www.tiktok.com/@rajistics/video/7063828800001396015",
          "view_count": 170,
          "upload_date": "2022-02-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Pie chart fails #stats #datascience #datavisualization #piechart #analytics #fails",
      "description": "Pie chart fails #stats #datascience #datavisualization #piechart #analytics #fails",
      "upload_date": "2022-01-15",
      "total_views": 169,
      "max_views": 169,
      "topics": [
        "analytics",
        "datascience",
        "datavisualization",
        "fails",
        "piechart",
        "stats"
      ],
      "search_text": "Pie chart fails #stats #datascience #datavisualization #piechart #analytics #fails analytics datascience datavisualization fails piechart stats Not my circus, not my monkeys, not my circus, not my monkeys, not my...",
      "platforms": {
        "tiktok": {
          "video_id": "7053483169185844527",
          "url": "https://www.tiktok.com/@rajistics/video/7053483169185844527",
          "view_count": 169,
          "upload_date": "2022-01-15",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/49a4f3871a5d44d992854e9cb292637d_1642267028~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=IfKdnL9BOI%2BVDstsuVSACfvZpc8%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6012,
      "title": "Curriculum Learning in Machine Learning - Ordering Training Data Improves Performance",
      "description": "Curriculum Learning in Machine Learning - Ordering Training Data Improves Performance",
      "upload_date": "2024-07-27",
      "total_views": 168,
      "max_views": 168,
      "topics": [
        "curriculum",
        "data",
        "die",
        "learning",
        "machine",
        "ordering",
        "sie",
        "snowflake",
        "training",
        "wie"
      ],
      "search_text": "Curriculum Learning in Machine Learning - Ordering Training Data Improves Performance curriculum data die learning machine ordering sie snowflake training wie",
      "platforms": {
        "instagram": {
          "video_id": "18261365758216710",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "LR6RFGOHdaI",
          "url": "https://www.youtube.com/watch?v=LR6RFGOHdaI",
          "view_count": 168,
          "upload_date": "2024-07-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6025,
      "title": "Rajistics on Gartner and Forrester research reports in AI",
      "description": "Rajistics on Gartner and Forrester research reports in AI",
      "upload_date": "2024-06-08",
      "total_views": 159,
      "max_views": 159,
      "topics": [
        "approach",
        "dspy",
        "forrester",
        "gartner",
        "like",
        "looks",
        "prompting",
        "reports",
        "research",
        "using"
      ],
      "search_text": "Rajistics on Gartner and Forrester research reports in AI approach dspy forrester gartner like looks prompting reports research using",
      "platforms": {
        "instagram": {
          "video_id": "18105906061395662",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Etbdj1jlc-I",
          "url": "https://www.youtube.com/watch?v=Etbdj1jlc-I",
          "view_count": 159,
          "upload_date": "2024-06-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6006,
      "title": "How to Read Github (and find the Best Projects)",
      "description": "How to Read Github (and find the Best Projects)",
      "upload_date": "2024-08-29",
      "total_views": 148,
      "max_views": 148,
      "topics": [
        "best",
        "evaluate",
        "find",
        "github",
        "hints",
        "projects",
        "read"
      ],
      "search_text": "How to Read Github (and find the Best Projects) best evaluate find github hints projects read",
      "platforms": {
        "instagram": {
          "video_id": "17867719920186769",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-28",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": " -hvLvMddLYA",
          "url": "https://www.youtube.com/watch?v= -hvLvMddLYA",
          "view_count": 148,
          "upload_date": "2024-08-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6133,
      "title": "Explaining how Emily Ocasio won second place with her project analyzing media coverage. I like her approach and highlights a growing trend of using prompting in data science. #datascience #machinelearning #promptengineering #societyforscience #emilyocasio",
      "description": "Explaining how Emily Ocasio won second place with her project analyzing media coverage. I like her approach and highlights a growing trend of using prompting in data science. #datascience #machinelearning #promptengineering #societyforscience #emilyocasio",
      "upload_date": "2023-03-29",
      "total_views": 145,
      "max_views": 145,
      "topics": [
        "data",
        "datascience",
        "emily",
        "emilyocasio",
        "explaining",
        "machinelearning",
        "project",
        "promptengineering",
        "prompting",
        "science",
        "second",
        "societyforscience"
      ],
      "search_text": "Explaining how Emily Ocasio won second place with her project analyzing media coverage. I like her approach and highlights a growing trend of using prompting in data science. #datascience #machinelearning #promptengineering #societyforscience #emilyocasio data datascience emily emilyocasio explaining machinelearning project promptengineering prompting science second societyforscience",
      "platforms": {
        "instagram": {
          "video_id": "17871419426822926",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "V5EYhSgB2bQ",
          "url": "https://www.youtube.com/watch?v=V5EYhSgB2bQ",
          "view_count": 145,
          "upload_date": "2023-03-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6262,
      "title": "Keras versus Pytorch Benchmarking Controversy",
      "description": "Keras versus Pytorch Benchmarking Controversy",
      "upload_date": "2024-04-06",
      "total_views": 144,
      "max_views": 144,
      "topics": [
        "benchmarking",
        "benchmarks",
        "compare",
        "controversy",
        "going",
        "keras",
        "python",
        "pytorch",
        "versus"
      ],
      "search_text": "Keras versus Pytorch Benchmarking Controversy benchmarking benchmarks compare controversy going keras python pytorch versus",
      "platforms": {
        "instagram": {
          "video_id": "17931312479835239",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "yJYh2ce--Nc",
          "url": "https://www.youtube.com/watch?v=yJYh2ce--Nc",
          "view_count": 144,
          "upload_date": "2024-04-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6250,
      "title": "Limits of AI: Compute, Memory, and Interconnection",
      "description": "Limits of AI: Compute, Memory, and Interconnection",
      "upload_date": "2024-09-06",
      "total_views": 143,
      "max_views": 143,
      "topics": [
        "arxiv",
        "compute",
        "dram",
        "interconnection",
        "limits",
        "memory",
        "org",
        "pdf",
        "wall"
      ],
      "search_text": "Limits of AI: Compute, Memory, and Interconnection arxiv compute dram interconnection limits memory org pdf wall",
      "platforms": {
        "instagram": {
          "video_id": "18046525870758432",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-06",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "AznbB_JIixI",
          "url": "https://www.youtube.com/watch?v=AznbB_JIixI",
          "view_count": 143,
          "upload_date": "2024-09-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6239,
      "title": "Embeddings, Context, and the Static Embeddings in Sentence Transformers",
      "description": "Embeddings, Context, and the Static Embeddings in Sentence Transformers",
      "upload_date": "2024-10-11",
      "total_views": 139,
      "max_views": 139,
      "topics": [
        "context",
        "den",
        "die",
        "embeddings",
        "ich",
        "nnen",
        "sentence",
        "sie",
        "static",
        "transformers",
        "wie"
      ],
      "search_text": "Embeddings, Context, and the Static Embeddings in Sentence Transformers context den die embeddings ich nnen sentence sie static transformers wie",
      "platforms": {
        "instagram": {
          "video_id": "18055617733876756",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "mj2idrn4U5g",
          "url": "https://www.youtube.com/watch?v=mj2idrn4U5g",
          "view_count": 139,
          "upload_date": "2024-10-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6024,
      "title": "Hyperparameter Optimization",
      "description": "Hyperparameter Optimization",
      "upload_date": "2024-06-20",
      "total_views": 137,
      "max_views": 137,
      "topics": [
        "algorithms",
        "hyperparameter",
        "hyperparameters",
        "learning",
        "like",
        "optimization",
        "search"
      ],
      "search_text": "Hyperparameter Optimization algorithms hyperparameter hyperparameters learning like optimization search",
      "platforms": {
        "instagram": {
          "video_id": "17867593767119192",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "cFtF85lCIAs",
          "url": "https://www.youtube.com/watch?v=cFtF85lCIAs",
          "view_count": 137,
          "upload_date": "2024-06-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6017,
      "title": "Pricing Optimization with Machine Learning - A funny summary",
      "description": "Pricing Optimization with Machine Learning - A funny summary",
      "upload_date": "2024-07-16",
      "total_views": 134,
      "max_views": 134,
      "topics": [
        "data",
        "funny",
        "learning",
        "machine",
        "one",
        "optimization",
        "pay",
        "price",
        "pricing",
        "summary",
        "use"
      ],
      "search_text": "Pricing Optimization with Machine Learning - A funny summary data funny learning machine one optimization pay price pricing summary use",
      "platforms": {
        "instagram": {
          "video_id": "18121225156342105",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "V37HgPf4uXU",
          "url": "https://www.youtube.com/watch?v=V37HgPf4uXU",
          "view_count": 134,
          "upload_date": "2024-07-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "Crowdsource labor for #ai #machinelearning - longer video explaining this coming out later today.",
      "description": "Crowdsource labor for #ai #machinelearning - longer video explaining this coming out later today.",
      "upload_date": "2022-01-22",
      "total_views": 129,
      "max_views": 129,
      "topics": [
        "ai",
        "crowdsource",
        "labor",
        "listen",
        "longer",
        "machinelearning"
      ],
      "search_text": "Crowdsource labor for #ai #machinelearning - longer video explaining this coming out later today. ai crowdsource labor listen longer machinelearning No, no, no, no, no. Listen, listen.",
      "platforms": {
        "tiktok": {
          "video_id": "7056038610415488303",
          "url": "https://www.tiktok.com/@rajistics/video/7056038610415488303",
          "view_count": 129,
          "upload_date": "2022-01-22",
          "thumbnail_url": "https://p16-common-sign.tiktokcdn-us.com/tos-useast5-p-0068-tx/9813ee9aebeb4328ba4a19d6f07d11d9_1642862014~tplv-tiktokx-origin.image?dr=9636&x-expires=1767506400&x-signature=51gJ3oBu%2F5ctsGEJIScL8iq3Mac%3D&t=4d5b0474&ps=13740610&shp=81f88b70&shcp=43f4a2f9&idc=useast5"
        }
      }
    },
    {
      "group_id": 6019,
      "title": "MobileLLM from Meta is full of efficient architecture ideas for LLMs",
      "description": "MobileLLM from Meta is full of efficient architecture ideas for LLMs",
      "upload_date": "2024-07-12",
      "total_views": 128,
      "max_views": 128,
      "topics": [
        "architecture",
        "blocks",
        "efficient",
        "full",
        "ideas",
        "meta",
        "mobilellm",
        "model",
        "swiglu",
        "using"
      ],
      "search_text": "MobileLLM from Meta is full of efficient architecture ideas for LLMs architecture blocks efficient full ideas meta mobilellm model swiglu using",
      "platforms": {
        "instagram": {
          "video_id": "18076883833456101",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "7WmfqOAgXvw",
          "url": "https://www.youtube.com/watch?v=7WmfqOAgXvw",
          "view_count": 128,
          "upload_date": "2024-07-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6140,
      "title": "Lets talk about why enterprises are considering alternatives to chatGPT by looking to open source. An open source strategy can affect lots of areas outside data science including data goverance, IT, legal, and accounting. Let me know what else i missed.",
      "description": "Lets talk about why enterprises are considering alternatives to chatGPT by looking to open source. An open source strategy can affect lots of areas outside data science including data goverance, IT, legal, and accounting. Let me know what else i missed.",
      "upload_date": "2023-03-18",
      "total_views": 126,
      "max_views": 126,
      "topics": [
        "api",
        "chatgpt",
        "data",
        "don",
        "enterprises",
        "know",
        "lets",
        "open",
        "opensource",
        "source",
        "talk"
      ],
      "search_text": "Lets talk about why enterprises are considering alternatives to chatGPT by looking to open source. An open source strategy can affect lots of areas outside data science including data goverance, IT, legal, and accounting. Let me know what else i missed. api chatgpt data don enterprises know lets open opensource source talk",
      "platforms": {
        "instagram": {
          "video_id": "17891986262729115",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "tEzJG13Ozdw",
          "url": "https://www.youtube.com/watch?v=tEzJG13Ozdw",
          "view_count": 126,
          "upload_date": "2023-03-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6274,
      "title": "Starting to see people productionizing GPT-3 workflows. I am a bug fan of using large language midels. Here is how one data science dealt with GPT3. #datascience #machinelearning #largelanguagemodels #gpt3",
      "description": "Starting to see people productionizing GPT-3 workflows. I am a bug fan of using large language midels. Here is how one data science dealt with GPT3. #datascience #machinelearning #largelanguagemodels #gpt3",
      "upload_date": "2023-03-11",
      "total_views": 120,
      "max_views": 120,
      "topics": [
        "datascience",
        "gpt3",
        "large",
        "largelanguagemodels",
        "machinelearning",
        "natdev",
        "see",
        "starting"
      ],
      "search_text": "Starting to see people productionizing GPT-3 workflows. I am a bug fan of using large language midels. Here is how one data science dealt with GPT3. #datascience #machinelearning #largelanguagemodels #gpt3 datascience gpt3 large largelanguagemodels machinelearning natdev see starting",
      "platforms": {
        "instagram": {
          "video_id": "18351168508035115",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-12",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "OB5y3l_Booc",
          "url": "https://www.youtube.com/watch?v=OB5y3l_Booc",
          "view_count": 120,
          "upload_date": "2023-03-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6035,
      "title": "Retrieval Augmented Generation - What it is and how it works",
      "description": "Retrieval Augmented Generation - What it is and how it works",
      "upload_date": "2024-05-10",
      "total_views": 119,
      "max_views": 119,
      "topics": [
        "algorithms",
        "augmented",
        "generation",
        "machine",
        "machinelearning",
        "retrieval",
        "tools",
        "understand",
        "visualizations",
        "works"
      ],
      "search_text": "Retrieval Augmented Generation - What it is and how it works algorithms augmented generation machine machinelearning retrieval tools understand visualizations works",
      "platforms": {
        "instagram": {
          "video_id": "18011442032150807",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "HITf24r4g-M",
          "url": "https://www.youtube.com/watch?v=HITf24r4g-M",
          "view_count": 119,
          "upload_date": "2024-05-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6304,
      "title": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization",
      "description": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization",
      "upload_date": "2023-01-08",
      "total_views": 113,
      "max_views": 113,
      "topics": [
        "data",
        "datascience",
        "datavisualization",
        "dealing",
        "groups",
        "like",
        "lot",
        "machinelearning",
        "plotting",
        "statistics",
        "use"
      ],
      "search_text": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization data datascience datavisualization dealing groups like lot machinelearning plotting statistics use",
      "platforms": {
        "instagram": {
          "video_id": "18257494111193127",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-09",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wm-wx_O97Vo",
          "url": "https://youtube.com/shorts/wm-wx_O97Vo?feature=share",
          "view_count": 113,
          "upload_date": "2023-01-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6134,
      "title": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt",
      "description": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt",
      "upload_date": "2023-03-23",
      "total_views": 109,
      "max_views": 109,
      "topics": [
        "chatgpt",
        "datascience",
        "huggingface",
        "langchain",
        "largelanguagemodels",
        "machinelearning",
        "models",
        "openai",
        "stablediffusion",
        "text2video"
      ],
      "search_text": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt chatgpt datascience huggingface langchain largelanguagemodels machinelearning models openai stablediffusion text2video",
      "platforms": {
        "instagram": {
          "video_id": "17980081679038425",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-29",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_yT3ReNJiYk",
          "url": "https://www.youtube.com/watch?v=_yT3ReNJiYk",
          "view_count": 109,
          "upload_date": "2023-03-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": null,
      "title": "AI Literacy, Q2, Driverless cars #ai #datascience #driverless #cars #tesla #fsd #alleninstitute",
      "description": "AI Literacy, Q2, Driverless cars #ai #datascience #driverless #cars #tesla #fsd #alleninstitute",
      "upload_date": "2022-01-18",
      "total_views": 109,
      "max_views": 109,
      "topics": [
        "alleninstitute",
        "cars",
        "datascience",
        "driverless",
        "fsd",
        "tesla"
      ],
      "search_text": "AI Literacy, Q2, Driverless cars #ai #datascience #driverless #cars #tesla #fsd #alleninstitute alleninstitute cars datascience driverless fsd tesla",
      "platforms": {
        "tiktok": {
          "video_id": "7054680119536864559",
          "url": "https://www.tiktok.com/@rajistics/video/7054680119536864559",
          "view_count": 109,
          "upload_date": "2022-01-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6400,
      "title": "Probe the data #dataanalysis #datascience #statistics #bias",
      "description": "Probe the data #dataanalysis #datascience #statistics #bias",
      "upload_date": "2022-08-01",
      "total_views": 104,
      "max_views": 104,
      "topics": [
        "bias",
        "code",
        "data",
        "dataanalysis",
        "datascience",
        "huggingface",
        "machinelearning",
        "model",
        "probe",
        "spreadsheet",
        "statistics"
      ],
      "search_text": "Probe the data #dataanalysis #datascience #statistics #bias bias code data dataanalysis datascience huggingface machinelearning model probe spreadsheet statistics",
      "platforms": {
        "instagram": {
          "video_id": "17957088002159612",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NvOhzaXjNeA",
          "url": "https://youtube.com/shorts/NvOhzaXjNeA",
          "view_count": 104,
          "upload_date": "2022-08-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6292,
      "title": "Try out these examples for yourself and lots more are available. It‚Äôs scary cool how these models are working. #datascience #machinelearning #gpt3 #largelanguagemodels #flanT5 #reasoningwithpeople https://huggingface.co/spaces/osanseviero/i-like-flan",
      "description": "Try out these examples for yourself and lots more are available. It‚Äôs scary cool how these models are working. #datascience #machinelearning #gpt3 #largelanguagemodels #flanT5 #reasoningwithpeople https://huggingface.co/spaces/osanseviero/i-like-flan",
      "upload_date": "2023-01-31",
      "total_views": 101,
      "max_views": 101,
      "topics": [
        "datascience",
        "flant5",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "models",
        "problem",
        "reasoningwithpeople",
        "solve"
      ],
      "search_text": "Try out these examples for yourself and lots more are available. It‚Äôs scary cool how these models are working. #datascience #machinelearning #gpt3 #largelanguagemodels #flanT5 #reasoningwithpeople https://huggingface.co/spaces/osanseviero/i-like-flan datascience flant5 gpt3 largelanguagemodels machinelearning models problem reasoningwithpeople solve",
      "platforms": {
        "instagram": {
          "video_id": "17944510742584455",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "teRu-ZT9XJs",
          "url": "https://www.youtube.com/watch?v=teRu-ZT9XJs",
          "view_count": 101,
          "upload_date": "2023-01-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6314,
      "title": "Learning curves, it‚Äôs a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique.",
      "description": "Learning curves, it‚Äôs a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique.",
      "upload_date": "2022-11-16",
      "total_views": 95,
      "max_views": 95,
      "topics": [
        "bigdata",
        "datascience",
        "galactica",
        "learning",
        "like",
        "machinelearning",
        "meta",
        "model",
        "say",
        "statistics",
        "stuff",
        "technique"
      ],
      "search_text": "Learning curves, it‚Äôs a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique. bigdata datascience galactica learning like machinelearning meta model say statistics stuff technique",
      "platforms": {
        "instagram": {
          "video_id": "17974891819849004",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "J2gKMGzuhEc",
          "url": "https://youtube.com/shorts/J2gKMGzuhEc?feature=share",
          "view_count": 95,
          "upload_date": "2022-11-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6003,
      "title": "Human Expertise in Text to SQL (Databricks, Snowflake, and Numbers Station)",
      "description": "Human Expertise in Text to SQL (Databricks, Snowflake, and Numbers Station)",
      "upload_date": "2024-09-01",
      "total_views": 91,
      "max_views": 91,
      "topics": [
        "automated",
        "databricks",
        "engineering",
        "expertise",
        "feature",
        "github",
        "human",
        "notebook",
        "openfe",
        "snowflake",
        "sql",
        "text"
      ],
      "search_text": "Human Expertise in Text to SQL (Databricks, Snowflake, and Numbers Station) automated databricks engineering expertise feature github human notebook openfe snowflake sql text",
      "platforms": {
        "instagram": {
          "video_id": "18042865802005391",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "q3KOXn4EG0s",
          "url": "https://www.youtube.com/watch?v=q3KOXn4EG0s",
          "view_count": 91,
          "upload_date": "2024-09-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6546,
      "title": "My data science setup for now #datascience #codetok #python #rstats #posit #vscode #googlecolab #digitalocean #conda",
      "description": "My data science setup for now #datascience #codetok #python #rstats #posit #vscode #googlecolab #digitalocean #conda",
      "upload_date": "2022-08-20",
      "total_views": 87,
      "max_views": 87,
      "topics": [
        "codetok",
        "datascience",
        "posit",
        "python",
        "rstats",
        "vscode"
      ],
      "search_text": "My data science setup for now #datascience #codetok #python #rstats #posit #vscode #googlecolab #digitalocean #conda codetok datascience posit python rstats vscode",
      "platforms": {
        "youtube": {
          "video_id": "XidPFGLs_rA",
          "url": "https://www.youtube.com/watch?v=XidPFGLs_rA",
          "view_count": 87,
          "upload_date": "2022-08-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6273,
      "title": "Data drift analysis is a must for production workloads. Here is Uber‚Äôs D3 system fie automated drift analysis. This video covers types of data drift issues, different approaches for addrssing them, and Ubers use of a Prophet model for anomaly detection. #datascience #machinelearning #mlops #datadrift #prophet",
      "description": "Data drift analysis is a must for production workloads. Here is Uber‚Äôs D3 system fie automated drift analysis. This video covers types of data drift issues, different approaches for addrssing them, and Ubers use of a Prophet model for anomaly detection. #datascience #machinelearning #mlops #datadrift #prophet",
      "upload_date": "2023-03-13",
      "total_views": 79,
      "max_views": 79,
      "topics": [
        "automated",
        "data",
        "datadrift",
        "datascience",
        "drift",
        "issues",
        "machinelearning",
        "mlops",
        "prophet"
      ],
      "search_text": "Data drift analysis is a must for production workloads. Here is Uber‚Äôs D3 system fie automated drift analysis. This video covers types of data drift issues, different approaches for addrssing them, and Ubers use of a Prophet model for anomaly detection. #datascience #machinelearning #mlops #datadrift #prophet automated data datadrift datascience drift issues machinelearning mlops prophet",
      "platforms": {
        "instagram": {
          "video_id": "17883119951768474",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "_AXzWcOgKwM",
          "url": "https://www.youtube.com/watch?v=_AXzWcOgKwM",
          "view_count": 79,
          "upload_date": "2023-03-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6547,
      "title": "Just how smart is ChatGPT and other #largelanguagemodels? Big Bench is a set of benchmark tests to asses the performance of the models. And the most recent models from Google are doing pretty good! #datascience #machinelearning #chatgpt",
      "description": "Just how smart is ChatGPT and other #largelanguagemodels? Big Bench is a set of benchmark tests to asses the performance of the models. And the most recent models from Google are doing pretty good! #datascience #machinelearning #chatgpt",
      "upload_date": "2023-01-04",
      "total_views": 79,
      "max_views": 79,
      "topics": [
        "chatgpt",
        "datascience",
        "largelanguagemodels",
        "machinelearning",
        "models",
        "smart"
      ],
      "search_text": "Just how smart is ChatGPT and other #largelanguagemodels? Big Bench is a set of benchmark tests to asses the performance of the models. And the most recent models from Google are doing pretty good! #datascience #machinelearning #chatgpt chatgpt datascience largelanguagemodels machinelearning models smart",
      "platforms": {
        "instagram": {
          "video_id": "CnCRlxbhXyn",
          "url": "https://www.instagram.com/reel/CnCRlxbhXyn/",
          "view_count": 0,
          "upload_date": "2023-01-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "GIFkjMMUVG4",
          "url": "https://www.youtube.com/watch?v=GIFkjMMUVG4",
          "view_count": 79,
          "upload_date": "2023-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6020,
      "title": "Challenging Benchmarks for LLMS: MUSR and Connections",
      "description": "Challenging Benchmarks for LLMS: MUSR and Connections",
      "upload_date": "2024-07-04",
      "total_views": 75,
      "max_views": 75,
      "topics": [
        "benchmarks",
        "challenging",
        "connections",
        "llms",
        "musr",
        "reasoning",
        "still"
      ],
      "search_text": "Challenging Benchmarks for LLMS: MUSR and Connections benchmarks challenging connections llms musr reasoning still",
      "platforms": {
        "instagram": {
          "video_id": "17845382832242911",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-04",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ubm3wSB2e74",
          "url": "https://www.youtube.com/watch?v=ubm3wSB2e74",
          "view_count": 75,
          "upload_date": "2024-07-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6018,
      "title": "ABCs of AI for Machine Learning and Generative AI - Anything But Chatbots",
      "description": "ABCs of AI for Machine Learning and Generative AI - Anything But Chatbots",
      "upload_date": "2024-07-13",
      "total_views": 73,
      "max_views": 73,
      "topics": [
        "abcs",
        "anything",
        "build",
        "chatbots",
        "generative",
        "learning",
        "machine",
        "rag"
      ],
      "search_text": "ABCs of AI for Machine Learning and Generative AI - Anything But Chatbots abcs anything build chatbots generative learning machine rag",
      "platforms": {
        "instagram": {
          "video_id": "18071947870526641",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-13",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ECKby70XAW0",
          "url": "https://www.youtube.com/watch?v=ECKby70XAW0",
          "view_count": 73,
          "upload_date": "2024-07-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6550,
      "title": "No big deal, use visualization #stats #datascience #datasaurus #datascience #analytics #anscombe #visualization",
      "description": "No big deal, use visualization #stats #datascience #datasaurus #datascience #analytics #anscombe #visualization",
      "upload_date": "2022-01-14",
      "total_views": 73,
      "max_views": 73,
      "topics": [
        "analytics",
        "anscombe",
        "datasaurus",
        "datascience",
        "stats",
        "visualization"
      ],
      "search_text": "No big deal, use visualization #stats #datascience #datasaurus #datascience #analytics #anscombe #visualization analytics anscombe datasaurus datascience stats visualization",
      "platforms": {
        "youtube": {
          "video_id": "7BWukRTXKgw",
          "url": "https://youtube.com/shorts/7BWukRTXKgw?feature=share",
          "view_count": 73,
          "upload_date": "2022-01-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6556,
      "title": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here. #datascience #machinelearning #twitter #recommenders",
      "description": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here. #datascience #machinelearning #twitter #recommenders",
      "upload_date": "2023-03-31",
      "total_views": 64,
      "max_views": 64,
      "topics": [
        "datascience",
        "machinelearning",
        "open",
        "recommenders",
        "sourced",
        "twitter"
      ],
      "search_text": "Twitter open sourced it's recommendation algorithm. It's fun to look at someone else's production code and will be useful to people studying recommender systems. But a lot of the important pieces aren't provided and there doesn't seem to be anything earthshattering or unexpected here. #datascience #machinelearning #twitter #recommenders datascience machinelearning open recommenders sourced twitter",
      "platforms": {
        "instagram": {
          "video_id": "Cqfq9iZgSdU",
          "url": "https://www.instagram.com/reel/Cqfq9iZgSdU/",
          "view_count": 0,
          "upload_date": "2023-03-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Exj1PdT9VzU",
          "url": "https://www.youtube.com/watch?v=Exj1PdT9VzU",
          "view_count": 64,
          "upload_date": "2023-03-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6030,
      "title": "Data Scientist versus Data Analyst - Police Misconduct",
      "description": "Data Scientist versus Data Analyst - Police Misconduct",
      "upload_date": "2024-05-27",
      "total_views": 63,
      "max_views": 63,
      "topics": [
        "analyst",
        "data",
        "learning",
        "machine",
        "misconduct",
        "model",
        "police",
        "scientist",
        "versus"
      ],
      "search_text": "Data Scientist versus Data Analyst - Police Misconduct analyst data learning machine misconduct model police scientist versus",
      "platforms": {
        "instagram": {
          "video_id": "18354926452100582",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-27",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "DJ5wmJTxeqw",
          "url": "https://www.youtube.com/watch?v=DJ5wmJTxeqw",
          "view_count": 63,
          "upload_date": "2024-05-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6284,
      "title": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It‚Äôs great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant",
      "description": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It‚Äôs great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant",
      "upload_date": "2023-02-19",
      "total_views": 63,
      "max_views": 63,
      "topics": [
        "chatgpt",
        "datascience",
        "feedback",
        "help",
        "information",
        "machinelearning",
        "open",
        "openai",
        "openassistant"
      ],
      "search_text": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It‚Äôs great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant chatgpt datascience feedback help information machinelearning open openai openassistant",
      "platforms": {
        "instagram": {
          "video_id": "17970138353154954",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-20",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Umgd4RgP1Qo",
          "url": "https://www.youtube.com/watch?v=Umgd4RgP1Qo",
          "view_count": 63,
          "upload_date": "2023-02-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6320,
      "title": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery",
      "description": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery",
      "upload_date": "2022-11-12",
      "total_views": 63,
      "max_views": 63,
      "topics": [
        "ablation",
        "ablationsurgery",
        "analytics",
        "datascience",
        "functions",
        "loss",
        "machinelearning",
        "regression",
        "rsme",
        "stablediffusion",
        "statistics"
      ],
      "search_text": "Applying a classic methodology of ablation when working with stable diffusion prompts. Ablation is very common in many techniques to understand how models are working. #machinelearning #datascience #statistics #stablediffusion #ablationsurgery ablation ablationsurgery analytics datascience functions loss machinelearning regression rsme stablediffusion statistics",
      "platforms": {
        "instagram": {
          "video_id": "17843502047884406",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-08",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "NVjOWU12R0k",
          "url": "https://youtube.com/shorts/NVjOWU12R0k?feature=share",
          "view_count": 63,
          "upload_date": "2022-11-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6263,
      "title": "How I added a list of my Tik Tok videos to my web site",
      "description": "How I added a list of my Tik Tok videos to my web site",
      "upload_date": "2024-04-02",
      "total_views": 61,
      "max_views": 61,
      "topics": [
        "added",
        "buzzlytics",
        "chatgpt",
        "data",
        "list",
        "rajivshah",
        "tik",
        "tok",
        "videos",
        "web",
        "website"
      ],
      "search_text": "How I added a list of my Tik Tok videos to my web site added buzzlytics chatgpt data list rajivshah tik tok videos web website",
      "platforms": {
        "instagram": {
          "video_id": "18014691113221304",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-02",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "RZYxG8PH0os",
          "url": "https://www.youtube.com/watch?v=RZYxG8PH0os",
          "view_count": 61,
          "upload_date": "2024-04-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6557,
      "title": "Cleanlab is open source and will improve your data quality. It‚Äôs so underrated. This was hard to record vertically, so go try it out. #datascience #machinelearning #cleanlab #labelerror #confidentlearning #dataquality",
      "description": "Cleanlab is open source and will improve your data quality. It‚Äôs so underrated. This was hard to record vertically, so go try it out. #datascience #machinelearning #cleanlab #labelerror #confidentlearning #dataquality",
      "upload_date": "2023-01-29",
      "total_views": 61,
      "max_views": 61,
      "topics": [
        "cleanlab",
        "confidentlearning",
        "dataquality",
        "datascience",
        "datasets",
        "explainability",
        "labelerror",
        "machinelearning",
        "synthetic",
        "syntheticdata"
      ],
      "search_text": "Cleanlab is open source and will improve your data quality. It‚Äôs so underrated. This was hard to record vertically, so go try it out. #datascience #machinelearning #cleanlab #labelerror #confidentlearning #dataquality cleanlab confidentlearning dataquality datascience datasets explainability labelerror machinelearning synthetic syntheticdata",
      "platforms": {
        "instagram": {
          "video_id": "Cn3StLcAmsv",
          "url": "https://www.instagram.com/p/Cn3StLcAmsv/",
          "view_count": 0,
          "upload_date": "2023-01-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "GA2AJEydFZ0",
          "url": "https://www.youtube.com/watch?v=GA2AJEydFZ0",
          "view_count": 61,
          "upload_date": "2023-01-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6621,
      "title": "Explaining AI",
      "description": "Explaining AI",
      "upload_date": "2023-06-26",
      "total_views": 60,
      "max_views": 60,
      "topics": [
        "alpaca",
        "datascience",
        "explaining",
        "largelanguagemodels",
        "llama",
        "machinelearning",
        "objaverse"
      ],
      "search_text": "Explaining AI alpaca datascience explaining largelanguagemodels llama machinelearning objaverse",
      "platforms": {
        "instagram": {
          "video_id": "CqQU4msAOiJ",
          "url": "https://www.instagram.com/p/CqQU4msAOiJ/",
          "view_count": 0,
          "upload_date": "2023-03-25",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "MTb08qSQCYw",
          "url": "https://www.youtube.com/watch?v=MTb08qSQCYw",
          "view_count": 60,
          "upload_date": "2023-06-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6587,
      "title": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning",
      "description": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning",
      "upload_date": "2023-01-03",
      "total_views": 58,
      "max_views": 58,
      "topics": [
        "data",
        "datascience",
        "machinelearning",
        "model",
        "models",
        "scientist"
      ],
      "search_text": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning data datascience machinelearning model models scientist Have you been cheated on? Well, as a data scientist, you get used to it. I have models that tell me that this is an elephant, there's no cow here, or there's a guitar here. So, let's talk about why these models cheat and what we can do about it. I train my AI model using these four images but then look what happens when I ask for a prediction. It gets it wrong. Can you figure out why the model cheated? It looked at the background of the image and this is what all AI models do is they look for the shortcuts. They look for the easiest way to classify the data that you've given them. Here's a real-world example where we built a classifier to identify pneumonia in these lung X rays and the model did great but then we gave it new data that was unlike the training data was data from different hospitals and you know what happened the model fell down it didn't do so well once we dug in and investigated we found that in the images there's a hospital specific token and that's what the AI was using because this hospital token gives you an idea if a hospital has a high likelihood of pneumonia or low and it was using as a proxy instead of actually taking the time to look at the long image. This happens everyday in the life of a data scientist it's often called leakage and there's a paper here on shortcuts that goes over a couple of the common ways it does this. Sometimes the AI finds a pattern where there's no pattern there. Or it's using other features or attributes than what you really believe the true signal is at. So how can we prevent this? So one thing is we can use explainability tools to better try to understand what our models doing but we can also",
      "platforms": {
        "instagram": {
          "video_id": "Cm9yaROh310",
          "url": "https://www.instagram.com/p/Cm9yaROh310/",
          "view_count": 58,
          "upload_date": "2023-01-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6347,
      "title": "tiktok e2a990da04ca9451707be3db1b1bbef66c5254a5",
      "description": "",
      "upload_date": "2024-09-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "block",
        "check",
        "earlier",
        "latest",
        "videos",
        "world"
      ],
      "search_text": "block check earlier latest videos world",
      "platforms": {
        "instagram": {
          "video_id": "18028290548098521",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6351,
      "title": "tiktok d7bef697298bb1d985ca3c6f7cbd11ed7ae7d689",
      "description": "",
      "upload_date": "2024-09-01",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "aiethics",
        "cigna",
        "human",
        "loop",
        "news",
        "tesla"
      ],
      "search_text": "aiethics cigna human loop news tesla",
      "platforms": {
        "instagram": {
          "video_id": "18035973920125911",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6352,
      "title": "tiktok 417158c1e75afdec1eb5bc35f46b7b138c6056ca",
      "description": "",
      "upload_date": "2024-08-31",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "performance",
        "snowflake",
        "source",
        "sql",
        "text",
        "work"
      ],
      "search_text": "performance snowflake source sql text work",
      "platforms": {
        "instagram": {
          "video_id": "18025072523341184",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6353,
      "title": "tiktok a6aa66a4f29979c305632aa3bf16159062777ba7",
      "description": "",
      "upload_date": "2024-08-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "reinforcementlearning",
        "rlhf",
        "techtok"
      ],
      "search_text": "chatgpt datascience machinelearning reinforcementlearning rlhf techtok",
      "platforms": {
        "instagram": {
          "video_id": "18006668549403070",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6354,
      "title": "tiktok 0e623c2cf183c1083703c98f65cd692b76e6aa5b",
      "description": "",
      "upload_date": "2024-08-29",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "evaluatingllms",
        "evaluation",
        "language",
        "large",
        "largelanguagemodels",
        "models"
      ],
      "search_text": "evaluatingllms evaluation language large largelanguagemodels models",
      "platforms": {
        "instagram": {
          "video_id": "18119208919379013",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6355,
      "title": "tiktok 4d213362842492fbc692ca591489cdcec809f3f7",
      "description": "",
      "upload_date": "2024-08-28",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "clustering",
        "kmeans"
      ],
      "search_text": "clustering kmeans",
      "platforms": {
        "instagram": {
          "video_id": "18279015736236877",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6356,
      "title": "tiktok b9a370cab2d3b0936212e9c2ce29d961e2ab3cd4",
      "description": "",
      "upload_date": "2024-08-22",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "courses",
        "data",
        "don",
        "getting",
        "overspend",
        "science"
      ],
      "search_text": "courses data don getting overspend science",
      "platforms": {
        "instagram": {
          "video_id": "18073333288559193",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6357,
      "title": "tiktok 9df3656300e252c0d3de70d269aad4bc567806c0",
      "description": "",
      "upload_date": "2024-08-20",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "key",
        "llms",
        "models",
        "planning",
        "reasoning",
        "weaknesses"
      ],
      "search_text": "key llms models planning reasoning weaknesses",
      "platforms": {
        "instagram": {
          "video_id": "18247683637251490",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6359,
      "title": "tiktok 5355e480148d4573f3462165cc2a3f67c5059ee6",
      "description": "",
      "upload_date": "2024-08-11",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "hungry",
        "makes",
        "mmm"
      ],
      "search_text": "hungry makes mmm",
      "platforms": {
        "instagram": {
          "video_id": "18002421731640057",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6360,
      "title": "tiktok 50b157f75104d353c7f4be7fc18ad0a497056b89",
      "description": "",
      "upload_date": "2024-08-09",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "das",
        "daten",
        "die",
        "ich",
        "ist",
        "sich"
      ],
      "search_text": "das daten die ich ist sich",
      "platforms": {
        "instagram": {
          "video_id": "17923027430931930",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6361,
      "title": "tiktok b659e06c1dd534fb395538bc618be1f8b7543e06",
      "description": "",
      "upload_date": "2024-08-06",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datos",
        "est",
        "los",
        "para",
        "pensando",
        "que"
      ],
      "search_text": "datos est los para pensando que",
      "platforms": {
        "instagram": {
          "video_id": "17927396300812323",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-08-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6362,
      "title": "tiktok afe4c9419d63f900e158d47033b367e3cccd8ab9",
      "description": "",
      "upload_date": "2024-08-04",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "das",
        "gpus",
        "ich",
        "ist",
        "pro",
        "sie"
      ],
      "search_text": "das gpus ich ist pro sie",
      "platforms": {
        "instagram": {
          "video_id": "17934217871887352",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6016,
      "title": "5 tiers of data centers",
      "description": "5 tiers of data centers",
      "upload_date": "2024-07-18",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "centers",
        "data",
        "higher",
        "pay",
        "tier",
        "tiers"
      ],
      "search_text": "5 tiers of data centers centers data higher pay tier tiers",
      "platforms": {
        "instagram": {
          "video_id": "18035108486066662",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6363,
      "title": "tiktok 0743cb2a2ae297e3c8ccc35ba27da2ec9ba16ecf",
      "description": "",
      "upload_date": "2024-07-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "openai",
        "revenue",
        "wow"
      ],
      "search_text": "openai revenue wow",
      "platforms": {
        "instagram": {
          "video_id": "18062518723593113",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6364,
      "title": "tiktok 0b122ac2e9694c92b91164414905566fdd2fc14f",
      "description": "",
      "upload_date": "2024-07-12",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "able",
        "behavior",
        "chicago",
        "crime",
        "numbers",
        "statistics"
      ],
      "search_text": "able behavior chicago crime numbers statistics",
      "platforms": {
        "instagram": {
          "video_id": "18040689514929526",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6365,
      "title": "tiktok 4cf7ae463f2bad712f25c9e362e25157c65521a6",
      "description": "",
      "upload_date": "2024-07-05",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "also",
        "cold",
        "concepts",
        "look",
        "see",
        "tensorboard"
      ],
      "search_text": "also cold concepts look see tensorboard",
      "platforms": {
        "instagram": {
          "video_id": "18029859749482467",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-07-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6366,
      "title": "tiktok 8c066ba61667767f6b38fe18b5dd8e1e281f6683",
      "description": "",
      "upload_date": "2024-07-01",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "ai",
        "aiexplained",
        "customers",
        "datascience",
        "machinelearning",
        "years"
      ],
      "search_text": "ai aiexplained customers datascience machinelearning years",
      "platforms": {
        "instagram": {
          "video_id": "18002404742558185",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6664,
      "title": "7 Baseline Models:\nTime Series: Previous Value\nAnomaly: p99\nSearch: BM25\nRecommendation: Popularity\nBuy recommendations: last viewed\nClassification: Majority class\nRegression: Mean\nThe idea from this thread comes from a post by Jo Kristian Bergum\nhttps://x.com/jobergum/status/1803674976173629865",
      "description": "7 Baseline Models:\nTime Series: Previous Value\nAnomaly: p99\nSearch: BM25\nRecommendation: Popularity\nBuy recommendations: last viewed\nClassification: Majority class\nRegression: Mean\nThe idea from this thread comes from a post by Jo Kristian Bergum\nhttps://x.com/jobergum/status/1803674976173629865",
      "upload_date": "2024-06-22",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "baseline",
        "models",
        "recommendation",
        "search",
        "use",
        "value"
      ],
      "search_text": "7 Baseline Models:\nTime Series: Previous Value\nAnomaly: p99\nSearch: BM25\nRecommendation: Popularity\nBuy recommendations: last viewed\nClassification: Majority class\nRegression: Mean\nThe idea from this thread comes from a post by Jo Kristian Bergum\nhttps://x.com/jobergum/status/1803674976173629865 baseline models recommendation search use value Baseline models you can use for predictions. This stuff is golden. You can beat a room full of PHDs. Save this. For time series when you're trying to predict the next value, use the previous value. What happened yesterday is probably your best predictor for what's going to happen tomorrow. For a nominal detection, use a P99 model which looks at anything that lies outside of what 99% of the observations are. Such a simple rule of thumb. Use this everywhere. For search, BM25. That's good old-fashioned tech search still that vector search in lots of use cases. For recommendation systems using popularity. People's choices aren't nearly as diverse as what they'll tell you. If you need a recommendation for what someone's going to buy, look at what they last viewed. This is a no-brainer. The classification, look at the majority class. And when someone tells you their models 96% accurate, ask how often it happens. Regression problems, use a mean or medium as your baseline model. Give it up for that mean. These baseline models should always be used as a starting point and as a reference when you're comparing more complicated approaches. This is going to humble a lot of data scientists.",
      "platforms": {
        "instagram": {
          "video_id": "17985093086682275",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6367,
      "title": "tiktok bb7abbecd2c5cd54f01726919f43c5d803879533",
      "description": "",
      "upload_date": "2024-06-06",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "decomposition",
        "let",
        "series",
        "start",
        "time"
      ],
      "search_text": "data decomposition let series start time",
      "platforms": {
        "instagram": {
          "video_id": "18320523106178320",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6368,
      "title": "tiktok 19e9c4dc3117a7c860ebb3df3e3f7ce69ef12488",
      "description": "",
      "upload_date": "2024-06-04",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "different",
        "leaderboards",
        "many",
        "model",
        "models",
        "use"
      ],
      "search_text": "different leaderboards many model models use",
      "platforms": {
        "instagram": {
          "video_id": "18002405546559137",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6369,
      "title": "tiktok 954676c979b9f2656d789b3e0f97f299509c178d",
      "description": "",
      "upload_date": "2024-06-01",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "algorithm",
        "approaches",
        "data",
        "failed",
        "like",
        "model"
      ],
      "search_text": "algorithm approaches data failed like model",
      "platforms": {
        "instagram": {
          "video_id": "18027827222293024",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6370,
      "title": "tiktok 775eb1bfdcf68644342be85b1c8c554005253e0c",
      "description": "",
      "upload_date": "2024-05-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "blog",
        "blogging",
        "jekyll",
        "posit",
        "post",
        "quarto"
      ],
      "search_text": "blog blogging jekyll posit post quarto",
      "platforms": {
        "instagram": {
          "video_id": "18060054916520482",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6371,
      "title": "tiktok 7fd83346daf58d13afdb3984dcb33cc9195efd7a",
      "description": "",
      "upload_date": "2024-05-29",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "doesn",
        "hallucinations",
        "legal",
        "llms",
        "paragraph",
        "rag"
      ],
      "search_text": "doesn hallucinations legal llms paragraph rag",
      "platforms": {
        "instagram": {
          "video_id": "18004806830530176",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-06-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6372,
      "title": "tiktok 00c676f59f8cf572ebe8336fe2df7c4b0bd5ca5e",
      "description": "",
      "upload_date": "2024-05-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "large",
        "like",
        "live",
        "open",
        "source",
        "world"
      ],
      "search_text": "large like live open source world",
      "platforms": {
        "instagram": {
          "video_id": "18025520840474763",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6374,
      "title": "tiktok cf32a89cf851869db4c43616089a42d2cf26b89f",
      "description": "",
      "upload_date": "2024-05-21",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "content",
        "copyright",
        "datascience",
        "datasets",
        "laoin",
        "llama"
      ],
      "search_text": "content copyright datascience datasets laoin llama",
      "platforms": {
        "instagram": {
          "video_id": "18038547808853187",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6044,
      "title": "tiktok d00bf504b17c83aa06976273e93d65b7b35d59e6",
      "description": "",
      "upload_date": "2025-01-13",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "ast",
        "code",
        "leak",
        "leaks",
        "resource",
        "right"
      ],
      "search_text": "ast code leak leaks resource right",
      "platforms": {
        "instagram": {
          "video_id": "18128009830494850",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-11-10",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Showing top 500 results",
          "url": "https://www.youtube.com/watch?v=Showing top 500 results",
          "view_count": 0,
          "upload_date": "",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6584,
      "title": "The politics of ChatGPT, it‚Äôs no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics",
      "description": "The politics of ChatGPT, it‚Äôs no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics",
      "upload_date": "2022-12-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "ago",
        "chatgpt",
        "datascience",
        "machinelearning",
        "openai",
        "politics",
        "technologyethics",
        "themes",
        "timeless",
        "two",
        "years"
      ],
      "search_text": "The politics of ChatGPT, it‚Äôs no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics ago chatgpt datascience machinelearning openai politics technologyethics themes timeless two years",
      "platforms": {
        "instagram": {
          "video_id": "18319252912163118",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-02-07",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Vr6Cmz6D-gE",
          "url": "https://www.youtube.com/watch?v=Vr6Cmz6D-gE",
          "view_count": 0,
          "upload_date": "2022-12-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6334,
      "title": "tiktok 990843282321f08d5aaf5a5932e6d003f6474e9d",
      "description": "",
      "upload_date": "2025-01-03",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "let",
        "like",
        "lot",
        "use",
        "viz"
      ],
      "search_text": "data let like lot use viz",
      "platforms": {
        "instagram": {
          "video_id": "18066715747754774",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6335,
      "title": "tiktok de97fd3138a8f8d4d080bb166276cd00c49af5ca",
      "description": "",
      "upload_date": "2025-01-02",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "baseline",
        "models",
        "recommendation",
        "search",
        "use",
        "value"
      ],
      "search_text": "baseline models recommendation search use value",
      "platforms": {
        "instagram": {
          "video_id": "18043450181239247",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6336,
      "title": "tiktok 64b0a4d7321788a7ba7541b749e80ecaffb9cd6f",
      "description": "",
      "upload_date": "2024-12-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "got",
        "learning",
        "really",
        "science",
        "somebody"
      ],
      "search_text": "data got learning really science somebody",
      "platforms": {
        "instagram": {
          "video_id": "17943770453925970",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6099,
      "title": "tiktok 68e99405e8971cfeeff7d1136218c61bfcd17415",
      "description": "",
      "upload_date": "2024-12-20",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "ready"
      ],
      "search_text": "ready",
      "platforms": {
        "instagram": {
          "video_id": "18104795587414644",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6132,
      "title": "My take on Objaverse Llama and Alpaca. Not a lot of respect for copyright or contract terms. #largelanguagemodels #datascience #machinelearning #objaverse #alleninstitute #openai #chatgpt #llama #alpaca #meta #databricks",
      "description": "My take on Objaverse Llama and Alpaca. Not a lot of respect for copyright or contract terms. #largelanguagemodels #datascience #machinelearning #objaverse #alleninstitute #openai #chatgpt #llama #alpaca #meta #databricks",
      "upload_date": "2023-03-25",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "alpaca",
        "data",
        "datascience",
        "largelanguagemodels",
        "lead",
        "llama",
        "machinelearning",
        "models",
        "objaverse",
        "open",
        "use",
        "vicuna"
      ],
      "search_text": "My take on Objaverse Llama and Alpaca. Not a lot of respect for copyright or contract terms. #largelanguagemodels #datascience #machinelearning #objaverse #alleninstitute #openai #chatgpt #llama #alpaca #meta #databricks alpaca data datascience largelanguagemodels lead llama machinelearning models objaverse open use vicuna",
      "platforms": {
        "instagram": {
          "video_id": "17934162983530677",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "sIDOLs6HN7s",
          "url": "https://www.youtube.com/watch?v=sIDOLs6HN7s",
          "view_count": 0,
          "upload_date": "2023-03-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6141,
      "title": "Ensembling is key method in machine learning. This video introduces ensembling through majority voting. #datascience #machinelearning #ensembling #kaggle #majorityvoting",
      "description": "Ensembling is key method in machine learning. This video introduces ensembling through majority voting. #datascience #machinelearning #ensembling #kaggle #majorityvoting",
      "upload_date": "2023-03-14",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "ensembling",
        "kaggle",
        "key",
        "langchain",
        "langflow",
        "largelanguagemodels",
        "machinelearning",
        "majorityvoting",
        "use"
      ],
      "search_text": "Ensembling is key method in machine learning. This video introduces ensembling through majority voting. #datascience #machinelearning #ensembling #kaggle #majorityvoting datascience ensembling kaggle key langchain langflow largelanguagemodels machinelearning majorityvoting use",
      "platforms": {
        "instagram": {
          "video_id": "17960865032516565",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-18",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Hh-Lqd-hdyY",
          "url": "https://www.youtube.com/watch?v=Hh-Lqd-hdyY",
          "view_count": 0,
          "upload_date": "2023-03-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6143,
      "title": "Best machine learning tools for competitions. Lots of great stuff here. #datascience #machinelearning #python #codetok",
      "description": "Best machine learning tools for competitions. Lots of great stuff here. #datascience #machinelearning #python #codetok",
      "upload_date": "2023-03-09",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "accuracy",
        "best",
        "codetok",
        "datascience",
        "ensembling",
        "machine",
        "machinelearning",
        "majority",
        "models",
        "python",
        "three",
        "voting"
      ],
      "search_text": "Best machine learning tools for competitions. Lots of great stuff here. #datascience #machinelearning #python #codetok accuracy best codetok datascience ensembling machine machinelearning majority models python three voting",
      "platforms": {
        "instagram": {
          "video_id": "17994431344754479",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-03-15",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "Vk5MLRHVisg",
          "url": "https://www.youtube.com/watch?v=Vk5MLRHVisg",
          "view_count": 0,
          "upload_date": "2023-03-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6146,
      "title": "tiktok 320b9177d62d2a93d69107a70c76c21db5a3060e",
      "description": "",
      "upload_date": "2025-01-14",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "17934963032973273",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6147,
      "title": "tiktok a69842409e16f28d371db823dc9ef60f1f15a440",
      "description": "",
      "upload_date": "2025-01-11",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18050947142027799",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6148,
      "title": "tiktok 2b77820d46cd2ce156e7a60d75267aaf69e765fb",
      "description": "",
      "upload_date": "2025-01-10",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18131375548377703",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6149,
      "title": "tiktok 5fbfda71a12d1952651ade3a93099c09706745ce",
      "description": "",
      "upload_date": "2025-01-09",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18174673267313286",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6150,
      "title": "tiktok a3ef42d8bf23f62babe6a6b4675133b84e859427",
      "description": "",
      "upload_date": "2025-01-08",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18161133178329295",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6151,
      "title": "tiktok d4771ab2c38fc049f245c2909abb761afe720662",
      "description": "",
      "upload_date": "2025-01-07",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18054130742067078",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6152,
      "title": "tiktok 8f831528b60167821cb18be81442f52f30a375c0",
      "description": "",
      "upload_date": "2025-01-05",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18011078153682625",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6153,
      "title": "tiktok d5d63e99298efa6c0423da9917fb4c0a2e3eab67",
      "description": "",
      "upload_date": "2025-01-04",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18003282170528144",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-01-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6154,
      "title": "tiktok 0d4cfd4336634201f295e09f99e92ef5fa6a54c7",
      "description": "",
      "upload_date": "2024-12-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "agents",
        "astra",
        "project",
        "sculpture",
        "using",
        "world"
      ],
      "search_text": "agents astra project sculpture using world",
      "platforms": {
        "instagram": {
          "video_id": "17862348132293993",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6155,
      "title": "tiktok 903c15d39aee91b5a50221f5e0de892c298d05f0",
      "description": "",
      "upload_date": "2024-12-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18046433255140966",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6156,
      "title": "tiktok 2c87e26ed66b006024fdc949b270eb5e9435b153",
      "description": "",
      "upload_date": "2024-12-17",
      "total_views": 0,
      "max_views": 0,
      "topics": [],
      "search_text": "",
      "platforms": {
        "instagram": {
          "video_id": "18094596679495132",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6668,
      "title": "tiktok 1c7fa2839202a58d61ba4c7afa4a28eae107cef1",
      "description": "",
      "upload_date": "2025-03-21",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "accurate",
        "hours",
        "model",
        "next",
        "spend",
        "spent"
      ],
      "search_text": "accurate hours model next spend spent",
      "platforms": {
        "instagram": {
          "video_id": "18088070542525377",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2025-03-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6337,
      "title": "tiktok 124c9d35666bbd220694533bad88908f4cb1bae7",
      "description": "",
      "upload_date": "2024-12-12",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "going",
        "let",
        "team",
        "time",
        "want"
      ],
      "search_text": "data going let team time want",
      "platforms": {
        "instagram": {
          "video_id": "18077396416602610",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-12-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6339,
      "title": "tiktok ba59e0260fcb7eee8f65083fe8390511e75acf3d",
      "description": "",
      "upload_date": "2024-10-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "distance",
        "levinci",
        "metrics",
        "science"
      ],
      "search_text": "data distance levinci metrics science",
      "platforms": {
        "instagram": {
          "video_id": "18143025370352338",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6340,
      "title": "tiktok ab70212cbc5914f13b1643b96d76fc60de8dea1d",
      "description": "",
      "upload_date": "2024-10-29",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "der",
        "die",
        "man",
        "revolution",
        "science"
      ],
      "search_text": "data der die man revolution science",
      "platforms": {
        "instagram": {
          "video_id": "18047760640956775",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6341,
      "title": "tiktok 08ffeb08df0e47f4d3fbbc5c7120071a912b9276",
      "description": "",
      "upload_date": "2024-10-28",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "evaluations",
        "half",
        "magic",
        "million",
        "neural",
        "ran"
      ],
      "search_text": "evaluations half magic million neural ran",
      "platforms": {
        "instagram": {
          "video_id": "18062693485671354",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6342,
      "title": "tiktok 96cb6701682ae307ce7eae6b28570743ec7bfafb",
      "description": "",
      "upload_date": "2024-10-25",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "das",
        "ist",
        "modell",
        "nicht",
        "sie",
        "und"
      ],
      "search_text": "das ist modell nicht sie und",
      "platforms": {
        "instagram": {
          "video_id": "18051156694924365",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6243,
      "title": "tiktok 425c38061777a01ab55105ee38c0b382b0884be6",
      "description": "",
      "upload_date": "2024-10-15",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "baselines",
        "remember",
        "rsc"
      ],
      "search_text": "baselines remember rsc",
      "platforms": {
        "instagram": {
          "video_id": "18034231661258537",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6343,
      "title": "tiktok caa90da79fbc05977cd1c19f1a42e9568bfa8bc6",
      "description": "",
      "upload_date": "2024-10-13",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "ber",
        "datascience",
        "die",
        "distanz",
        "ist",
        "man"
      ],
      "search_text": "ber datascience die distanz ist man",
      "platforms": {
        "instagram": {
          "video_id": "17880623631145889",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6344,
      "title": "tiktok 3f0c6902fa5a79371eb5d6cda9f068ad9f77c706",
      "description": "",
      "upload_date": "2024-10-11",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "des",
        "diction",
        "dictions",
        "donn",
        "les",
        "pour"
      ],
      "search_text": "des diction dictions donn les pour",
      "platforms": {
        "instagram": {
          "video_id": "18101941750452318",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-10-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6345,
      "title": "tiktok 48d3b4d7d189f76a66980535eae81feeb890348e",
      "description": "",
      "upload_date": "2024-10-06",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "alternatives",
        "copilot",
        "github",
        "gpt",
        "tgi"
      ],
      "search_text": "alternatives copilot github gpt tgi",
      "platforms": {
        "instagram": {
          "video_id": "17911487102918114",
          "url": "https://www.instagram.com/reel/DAihK0fuqKc/",
          "view_count": 0,
          "upload_date": "2024-09-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6346,
      "title": "tiktok b030263c4a5b1a509660564c61b6c54d2b265b4d",
      "description": "",
      "upload_date": "2024-10-02",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "algoritmo",
        "anomal",
        "datos",
        "detecci",
        "mejor",
        "que"
      ],
      "search_text": "algoritmo anomal datos detecci mejor que",
      "platforms": {
        "instagram": {
          "video_id": "18042175877039235",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6348,
      "title": "tiktok 2b26a9a7fe39c1f929b2bce1f79b9b6987aa601d",
      "description": "",
      "upload_date": "2024-09-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "die",
        "ein",
        "eine",
        "ist",
        "und",
        "wissen"
      ],
      "search_text": "die ein eine ist und wissen",
      "platforms": {
        "instagram": {
          "video_id": "18041428999891266",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6249,
      "title": "tiktok ab5517dd732ef557e417f11c56b665c433d22982",
      "description": "",
      "upload_date": "2024-09-26",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "das",
        "die",
        "haben",
        "sie",
        "und",
        "wie"
      ],
      "search_text": "das die haben sie und wie",
      "platforms": {
        "instagram": {
          "video_id": "17882165784104238",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6349,
      "title": "tiktok 1b5cf92f9eea6609cd7e9a85d4815881356dace8",
      "description": "",
      "upload_date": "2024-09-25",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "cat",
        "des",
        "donn",
        "est",
        "une",
        "vous"
      ],
      "search_text": "cat des donn est une vous",
      "platforms": {
        "instagram": {
          "video_id": "18025515578124593",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6350,
      "title": "tiktok d3e1d6b2b51a4f6d63a13025820aa59e02e742b1",
      "description": "",
      "upload_date": "2024-09-20",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "abs",
        "arxiv",
        "detection",
        "object",
        "org",
        "yolo"
      ],
      "search_text": "abs arxiv detection object org yolo",
      "platforms": {
        "instagram": {
          "video_id": "18320422567197895",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-09-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6373,
      "title": "tiktok 709161c3d2236bdfbdafab1b6ad710a6d29ef9ad",
      "description": "",
      "upload_date": "2024-05-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "four",
        "good",
        "gpt",
        "labeling",
        "well"
      ],
      "search_text": "data four good gpt labeling well",
      "platforms": {
        "instagram": {
          "video_id": "18267781537224297",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6252,
      "title": "tiktok ffcd51d466fbaf06c6b067719c672165f926cd17",
      "description": "",
      "upload_date": "2024-05-25",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "bojan",
        "hopefully",
        "linkedin",
        "post",
        "saw",
        "tunguz"
      ],
      "search_text": "bojan hopefully linkedin post saw tunguz",
      "platforms": {
        "instagram": {
          "video_id": "18308133880199591",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6375,
      "title": "tiktok 844e96adce97f89ef0cc605924da96b094d0fe0f",
      "description": "",
      "upload_date": "2024-05-17",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "documents",
        "get",
        "largelanguagemodels",
        "rag",
        "retrievalaugmentedgeneration",
        "snowflake"
      ],
      "search_text": "documents get largelanguagemodels rag retrievalaugmentedgeneration snowflake",
      "platforms": {
        "instagram": {
          "video_id": "17998462169374803",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-05-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6376,
      "title": "tiktok b9c0b9b5b5823a6d96689096093f16871b5f156e",
      "description": "",
      "upload_date": "2024-05-04",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "credit",
        "entropy",
        "know",
        "liability",
        "look",
        "rating"
      ],
      "search_text": "credit entropy know liability look rating",
      "platforms": {
        "instagram": {
          "video_id": "18031284685947328",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6377,
      "title": "tiktok 325733951ba1b2b080519863609b58df0876fcc6",
      "description": "",
      "upload_date": "2024-05-02",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "largelanguagemodels",
        "llama",
        "really",
        "training",
        "trainingdata"
      ],
      "search_text": "data largelanguagemodels llama really training trainingdata",
      "platforms": {
        "instagram": {
          "video_id": "17844200382201390",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6378,
      "title": "tiktok a5562e05a4234cd0e1c6c939c12f26c0cc677602",
      "description": "",
      "upload_date": "2024-04-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "reinforcementlearning",
        "rlhf",
        "techtok"
      ],
      "search_text": "chatgpt datascience machinelearning reinforcementlearning rlhf techtok",
      "platforms": {
        "instagram": {
          "video_id": "18125215720314922",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6379,
      "title": "tiktok 54aba190d1b3c5efca9aef8da61c0123890bc641",
      "description": "",
      "upload_date": "2024-04-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "databases",
        "datascience",
        "embeddings",
        "faiss",
        "pinecone",
        "vector"
      ],
      "search_text": "databases datascience embeddings faiss pinecone vector",
      "platforms": {
        "instagram": {
          "video_id": "18114686812368108",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6380,
      "title": "tiktok 35db4b850d175713598b9f83c91925299673d3a8",
      "description": "",
      "upload_date": "2024-04-18",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "crowdai",
        "data",
        "leakage",
        "machinelearning",
        "sarcos",
        "target"
      ],
      "search_text": "crowdai data leakage machinelearning sarcos target",
      "platforms": {
        "instagram": {
          "video_id": "17919905684869890",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6381,
      "title": "tiktok b8f5c81c0a4f7eb4981a87d3af5d05a2b3ef5f5b",
      "description": "",
      "upload_date": "2024-04-14",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "baseline",
        "baselinemodel",
        "benchmark",
        "datascience",
        "machinelearning",
        "model"
      ],
      "search_text": "baseline baselinemodel benchmark datascience machinelearning model",
      "platforms": {
        "instagram": {
          "video_id": "18090082459385891",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6382,
      "title": "tiktok 39fe20eb758016a4a43ecd1d3b552263c0a86de4",
      "description": "",
      "upload_date": "2024-04-12",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "cohere",
        "model",
        "multi",
        "tool",
        "tools",
        "use"
      ],
      "search_text": "cohere model multi tool tools use",
      "platforms": {
        "instagram": {
          "video_id": "18006275363454889",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6383,
      "title": "tiktok 2de5791aaafd1430249a1f1b6a97b40454657c60",
      "description": "",
      "upload_date": "2024-04-10",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "machinelearning",
        "matrixalgebra",
        "one",
        "singularvaluedecomposition",
        "svd"
      ],
      "search_text": "datascience machinelearning matrixalgebra one singularvaluedecomposition svd",
      "platforms": {
        "instagram": {
          "video_id": "18005804321207015",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2024-04-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6282,
      "title": "Data Centric AI helps to remind us not to focus too much on the model or algorithms. In real data science, it‚Äôs more about understanding your data and having high quality labeled training data. #datascience #machinelearning #datacentricai #cleanlab #erroranalysis",
      "description": "Data Centric AI helps to remind us not to focus too much on the model or algorithms. In real data science, it‚Äôs more about understanding your data and having high quality labeled training data. #datascience #machinelearning #datacentricai #cleanlab #erroranalysis",
      "upload_date": "2023-02-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "cleanlab",
        "data",
        "datacentricai",
        "datascience",
        "erroranalysis",
        "machinelearning",
        "model"
      ],
      "search_text": "Data Centric AI helps to remind us not to focus too much on the model or algorithms. In real data science, it‚Äôs more about understanding your data and having high quality labeled training data. #datascience #machinelearning #datacentricai #cleanlab #erroranalysis cleanlab data datacentricai datascience erroranalysis machinelearning model",
      "platforms": {
        "instagram": {
          "video_id": "17974421288103931",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-24",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "rxlBz7Z-TLs",
          "url": "https://www.youtube.com/watch?v=rxlBz7Z-TLs",
          "view_count": 0,
          "upload_date": "2023-02-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6285,
      "title": "Wrap up of current events going on with chat including #openai #chatgpt #bing #amazon #datascience #machinelearning",
      "description": "Wrap up of current events going on with chat including #openai #chatgpt #bing #amazon #datascience #machinelearning",
      "upload_date": "2023-02-16",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "amazon",
        "bing",
        "chatgpt",
        "datascience",
        "like",
        "machinelearning",
        "models",
        "openai"
      ],
      "search_text": "Wrap up of current events going on with chat including #openai #chatgpt #bing #amazon #datascience #machinelearning amazon bing chatgpt datascience like machinelearning models openai",
      "platforms": {
        "instagram": {
          "video_id": "17903290385717019",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-17",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "V8w4qQAj6zE",
          "url": "https://www.youtube.com/watch?v=V8w4qQAj6zE",
          "view_count": 0,
          "upload_date": "2023-02-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6669,
      "title": "tiktok c50c73941e834e5984e3e0c1e90117d5cbebf631",
      "description": "",
      "upload_date": "2023-02-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "let",
        "lot",
        "mediocre",
        "science",
        "videos"
      ],
      "search_text": "data let lot mediocre science videos",
      "platforms": {
        "instagram": {
          "video_id": "18349319596035537",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-12",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6287,
      "title": "Roundup of this weeks news, let me know if you all like this format. I had a lot of fun making this. #datascience #machinelearning #dumbtechnews #openai #google #microsoft #stabilityai #meta #apple",
      "description": "Roundup of this weeks news, let me know if you all like this format. I had a lot of fun making this. #datascience #machinelearning #dumbtechnews #openai #google #microsoft #stabilityai #meta #apple",
      "upload_date": "2023-02-10",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "dumbtechnews",
        "google",
        "machinelearning",
        "microsoft",
        "openai"
      ],
      "search_text": "Roundup of this weeks news, let me know if you all like this format. I had a lot of fun making this. #datascience #machinelearning #dumbtechnews #openai #google #microsoft #stabilityai #meta #apple datascience dumbtechnews google machinelearning microsoft openai",
      "platforms": {
        "instagram": {
          "video_id": "17996789428638638",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-02-11",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "wqAsuSuec3o",
          "url": "https://www.youtube.com/watch?v=wqAsuSuec3o",
          "view_count": 0,
          "upload_date": "2023-02-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6293,
      "title": "OpenAI AI classifier is a great example to remind people of the limitations when detecting rare events. It‚Äôs not intuitive, so I showed the math and need you all to get it. This happens in many contexts like detecting terrorizes or diseases. #datascience #statistics #openai #baseratefallacy",
      "description": "OpenAI AI classifier is a great example to remind people of the limitations when detecting rare events. It‚Äôs not intuitive, so I showed the math and need you all to get it. This happens in many contexts like detecting terrorizes or diseases. #datascience #statistics #openai #baseratefallacy",
      "upload_date": "2023-02-04",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "baseratefallacy",
        "cheating",
        "classifier",
        "competition",
        "datascience",
        "detecting",
        "kaggle",
        "machinelearning",
        "openai",
        "ottocompetition",
        "statistics"
      ],
      "search_text": "OpenAI AI classifier is a great example to remind people of the limitations when detecting rare events. It‚Äôs not intuitive, so I showed the math and need you all to get it. This happens in many contexts like detecting terrorizes or diseases. #datascience #statistics #openai #baseratefallacy baseratefallacy cheating classifier competition datascience detecting kaggle machinelearning openai ottocompetition statistics",
      "platforms": {
        "instagram": {
          "video_id": "18055130989355605",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-31",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "RtSTS7U1qqM",
          "url": "https://www.youtube.com/watch?v=RtSTS7U1qqM",
          "view_count": 0,
          "upload_date": "2023-02-04",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6301,
      "title": "Picking the right GPU for deep learning based on Tim Dettmers blog post.",
      "description": "Picking the right GPU for deep learning based on Tim Dettmers blog post.",
      "upload_date": "2023-01-17",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "gpu",
        "makes",
        "right",
        "sense",
        "tim",
        "using"
      ],
      "search_text": "Picking the right GPU for deep learning based on Tim Dettmers blog post. gpu makes right sense tim using So, which GPU is right for you? The first question is, does it economically make sense for me to buy a GPU versus using one in the cloud? Tim makes some assumptions but then crunches the numbers and figures out that the over under is at about 300 days for where it makes sense to have your own GPU versus using one in the cloud. For teams, it often makes sense to have your own cluster rather than putting GPU on individual computers. Once you've decided on the GPU, people often gravitate towards looking at the most powerful GPUs but for those of on a budget. Tim also crunches the number here to give us a overall relative cost performance. There's also a flowchart that goes to a lot of the common decisions that people have when purchasing a GPU like, are they part of a large organization? Are they using this for training? Are they using this for large language models? All of this will help you better pick your next GPU. This is just a couple of the highlights. Go read the blog post",
      "platforms": {
        "instagram": {
          "video_id": "17847819350898909",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6306,
      "title": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law",
      "description": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law",
      "upload_date": "2022-12-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "barexam",
        "chatgpt",
        "datascience",
        "gpt",
        "gpt3",
        "large",
        "law",
        "like",
        "machinelearning",
        "openai",
        "reasoning"
      ],
      "search_text": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law barexam chatgpt datascience gpt gpt3 large law like machinelearning openai reasoning",
      "platforms": {
        "instagram": {
          "video_id": "18002155381560383",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-05",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "C6AJoo3TRjI",
          "url": "https://www.youtube.com/watch?v=C6AJoo3TRjI",
          "view_count": 0,
          "upload_date": "2022-12-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6307,
      "title": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering",
      "description": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering",
      "upload_date": "2022-12-31",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "chatgptp",
        "clustering",
        "datascience",
        "kmeans",
        "like",
        "machinelearning",
        "machinelearnjng",
        "means",
        "openai",
        "statistics"
      ],
      "search_text": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering chatgpt chatgptp clustering datascience kmeans like machinelearning machinelearnjng means openai statistics",
      "platforms": {
        "instagram": {
          "video_id": "17948829074269779",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2023-01-01",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "bPYZ6Quf0bM",
          "url": "https://www.youtube.com/watch?v=bPYZ6Quf0bM",
          "view_count": 0,
          "upload_date": "2022-12-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6309,
      "title": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning",
      "description": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning",
      "upload_date": "2023-01-03",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "bar",
        "cheat",
        "data",
        "datascience",
        "exam",
        "gpt",
        "machinelearning",
        "models",
        "openai",
        "pretty",
        "scientist"
      ],
      "search_text": "Models that cheat, take shortcuts, and leak information are all part of the data scientist life style. Ever my data scientist has a story like this. #datascience #machinelearning bar cheat data datascience exam gpt machinelearning models openai pretty scientist",
      "platforms": {
        "instagram": {
          "video_id": "18122245783291428",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-12-30",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "4rvywn2D9Vo",
          "url": "https://www.youtube.com/watch?v=4rvywn2D9Vo",
          "view_count": 0,
          "upload_date": "2023-01-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6312,
      "title": "Metaâs Cicero AI that plays Diplomacy and knows how to get its way with people. #datascience",
      "description": "Metaâs Cicero AI that plays Diplomacy and knows how to get its way with people. #datascience",
      "upload_date": "2022-11-24",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "diplomacy",
        "figured",
        "get",
        "meta",
        "trained"
      ],
      "search_text": "Metaâs Cicero AI that plays Diplomacy and knows how to get its way with people. #datascience datascience diplomacy figured get meta trained Did you ever get so mad that later you look back and figured out you were being manipulated the whole time? Well, the folks at Meta did that. They trained an AI how to play diplomacy which is a strategy game based on negotiation. The AI they trained is both ruthless and approachable. It's a master manipulator and great at diplomacy. Just wait till the advertisers and marketers figured this out. Here's it showing some sympathy for France. Here's how it coordinates a ceasefire with England. In this example, it predicts Italy is going to attack in coordinates a defense with Austria. It's good. The kicker, the other humans like to play with the AI more than they like other humans. AI that can learn from tens of thousands of interactions. It's probably not surprisingly pretty good at knowing how to interact.",
      "platforms": {
        "instagram": {
          "video_id": "17985465631666206",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-24",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6582,
      "title": "DiffusionDet is bringing generative approaches to object detection #computervision",
      "description": "DiffusionDet is bringing generative approaches to object detection #computervision",
      "upload_date": "2022-11-21",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "computervision",
        "detection",
        "generative",
        "object",
        "paper",
        "shows"
      ],
      "search_text": "DiffusionDet is bringing generative approaches to object detection #computervision computervision detection generative object paper shows This week, we've got a new model that shows how we can use the concept of diffusion for object detection. The DiffusionDet paper shows that a generative process can work for object detection that's on par with other object detection methods. This is super cool to me because it shows how diffusion can be used for other tasks that we wouldn't necessarily think of as generative. This paper also shows the value of using a random process to generate the object detection boxes which is different than traditional processes for object detection. Just another reminder of how useful random can be in data science. Check out the paper to learn more and there's even code if you want to try it yourself.",
      "platforms": {
        "instagram": {
          "video_id": "17989523962624403",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6313,
      "title": "Regularization is something I need more in my everyday life.",
      "description": "Regularization is something I need more in my everyday life.",
      "upload_date": "2022-11-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "hammering",
        "maybe",
        "regularization",
        "shirt",
        "shoes",
        "videos"
      ],
      "search_text": "Regularization is something I need more in my everyday life. hammering maybe regularization shirt shoes videos Hey, boss. Just ordering this T-shirt and shoes from this video and I'll be ready for our one on one. Is that video about hammering? Yeah, I'm studying hammering for my girlfriend. Why the T shirt and shoes? I've watched a couple of videos and analyze them and those were some of the variables to do hammering. I've also ordered a hammer and I've been practicing the correct angle and transfer of power to the nail. There's just so many things to consider. Can you remind me what regularization is? Sure, in machine learning, algorithms can overfit to their training data like this and when they do that, they don't generalize well to new examples. So, regularization is a technique where we penalize coefficients from getting too big and this technique helps us improve how models generalize. If you only watch a few videos on hammering, would it be possible to overfit to those videos and learn maybe some actions, behavior, clothing that aren't generalizable? Hmm, yeah, I pose it would be. Maybe I should watch a few more videos. What I want you to do is apply that regularization methodology here. So when you have a lot of features, ideas that you think, just try to keep cool overall, don't over focus and maybe even drop some of those ideas. Wow. Oh, that makes sense. It's kind of like how Lasso helps those pea greater than end type problems where the number of features is much greater than the number of observations arose. Exactly. By penalizing those coefficients, we",
      "platforms": {
        "instagram": {
          "video_id": "17961857488968038",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6315,
      "title": "Automatic speech recognition using transformers. It is that easy!",
      "description": "Automatic speech recognition using transformers. It is that easy!",
      "upload_date": "2022-11-17",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "automatic",
        "recognition",
        "speech",
        "start",
        "transformers",
        "using"
      ],
      "search_text": "Automatic speech recognition using transformers. It is that easy! automatic recognition speech start transformers using Me show you how to do automatic speech recognition in just a few lines of code. I had to do this for a customer earlier today. Thought I'd share it with all of you. Start by grabbing the pipeline out of Transformers and then we're going to use Facebook's Wave to Vec. It's a great kind of starting point for doing this type of automatic speech recognition. Whispers another popular model. It'll be supported soon inside Transformers. It's not there yet couple weeks away but in the meantime, start with this and then you can see it's just one line of command to go ahead and do the transcription. Sit back, wait, and it'll happen. I've seen people apply this to lots of different things from financial earning calls to YouTube videos so go have some fun.",
      "platforms": {
        "instagram": {
          "video_id": "18024147409442261",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6321,
      "title": "Editing facts in large language models. An exciting approach that is probing LLMs. #largelanguagemodels #datascience  #machinelearning",
      "description": "Editing facts in large language models. An exciting approach that is probing LLMs. #largelanguagemodels #datascience  #machinelearning",
      "upload_date": "2022-11-06",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "editing",
        "facts",
        "large",
        "largelanguagemodels",
        "machinelearning"
      ],
      "search_text": "Editing facts in large language models. An exciting approach that is probing LLMs. #largelanguagemodels #datascience  #machinelearning datascience editing facts large largelanguagemodels machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "17930727731451909",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6325,
      "title": "Software exec at the end is the best. Your quick intro to patents, trademarks, and licenses. I see too many comments where people get confused on these.",
      "description": "Software exec at the end is the best. Your quick intro to patents, trademarks, and licenses. I see too many comments where people get confused on these.",
      "upload_date": "2022-11-01",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "licenses",
        "patents",
        "people",
        "software",
        "trademarks",
        "use"
      ],
      "search_text": "Software exec at the end is the best. Your quick intro to patents, trademarks, and licenses. I see too many comments where people get confused on these. licenses patents people software trademarks use I am an inventor and by sharing exactly what I did, I can use patents to keep anyone else from making it for a while. I am an artist and copyright protects my creative expression to display, share, perform for a very long time. As a business owner, I've invested a lot into this logo and these words. Trademark Law lets me protect that as long as I'm actively using it and they're mine. I'm a programmer and I create software products. I use licenses other people can use my products but on terms that we agree on. As a software executive, I try to gather patents. They're kind of like nukes. We don't really want to use them at least publicly but it's good to be in the club that has lots of them. I don't worry about copyright. All we have to do is slightly change the look and feel of a program, it'll be okay but trademarks, they're a big deal. We want to establish our brand and be very careful about stepping on others and we spend a lot of time on licensing whether it's making sure we're careful about using other people's licenses or crafting our own licenses with terms that we like.",
      "platforms": {
        "instagram": {
          "video_id": "17920381517622286",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-11-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6327,
      "title": "Stable diffusion for markup. This is about better understanding how to go from text to image, not a practical solution. #stablediffusion #datascience #statistics",
      "description": "Stable diffusion for markup. This is about better understanding how to go from text to image, not a practical solution. #stablediffusion #datascience #statistics",
      "upload_date": "2022-10-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "diffusion",
        "markup",
        "model",
        "stablediffusion",
        "statistics"
      ],
      "search_text": "Stable diffusion for markup. This is about better understanding how to go from text to image, not a practical solution. #stablediffusion #datascience #statistics datascience diffusion markup model stablediffusion statistics So we've seen how stablediffusion can go from text to images. Wow. Now it's been extended to markup. So we're going to start with some markup. This is La Tech and what we'll see is that this markup actually represents an equation. And the model itself will generate and give you an image of the final equation. They also train the model on other types of layouts from table layouts, sheet music, and molecular images. The model isn't perfect in representing the markdown but it just shows you the power of diffusion. If you want to, go check out the space. You can play around with different Letech commands. See how it's visualized. There's code and a paper available as well.",
      "platforms": {
        "instagram": {
          "video_id": "17926893620555679",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-10-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6328,
      "title": "Mixing in some law with data science. #craiyon #dallemini #stablediffusion #texttoimage #machinelearning #datascience",
      "description": "Mixing in some law with data science. #craiyon #dallemini #stablediffusion #texttoimage #machinelearning #datascience",
      "upload_date": "2022-10-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "craiyon",
        "dallemini",
        "dolly",
        "open",
        "stablediffusion",
        "trademark"
      ],
      "search_text": "Mixing in some law with data science. #craiyon #dallemini #stablediffusion #texttoimage #machinelearning #datascience craiyon dallemini dolly open stablediffusion trademark Wanna hear how a $20 billion dollar AI company was able to get its way with the startup? I don't have any inside knowledge and although I have a law degree, I wouldn't take that too seriously but what I want to do is I want to give you the story that I know and share some of the documents I found along the way. Open AI created Dolly about over a year ago, very cool text to image but they didn't make it publicly available. So, Boris came along, read the original paper, said, hey, I can make a better version of this. Figured out how to make a simpler version of this that ran faster and he made it available to the public as Dolly Mini and we all had a lot of fun with Dolly Mini. It blew up even was higher than the open AI Dolly in Google search for a little while but then, Boris flew a little bit too close to the sun when he filed a trademark for Dolly Mini. Now, trademark protection is very useful in many cases where it allows buyers to distinctively identified the sellers but sometimes in communities like open source or fan communities where lots of people have contributed, it can feel like people are stealing something out of public discourse when they trademark part of that. My guess is is once Open AI saw that trademark application, their lawyers immediately showed up, knocked on Boris' door and politely said, you better change your name and the reason I believe this set of events is because two weeks after Boris' filing, Open AI decided to file the trademark as well. So, be careful when you name your projects and hey, if you're bored, check out the trademark search database. Lot of",
      "platforms": {
        "instagram": {
          "video_id": "18326404447050905",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-10-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6329,
      "title": "Reminder to visualize your data #datascience #datavisualization #statistics",
      "description": "Reminder to visualize your data #datascience #datavisualization #statistics",
      "upload_date": "2022-10-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "analyze",
        "data",
        "datascience",
        "datavisualization",
        "statistics",
        "visualize"
      ],
      "search_text": "Reminder to visualize your data #datascience #datavisualization #statistics analyze data datascience datavisualization statistics visualize Hey intern. I need you to analyze these data sets and tell me what's different. Yes sir. Everyone of these data sets is different but when I analyze them and I look at the mean, the standard deviation, even the correlation between variables, they're all the same. I'm not sure what to do. I've been there. Try the plot function. Let me try that. Glad you see it now. It's always important to visualize your data. You can't just trust the numbers alone.",
      "platforms": {
        "instagram": {
          "video_id": "17965161760956520",
          "url": "https://www.instagram.com/rajistics/",
          "view_count": 0,
          "upload_date": "2022-10-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6624,
      "title": "tiktok c117fdd786b5fa4b9ca9bf6e63bb5784bbaa2f2b",
      "description": "",
      "upload_date": "2023-04-05",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "emilyocasio",
        "explaining",
        "machinelearning",
        "promptengineering",
        "societyforscience"
      ],
      "search_text": "datascience emilyocasio explaining machinelearning promptengineering societyforscience",
      "platforms": {
        "instagram": {
          "video_id": "CqaleFKgsrj",
          "url": "https://www.instagram.com/reel/CqaleFKgsrj/",
          "view_count": 0,
          "upload_date": "2023-03-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6622,
      "title": "tiktok 09d58e11fb8f71d12e407b242351d657cb0331c5",
      "description": "",
      "upload_date": "2023-04-02",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "huggingface",
        "machinelearning",
        "models",
        "stablediffusion",
        "text2video"
      ],
      "search_text": "datascience huggingface machinelearning models stablediffusion text2video",
      "platforms": {
        "instagram": {
          "video_id": "CqZEac2AI9N",
          "url": "https://www.instagram.com/reel/CqZEac2AI9N/",
          "view_count": 0,
          "upload_date": "2023-03-26",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6623,
      "title": "tiktok 1bfd35d388a29397ed0c8b2413093d3e6846b824",
      "description": "",
      "upload_date": "2023-04-01",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "flant5",
        "largelanguagemodels",
        "lora",
        "machinelearning",
        "peft"
      ],
      "search_text": "datascience flant5 largelanguagemodels lora machinelearning peft",
      "platforms": {
        "instagram": {
          "video_id": "CqWagEHgvGp",
          "url": "https://www.instagram.com/reel/CqWagEHgvGp/",
          "view_count": 0,
          "upload_date": "2023-03-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6619,
      "title": "tiktok 91cfc737c7bfbab33af3e1938be71a135a91b710",
      "description": "",
      "upload_date": "2023-03-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "gpt3",
        "gpt4",
        "largelanguagemodels",
        "machinelearning",
        "temperature"
      ],
      "search_text": "datascience gpt3 gpt4 largelanguagemodels machinelearning temperature",
      "platforms": {
        "instagram": {
          "video_id": "CqIwQrqgG8j",
          "url": "https://www.instagram.com/p/CqIwQrqgG8j/",
          "view_count": 0,
          "upload_date": "2023-03-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6618,
      "title": "tiktok 0386ff01a07604d0d2939e3c1a395093b9918e48",
      "description": "",
      "upload_date": "2023-03-26",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "codex",
        "datascience",
        "machinelearning",
        "pair",
        "pairprogramming"
      ],
      "search_text": "chatgpt codex datascience machinelearning pair pairprogramming",
      "platforms": {
        "instagram": {
          "video_id": "CqCFhFfA_lZ",
          "url": "https://www.instagram.com/p/CqCFhFfA_lZ/",
          "view_count": 0,
          "upload_date": "2023-03-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6617,
      "title": "tiktok 23ef998b251599c7ad0769b331d3c7388ef46b0b",
      "description": "",
      "upload_date": "2023-03-25",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "enterprises",
        "lets",
        "open",
        "source",
        "talk"
      ],
      "search_text": "data enterprises lets open source talk",
      "platforms": {
        "instagram": {
          "video_id": "Cp_goHhgB4I",
          "url": "https://www.instagram.com/p/Cp_goHhgB4I/",
          "view_count": 0,
          "upload_date": "2023-03-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6384,
      "title": "tiktok 92b2c950113e3e344c676be2387696f7e2760e53",
      "description": "",
      "upload_date": "2023-03-22",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "imagecaptioning",
        "machinelearning",
        "nvidia",
        "prismer",
        "visualquestionanswering"
      ],
      "search_text": "datascience imagecaptioning machinelearning nvidia prismer visualquestionanswering",
      "platforms": {
        "instagram": {
          "video_id": "Cp2esPQglji",
          "url": "https://www.instagram.com/reel/Cp2esPQglji/",
          "view_count": 0,
          "upload_date": "2023-03-15",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6385,
      "title": "tiktok 7600368163371f79348dbde7355ad7b2624f8a25",
      "description": "",
      "upload_date": "2023-03-22",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "gpt4",
        "langchain",
        "langflow",
        "largelanguagemodels",
        "machinelearning"
      ],
      "search_text": "datascience gpt4 langchain langflow largelanguagemodels machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "Cp7_JNIgSyP",
          "url": "https://www.instagram.com/reel/Cp7_JNIgSyP/",
          "view_count": 0,
          "upload_date": "2023-03-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6386,
      "title": "tiktok b01c415cbe197395d672a1697f15f8bcce0f6c62",
      "description": "",
      "upload_date": "2023-03-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datadrift",
        "datascience",
        "drift",
        "machinelearning",
        "mlops",
        "prophet"
      ],
      "search_text": "datadrift datascience drift machinelearning mlops prophet",
      "platforms": {
        "instagram": {
          "video_id": "Cpyk9mFA1oY",
          "url": "https://www.instagram.com/reel/Cpyk9mFA1oY/",
          "view_count": 0,
          "upload_date": "2023-03-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6387,
      "title": "tiktok 57f46e0b2a673ca85ce1c18b516dc3fb951315d4",
      "description": "",
      "upload_date": "2023-03-18",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "see",
        "starting"
      ],
      "search_text": "datascience gpt3 largelanguagemodels machinelearning see starting",
      "platforms": {
        "instagram": {
          "video_id": "CpsPHPbgGuD",
          "url": "https://www.instagram.com/reel/CpsPHPbgGuD/",
          "view_count": 0,
          "upload_date": "2023-03-11",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6388,
      "title": "tiktok 9afa0af2e2d89cec29cc5ce519ad2fce9f569801",
      "description": "",
      "upload_date": "2023-03-17",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "gpt3",
        "largelanguagemodels",
        "machinelearning",
        "nat",
        "natdev"
      ],
      "search_text": "datascience gpt3 largelanguagemodels machinelearning nat natdev",
      "platforms": {
        "instagram": {
          "video_id": "CpoifhtgOHZ",
          "url": "https://www.instagram.com/reel/CpoifhtgOHZ/",
          "view_count": 0,
          "upload_date": "2023-03-10",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6389,
      "title": "tiktok df04d2a352228dd8fe797b2f60216402bea19596",
      "description": "",
      "upload_date": "2023-03-15",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "best",
        "codetok",
        "datascience",
        "machine",
        "machinelearning",
        "python"
      ],
      "search_text": "best codetok datascience machine machinelearning python",
      "platforms": {
        "instagram": {
          "video_id": "Cpl-xKwgwim",
          "url": "https://www.instagram.com/reel/Cpl-xKwgwim/",
          "view_count": 0,
          "upload_date": "2023-03-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6390,
      "title": "tiktok b6bc845ad22ac829802bc2b587b1104739cb7b37",
      "description": "",
      "upload_date": "2023-03-15",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "ensembling",
        "kaggle",
        "key",
        "machinelearning",
        "majorityvoting"
      ],
      "search_text": "datascience ensembling kaggle key machinelearning majorityvoting",
      "platforms": {
        "instagram": {
          "video_id": "Cp0oWVcgTnl",
          "url": "https://www.instagram.com/reel/Cp0oWVcgTnl/",
          "view_count": 0,
          "upload_date": "2023-03-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6616,
      "title": "tiktok d4f56cd69671cddaf9659ed34261197c90be41f9",
      "description": "",
      "upload_date": "2023-03-13",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "fonts",
        "generativeai",
        "machinelearning",
        "stablediffusion",
        "word"
      ],
      "search_text": "datascience fonts generativeai machinelearning stablediffusion word",
      "platforms": {
        "instagram": {
          "video_id": "CpiiIIGgczi",
          "url": "https://www.instagram.com/reel/CpiiIIGgczi/",
          "view_count": 0,
          "upload_date": "2023-03-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6615,
      "title": "tiktok 0e24ee241dd470ca7526ee5f0b53e99e277820d0",
      "description": "",
      "upload_date": "2023-03-11",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "largelanguagemodels",
        "less",
        "machinelearning",
        "meta",
        "opensource"
      ],
      "search_text": "datascience largelanguagemodels less machinelearning meta opensource",
      "platforms": {
        "instagram": {
          "video_id": "Cpbg_klg9RN",
          "url": "https://www.instagram.com/reel/Cpbg_klg9RN/",
          "view_count": 0,
          "upload_date": "2023-03-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6613,
      "title": "tiktok 5cc3823f5e80faa4ba21bb0be4549d9f381c8381",
      "description": "",
      "upload_date": "2023-03-10",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "google",
        "machinelearning",
        "meta",
        "openai",
        "stabilityai"
      ],
      "search_text": "datascience google machinelearning meta openai stabilityai",
      "platforms": {
        "instagram": {
          "video_id": "CpWcX_HAyoW",
          "url": "https://www.instagram.com/reel/CpWcX_HAyoW/",
          "view_count": 0,
          "upload_date": "2023-03-03",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6609,
      "title": "tiktok ce63b22e2a11d8eac544f2b46a8723093f0f35bd",
      "description": "",
      "upload_date": "2023-03-02",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "cleanlab",
        "data",
        "datacentricai",
        "datascience",
        "erroranalysis",
        "machinelearning"
      ],
      "search_text": "cleanlab data datacentricai datascience erroranalysis machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "CpC90VvglEV",
          "url": "https://www.instagram.com/reel/CpC90VvglEV/",
          "view_count": 0,
          "upload_date": "2023-02-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6607,
      "title": "tiktok 7c4249bfb070958df2e8dfd21c1651cf7da25266",
      "description": "",
      "upload_date": "2023-02-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "gpt",
        "gpt3",
        "gpt4",
        "machinelearning",
        "speculating"
      ],
      "search_text": "datascience gpt gpt3 gpt4 machinelearning speculating",
      "platforms": {
        "instagram": {
          "video_id": "Co6AedbgA34",
          "url": "https://www.instagram.com/p/Co6AedbgA34/",
          "view_count": 0,
          "upload_date": "2023-02-20",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6402,
      "title": "Is explainability important for you? #datascience #explainability #interpretability #statistics #codetalk #machinelearning",
      "description": "Is explainability important for you? #datascience #explainability #interpretability #statistics #codetalk #machinelearning",
      "upload_date": "2022-08-06",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "codetalk",
        "datascience",
        "explainability",
        "interpretability",
        "machinelearning",
        "statistics"
      ],
      "search_text": "Is explainability important for you? #datascience #explainability #interpretability #statistics #codetalk #machinelearning codetalk datascience explainability interpretability machinelearning statistics",
      "platforms": {
        "youtube": {
          "video_id": "iE1eQ2rDAw0",
          "url": "https://www.youtube.com/watch?v=iE1eQ2rDAw0",
          "view_count": 0,
          "upload_date": "2022-08-06",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6404,
      "title": "tiktok b16b91c28d975c34028f8e7f66812744ed670595",
      "description": "",
      "upload_date": "2022-07-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "adversarial",
        "codetok",
        "datacentricai",
        "datascience",
        "huggingface",
        "offering"
      ],
      "search_text": "adversarial codetok datacentricai datascience huggingface offering",
      "platforms": {
        "youtube": {
          "video_id": "YKwEW4WxLEY",
          "url": "https://www.youtube.com/watch?v=YKwEW4WxLEY",
          "view_count": 0,
          "upload_date": "2022-07-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6559,
      "title": "Learn about foundational models, especially in #nlp #naturallanguageprocessing #datascience #deeplearning #analytics #techtok #openai",
      "description": "Learn about foundational models, especially in #nlp #naturallanguageprocessing #datascience #deeplearning #analytics #techtok #openai",
      "upload_date": "2022-04-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "analytics",
        "datascience",
        "deeplearning",
        "naturallanguageprocessing",
        "nlp",
        "techtok"
      ],
      "search_text": "Learn about foundational models, especially in #nlp #naturallanguageprocessing #datascience #deeplearning #analytics #techtok #openai analytics datascience deeplearning naturallanguageprocessing nlp techtok",
      "platforms": {
        "youtube": {
          "video_id": "G4byELk_EUo",
          "url": "https://www.youtube.com/watch?v=G4byELk_EUo",
          "view_count": 0,
          "upload_date": "2022-04-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6560,
      "title": "Loss Functions - simple example of MAE versus RSME #datascience #statistics #analytics #codetok #regression",
      "description": "Loss Functions - simple example of MAE versus RSME #datascience #statistics #analytics #codetok #regression",
      "upload_date": "2022-08-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "analytics",
        "codetok",
        "datascience",
        "loss",
        "regression",
        "statistics"
      ],
      "search_text": "Loss Functions - simple example of MAE versus RSME #datascience #statistics #analytics #codetok #regression analytics codetok datascience loss regression statistics",
      "platforms": {
        "youtube": {
          "video_id": "DEt0Dcs26Ds",
          "url": "https://www.youtube.com/watch?v=DEt0Dcs26Ds",
          "view_count": 0,
          "upload_date": "2022-08-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6561,
      "title": "Rust for machine learning. It‚Äôs useful in some cases for ML, but learn python first. #datascience #codetok #python #machinelearning #rust",
      "description": "Rust for machine learning. It‚Äôs useful in some cases for ML, but learn python first. #datascience #codetok #python #machinelearning #rust",
      "upload_date": "2022-09-25",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "codetok",
        "datascience",
        "machine",
        "machinelearning",
        "python",
        "rust"
      ],
      "search_text": "Rust for machine learning. It‚Äôs useful in some cases for ML, but learn python first. #datascience #codetok #python #machinelearning #rust codetok datascience machine machinelearning python rust",
      "platforms": {
        "instagram": {
          "video_id": "CkdbEdzgn9c",
          "url": "https://www.instagram.com/reel/CkdbEdzgn9c/",
          "view_count": 0,
          "upload_date": "2022-09-25",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6562,
      "title": "Diffusion models for markup. #datascience #machinelearning #stablediffusion",
      "description": "Diffusion models for markup. #datascience #machinelearning #stablediffusion",
      "upload_date": "2022-10-13",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "diffusion",
        "machinelearning",
        "markup",
        "models",
        "stablediffusion"
      ],
      "search_text": "Diffusion models for markup. #datascience #machinelearning #stablediffusion datascience diffusion machinelearning markup models stablediffusion",
      "platforms": {
        "instagram": {
          "video_id": "CkV7dh8A2fZ",
          "url": "https://www.instagram.com/reel/CkV7dh8A2fZ/",
          "view_count": 0,
          "upload_date": "2022-10-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6563,
      "title": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at ü§ó hugging face spaces by Ella2323 called Positive Reframing. #machinelearning #datascience #codetok",
      "description": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at ü§ó hugging face spaces by Ella2323 called Positive Reframing. #machinelearning #datascience #codetok",
      "upload_date": "2022-10-14",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "codetok",
        "datascience",
        "machinelearning",
        "makes",
        "positive",
        "reframing"
      ],
      "search_text": "AI that makes you feel better. The paper is Inducing Positive Perspectives with Text Reframing. You can find a demo over at ü§ó hugging face spaces by Ella2323 called Positive Reframing. #machinelearning #datascience #codetok codetok datascience machinelearning makes positive reframing",
      "platforms": {
        "instagram": {
          "video_id": "Cj6NBFgAbQf",
          "url": "https://www.instagram.com/reel/Cj6NBFgAbQf/",
          "view_count": 0,
          "upload_date": "2022-10-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6564,
      "title": "TabPFN revolution in data science. Please don‚Äôt your time on all this hype. Every week there is a revolution announced on Twitter. Ignore it. True greatness takes time. #datascience #machinelearning #statistics #tabpfn",
      "description": "TabPFN revolution in data science. Please don‚Äôt your time on all this hype. Every week there is a revolution announced on Twitter. Ignore it. True greatness takes time. #datascience #machinelearning #statistics #tabpfn",
      "upload_date": "2022-10-22",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "machinelearning",
        "revolution",
        "statistics",
        "tabpfn",
        "time"
      ],
      "search_text": "TabPFN revolution in data science. Please don‚Äôt your time on all this hype. Every week there is a revolution announced on Twitter. Ignore it. True greatness takes time. #datascience #machinelearning #statistics #tabpfn datascience machinelearning revolution statistics tabpfn time",
      "platforms": {
        "instagram": {
          "video_id": "CkYUFjzAssa",
          "url": "https://www.instagram.com/reel/CkYUFjzAssa/",
          "view_count": 0,
          "upload_date": "2022-10-22",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6567,
      "title": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics",
      "description": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics",
      "upload_date": "2022-10-29",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "anscombesquartet",
        "datascience",
        "datavisualization",
        "reminder",
        "statistics",
        "visualize"
      ],
      "search_text": "Reminder to visualize your data with one of my favorites #anscombesquartet #datavisualization #datascience #statistics anscombesquartet datascience datavisualization reminder statistics visualize",
      "platforms": {
        "instagram": {
          "video_id": "CkUW4qNAtb1",
          "url": "https://www.instagram.com/reel/CkUW4qNAtb1/",
          "view_count": 0,
          "upload_date": "2022-10-29",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6568,
      "title": "Software exec at the end is the best. Your quick intro to patents, trademarks, copyright, and licenses. I see too many comments where people get these confused.",
      "description": "Software exec at the end is the best. Your quick intro to patents, trademarks, copyright, and licenses. I see too many comments where people get these confused.",
      "upload_date": "2022-10-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "best",
        "end",
        "exec",
        "intro",
        "quick",
        "software"
      ],
      "search_text": "Software exec at the end is the best. Your quick intro to patents, trademarks, copyright, and licenses. I see too many comments where people get these confused. best end exec intro quick software",
      "platforms": {
        "instagram": {
          "video_id": "Ckbhv43Ai9I",
          "url": "https://www.instagram.com/reel/Ckbhv43Ai9I/",
          "view_count": 0,
          "upload_date": "2022-10-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6572,
      "title": "Checking out Flan T5 large language models. Let me know what wisdom you can find in this model. #machinelearning #datascience #largelanguagemodels",
      "description": "Checking out Flan T5 large language models. Let me know what wisdom you can find in this model. #machinelearning #datascience #largelanguagemodels",
      "upload_date": "2022-11-09",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "checking",
        "datascience",
        "flan",
        "large",
        "largelanguagemodels",
        "machinelearning"
      ],
      "search_text": "Checking out Flan T5 large language models. Let me know what wisdom you can find in this model. #machinelearning #datascience #largelanguagemodels checking datascience flan large largelanguagemodels machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "CkudQcIAYrR",
          "url": "https://www.instagram.com/reel/CkudQcIAYrR/",
          "view_count": 0,
          "upload_date": "2022-11-09",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6575,
      "title": "New style of content, let me know if you want more like this. Predict sentiment #machinelearning #datascience #transformers #huggingface",
      "description": "New style of content, let me know if you want more like this. Predict sentiment #machinelearning #datascience #transformers #huggingface",
      "upload_date": "2022-11-13",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "huggingface",
        "machinelearning",
        "new",
        "style",
        "transformers"
      ],
      "search_text": "New style of content, let me know if you want more like this. Predict sentiment #machinelearning #datascience #transformers #huggingface datascience huggingface machinelearning new style transformers",
      "platforms": {
        "instagram": {
          "video_id": "Ck7ED4wgfwr",
          "url": "https://www.instagram.com/reel/Ck7ED4wgfwr/",
          "view_count": 0,
          "upload_date": "2022-11-13",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6576,
      "title": "Learning curves, it‚Äôs a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique.",
      "description": "Learning curves, it‚Äôs a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique.",
      "upload_date": "2022-11-16",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "bigdata",
        "datascience",
        "learning",
        "machinelearning",
        "statistics",
        "technique"
      ],
      "search_text": "Learning curves, it‚Äôs a technique I use all the time when training models. Thanks to Todd C for showing me the best way to explain this. #datascience #machinelearning #statistics #bigdata. This video is inspired by the many machine learning experts that I had to explain that sampling is a useful and valid technique. bigdata datascience learning machinelearning statistics technique",
      "platforms": {
        "instagram": {
          "video_id": "ClBjtW-gZfy",
          "url": "https://www.instagram.com/reel/ClBjtW-gZfy/",
          "view_count": 0,
          "upload_date": "2022-11-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6577,
      "title": "Automatic Speech recognition in 3 lines of code using wav2vec2 in transformers #datascience #machinelearning #huggingface #automaticspeechrecognition #asr",
      "description": "Automatic Speech recognition in 3 lines of code using wav2vec2 in transformers #datascience #machinelearning #huggingface #automaticspeechrecognition #asr",
      "upload_date": "2022-11-17",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "asr",
        "automatic",
        "automaticspeechrecognition",
        "datascience",
        "huggingface",
        "machinelearning"
      ],
      "search_text": "Automatic Speech recognition in 3 lines of code using wav2vec2 in transformers #datascience #machinelearning #huggingface #automaticspeechrecognition #asr asr automatic automaticspeechrecognition datascience huggingface machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "ClEqAApgnWO",
          "url": "https://www.instagram.com/reel/ClEqAApgnWO/",
          "view_count": 0,
          "upload_date": "2022-11-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6578,
      "title": "Galactica by meta. Cool model, poor form on sharing it out. #datascience #machinelearning I feel for students, it was going to write a lot of papers.",
      "description": "Galactica by meta. Cool model, poor form on sharing it out. #datascience #machinelearning I feel for students, it was going to write a lot of papers.",
      "upload_date": "2022-11-17",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "cool",
        "datascience",
        "galactica",
        "machinelearning",
        "meta",
        "model"
      ],
      "search_text": "Galactica by meta. Cool model, poor form on sharing it out. #datascience #machinelearning I feel for students, it was going to write a lot of papers. cool datascience galactica machinelearning meta model",
      "platforms": {
        "instagram": {
          "video_id": "ClHPnuqgZpb",
          "url": "https://www.instagram.com/reel/ClHPnuqgZpb/",
          "view_count": 0,
          "upload_date": "2022-11-17",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6579,
      "title": "I need to focus on adding more Regularization to my life. #datascience #statistics #regularization",
      "description": "I need to focus on adding more Regularization to my life. #datascience #statistics #regularization",
      "upload_date": "2022-11-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "adding",
        "datascience",
        "focus",
        "need",
        "regularization",
        "statistics"
      ],
      "search_text": "I need to focus on adding more Regularization to my life. #datascience #statistics #regularization adding datascience focus need regularization statistics",
      "platforms": {
        "instagram": {
          "video_id": "ClKHJiZAGRG",
          "url": "https://www.instagram.com/reel/ClKHJiZAGRG/",
          "view_count": 0,
          "upload_date": "2022-11-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6581,
      "title": "Meta‚Äôs Cicero for playing Diplomacy is impressive and a bit scary. #statistics #datascience #machinelearning #diplomacy",
      "description": "Meta‚Äôs Cicero for playing Diplomacy is impressive and a bit scary. #statistics #datascience #machinelearning #diplomacy",
      "upload_date": "2022-11-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "cicero",
        "datascience",
        "diplomacy",
        "machinelearning",
        "meta",
        "statistics"
      ],
      "search_text": "Meta‚Äôs Cicero for playing Diplomacy is impressive and a bit scary. #statistics #datascience #machinelearning #diplomacy cicero datascience diplomacy machinelearning meta statistics",
      "platforms": {
        "instagram": {
          "video_id": "ClU15WLAcTg",
          "url": "https://www.instagram.com/reel/ClU15WLAcTg/",
          "view_count": 0,
          "upload_date": "2022-11-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6583,
      "title": "A couple of examples of what not to do and what you should do when presenting your data science results to the business. #datascience #statistics #machinelearning #enterpriseai",
      "description": "A couple of examples of what not to do and what you should do when presenting your data science results to the business. #datascience #statistics #machinelearning #enterpriseai",
      "upload_date": "2022-12-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "couple",
        "datascience",
        "enterpriseai",
        "examples",
        "machinelearning",
        "statistics"
      ],
      "search_text": "A couple of examples of what not to do and what you should do when presenting your data science results to the business. #datascience #statistics #machinelearning #enterpriseai couple datascience enterpriseai examples machinelearning statistics",
      "platforms": {
        "youtube": {
          "video_id": "OUX4Sy8FxZw",
          "url": "https://www.youtube.com/watch?v=OUX4Sy8FxZw",
          "view_count": 0,
          "upload_date": "2022-12-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6586,
      "title": "The politics of ChatGPT, it‚Äôs no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics",
      "description": "The politics of ChatGPT, it‚Äôs no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics",
      "upload_date": "2022-12-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "openai",
        "politics",
        "technologyethics"
      ],
      "search_text": "The politics of ChatGPT, it‚Äôs no different than any other technology and is not neutral. If you want a simple explanation of how ChatGTP works check out @rajistics Open source language models have a role here as well. #datascience #machinelearning #chatgpt #openai #technologyethics chatgpt datascience machinelearning openai politics technologyethics",
      "platforms": {
        "instagram": {
          "video_id": "Cm4hMsohIzM",
          "url": "https://www.instagram.com/p/Cm4hMsohIzM/",
          "view_count": 0,
          "upload_date": "2022-12-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6589,
      "title": "Dtreeviz 2.0 - Visualizing Decision Trees",
      "description": "Dtreeviz 2.0 - Visualizing Decision Trees",
      "upload_date": "2022-12-28",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "decision",
        "dtreeviz",
        "trees",
        "visualizing"
      ],
      "search_text": "Dtreeviz 2.0 - Visualizing Decision Trees decision dtreeviz trees visualizing",
      "platforms": {
        "instagram": {
          "video_id": "Cm0Xqh3BEvm",
          "url": "https://www.instagram.com/p/Cm0Xqh3BEvm/",
          "view_count": 0,
          "upload_date": "2022-12-28",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6585,
      "title": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law",
      "description": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law",
      "upload_date": "2022-12-30",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "barexam",
        "datascience",
        "gpt",
        "gpt3",
        "law",
        "machinelearning"
      ],
      "search_text": "GPT3.5 takes the bar exam with very little tuning. It does pretty well. #gpt #datascience #machinelearning #barexam #law barexam datascience gpt gpt3 law machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "CmzaOxQhD0o",
          "url": "https://www.instagram.com/p/CmzaOxQhD0o/",
          "view_count": 0,
          "upload_date": "2022-12-30",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6588,
      "title": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering",
      "description": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering",
      "upload_date": "2022-12-31",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "clustering",
        "datascience",
        "kmeans",
        "machinelearning",
        "means",
        "statistics"
      ],
      "search_text": "Clustering with k-means. This skit was inspired by the examples in Schubert paper on stop using the elbow criterion for kmeans. Any other clustering fails out there? #datascience #statistics #machinelearning #kmeans #clustering clustering datascience kmeans machinelearning means statistics",
      "platforms": {
        "instagram": {
          "video_id": "Cm2QIgyBpTq",
          "url": "https://www.instagram.com/p/Cm2QIgyBpTq/",
          "view_count": 0,
          "upload_date": "2022-12-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6591,
      "title": "Image captioning models - GIT from Microsoft and BLIP from salesforce #datascience #machinelearning #imagecaptioning",
      "description": "Image captioning models - GIT from Microsoft and BLIP from salesforce #datascience #machinelearning #imagecaptioning",
      "upload_date": "2023-01-05",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "captioning",
        "datascience",
        "image",
        "imagecaptioning",
        "machinelearning",
        "models"
      ],
      "search_text": "Image captioning models - GIT from Microsoft and BLIP from salesforce #datascience #machinelearning #imagecaptioning captioning datascience image imagecaptioning machinelearning models",
      "platforms": {
        "instagram": {
          "video_id": "CnEgMuIBPrv",
          "url": "https://www.instagram.com/p/CnEgMuIBPrv/",
          "view_count": 0,
          "upload_date": "2023-01-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6592,
      "title": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt",
      "description": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt",
      "upload_date": "2023-01-07",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "deepmind",
        "largelanguagemodels",
        "machinelearning",
        "nvidia",
        "openai"
      ],
      "search_text": "Scaling laws help us figure out how manage the amount of training data versus the model size. DeepMind showed with Chinchilla by using more data, you can use a smaller model. This went against the known wisdom from OpenAI‚Äôs research. This is a big deal because lots of resources are spent on building those models. Ask more questions in the comments. #datascience #machinelearning #largelanguagemodels #openai #deepmind #nvidia #microsoft #azure #huggingface #chatgpt datascience deepmind largelanguagemodels machinelearning nvidia openai",
      "platforms": {
        "instagram": {
          "video_id": "CnHXb69BzH8",
          "url": "https://www.instagram.com/p/CnHXb69BzH8/",
          "view_count": 0,
          "upload_date": "2023-01-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6590,
      "title": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization",
      "description": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization",
      "upload_date": "2023-01-08",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "datavisualization",
        "dealing",
        "machinelearning",
        "plotting",
        "statistics"
      ],
      "search_text": "Dealing with over plotting, another visualization tips from data to viz #datascience #machinelearning #statistics #datavisualization datascience datavisualization dealing machinelearning plotting statistics",
      "platforms": {
        "instagram": {
          "video_id": "CnL_pLeB6uO",
          "url": "https://www.instagram.com/p/CnL_pLeB6uO/",
          "view_count": 0,
          "upload_date": "2023-01-08",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6593,
      "title": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It‚Äôs an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain",
      "description": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It‚Äôs an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain",
      "upload_date": "2023-01-14",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "gpt3",
        "langchain",
        "largelanguagemodels",
        "machinelearning",
        "using"
      ],
      "search_text": "Using LangChain with GPT3. I am seeing lots of cool demos based on LangChain and needed to make I covered it. It‚Äôs an easy way to take advantage of #largelanguagemodels #datascience #machinelearning #gpt3 #langchain datascience gpt3 langchain largelanguagemodels machinelearning using",
      "platforms": {
        "instagram": {
          "video_id": "CncO3s3hfwC",
          "url": "https://www.instagram.com/p/CncO3s3hfwC/",
          "view_count": 0,
          "upload_date": "2023-01-14",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6595,
      "title": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #deeplearning #gpu",
      "description": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #deeplearning #gpu",
      "upload_date": "2023-01-16",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "deep",
        "deeplearning",
        "gpu",
        "machinelearning",
        "picking"
      ],
      "search_text": "Picking a GPU for deep learning based on Tim Dettmers classic blog post. #datascience #machinelearning #deeplearning #gpu datascience deep deeplearning gpu machinelearning picking",
      "platforms": {
        "instagram": {
          "video_id": "Cnf8HKJhU_G",
          "url": "https://www.instagram.com/reel/Cnf8HKJhU_G/",
          "view_count": 0,
          "upload_date": "2023-01-16",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6594,
      "title": "How companies use your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience",
      "description": "How companies use your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience",
      "upload_date": "2023-01-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "bigcode",
        "companies",
        "copilot",
        "datascience",
        "github",
        "huggingface"
      ],
      "search_text": "How companies use your data for training models will be a big issue this year. GitHub is being sued for Copilot and Hugging Face has been building out datasets that respect creators. #huggingface #bigcode #github #copilot #datascience bigcode companies copilot datascience github huggingface",
      "platforms": {
        "instagram": {
          "video_id": "CnnzifPByc8",
          "url": "https://www.instagram.com/p/CnnzifPByc8/",
          "view_count": 0,
          "upload_date": "2023-01-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6597,
      "title": "Google‚Äôs sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels",
      "description": "Google‚Äôs sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels",
      "upload_date": "2023-01-21",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "google",
        "googlesparrow",
        "machinelearning",
        "openai"
      ],
      "search_text": "Google‚Äôs sparrow is the rumored competitor to OpenAI ChatGPT. Check out the paper to see lots of examples of it chatting. It looks really good! #datascience #machinelearning #chatgpt #openai #google #googlesparrow #largelanguagemodels chatgpt datascience google googlesparrow machinelearning openai",
      "platforms": {
        "instagram": {
          "video_id": "CntCbuohh2N",
          "url": "https://www.instagram.com/p/CntCbuohh2N/",
          "view_count": 0,
          "upload_date": "2023-01-21",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6596,
      "title": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes",
      "description": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes",
      "upload_date": "2023-01-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "datascience",
        "kubernetes",
        "machinelearning",
        "take",
        "time"
      ],
      "search_text": "Should you take the time to learn Kubernetes as a data scientist? Or you already overloaded learning data science? #datascience #machinelearning #kubernetes data datascience kubernetes machinelearning take time",
      "platforms": {
        "instagram": {
          "video_id": "CnxaYwKBBDY",
          "url": "https://www.instagram.com/p/CnxaYwKBBDY/",
          "view_count": 0,
          "upload_date": "2023-01-23",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6598,
      "title": "I can‚Äôt make this stuff up. OpenAI released their classifier and I saw all these messages about how ineffective it is. Wanted to get this news out. #datascience #machinelearning #openai I am just having fun here, so let‚Äôs not get too worked up over my jokes.",
      "description": "I can‚Äôt make this stuff up. OpenAI released their classifier and I saw all these messages about how ineffective it is. Wanted to get this news out. #datascience #machinelearning #openai I am just having fun here, so let‚Äôs not get too worked up over my jokes.",
      "upload_date": "2023-01-31",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "get",
        "gpt3",
        "huggingface",
        "machinelearning",
        "make",
        "openai",
        "stuff"
      ],
      "search_text": "I can‚Äôt make this stuff up. OpenAI released their classifier and I saw all these messages about how ineffective it is. Wanted to get this news out. #datascience #machinelearning #openai I am just having fun here, so let‚Äôs not get too worked up over my jokes. chatgpt datascience get gpt3 huggingface machinelearning make openai stuff",
      "platforms": {
        "instagram": {
          "video_id": "Cn53Uplg_7w",
          "url": "https://www.instagram.com/p/Cn53Uplg_7w/",
          "view_count": 0,
          "upload_date": "2023-01-26",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "-T1zjxvrVrY",
          "url": "https://www.youtube.com/watch?v=-T1zjxvrVrY",
          "view_count": 0,
          "upload_date": "2023-01-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6599,
      "title": "My second try to explain in context learning or few shot learning with large language models. It‚Äôs very cool and why these models are so exciting. My older video is here @rajistics #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning",
      "description": "My second try to explain in context learning or few shot learning with large language models. It‚Äôs very cool and why these models are so exciting. My older video is here @rajistics #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning",
      "upload_date": "2023-01-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "fewshotlearning",
        "gpt3",
        "incontextlearning",
        "largelanguagemodels",
        "machinelearning"
      ],
      "search_text": "My second try to explain in context learning or few shot learning with large language models. It‚Äôs very cool and why these models are so exciting. My older video is here @rajistics #datascience #machinelearning #gpt3 #largelanguagemodels #fewshotlearning #incontextlearning datascience fewshotlearning gpt3 incontextlearning largelanguagemodels machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "Cn8RJ6Sg5l_",
          "url": "https://www.instagram.com/p/Cn8RJ6Sg5l_/",
          "view_count": 0,
          "upload_date": "2023-01-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6601,
      "title": "I can‚Äôt make this stuff up. OpenAI released their classifier and I saw all these messages about how ineffective it is. Wanted to get this news out. #datascience #machinelearning #openai I am just having fun here, so let‚Äôs not get too worked up over my jokes.",
      "description": "I can‚Äôt make this stuff up. OpenAI released their classifier and I saw all these messages about how ineffective it is. Wanted to get this news out. #datascience #machinelearning #openai I am just having fun here, so let‚Äôs not get too worked up over my jokes.",
      "upload_date": "2023-01-31",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "get",
        "machinelearning",
        "make",
        "openai",
        "stuff"
      ],
      "search_text": "I can‚Äôt make this stuff up. OpenAI released their classifier and I saw all these messages about how ineffective it is. Wanted to get this news out. #datascience #machinelearning #openai I am just having fun here, so let‚Äôs not get too worked up over my jokes. datascience get machinelearning make openai stuff",
      "platforms": {
        "tiktok": {
          "video_id": "7195036379233979690",
          "url": "https://www.tiktok.com/@rajistics/video/7195036379233979690",
          "view_count": 0,
          "upload_date": "2023-01-31",
          "thumbnail_url": ""
        },
        "instagram": {
          "video_id": "CoH-1W-gkjK",
          "url": "https://www.instagram.com/p/CoH-1W-gkjK/",
          "view_count": 0,
          "upload_date": "2023-01-31",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6602,
      "title": "How enterprises are dealing with ChatGPT it‚Äôs a pretty familiar cycle of grief. The good thing is it does open up lots of cool use cases. #datascience #machinelearning #chatgpt #enterprisearchitecture",
      "description": "How enterprises are dealing with ChatGPT it‚Äôs a pretty familiar cycle of grief. The good thing is it does open up lots of cool use cases. #datascience #machinelearning #chatgpt #enterprisearchitecture",
      "upload_date": "2023-02-05",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "dealing",
        "enterprisearchitecture",
        "enterprises",
        "machinelearning"
      ],
      "search_text": "How enterprises are dealing with ChatGPT it‚Äôs a pretty familiar cycle of grief. The good thing is it does open up lots of cool use cases. #datascience #machinelearning #chatgpt #enterprisearchitecture chatgpt datascience dealing enterprisearchitecture enterprises machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "CoSosR8AN7D",
          "url": "https://www.instagram.com/reel/CoSosR8AN7D/",
          "view_count": 0,
          "upload_date": "2023-02-05",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6603,
      "title": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches. #datascience #machinelearning #transformers #climatemodel",
      "description": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches. #datascience #machinelearning #transformers #climatemodel",
      "upload_date": "2023-02-07",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "based",
        "climatemodel",
        "climax",
        "datascience",
        "machinelearning",
        "transformers"
      ],
      "search_text": "Climax, a new transformer based model for predicting weather and climate forecasting. Great example of the flexibility of transformers based approaches. #datascience #machinelearning #transformers #climatemodel based climatemodel climax datascience machinelearning transformers",
      "platforms": {
        "instagram": {
          "video_id": "CoZ6kV9ghhl",
          "url": "https://www.instagram.com/reel/CoZ6kV9ghhl/",
          "view_count": 0,
          "upload_date": "2023-02-07",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6604,
      "title": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran",
      "description": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran",
      "upload_date": "2023-02-18",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "amazon",
        "bing",
        "chatgpt",
        "dataprep",
        "datascience",
        "decisiontree",
        "machinelearning",
        "openai",
        "randomforest",
        "statistics"
      ],
      "search_text": "Random forests and their ease of use are important in understanding modern data science. #datascience #machinelearning #statistics #randomforest #dataprep #decisiontree #fortran amazon bing chatgpt dataprep datascience decisiontree machinelearning openai randomforest statistics",
      "platforms": {
        "instagram": {
          "video_id": "CoxpMgaA3Pt",
          "url": "https://www.instagram.com/reel/CoxpMgaA3Pt/",
          "view_count": 0,
          "upload_date": "2023-02-16",
          "thumbnail_url": ""
        },
        "youtube": {
          "video_id": "ekrUzppE39s",
          "url": "https://www.youtube.com/watch?v=ekrUzppE39s",
          "view_count": 0,
          "upload_date": "2023-02-18",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6606,
      "title": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It‚Äôs great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant",
      "description": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It‚Äôs great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant",
      "upload_date": "2023-02-19",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "machinelearning",
        "open",
        "openai",
        "openassistant"
      ],
      "search_text": "Replying to @anansaadi OpenAssistant is an open source project that aims to provide a chat based assistant that connects to other sources of information. It‚Äôs great to see these open source projects, but just know they are very early in the development cycle. #datascience #machinelearning #openai #chatgpt #openassistant chatgpt datascience machinelearning open openai openassistant",
      "platforms": {
        "instagram": {
          "video_id": "Co3QD4dgtYW",
          "url": "https://www.instagram.com/reel/Co3QD4dgtYW/",
          "view_count": 0,
          "upload_date": "2023-02-19",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6610,
      "title": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering",
      "description": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering",
      "upload_date": "2023-02-27",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "data",
        "datascience",
        "engineering",
        "feature",
        "featureengineering",
        "machinelearning"
      ],
      "search_text": "Feature engineering and data preprocessing are an important part of the machine learning process. #datascience #machinelearning #featureengineering data datascience engineering feature featureengineering machinelearning",
      "platforms": {
        "instagram": {
          "video_id": "CpMBpqNgPeA",
          "url": "https://www.instagram.com/reel/CpMBpqNgPeA/",
          "view_count": 0,
          "upload_date": "2023-02-27",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6611,
      "title": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr #datatable",
      "description": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr #datatable",
      "upload_date": "2023-03-01",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "datascience",
        "dplyr",
        "machinelearning",
        "pandas",
        "polars",
        "rstats"
      ],
      "search_text": "Pandas 2.0 combing with arrow. A short recap on how it fits in with polars, dplyr, and data.table. #datascience #machinelearning #rstats #python #pandas #polars #dplyr #datatable datascience dplyr machinelearning pandas polars rstats",
      "platforms": {
        "instagram": {
          "video_id": "CpRdQ8XAReO",
          "url": "https://www.instagram.com/reel/CpRdQ8XAReO/",
          "view_count": 0,
          "upload_date": "2023-03-01",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6612,
      "title": "ChatGPT price drop. Let‚Äôs break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain",
      "description": "ChatGPT price drop. Let‚Äôs break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain",
      "upload_date": "2023-03-02",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "cohere",
        "datascience",
        "langchain",
        "machinelearning",
        "openai"
      ],
      "search_text": "ChatGPT price drop. Let‚Äôs break down how much the price dropped, how OpenAI could drop the price, the effects on performance, what is going on with langchain, and the open source contenders. #datascience #machinelearning #chatgpt #openai #cohere #anthropic #flant5 #langchain chatgpt cohere datascience langchain machinelearning openai",
      "platforms": {
        "instagram": {
          "video_id": "CpTwaJYA2n2",
          "url": "https://www.instagram.com/reel/CpTwaJYA2n2/",
          "view_count": 0,
          "upload_date": "2023-03-02",
          "thumbnail_url": ""
        }
      }
    },
    {
      "group_id": 6620,
      "title": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt",
      "description": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt",
      "upload_date": "2023-03-23",
      "total_views": 0,
      "max_views": 0,
      "topics": [
        "chatgpt",
        "datascience",
        "langchain",
        "largelanguagemodels",
        "machinelearning",
        "openai"
      ],
      "search_text": "OpenAI plugins! Lets get everyones APIs working with LLMs! This isa good thing. #largelanguagemodels #langchain #openai #datascience #machinelearning #chatgpt chatgpt datascience langchain largelanguagemodels machinelearning openai",
      "platforms": {
        "instagram": {
          "video_id": "CqPLA1_gs0u",
          "url": "https://www.instagram.com/p/CqPLA1_gs0u/",
          "view_count": 0,
          "upload_date": "2023-03-23",
          "thumbnail_url": ""
        }
      }
    }
  ]
}