{
  "metadata": {
    "title": "AI Problem Framing for AI Practitioners - Case Studies Database",
    "description": "Comprehensive collection of AI architectural pivots, reframing examples, and first principles thinking patterns",
    "version": "7.14",
    "last_updated": "2026-01-21",
    "total_case_studies": 216,
    "breakdown": {
      "detailed_module_6": 7,
      "detailed_architectural_pivots": 10,
      "generic_teaching_examples": 25,
      "new_llm_era_cases": 15,
      "batch_1_650_dataset": 10,
      "batch_2_650_dataset_websearch": 5,
      "batch_3_tier2_high_value": 10,
      "batch_9_tier1_800_dataset": 12,
      "batch_10_tier2_800_dataset": 9,
      "batch_11_tier2_800_dataset": 11,
      "batch_12_manual_cases": 8,
      "batch_13_fetched_cases": 13
    },
    "categories": [
      "Optimization Pivot",
      "Relational Pivot",
      "Retrieval/Knowledge Architecture",
      "Reasoning Pivot",
      "Pragmatic Pivot",
      "Business Outcome",
      "Archetype Selection",
      "GenAI & Eval"
    ],
    "sources": [
      "Lesson 6: First Principles Case Studies (course materials)",
      "AI Architectural Pivots: Case Studies (detailed research document)",
      "examples.txt (25 teaching examples)",
      "Primary academic papers and engineering blogs",
      "Claude AI knowledge base for technical enrichment",
      "15 validated LLM-era case studies (2023-2025)"
    ],
    "by_category": {
      "Architectural Pivot - Prediction Paradigm Shift": 1,
      "Architectural Pivot - Knowledge Architecture": 1,
      "Architectural Pivot - Objective Reframing": 1,
      "Architectural Pivot - Prediction to Optimization": 1,
      "Illustrative Case - Teaching Example": 1,
      "Architectural Pivot - Hardware Acceleration Paradigm": 1,
      "Architectural Pivot - Computational Complexity Reduction": 1,
      "Relational Pivot - Visual to Graph": 1,
      "Alternative Pattern - Scale and Filter": 1,
      "Alternative Pattern - Hybrid Approach": 1,
      "Alternative Pattern - Paradigm Shift": 1,
      "Pragmatic Pivot - Complexity to Robustness": 1,
      "Cautionary Tale - Anti-Pattern": 1,
      "First Principles Pivot": 1,
      "Evaluation Pivot - Offline to Online Experimentation": 1,
      "Relational Pivot - Tabular to Graph": 1,
      "Optimization Pivot - Local to Global": 1,
      "Business Outcome": 8,
      "Archetype Selection": 8,
      "GenAI & Eval": 9,
      "Architectural Pivot - Data Representation (Classification)": 2,
      "Architectural Pivot - Ranking Granularity": 1,
      "Architectural Pivot - GenAI for Data Enrichment": 4,
      "Architectural Pivot - GenAI / Semantic Layer": 1,
      "Architectural Pivot - Recommendation Logic": 1,
      "Architectural Pivot - GenAI as Data Infrastructure": 2,
      "Architectural Pivot - LLM as Compiler": 1,
      "Architectural Pivot - Agentic Workflow": 1,
      "Architectural Pivot - From Search to Agents": 1,
      "Architectural Pivot - RAG & Privacy": 1,
      "Architectural Pivot - Statistical Signal (Uncertainty)": 1,
      "Architectural Pivot - Metric Reframe (The Proxy Trap)": 1,
      "Architectural Pivot - Signal Reframe (Graph)": 1,
      "Architectural Pivot - Data Quality Reframe": 1,
      "Architectural Pivot - Correlation to Causality": 1,
      "Optimization Pivot": 17,
      "Architectural Pivot": 24,
      "Pragmatic Pivot": 3,
      "Reasoning Pivot": 2,
      "Relational Pivot": 4,
      "Alternative Pattern - Model Evolution": 4,
      "Architectural Pivot - RAG Reframe": 4,
      "Architectural Pivot - Memory Architecture": 1,
      "Architectural Pivot - Agent Architecture": 14,
      "Pragmatic Pivot - Agent Architecture": 1,
      "Architectural Pivot - Code Generation": 2,
      "Architectural Pivot - Prompt Engineering": 2,
      "Architectural Pivot - Search Reframe": 5,
      "Architectural Pivot - Rules vs LLM Hybrid": 1,
      "Architectural Pivot - Context vs Retrieval Decision": 1,
      "Cautionary Tale - Agent Liability": 1,
      "Cautionary Tale - Demo to Production Gap": 1,
      "Architectural Pivot - Multi-Model Architecture": 3,
      "Architectural Pivot - Graceful Degradation": 1,
      "Architectural Pivot - Evaluation Reframe": 3,
      "Cautionary Tale - Behavioral Control": 1,
      "Cautionary Tale - Agent Architecture": 1,
      "Cautionary Tale - Demo to Production": 1,
      "Cautionary Tale - Adversarial Users": 1,
      "Architectural Pivot - RAG Decision Framework": 1,
      "Pragmatic Pivot - Infrastructure First": 4,
      "Cautionary Tale - Stakeholder Failure": 1,
      "Cautionary Tale - Strategy vs Reality": 1,
      "Architectural Pivot - Privacy Architecture": 2,
      "Architectural Pivot - Transfer Learning Paradigm": 2,
      "Debatable Case - No Clear Winner": 1,
      "Research Insight - Training Dynamics": 1,
      "Architectural Pivot - Agent Evaluation": 1,
      "Business Outcome - Metric Reframe": 1,
      "Architectural Pivot - Atomic Unit Reframe": 1,
      "Architectural Pivot - Infrastructure First": 2,
      "Architectural Pivot - Multi-Agent Systems": 1,
      "Architectural Pivot - Efficient LLM at Scale": 1,
      "Architectural Pivot - Unified Ranking": 1,
      "Pragmatic Pivot - Unified Architecture": 1,
      "Pragmatic Pivot - Simplified Agent Architecture": 2,
      "Architectural Pivot - Temporal Understanding": 1,
      "Cautionary Tale - Agent Memory Architecture": 1,
      "Cautionary Tale - Interpretability Validation": 1,
      "Pragmatic Pivot - Infrastructure Control": 1,
      "Relational Pivot - Individual to Graph": 1,
      "Pragmatic Pivot - AI Bridge Over Legacy Infrastructure": 1,
      "Architectural Pivot - Metric Reframe": 1,
      "Architectural Pivot - Knowledge Distillation": 1,
      "Architectural Pivot - Agent Reliability": 1,
      "Architectural Pivot - Expert Augmentation": 1,
      "Architectural Pivot - LLM Application Patterns": 1,
      "Architectural Pivot - Knowledge Base Automation": 1,
      "Architectural Pivot - Cross-Vertical Personalization": 1
    },
    "by_year": {
      "Unknown": 39,
      "2025": 12,
      "2024": 64,
      "2023": 13,
      "2022": 6,
      "2021": 1,
      "2020": 2,
      "2018": 1,
      "2017": 1,
      "2012": 1,
      "1955": 1
    },
    "by_company": {
      "Generic Example": 19,
      "LinkedIn": 10,
      "Pinterest": 10,
      "Uber": 10,
      "Instacart": 5,
      "DoorDash": 8,
      "Netflix": 3,
      "Google": 3,
      "OpenAI": 5,
      "Stripe": 3,
      "Shopify": 6,
      "Grab": 6,
      "Harvey AI": 6,
      "Glean": 2,
      "DeepMind": 2,
      "Amazon": 2,
      "Booking.com": 2,
      "Spotify": 5,
      "Microsoft Research": 2,
      "Slack": 3,
      "Swiggy": 2,
      "Yelp": 2,
      "Wayfair": 4,
      "Airbnb": 2,
      "Meta": 5,
      "Merck & Co.": 2,
      "Est\u00e9e Lauder Companies": 1,
      "Sony Music Entertainment": 1,
      "Q2 Holdings": 1,
      "Protopia AI": 1,
      "fast.ai": 1,
      "LangChain": 1,
      "Anthropic": 2,
      "METR": 1,
      "Vellum": 1,
      "Sony": 1,
      "RCA": 1,
      "Salesforce": 2,
      "Nubank": 2,
      "Target": 1,
      "HubSpot": 1,
      "Vinted": 1,
      "Zalando": 1,
      "Etsy": 2,
      "Flipkart": 1,
      "Whatnot": 1,
      "Duolingo": 1,
      "Digits": 1,
      "Bayezian Limited": 1,
      "FuzzyLabs": 1,
      "Airtable": 1,
      "Coupang": 1
    },
    "source_breakdown": {
      "batch_4_tier2_midrange": {
        "count": 7,
        "description": "Tier 2 mid-range cases (score 4.0) from 650+ dataset",
        "validation_score": "93.8%"
      },
      "batch_5_tier2_plus_finds": {
        "count": 4,
        "description": "Tier 2 remaining high-value (4.5) + strong architectural cases",
        "validation_score": "93.8%"
      },
      "batch_6_tier2_plus_finds": {
        "count": 4,
        "description": "Tier 2 mid-range cases (3.5-4.0) with strong production metrics",
        "validation_score": "93.8%"
      },
      "batch_7_tier2_plus_finds": {
        "count": 6,
        "description": "Tier 2 cases (3.0-4.0) from new companies with strong technical depth",
        "validation_score": "93.8%"
      },
      "batch_8_tier2_plus_finds": {
        "count": 5,
        "description": "Final Tier 2 cases (score 3.0) completing Tier 2 processing",
        "validation_score": "93.8%"
      },
      "genai_first_principles": {
        "count": 8,
        "description": "Gen AI first principles reframes: RAG, memory, code generation, agents, search, business model",
        "date": "2025-12-23"
      },
      "genai_gaps_system2": {
        "count": 7,
        "description": "Gen AI System 2 gaps: rules vs LLM, context vs RAG, agent failures, production gaps, model routing, safety fallbacks, eval reframes",
        "date": "2025-12-23"
      },
      "genai_gaps_round2": {
        "count": 6,
        "description": "Cautionary tales (Sydney, Galactica, DAN, $47K), RAG decision framework (Pinecone), Eval gap (Coda)",
        "date": "2025-12-23"
      },
      "sol_rashidi_cases": {
        "count": 5,
        "description": "Enterprise AI case studies from Sol Rashidi: Merck (infrastructure-first + strategy audit), Est\u00e9e Lauder (stakeholder failure), Sony Music (art of practical), Q2/Protopia (privacy-preserving inference)",
        "date": "2025-12-24"
      },
      "batch_9_tier1_800_dataset": {
        "count": 12,
        "description": "Tier 1 high-priority cases from 800+ ML/LLM use cases dataset (Spotify, Shopify, LinkedIn, Harvey, Uber, Grab, Salesforce)",
        "date": "2026-01-04",
        "validation_score": "95%"
      },
      "batch_10_tier2_800_dataset": {
        "count": 9,
        "description": "Tier 2 cases from 800+ ML/LLM use cases dataset (Uber, Harvey, Grab, Meta, Salesforce, Nubank, Slack)",
        "date": "2026-01-04",
        "validation_score": "90%"
      },
      "batch_11_tier2_800_dataset": {
        "count": 11,
        "description": "Additional Tier 2 cases (Meta, Shopify, Wayfair, Target, Uber, HubSpot, Vinted, Zalando)",
        "date": "2026-01-04"
      }
    }
  },
  "case_studies": [
    {
      "id": "yolo-object-detection",
      "title": "YOLO: Deconstructing Object Detection",
      "category": "Architectural Pivot - Prediction Paradigm Shift",
      "companies_involved": [
        "Research Community",
        "Joseph Redmon et al."
      ],
      "initial_problem": "Detect and localize multiple objects in images with their class labels and bounding box coordinates in real-time applications like autonomous driving and surveillance",
      "initial_assumptions": [
        "Object detection is fundamentally a classification problem applied at multiple scales",
        "The atomic unit is a 'window' or 'region' that needs to be classified",
        "Must use a sliding window approach or region proposal methods (R-CNN family)",
        "Detection requires a two-stage pipeline: first propose regions, then classify them",
        "High accuracy requires examining many overlapping regions independently"
      ],
      "why_it_fails": [
        "Two-stage pipeline (propose regions, then classify) is inherently slow - can't optimize end-to-end",
        "Region proposal methods examine thousands of overlapping windows, creating massive redundant computation",
        "Real-time applications like autonomous driving are impossible with multi-second inference times"
      ],
      "first_principle_insight": "Object detection doesn't require examining thousands of candidate regions. By treating the image as a grid where each cell makes predictions directly, you can do detection in a single pass - turning a slow two-stage process into real-time inference.",
      "reframe": {
        "new_atomic_unit": "Grid cell with direct prediction of (x, y, w, h, class_probabilities)",
        "new_problem_type": "Single-stage spatial regression with classification",
        "new_objective": "Jointly optimize localization (bounding box regression) and classification in a single forward pass",
        "architectural_changes": [
          {
            "change": "Single Unified Network",
            "description": "YOLO divides the image into an S\u00d7S grid. Each grid cell directly predicts B bounding boxes (with confidence scores) and C class probabilities. This is a single CNN that outputs a tensor of shape (S, S, B\u00d75 + C)."
          },
          {
            "change": "Global Context",
            "description": "Because the entire image is processed in one pass, the network has access to global context when making predictions. A cell can 'see' the entire image through the receptive field of the deeper layers, reducing false positives on background patches."
          },
          {
            "change": "Direct Coordinate Regression",
            "description": "Instead of classifying pre-defined boxes, YOLO regresses directly to bounding box coordinates (x, y, width, height) relative to grid cells. This treats localization as a regression problem with L2 loss."
          },
          {
            "change": "End-to-End Training",
            "description": "The entire pipeline is a single neural network trained end-to-end with a multi-part loss function (localization loss + confidence loss + classification loss). No separate region proposal stage means gradients flow through the entire system."
          }
        ],
        "results": "YOLOv1 achieved 45 FPS (real-time) on GPU with 63.4% mAP on PASCAL VOC. Fast YOLO reached 155 FPS. This was 1000x faster than R-CNN while maintaining competitive accuracy. The reframe from 'classification of regions' to 'spatial regression on grid' fundamentally solved the speed problem."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. CVPR 2016.",
          "url": "https://arxiv.org/abs/1506.02640",
          "note": "Original YOLO paper"
        },
        {
          "type": "primary",
          "citation": "Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. CVPR 2014.",
          "url": "https://arxiv.org/abs/1311.2524",
          "note": "R-CNN baseline for comparison"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical details about YOLO architecture and computational analysis"
        }
      ],
      "key_lesson": "When your solution requires processing the same data multiple times in slightly different ways (sliding windows, overlapping regions), look for a formulation that processes it once with a richer output representation. The shift from 'thousands of binary questions' to 'one structured question' is often the key to both speed and accuracy.",
      "how_to_reframe": {
        "old_atomic_unit": "Region/window to classify - treating object detection as classifying thousands of independently proposed regions through a two-stage pipeline (region proposal \u2192 classification)",
        "new_atomic_unit": "Grid cell with direct predictions - treating the entire image as a unified grid where each cell directly predicts bounding boxes, objectness scores, and class probabilities in a single forward pass"
      },
      "architectural_changes": [
        "Replaced two-stage pipeline (region proposal \u2192 classification) with single unified network",
        "Process entire image in one forward pass instead of examining thousands of regions separately",
        "Divide image into grid where each cell directly predicts bounding boxes and class probabilities",
        "Enabled real-time inference (45+ FPS vs R-CNN's 47 seconds per image)"
      ],
      "comments": [
        {
          "text": "Part of the classic development of image algorithms is the movement from R-CNN to YOLO -- Great example to use",
          "timestamp": "2025-12-21T21:33:41.644116",
          "author": "User"
        }
      ],
      "tags": [
        "computer-vision",
        "deep-learning",
        "real-time",
        "architecture",
        "optimization"
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "inspiration"
        ],
        "transferable_patterns": [
          "Reframing the atomic unit can unlock 1000x speedups",
          "End-to-end learning often beats staged pipelines",
          "Real-time constraints force creative solutions"
        ],
        "non_generalizable_context": [
          "Computer vision specific architecture choices",
          "2015-era GPU constraints",
          "Object detection specific evaluation metrics"
        ]
      }
    },
    {
      "id": "rag-memory-decomposition",
      "title": "RAG: Retrieve-and-Dump vs. Structured Reasoning",
      "category": "Architectural Pivot - Knowledge Architecture",
      "companies_involved": [
        "Harvey AI",
        "Glean",
        "Meta AI Research",
        "Enterprise LLM Community"
      ],
      "initial_problem": "Get LLMs to answer questions using external knowledge",
      "initial_assumptions": [
        "Retrieve relevant documents and dump them into the prompt",
        "The LLM will figure out how to use the context",
        "More retrieved content = better answers"
      ],
      "why_it_fails": [
        "Dumping documents doesn't guide reasoning",
        "LLM may ignore relevant parts or hallucinate",
        "No verification that retrieved content was actually used correctly"
      ],
      "first_principle_insight": "Don't just dump documents into the prompt. Structure the retrieved content to guide the LLM's reasoning step by step, and verify the answer traces back to sources.",
      "reframe": {
        "new_atomic_unit": "Query \u2192 Retrieved Context + LLM Reasoning \u2192 Grounded Answer + Citations",
        "new_problem_type": "Information retrieval + conditional generation (RAG pipeline)",
        "new_objective": "Retrieve relevant documents, then generate answers conditioned only on retrieved context, with verifiable attribution",
        "architectural_changes": [
          {
            "change": "Knowledge Externalization",
            "description": "Documents are stored in a vector database (Pinecone, Weaviate, Chroma) or hybrid search index. Each document/chunk is embedded into a dense vector space using models like sentence-transformers or OpenAI embeddings. The LLM's weights remain frozen\u2014it never 'learns' the facts."
          },
          {
            "change": "Retrieval-First Pipeline",
            "description": "When a query arrives ('What is the vacation policy?'), a retriever performs semantic search to fetch the top-k most relevant document chunks (typically k=3-10). The retriever uses cosine similarity in embedding space or hybrid search (semantic + keyword BM25)."
          },
          {
            "change": "Context Injection",
            "description": "Retrieved documents are injected into the LLM prompt as context: 'Based on the following documents: [chunks], answer: [question]'. The LLM acts purely as a reader and synthesizer, not a memorizer. It generates answers by reasoning over provided evidence."
          },
          {
            "change": "Grounding and Attribution",
            "description": "The system can return source citations alongside answers because it knows which documents were used. Advanced systems (like Harvey AI) decompose answers into factual claims and cross-reference each claim against authoritative sources, reducing hallucination to ~0.2%."
          },
          {
            "change": "Instant Knowledge Updates",
            "description": "When a document changes or new information arrives, you update the vector database\u2014a matter of seconds. No retraining required. The next query automatically has access to the new information."
          },
          {
            "change": "Mature Hybrid Architecture",
            "description": "Modern systems use fine-tuning for behavior/style (how to format legal memos, citation style, tone) and RAG for facts. This separates 'how to behave' (weights) from 'what to know' (retrieval)."
          }
        ],
        "results": "Harvey AI reduced hallucination rates to ~0.2% in legal applications. Glean enabled enterprise search that stays up-to-date in real-time. The architecture is now the industry standard for knowledge-intensive applications. Studies show General LLM + RAG consistently outperforms Domain-Specific Fine-Tuned models on factual QA tasks while being cheaper and more maintainable."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 2020.",
          "url": "https://arxiv.org/abs/2005.11401",
          "note": "Original RAG paper from Meta AI"
        },
        {
          "type": "primary",
          "citation": "Harvey AI Engineering Blog: Enterprise-Grade RAG Systems",
          "url": "https://www.harvey.ai/blog/enterprise-grade-rag-systems",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "primary",
          "citation": "Glean Blog: Retrieval Augmented Generation (RAG) \u2013 The key to enabling generative AI for the enterprise",
          "url": "https://www.glean.com/blog/retrieval-augmented-generation-rag-the-key-to-enabling-generative-ai-for-the-enterprise",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "supplementary",
          "citation": "Ovadia, G., et al. (2023). Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. arXiv:2312.05934",
          "url": "https://arxiv.org/abs/2312.05934",
          "note": "Empirical comparison showing RAG advantages"
        }
      ],
      "key_lesson": "Don't conflate capabilities. When a system needs both X and Y (reasoning and memory, creativity and accuracy), and your solution bakes them both into one component, you create irreconcilable tradeoffs. Decompose the system: use specialized components for each capability. The model reasons, the database remembers.",
      "how_to_reframe": {
        "old_atomic_unit": "Retrieved documents as raw context - dump docs into prompt",
        "new_atomic_unit": "Retrieved documents as reasoning scaffold - structure retrieval to guide step-by-step reasoning"
      },
      "architectural_changes": [
        "Structure retrieved content as reasoning steps",
        "Add verification that answers trace to sources",
        "Iterate retrieval based on reasoning needs"
      ],
      "tags": [
        "RAG",
        "LLM",
        "retrieval",
        "reasoning",
        "architecture"
      ]
    },
    {
      "id": "netflix-page-generation",
      "title": "Netflix: Deconstructing Rating Prediction (Stars vs. Thumbs)",
      "category": "Architectural Pivot - Objective Reframing",
      "companies_involved": [
        "Netflix"
      ],
      "initial_problem": "Recommend movies and TV shows that users will enjoy watching from a large catalog",
      "initial_assumptions": [
        "Users know how much they'll like something on a 1-5 star scale",
        "Predicting exact star ratings accurately leads to better recommendations",
        "Minimizing rating prediction error (RMSE) is the right optimization target",
        "The Netflix Prize approach (beat Cinematch by 10% RMSE) solves the recommendation problem"
      ],
      "why_it_fails": [
        "Users don't rate movies they don't watch - so you're optimizing on a biased sample",
        "The difference between predicting 4.1 vs 4.3 stars doesn't change whether someone watches",
        "Rating accuracy doesn't correlate well with engagement or satisfaction",
        "Users find 5-star ratings cognitively demanding - they overthink instead of giving gut reactions"
      ],
      "first_principle_insight": "The goal isn't to predict how users will rate content - it's to predict whether they'll enjoy watching it. A binary thumbs up/down captures the real decision ('will I like this?') better than a 5-point scale, and aligns the model with actual engagement rather than stated preferences.",
      "reframe": {
        "new_atomic_unit": "The Page/Slate (entire two-dimensional composition of rows and items)",
        "new_problem_type": "Learning to Rank (LTR) + Whole-Page Optimization with Diversity Constraints",
        "new_objective": "Maximize probability of play event, watch time, and session satisfaction while enforcing diversity, coherence, and navigability constraints",
        "architectural_changes": [
          {
            "change": "Two-Stage Page Generation",
            "description": "Instead of independently scoring all items, Netflix uses a staged selection process. First, select which row category to show (e.g., 'Trending Now'). Then, re-score remaining candidates conditioned on what's already been selected. This allows the system to enforce diversity\u2014if a 'Comedy' row is selected, the probability of selecting another 'Comedy' row immediately below is penalized."
          },
          {
            "change": "Listwise Ranking, Not Pointwise Prediction",
            "description": "The system optimizes the entire list of recommendations jointly, not each item independently. It uses ranking loss functions (listwise loss) that consider relative ordering. The goal is 'Is this set of items better than that set?' not 'How many stars will they rate this?'"
          },
          {
            "change": "Evidence Optimization (Item + Context)",
            "description": "The recommendation is not just the item ID\u2014it's the combination of item + thumbnail + synopsis. The system selects which artwork to show alongside the recommendation. This doubles the optimization surface: not just 'what to show' but 'how to show it.'"
          },
          {
            "change": "Multi-Objective Balancing",
            "description": "The page generation system explicitly balances multiple objectives using a weighted utility function: relevance (will they like it?), diversity (is the page varied?), discovery (are we showing new content?), continuation (are we helping them finish series?), and freshness (are we showing recently added content?)."
          },
          {
            "change": "Row-Level Personalization",
            "description": "Each row has a theme/intent (e.g., 'Trending,' 'Because you watched X,' 'Award Winners'). The system personalizes which rows to show and in what order, creating a navigable structure that lets users self-select based on their current mood."
          }
        ],
        "results": "This pivot was fundamental to Netflix's success as a streaming platform. The shift from 'predicting stars' to 'generating pages' increased engagement, watch time, and subscriber retention. Netflix can now balance showing users safe picks they'll love with introducing them to new genres, all while maintaining a coherent, navigable interface."
      },
      "sources": [
        {
          "citation": "Netflix (2017). Goodbye, Stars: How Netflix Replaced 5-Star Ratings with Thumbs.",
          "url": "https://about.netflix.com/en/news/goodbye-five-stars-hello-thumbs"
        },
        {
          "citation": "Netflix Tech Blog (2017). The Netflix Prize and Why RMSE Isn't Everything.",
          "url": "https://netflixtechblog.com/"
        },
        {
          "citation": "Todd Yellin, VP of Product Innovation, Netflix (2017). Explaining the shift from star ratings to thumbs up/down.",
          "url": "N/A"
        }
      ],
      "key_lesson": "Beware of optimizing proxy metrics. When you optimize for what's easy to measure (RMSE, accuracy) rather than what actually matters (engagement, satisfaction), you'll hit a ceiling. The key diagnostic: if your model is 'accurate' but users aren't happy, your objective function is wrong. Reframe around the true business outcome.",
      "how_to_reframe": {
        "old_atomic_unit": "Predicted star rating (1-5 scale) - treating recommendation as a regression problem to minimize rating prediction error",
        "new_atomic_unit": "Predicted thumbs up/down (binary) - treating recommendation as predicting whether someone will enjoy content enough to engage with it"
      },
      "architectural_changes": [
        "Replaced 5-star rating UI with simple thumbs up/thumbs down",
        "Changed from regression (predict exact rating) to classification (predict positive/negative)",
        "Shifted optimization target from RMSE to engagement metrics (plays, completion rate)",
        "Reduced cognitive load on users - gut reaction instead of deliberation"
      ],
      "comments": [
        {
          "text": "I think we need to red this one, I am sure Netflix has had many pivots but going from 5 stars to thumbs up was a first principles - but look at this: \"Netflix has had star ratings for much of our history, but we\u2019ve learned through over a year of testing that while we\u2019ve used stars to help you personalize your suggestions, many of our members are confused about what they do.\n\nThat\u2019s because we\u2019ve all gotten used to star ratings on e-commerce and review apps, where rating contributes to an overall average, and the star rating shown next to a restaurant or a pair of shoes is an average of all the reviewers. On those apps, being a reviewer can be fun and helpful to others, but the primary goal isn\u2019t always to help you get better suggestions.\n\nIn contrast, when people see thumbs, they know that they are used to teach the system about their tastes with the goal of finding more great content. That\u2019s why when we tested replacing stars with thumbs we saw an astounding 200% increase in ratings activity.\" https://about.netflix.com/en/news/goodbye-stars-hello-thumbs\n\nSo its a good examples of chaning a rating scale and rethinking something obvious  --- \nI think we might need to redo this example along that -\n\nWe could split up the page organization as a second example, where the first principles starts form a genre approach for a home page and then moves to this optimization problem",
          "timestamp": "2025-12-21T21:45:36.173711",
          "author": "User"
        }
      ],
      "year": 2017,
      "tags": [
        "recommendations",
        "personalization",
        "metrics",
        "evaluation",
        "experimentation"
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching"
        ],
        "transferable_patterns": [
          "Optimizing the wrong metric can hurt user experience",
          "Simpler interfaces can outperform 'smarter' ones",
          "Business metric alignment requires constant vigilance"
        ],
        "non_generalizable_context": [
          "Streaming-specific engagement patterns",
          "Netflix's specific A/B testing infrastructure",
          "Entertainment consumption vs other domains"
        ]
      }
    },
    {
      "id": "uber-demand-optimization",
      "title": "Uber: Deconstructing Dispatch (Greedy Matching vs. Future Value)",
      "category": "Architectural Pivot - Prediction to Optimization",
      "companies_involved": [
        "Uber"
      ],
      "initial_problem": "Match riders with drivers to minimize wait times and maximize driver earnings",
      "initial_assumptions": [
        "Send the nearest available driver to each ride request",
        "Each dispatch decision is independent - optimize for the current trip",
        "Minimizing pickup time for this trip is the goal",
        "Drivers are interchangeable - just find the closest one"
      ],
      "why_it_fails": [
        "Greedy matching ignores where the driver ends up after the trip",
        "Sending a driver to a low-demand area leaves them stranded with no future rides",
        "Short-term efficiency (fast pickup) can hurt long-term earnings and availability",
        "Doesn't account for upcoming demand - might need that driver elsewhere soon"
      ],
      "first_principle_insight": "Dispatch isn't about optimizing one trip at a time. Each assignment changes where drivers are positioned for future trips. The real question isn't 'who's closest?' but 'which assignment maximizes total value including future rides?'",
      "reframe": {
        "new_atomic_unit": "Driver state trajectory (sequence of locations over time) with associated value function",
        "new_problem_type": "Reinforcement Learning for marketplace optimization (MDP)",
        "new_objective": "Maximize long-term marketplace utility: total trips completed, driver earnings, rider wait times, and supply-demand balance",
        "architectural_changes": [
          {
            "change": "Value Function Learning",
            "description": "Uber trains a Deep Q-Network (DQN) to estimate V(s), the long-term earnings potential of being in spatiotemporal state s (location + time of day). States are encoded using H3 (hexagonal hierarchical spatial index) for geospatial representation. The value function is learned from historical marketplace data across millions of trips."
          },
          {
            "change": "Lambda Dispatch Formula",
            "description": "The dispatch decision is no longer just 'minimize ETA.' It becomes: maximize [Immediate Fare + \u03b3 \u00d7 V(destination_state)] where \u03b3 (gamma) is a discount factor weighting future value. If a trip ends in a dead zone, V(destination) is low, making the dispatch less attractive even if the immediate fare is decent."
          },
          {
            "change": "Proactive Supply Positioning",
            "description": "The system 'nudges' drivers toward high-future-value areas by adjusting dispatch priorities. It might intentionally not assign a driver to a low-value trip, letting them remain available for a better opportunity arriving soon. Or it might favor trips that end near anticipated demand (e.g., near a concert venue 30 minutes before the event ends)."
          },
          {
            "change": "Dynamic Incentive Shaping",
            "description": "Uber can offer small bonuses for trips that end in strategically valuable locations, effectively using the value function to shape driver behavior. Instead of forcing assignments, the system uses economic incentives aligned with learned marketplace dynamics."
          },
          {
            "change": "Surge Pricing as Last Resort",
            "description": "By optimizing dispatch to maintain supply-demand balance, the system reduces reliance on surge pricing. Surge becomes a signal of failure, not the primary balancing mechanism. The RL policy aims to avoid surge by preemptively positioning supply."
          }
        ],
        "results": "This architectural shift reduced supply-demand imbalance, decreased surge pricing volatility, increased driver utilization and earnings, and improved total trip throughput. The marketplace became more stable and efficient. By treating dispatch as sequential decision-making rather than greedy matching, Uber transformed from a reactive system to an anticipatory one."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog: Reinforcement Learning for Modeling Marketplace Balance",
          "url": "https://www.uber.com/blog/reinforcement-learning-for-modeling-marketplace-balance/",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "primary",
          "citation": "Uber Research: Dynamic Pricing and Matching in Ride-Hailing Platforms",
          "url": "https://www.uber.com/blog/research/dynamic-pricing-and-matching-in-ride-hailing-platforms/",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Reinforcement learning and MDP formulation details"
        }
      ],
      "key_lesson": "When your problem involves sequential decisions where actions have delayed consequences, greedy/myopic optimization will fail. Reframe as a Markov Decision Process. The diagnostic signal: if optimizing each step independently leads to system-level degradation, you need to optimize for trajectories, not steps.",
      "how_to_reframe": {
        "old_atomic_unit": "Single trip assignment - send nearest driver to each request independently",
        "new_atomic_unit": "Driver trajectory over time - consider where the driver ends up and their future earning potential"
      },
      "architectural_changes": [
        "Added destination value scoring - how good is the dropoff location for future trips?",
        "Consider future demand when making current assignments",
        "Balance immediate pickup time against driver's future earning potential",
        "Model how each dispatch affects overall marketplace supply distribution"
      ],
      "comments": [
        {
          "text": "This seems like a good example, we need to better simmply explain what is going on - but the basics look good",
          "timestamp": "2025-12-21T21:48:24.193086",
          "author": "User"
        }
      ],
      "tags": [
        "optimization",
        "marketplace",
        "real-time",
        "operations"
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "inspiration"
        ],
        "transferable_patterns": [
          "Prediction enables optimization, not replaces it",
          "Marketplace dynamics require systems thinking",
          "Incentive design is as important as model design"
        ],
        "non_generalizable_context": [
          "Ride-sharing specific supply/demand dynamics",
          "Real-time pricing specific constraints",
          "Driver behavior specific to gig economy"
        ]
      }
    },
    {
      "id": "tomato-sensor-vision",
      "title": "Tomato Sorting: ML vs. Simple Sensors (Illustrative Case)",
      "category": "Illustrative Case - Teaching Example",
      "companies_involved": [
        "Agriculture/Food Processing Industry (Generic Example)"
      ],
      "initial_problem": "Sort tomatoes by ripeness on a processing line",
      "initial_assumptions": [
        "This is a computer vision problem - we need AI to 'see' ripeness",
        "Train a CNN or image classifier on pictures of tomatoes",
        "Deploy GPUs and ML infrastructure for inference",
        "The problem is teaching a model to recognize what ripe looks like"
      ],
      "why_it_fails": [
        "Overcomplicates a problem that has a direct physical solution",
        "Ripeness isn't a subjective pattern - it's a measurable color change",
        "ML adds complexity, latency, and cost where a simple sensor works",
        "You don't need to 'learn' what ripe looks like if you can measure it directly"
      ],
      "first_principle_insight": "Ripeness isn't a pattern to learn - it's a color change you can measure. When tomatoes ripen, they go from green to red. A simple color sensor can detect this directly without any ML. Ask: 'Is this actually a pattern recognition problem, or is there a direct measurement?'",
      "reframe": {
        "new_atomic_unit": "Spectral reflectance measurement at specific wavelengths",
        "new_problem_type": "Optical sensor measurement with threshold-based classification",
        "new_objective": "Measure light reflectance in the red/near-infrared spectrum to quantify chlorophyll/lycopene ratio, then apply simple threshold rules",
        "architectural_changes": [
          {
            "change": "Optical Sensor Deployment",
            "description": "Replace cameras and GPUs with simple optical sensors (photodiodes or spectrometers) that measure light reflectance at specific wavelengths (e.g., 660nm for chlorophyll absorption, 700nm for reflectance). These sensors cost $5-$50 each, consume milliwatts of power, and provide measurements in microseconds."
          },
          {
            "change": "Physics-Based Thresholding",
            "description": "Ripeness classification becomes trivial: if (red_reflectance > threshold_1 AND nir_reflectance < threshold_2) \u2192 ripe. These thresholds can be calibrated once using a handful of reference tomatoes. No training data, no GPUs, no models. Just physics and simple Boolean logic."
          },
          {
            "change": "Hardware Simplification",
            "description": "The entire system becomes an embedded circuit: LED light source \u2192 tomato passes by \u2192 photodiode measures reflected light \u2192 microcontroller applies threshold logic \u2192 actuator sorts tomato. This can run on a $2 microcontroller (Arduino/ESP32) with no internet connection, no cloud, no ML infrastructure."
          },
          {
            "change": "Robustness Through Physics",
            "description": "Because the system measures physical properties, not learned patterns, it's inherently robust. Changing lighting conditions? The sensor measures absolute reflectance ratios, which are invariant. Dust? Use a cleaning air jet. The failure modes are mechanical/electrical, not statistical, making debugging and maintenance straightforward."
          },
          {
            "change": "Cost and Complexity Reduction",
            "description": "Total system cost drops from $100K-$5M to $500-$5K. Installation is simple. There's no ongoing compute cost, no model retraining, no ML engineers needed. A technician can calibrate thresholds in 10 minutes. The solution is transparent and explainable\u2014everyone understands 'measure red light reflection.'"
          }
        ],
        "results": "The optical sensor solution costs 100-1000x less, responds in microseconds (1000x faster), requires zero training data, never needs retraining, and is more robust to environmental variation. It's also explainable and maintainable by non-experts. This is a canonical example of 'de-escalation'\u2014solving a problem with the simplest mechanism possible."
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Course materials: First Principles AI Course.md, Lesson 1 & Lesson 6",
          "note": "Tomato sensor example appears as a teaching case study"
        },
        {
          "type": "supplementary",
          "citation": "Abbott, J. A. (1999). Quality measurement of fruits and vegetables. Postharvest Biology and Technology, 15(3), 207-225.",
          "note": "Academic reference on optical sensing for produce quality"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical details on spectral reflectance and optical sensing principles"
        }
      ],
      "key_lesson": "Before reaching for AI/ML, ask 'Does this problem require intelligence, or does it require measurement?' Many problems framed as 'perception' (classification, detection) are actually 'physics' (sensor measurement). The diagnostic: if the ground truth has a direct physical correlate (chemical composition, temperature, mass), measure it directly instead of training a model to infer it from proxies. This is 'de-escalation'\u2014the art of choosing the simplest mechanism that solves the problem.",
      "how_to_reframe": {
        "old_atomic_unit": "Image to classify - treat ripeness as a visual pattern recognition problem",
        "new_atomic_unit": "Color measurement - treat ripeness as a physical property you can measure directly"
      },
      "architectural_changes": [
        "Replace CNN/image classifier with simple color sensor",
        "Measure color wavelength directly instead of classifying images",
        "Remove GPU/ML infrastructure - use basic threshold on sensor reading",
        "Faster, cheaper, more reliable than ML approach"
      ],
      "comments": [
        {
          "text": "ok, but to be fair, no one started with CNN for fruit - this was a hypothical video I made to illustrate the differences between the two - its an example of reframing a problem, but not necesaarly first pinciples in action",
          "timestamp": "2025-12-21T21:49:36.133611",
          "author": "User"
        }
      ],
      "tags": [
        "computer-vision",
        "cost-optimization",
        "pragmatic-pivot",
        "manufacturing"
      ]
    },
    {
      "id": "alexnet-deep-learning-pivot",
      "title": "AlexNet: Hand-Crafted Features vs. Learned Representations",
      "category": "Architectural Pivot - Hardware Acceleration Paradigm",
      "companies_involved": [
        "University of Toronto (Krizhevsky, Sutskever, Hinton)",
        "NVIDIA"
      ],
      "initial_problem": "Classify images into categories (e.g., recognize objects in photos)",
      "initial_assumptions": [
        "Experts must design features that capture what matters in images (edges, textures, shapes)",
        "Use hand-crafted features like SIFT, HOG, or SURF as inputs to classifiers",
        "Train an SVM or similar classifier on top of these engineered features",
        "Better features require more domain expertise and manual tuning",
        "Deep neural networks don't work well - they're hard to train and overfit"
      ],
      "why_it_fails": [
        "Hand-crafted features are limited by human imagination - we can only design what we can conceive",
        "Feature engineering is slow, expensive, and doesn't transfer well across tasks",
        "SVMs hit a ceiling - more data doesn't help much once features are fixed",
        "Each new problem requires starting over with new feature design"
      ],
      "first_principle_insight": "Don't tell the model what to look for - let it learn. Instead of humans designing features (edges, textures), train a deep network end-to-end on raw pixels. With enough data and compute, learned features outperform anything humans can hand-design.",
      "reframe": {
        "new_atomic_unit": "Parallel execution of matrix operations across thousands of GPU cores",
        "new_problem_type": "Massively parallel computation on specialized hardware (GPGPU)",
        "new_objective": "Accelerate training by exploiting the inherent parallelism in convolution and matrix multiplication operations",
        "architectural_changes": [
          {
            "change": "GPU Acceleration (CUDA)",
            "description": "AlexNet was trained on NVIDIA GTX 580 GPUs with 512 CUDA cores each. GPUs are designed for graphics rendering, which requires applying the same operation (shader) to millions of pixels simultaneously. This SIMD (Single Instruction, Multiple Data) architecture is a perfect match for CNNs. Convolution becomes: load filter into GPU memory, apply it to all pixels in parallel."
          },
          {
            "change": "Custom CUDA Kernels",
            "description": "Krizhevsky hand-wrote optimized CUDA kernels for 2D convolution operations, exploiting GPU memory hierarchy (shared memory, registers) and thread block organization. This achieved 10-40x speedup over CPU implementations for convolutional layers, which dominate CNN computation."
          },
          {
            "change": "Data Parallelism with Model Splitting",
            "description": "AlexNet was too large to fit on a single GPU (memory constraints). The team split the model across two GPUs, with communication only at specific layers. This was an early example of model parallelism, enabling larger networks than hardware memory would normally allow."
          },
          {
            "change": "Deeper Networks Become Practical",
            "description": "With GPU acceleration, AlexNet used 8 learned layers (5 convolutional, 3 fully connected) with 60M parameters\u2014orders of magnitude deeper and larger than previous networks. Training on ImageNet dropped from months to 5-6 days on two GPUs. This made rapid iteration possible."
          },
          {
            "change": "Architectural Innovations Enabled by Speed",
            "description": "Because training was now feasible, the team could experiment with innovations: ReLU activations (faster than sigmoid/tanh), Dropout for regularization, data augmentation, overlapping pooling. These innovations, combined with depth, led to breakthrough performance."
          }
        ],
        "results": "AlexNet won the ImageNet 2012 competition with a top-5 error rate of 15.3%, compared to 26.2% for the second-place entry (traditional computer vision methods). This 10+ percentage point gap was unprecedented and shocked the community. The result triggered the 'deep learning revolution'\u2014within 2 years, every ImageNet winner used deep CNNs. The key insight: the breakthrough was not just the algorithm, but the hardware-algorithm co-design. AlexNet proved that GPUs democratized deep learning research."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. NeurIPS 2012.",
          "url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks",
          "note": "Original AlexNet paper"
        },
        {
          "type": "primary",
          "citation": "IEEE Spectrum: AlexNet Source Code",
          "url": "https://spectrum.ieee.org/alexnet-source-code",
          "note": "From course materials: First Principles AI Course.md"
        },
        {
          "type": "supplementary",
          "citation": "NVIDIA Developer Blog: GPU-Accelerated Deep Learning",
          "note": "Technical details on CUDA and GPU computing for neural networks"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Historical context and technical details about AlexNet architecture"
        }
      ],
      "key_lesson": "When your algorithm has a clear computational structure (e.g., massive parallelism), match it to hardware with the same structure. Don't force parallel algorithms onto serial hardware. This is a meta-lesson about 'problem-solution topology matching'\u2014the architecture of your solution (hardware, software) should mirror the structure of the problem. AlexNet's success was 30% algorithm innovation and 70% realizing GPUs were the right computational substrate for deep learning.",
      "how_to_reframe": {
        "old_atomic_unit": "Hand-crafted features - experts design what the model should look for, then train a classifier on those features",
        "new_atomic_unit": "Learned features - let the neural network discover what features matter directly from raw pixels and lots of data"
      },
      "architectural_changes": [
        "Replaced hand-crafted features (SIFT, HOG) with convolutional layers that learn features",
        "Train end-to-end from raw pixels instead of pre-extracted features",
        "Use large dataset (ImageNet) to learn rich representations",
        "GPUs made training deep networks practical at scale"
      ],
      "comments": [
        {
          "text": "Its not just parallel, its versus SVMs and the feature engineering approach that was widely used",
          "timestamp": "2025-12-21T21:50:35.026739",
          "author": "User"
        }
      ],
      "year": 2012,
      "tags": [
        "computer-vision",
        "deep-learning",
        "ML",
        "architecture"
      ]
    },
    {
      "id": "fast-attention-mechanisms",
      "title": "Flash Attention: Rethinking Attention for GPU Memory",
      "category": "Architectural Pivot - Computational Complexity Reduction",
      "companies_involved": [
        "Research Community: Google, Stanford, UC Berkeley, OpenAI"
      ],
      "initial_problem": "Run transformer attention on long sequences without running out of GPU memory",
      "initial_assumptions": [
        "Attention requires computing the full N\u00d7N attention matrix",
        "Must store this entire matrix in GPU memory before applying softmax",
        "Long sequences need more GPU memory - that's just how attention works",
        "To handle longer sequences, you need bigger GPUs or approximations that sacrifice accuracy"
      ],
      "why_it_fails": [
        "GPU memory is the bottleneck, not compute - you're memory-bound",
        "Storing the full N\u00d7N matrix wastes memory bandwidth moving data around",
        "Standard attention doesn't take advantage of GPU memory hierarchy (fast SRAM vs slow HBM)",
        "Approximate attention methods trade off accuracy to save memory"
      ],
      "first_principle_insight": "You don't need to store the full attention matrix. By breaking the computation into small tiles that fit in fast GPU SRAM, you can compute exact attention while using far less memory. The insight came from earlier work on memory-efficient matrix multiplication - the same tiling principle applies to attention.",
      "reframe": {
        "new_atomic_unit": "Sparse or structured attention patterns that capture most information with sub-quadratic complexity",
        "new_problem_type": "Approximate attention mechanisms with complexity O(n log n) or O(n)",
        "new_objective": "Maintain Transformer quality while reducing attention complexity to enable long-context processing",
        "architectural_changes": [
          {
            "change": "Sparse Attention Patterns",
            "description": "Methods like Sparse Transformers (OpenAI, 2019) restrict attention to predetermined sparse patterns: (1) Local attention: each token attends to k nearest neighbors. (2) Strided attention: attend to every k-th token. (3) Global attention: all tokens attend to a few special tokens. This reduces complexity to O(n \u00d7 k) where k << n."
          },
          {
            "change": "Linformer: Low-Rank Projection",
            "description": "Linformer (Facebook, 2020) observes that attention matrices are often low-rank. It projects the n\u00d7n attention matrix to a smaller k\u00d7k matrix (where k is fixed, e.g., 256) using learned linear projections. This achieves O(n) complexity while maintaining quality on many tasks. The insight: full-rank attention is redundant."
          },
          {
            "change": "Performer/Linear Attention: Kernel Approximation",
            "description": "Performer (Google, 2020) reframes attention as a kernel operation and approximates softmax with random feature maps. This allows attention to be computed as (Q \u00d7 K^T) \u00d7 V = Q \u00d7 (K^T \u00d7 V), changing order of operations to avoid materializing the n\u00d7n matrix. Complexity becomes O(n \u00d7 d\u00b2) \u2248 O(n) for practical d."
          },
          {
            "change": "Longformer: Sliding Window + Global Attention",
            "description": "Longformer (AllenAI, 2020) combines local sliding window attention (each token attends to w neighbors: O(n \u00d7 w)) with global attention for a few task-specific tokens (e.g., [CLS]). This hybrid approach achieves O(n) complexity and handles documents up to 4K-16K tokens efficiently."
          },
          {
            "change": "BigBird: Sparse Random + Window + Global",
            "description": "BigBird (Google, 2020) uses a three-part pattern: (1) local window attention, (2) global tokens, (3) random attention to a few tokens. It proves this sparse pattern is theoretically equivalent to full attention (universal approximator property) while achieving O(n) complexity. Can handle sequences up to 4K-8K tokens."
          },
          {
            "change": "Flash Attention: Hardware-Aware Optimization",
            "description": "FlashAttention (Stanford, 2022) doesn't change the algorithm\u2014it optimizes the standard attention for GPU memory hierarchy. By fusing operations and using tiling to minimize memory reads/writes (I/O complexity), it achieves 2-4x speedup and 10-20x memory reduction, enabling longer sequences on the same hardware. This is algorithm-hardware co-design."
          }
        ],
        "results": "These innovations enabled Transformers to scale from 512-1024 tokens to 4K-100K+ tokens. Models like GPT-4 (128K context), Claude 2.1 (200K context), and Gemini 1.5 (1M context) all rely on efficient attention mechanisms. The reframe from 'compute all pairs' to 'compute important pairs' unlocked new use cases: long document understanding, few-shot learning with many examples, code repositories as context, entire books as input."
      },
      "sources": [
        {
          "citation": "Dao, T. et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.",
          "url": "https://arxiv.org/abs/2205.14135"
        },
        {
          "citation": "Dao, T. (2022). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.",
          "url": "https://arxiv.org/abs/2307.08691"
        }
      ],
      "key_lesson": "When you hit a fundamental complexity barrier (O(n\u00b2)), don't just accept it\u2014question the assumptions. Full pairwise computation is often overkill. Look for structure or approximations. The diagnostic: if your bottleneck is 'we need to compute X for all pairs,' ask 'Do we really need all pairs, or just the important ones?' Sparsity and approximation are powerful tools. Additionally, algorithmic improvements can be complemented by hardware-aware optimization (FlashAttention)\u2014never ignore the hardware-software boundary.",
      "how_to_reframe": {
        "old_atomic_unit": "Full attention matrix - compute and store the entire N\u00d7N matrix, then apply softmax",
        "new_atomic_unit": "Tiled attention blocks - compute attention in small chunks that fit in fast GPU SRAM, never materializing the full matrix"
      },
      "architectural_changes": [
        "Break attention into tiles/blocks that fit in GPU SRAM",
        "Compute softmax incrementally without storing full N\u00d7N matrix",
        "Fuse operations to minimize memory reads/writes to slow HBM",
        "Get exact attention (not approximate) with O(N) memory instead of O(N\u00b2)"
      ],
      "comments": [
        {
          "text": "This is confusing a few things -- fast attention was about breaking up attention matrix calcuations and using GPU - this is a first pirnciples\nUsing linear encodings in trasnformers was not first pirnciples, it seemed like a more general continous learning",
          "timestamp": "2025-12-21T21:52:36.229195",
          "author": "User"
        }
      ],
      "year": 2022,
      "tags": [
        "LLM",
        "optimization",
        "architecture",
        "deep-learning",
        "efficiency"
      ]
    },
    {
      "id": "flashattention-tri-dao-reframe",
      "title": "FlashAttention: From FLOPs to Memory Access - Tri Dao's Reframe",
      "category": "Architectural Pivot - Hardware-Aware Optimization",
      "companies_involved": [
        "Stanford University",
        "Together AI"
      ],
      "initial_problem": "Make transformer attention faster on GPUs",
      "initial_assumptions": [
        "To make models faster, reduce theoretical complexity (Big-O notation)",
        "Fewer calculations (FLOPs) = faster execution",
        "The bottleneck is the amount of math being done",
        "Sparse or approximate attention is the path to efficiency"
      ],
      "why_it_fails": [
        "Algorithms with better theoretical complexity (O(N log N)) ran SLOWER on real GPUs than brute-force O(N) methods",
        "GPU performance is memory-bound, not compute-bound for attention",
        "The bottleneck isn't the amount of math - it's how hardware accesses data",
        "Approximate methods sacrifice accuracy unnecessarily"
      ],
      "first_principle_insight": "The atomic unit of cost isn't FLOPs - it's memory access. Moving data between slow HBM (main GPU memory) and fast SRAM (cache) is the actual bottleneck. Re-doing calculations can be 'cheaper' than storing results if it avoids memory access. This required combining two existing ideas: Tiling (decades-old HPC technique) + Online Softmax (obscure 2018 NVIDIA paper proving Softmax can be computed incrementally).",
      "how_to_reframe": {
        "old_atomic_unit": "FLOPs - minimize the number of floating point operations",
        "new_atomic_unit": "HBM reads/writes - minimize data movement between GPU memory tiers"
      },
      "reframe": {
        "new_atomic_unit": "Memory access patterns (HBM  SRAM transfers)",
        "new_problem_type": "IO-aware algorithm design",
        "new_objective": "Minimize memory bandwidth usage while computing exact attention",
        "architectural_changes": [
          {
            "change": "Tiling for Attention",
            "description": "Break the NN attention computation into small tiles that fit entirely in fast SRAM. This is a decades-old HPC technique, but it 'broke' when applied to attention because of Softmax."
          },
          {
            "change": "Online Softmax",
            "description": "Use the Online Normalizer Calculation (Milakov & Gimelshein, NVIDIA 2018) to compute Softmax incrementally in tiles without needing to see the entire row. This was the missing link that enabled tiling for attention."
          },
          {
            "change": "Recomputation over Storage",
            "description": "Don't store the NN attention matrix during the forward pass. Instead, recompute it during the backward pass from tiles still in SRAM. More math, but no HBM access = faster."
          },
          {
            "change": "Kernel Fusion",
            "description": "Fuse multiple operations (QK, Softmax, V) into a single GPU kernel to avoid intermediate memory writes."
          }
        ],
        "results": "3-10x speedup on standard attention. Exact attention (not approximate) with O(N) memory instead of O(N). Enabled scaling from 2K context to 32K, 100K, and eventually 1M+ context windows. FlashAttention-2 added further optimizations through better GPU thread parallelization."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Dao, T. et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS 2022.",
          "url": "https://arxiv.org/abs/2205.14135"
        },
        {
          "type": "primary",
          "citation": "Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.",
          "url": "https://arxiv.org/abs/2307.08691"
        },
        {
          "type": "enabling_research",
          "citation": "Milakov, M. & Gimelshein, N. (2018). Online normalizer calculation for softmax. arXiv:1805.02867.",
          "url": "https://arxiv.org/abs/1805.02867",
          "note": "The mathematical trick that enabled incremental Softmax computation in tiles"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2: Deconstruction - questioning what the actual atomic unit of cost is",
        "common_mistake_avoided": "Optimizing the wrong thing (FLOPs instead of memory access)",
        "when_to_use": "When your theoretical improvements don't translate to real-world speedups - question whether you're measuring the right bottleneck",
        "cross_pollination": "Tri Dao's breakthrough came from cross-pollinating ML with HPC/Systems knowledge. He didn't invent Tiling or Online Softmax - he recognized they could be combined to solve an ML problem."
      },
      "key_lesson": "Tri Dao's genius wasn't inventing new math - it was reframing the problem. He stopped seeing attention as a Math problem (minimize FLOPs) and started seeing it as a Data Movement problem (minimize HBM access). The diagnostic question: 'Is my theoretical improvement actually hitting the real bottleneck, or am I optimizing something that doesn't matter on real hardware?'",
      "journey_phases": [
        {
          "phase": "1. The Theory Frame (2018-2020)",
          "frame": "Reduce theoretical complexity via structured/sparse matrices",
          "outcome": "Better Big-O, but slower on real GPUs"
        },
        {
          "phase": "2. Cross-Pollination (2020-2021)",
          "frame": "ML problems are Systems problems in disguise",
          "outcome": "Learned Tiling from HPC, discovered Online Softmax paper"
        },
        {
          "phase": "3. The IO-Aware Reframe (2022)",
          "frame": "Memory access is so expensive that recomputation is cheaper than storage",
          "outcome": "FlashAttention - 3-10x faster with exact attention"
        },
        {
          "phase": "4. Iteration (2023+)",
          "frame": "IO-aware algorithms enable infinite context scaling",
          "outcome": "FlashAttention-2, 1M+ context windows"
        }
      ],
      "year": 2022,
      "tags": [
        "LLM",
        "optimization",
        "hardware-aware",
        "reframe",
        "cross-pollination",
        "deep-learning"
      ]
    },
    {
      "id": "pinterest-pinsage-gnn",
      "title": "Pinterest PinSage: Image Features vs. User Context",
      "category": "Relational Pivot - Visual to Graph",
      "companies_involved": [
        "Pinterest"
      ],
      "initial_problem": "Recommend similar pins to users on Pinterest",
      "initial_assumptions": [
        "Similar-looking images are similar content - use visual features from CNNs",
        "If two images have the same colors, shapes, objects, they're related",
        "The atomic unit is the image itself and what it looks like"
      ],
      "why_it_fails": [
        "A bed rail and a fence look visually similar but mean completely different things",
        "Visual features miss context - the same image means different things in different contexts",
        "What an image looks like isn't the same as what it means to users"
      ],
      "first_principle_insight": "The meaning of a pin isn't what it looks like - it's how users organize it. A bed rail appears on 'Baby Safety' boards; a fence appears on 'Garden' boards. Even if they look similar, the boards tell you what the pin actually means to users.",
      "reframe": {
        "new_atomic_unit": "Pin embedding learned from graph neighborhood (board co-occurrence)",
        "new_problem_type": "Graph Neural Network with random walk sampling (PinSage)",
        "new_objective": "Learn embeddings that capture semantic similarity through graph structure, not just visual features",
        "architectural_changes": [
          {
            "change": "Random Walk Sampling",
            "description": "Instead of computing on full Laplacian, PinSage uses random walks to sample neighborhoods. Simulate user clicking pin\u2192board\u2192pin. Nodes visited most frequently are 'important neighbors.' This makes GCNs tractable at web scale."
          },
          {
            "change": "Importance Pooling",
            "description": "Generate pin embedding by aggregating information from important neighbors discovered via random walks. A pin learns it's semantically close to other pins it shares boards with, regardless of visual similarity."
          },
          {
            "change": "Scalable Architecture",
            "description": "PinSage operates on billions of nodes and edges, making it the first production GNN at web scale. Avoids the O(n\u00b3) Laplacian inversion problem through sampling."
          }
        ],
        "results": "46% performance gain over traditional CNN+collaborative filtering methods. Successfully disambiguated visually similar but semantically different content. Significant lift in user engagement. Bed rails and garden fences now live in separate embedding spaces despite visual similarity."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W. L., & Leskovec, J. (2018). Graph Convolutional Neural Networks for Web-Scale Recommender Systems. KDD 2018.",
          "url": "https://arxiv.org/abs/1806.01973",
          "note": "Original PinSage paper"
        },
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog: PinSage: A new graph convolutional neural network for web-scale recommender systems",
          "url": "https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48",
          "note": "From research file: AI Architectural Pivots"
        }
      ],
      "key_lesson": "When objects have relationships and context matters more than attributes, move from feature-based models to graph-based models. Visual/textual features are often proxies\u2014the real signal is in the topology of how things connect.",
      "how_to_reframe": {
        "old_atomic_unit": "Image features - what does this image look like?",
        "new_atomic_unit": "User context - what boards do users save this image to? What else do they save alongside it?"
      },
      "architectural_changes": [
        "Use the graph of which pins appear on which boards together",
        "Learn pin similarity from user organization patterns, not just pixels",
        "Combine visual features with graph context for better recommendations",
        "Scale to billions of pins using sampled neighborhoods"
      ],
      "comments": [
        {
          "text": "i like this reframing and maybe first principles",
          "timestamp": "2025-12-21T21:53:22.554138",
          "author": "User"
        }
      ],
      "tags": [
        "graph-neural-networks",
        "recommendations",
        "embeddings",
        "e-commerce"
      ]
    },
    {
      "id": "alphacode-generate-filter",
      "title": "AlphaCode: Generate Many, Filter to Find Correct (Alternative Pattern)",
      "category": "Alternative Pattern - Scale and Filter",
      "companies_involved": [
        "DeepMind",
        "Google"
      ],
      "initial_problem": "Generate correct solutions to competitive programming problems that require complex algorithms, edge case handling, and strict time/memory constraints",
      "initial_assumptions": [
        "Code generation is a translation task (English \u2192 Python)",
        "A single forward pass through a Transformer can produce the correct solution",
        "The model should output the 'best guess' solution directly",
        "One attempt per problem is sufficient",
        "The atomic unit is 'one generated program'"
      ],
      "why_it_fails": [
        "Single-shot generation rarely produces correct competitive programming solutions",
        "Complex problems require exploring many possible approaches",
        "Hard to know if a solution is correct without testing it"
      ],
      "first_principle_insight": "Code is testable - unlike essays, you can objectively verify if code is correct. Instead of trying to generate the perfect solution, generate many diverse candidates and use testing to filter down to correct ones. This is a scale-based approach rather than a fundamental reframe.",
      "reframe": {
        "new_atomic_unit": "Large set of candidate programs filtered by execution",
        "new_problem_type": "Massively parallel generation + test-based filtering + clustering",
        "new_objective": "Generate millions of candidates, execute against tests, cluster by behavior, select diverse representatives",
        "architectural_changes": [
          {
            "change": "Massive Sampling (millions)",
            "description": "Instead of 1 solution, generate millions of candidate programs using Transformer with sampling. Accept that model is noisy\u2014compensate with compute-at-inference."
          },
          {
            "change": "Execution-Based Filtering",
            "description": "Run all candidates against sample tests. Eliminate ~99% that are syntactically incorrect or fail basic logic. Code executability enables hard filtering impossible in other modalities."
          },
          {
            "change": "Output-Based Clustering",
            "description": "Cluster remaining candidates by their outputs on model-generated hidden tests. Select diverse representatives (DP solution, greedy solution, etc.) to maximize probability of covering true logic."
          }
        ],
        "results": "AlphaCode performed in top 54% of human competitive programmers. The pivot from 'generate once' to 'generate millions and filter' was key. Accepting model noisiness and using inference-time compute to compensate proved more effective than trying to make a single pass perfect."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Li, Y., et al. (2022). Competition-Level Code Generation with AlphaCode. Science, 378(6624), 1092-1097.",
          "url": "https://www.science.org/doi/10.1126/science.abq1158",
          "note": "Original AlphaCode paper in Science"
        },
        {
          "type": "primary",
          "citation": "DeepMind Blog: Competitive programming with AlphaCode",
          "url": "https://deepmind.google/discover/blog/competitive-programming-with-alphacode/",
          "note": "From research file: AI Architectural Pivots"
        }
      ],
      "key_lesson": "When outputs are objectively verifiable (code, math, structured data), reframe from 'generate best answer' to 'generate many candidates and filter.' Use the verifiability to your advantage. Inference-time compute (sampling) can compensate for model uncertainty.",
      "how_to_reframe": {
        "old_atomic_unit": "Code generation as single-pass synthesis - generating one best program directly from the problem description",
        "new_atomic_unit": "Code generation as massive generation + learned filtering - generating millions of candidate programs and using learned models to identify correct solutions"
      },
      "architectural_changes": [
        "Implemented massive-scale code generation producing millions of candidates",
        "Built learned filtering model trained on execution traces and test results",
        "Added clustering to identify semantically distinct solution approaches",
        "Implemented test-based verification for each generated candidate",
        "Created ranking model combining syntax validity, test passage, and efficiency",
        "Built iterative refinement where top candidates inform next generation round",
        "Added ensemble scoring across multiple generation strategies"
      ],
      "comments": [
        {
          "text": "this isn't very convincing to me that it ws a reallly different approach to a problem",
          "timestamp": "2025-12-21T21:54:32.830986",
          "author": "User"
        }
      ],
      "tags": [
        "code-generation",
        "LLM",
        "reasoning",
        "evaluation"
      ]
    },
    {
      "id": "alphageometry-neurosymbolic",
      "title": "AlphaGeometry: Neural + Symbolic Hybrid (Alternative Pattern)",
      "category": "Alternative Pattern - Hybrid Approach",
      "companies_involved": [
        "DeepMind",
        "Google"
      ],
      "initial_problem": "Solve Olympiad-level geometry proofs that require both creative construction (adding auxiliary lines) and rigorous logical deduction",
      "initial_assumptions": [
        "Pure Transformers (like GPT-4) can learn to do mathematical reasoning",
        "Language models can generate valid geometric proofs through next-token prediction",
        "Scale and training data are sufficient for logical reasoning",
        "The atomic unit is 'generated proof text'",
        "Deep learning alone can achieve human-level theorem proving"
      ],
      "why_it_fails": {
        "summary": "Pure Transformers achieve 0% success on Olympiad geometry. They hallucinate non-existent constructions, use circular reasoning, and lack soundness guarantees. Pure symbolic AI is rigorous but brittle, getting stuck in explosive search spaces. Neither approach alone works.",
        "detailed_failures": [
          {
            "failure": "Neural Hallucination",
            "description": "LLMs hallucinate invalid geometric constructions ('add line XY perpendicular to Z' when Z doesn't exist) or use circular reasoning (assume what they're trying to prove). No soundness guarantee\u2014outputs aren't guaranteed valid."
          },
          {
            "failure": "Symbolic Brittleness",
            "description": "Pure deduction engines (symbolic AI) are rigorous but get stuck. Branching factor of possible constructions (add lines, circles, points) is explosive. Engine doesn't know which auxiliary construction enables the proof."
          },
          {
            "failure": "Data Bottleneck",
            "description": "Very few human-written Olympiad proofs exist for training. Supervised learning on scarce data doesn't generalize to novel problems."
          }
        ]
      },
      "first_principle_insight": "Geometry proofs need both creative intuition (where to add auxiliary lines) and rigorous logic (verify the proof is valid). Neural networks are good at creativity, symbolic systems are good at rigor - combine them. This is a hybrid approach rather than a fundamental reframe.",
      "reframe": {
        "new_atomic_unit": "Hybrid loop: symbolic deduction + neural construction suggestion",
        "new_problem_type": "Neuro-symbolic architecture with feedback loop",
        "new_objective": "Use symbolic engine for sound deduction, neural model for creative construction hints",
        "architectural_changes": [
          {
            "change": "Hybrid Loop Architecture",
            "description": "Symbolic engine (DDAR) attempts deduction from axioms. When stuck, neural model suggests constructions ('add midpoint D of BC'). Symbolic engine consumes suggestion and continues. Iterate until proof complete."
          },
          {
            "change": "Synthetic Data Generation",
            "description": "Avoid data bottleneck by generating 100M synthetic theorems. Run symbolic engine in reverse: create random diagrams, deduce true facts. Train neural model on this synthetic data to learn construction heuristics."
          },
          {
            "change": "Soundness Preservation",
            "description": "All deduction happens in symbolic engine (sound). Neural model only suggests constructions (creative, can be wrong). If suggestion is invalid, symbolic engine ignores it. This preserves formal correctness while adding creativity."
          }
        ],
        "results": "Solved 25/30 Olympiad problems, matching Gold Medalist performance. Pure Transformer: 0%. Pure symbolic: gets stuck. Hybrid: achieves human expert level. The pivot was acknowledging neural networks should guide the search, not perform the deduction."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Trinh, T. H., et al. (2024). Solving Olympiad Geometry without Human Demonstrations. Nature, 625, 476-482.",
          "url": "https://www.nature.com/articles/s41586-023-06747-5",
          "note": "Original AlphaGeometry paper in Nature"
        },
        {
          "type": "primary",
          "citation": "DeepMind Blog: AlphaGeometry: An Olympiad-level AI system for geometry",
          "url": "https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/",
          "note": "From research file: AI Architectural Pivots"
        }
      ],
      "key_lesson": "When a problem requires both creativity and rigor, don't force one model to do both. Decompose: use neural networks for creative search/heuristics, symbolic systems for sound execution. Neuro-symbolic architectures combine strengths while avoiding weaknesses.",
      "how_to_reframe": {
        "old_atomic_unit": "Theorem proving as pure symbolic search - using rule-based systems and search algorithms to construct proofs",
        "new_atomic_unit": "Theorem proving as neuro-symbolic synthesis - combining neural language models to generate construction hints with symbolic engines to verify rigorous proofs"
      },
      "architectural_changes": [
        "Combined transformer-based auxiliary construction generator with symbolic prover",
        "Implemented symbolic geometry engine for rigorous proof verification",
        "Added neural model to suggest construction steps based on diagram state",
        "Built iterative loop alternating between neural hints and symbolic deduction",
        "Created synthetic training data generator using symbolic geometry rules",
        "Implemented proof search guided by neural construction suggestions",
        "Added verification layer ensuring all neural suggestions are symbolically valid"
      ],
      "comments": [
        {
          "text": "maybe, not sure I understandi this",
          "timestamp": "2025-12-21T21:55:13.441831",
          "author": "User"
        }
      ],
      "tags": [
        "reasoning",
        "hybrid-systems",
        "architecture"
      ]
    },
    {
      "id": "openai-o1-chain-of-thought",
      "title": "OpenAI o1: Inference-Time Reasoning (Alternative Pattern)",
      "category": "Alternative Pattern - Paradigm Shift",
      "companies_involved": [
        "OpenAI"
      ],
      "initial_problem": "Get language models to solve complex reasoning problems (math, coding, logic)",
      "initial_assumptions": [
        "Train bigger models on more data to improve reasoning",
        "The model should output the answer directly",
        "Reasoning ability comes from pre-training and fine-tuning"
      ],
      "why_it_fails": [
        "Direct answers skip the reasoning steps humans use to solve hard problems",
        "Scaling pre-training has diminishing returns for reasoning tasks",
        "Models make mistakes they could catch if they 'thought' more carefully"
      ],
      "first_principle_insight": "Instead of just training models to be smarter, let them think longer at inference time. Generate explicit reasoning steps before answering - trading compute for accuracy. This is a strong paradigm shift, though the internal decision-making process isn't publicly documented.",
      "reframe": {
        "new_atomic_unit": "Chain of thought (hidden reasoning tokens) \u2192 final answer",
        "new_problem_type": "Reinforcement learning on reasoning traces, not just final answers",
        "new_objective": "Train model to generate intermediate reasoning steps before answering, rewarding correct reasoning paths",
        "architectural_changes": [
          {
            "change": "Chain of Thought Generation",
            "description": "Model generates hidden reasoning tokens (not shown to user) representing step-by-step thinking before producing final answer. These tokens allow planning, decomposition, and error checking."
          },
          {
            "change": "Reinforcement Learning on Reasoning",
            "description": "Train via RL to reward not just correct answers but correct reasoning paths. Model learns to 'think through' problems rather than guess answers."
          },
          {
            "change": "Inference-Time Compute",
            "description": "Accept longer inference time (more tokens generated) in exchange for better reasoning. Quality improves with thinking time\u2014more reasoning tokens = better answers on hard problems."
          }
        ],
        "results": "o1 achieves significant improvements on math, coding, and scientific reasoning benchmarks compared to GPT-4. The shift from 'answer immediately' to 'think then answer' unlocks reasoning capabilities impossible with standard language modeling."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "OpenAI (2024). OpenAI o1 System Card.",
          "url": "https://cdn.openai.com/o1-system-card.pdf",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "supplementary",
          "citation": "OpenAI Blog: Learning to Reason with LLMs",
          "url": "https://openai.com/index/learning-to-reason-with-llms/",
          "note": "Official o1 announcement"
        }
      ],
      "key_lesson": "When problems require multi-step reasoning, don't force the model to answer immediately. Give it 'thinking time' via chain of thought. This is an architectural shift from Language Modeling to Thought Modeling\u2014predicting reasoning steps, not just words.",
      "how_to_reframe": {
        "old_atomic_unit": "Direct answer - model outputs the final answer immediately",
        "new_atomic_unit": "Reasoning chain - model generates step-by-step thinking before the answer, using more compute at inference time"
      },
      "architectural_changes": [
        "Model generates chain-of-thought reasoning before final answer",
        "Inference-time compute scales with problem difficulty",
        "Train models to reason step-by-step, not just produce answers",
        "Hidden reasoning tokens used during thinking process"
      ],
      "comments": [
        {
          "text": "I agree the idea of doing post training reasoning was a big change, however, I don't know enough about the actual decision making that I can tell the first principles story -- it is a reframing -- but we don't why they went down this road.",
          "timestamp": "2025-12-22T21:05:29.524061",
          "author": "User"
        }
      ],
      "tags": [
        "LLM",
        "reasoning",
        "architecture",
        "evaluation"
      ]
    },
    {
      "id": "instacart-lost-demand",
      "title": "Instacart: Modeling Demand You Can't See",
      "category": "Pragmatic Pivot - Complexity to Robustness",
      "companies_involved": [
        "Instacart"
      ],
      "initial_problem": "Predict customer demand when you only see completed orders, not customers who left because items were unavailable",
      "initial_assumptions": [
        "Demand = orders we receive",
        "Train models on historical order data to predict future demand",
        "If we don't see an order, there was no demand"
      ],
      "why_it_fails": [
        "You're only seeing demand that was fulfilled - missing 'lost demand'",
        "Customers who couldn't get what they wanted just left without ordering",
        "Your data is biased toward what was available, not what was wanted"
      ],
      "first_principle_insight": "You can't optimize for demand you're not measuring. If customers leave because items aren't available, that's lost demand - and your order data doesn't capture it. You need to model the demand you're NOT seeing.",
      "reframe": {
        "new_atomic_unit": "Distribution of possible outcomes (via simulation), not point prediction",
        "new_problem_type": "Monte Carlo simulation + heuristic-based fallbacks",
        "new_objective": "Optimize for risk of failure (understaffing), not expected value. Handle variance explicitly.",
        "architectural_changes": [
          {
            "change": "Availability Heuristics",
            "description": "Replace complex ML with simple conversion funnels using 'visits' as proxy for lost demand. More robust than inferring intent from complex features. Simpler model = less overfitting to spurious patterns."
          },
          {
            "change": "Monte Carlo Simulation",
            "description": "For shopper staffing, simulate 10,000 possible days with varying demand/supply distributions. Optimize for risk of understaffing (lost deliveries), not just expectation. This explicitly handles variance."
          },
          {
            "change": "System-Level Robustness",
            "description": "Accept slight accuracy loss for massive robustness gain. Simple heuristics don't create feedback loops. Simulation-based approach handles black swan events model never saw."
          }
        ],
        "results": "Escaped vicious feedback loops. System became robust to unexpected events. Shifted from 'prediction architecture' to 'simulation architecture.' Variance-aware optimization prevented catastrophic under-staffing while avoiding over-staffing waste."
      },
      "sources": [
        {
          "citation": "Krishnan, G. (2020). Modeling the unseen: How Instacart uses Machine Learning to predict lost demand. Instacart Tech Blog.",
          "url": "https://tech.instacart.com/modeling-the-unseen"
        }
      ],
      "key_lesson": "When variance matters more than mean (high-stakes, tail risks, adversarial/chaotic environments), simpler robust methods beat complex accurate-on-average methods. Watch for feedback loops where model's predictions affect future data. Sometimes the first principle is 'robustness' not 'accuracy.'",
      "how_to_reframe": {
        "old_atomic_unit": "Observed orders - demand equals what customers actually ordered",
        "new_atomic_unit": "True demand including lost demand - model what customers wanted, not just what they got"
      },
      "architectural_changes": [
        "Model customer intent, not just completed orders",
        "Track session data to identify customers who browsed but didn't buy",
        "Estimate counterfactual: what would they have ordered if items were available?",
        "Use lost demand estimates to improve inventory and staffing"
      ],
      "comments": [
        {
          "text": "The sources here point to two different use cases -- the first is the evolution of modeling which is very useful reframing . The second use case is monte carlo simulation, which is very cool and another goodl reframing. So lets split this into two examples.",
          "timestamp": "2025-12-22T21:08:34.950263",
          "author": "User"
        }
      ],
      "tags": [
        "e-commerce",
        "optimization",
        "marketplace",
        "metrics"
      ]
    },
    {
      "id": "zillow-offers-collapse",
      "title": "Zillow Offers: When You Don't Question Assumptions (Cautionary Tale)",
      "category": "Cautionary Tale - Anti-Pattern",
      "companies_involved": [
        "Zillow"
      ],
      "initial_problem": "Use ML to predict home prices and automate home buying (iBuying)",
      "initial_assumptions": [
        "Our Zestimate model predicts home values accurately",
        "We can use this model to buy homes at scale and profit",
        "Point predictions are good enough for high-stakes financial decisions",
        "Historical accuracy means future accuracy"
      ],
      "why_it_fails": [
        "Used a model designed for estimates in a context requiring precision",
        "Point predictions hid the uncertainty - homes have wide value ranges",
        "Market shifted and model couldn't adapt fast enough",
        "Bought thousands of homes at prices that turned out to be too high"
      ],
      "first_principle_insight": "This is an ANTI-PATTERN: Zillow used a model designed for consumer estimates to make high-stakes purchasing decisions without questioning whether the model was appropriate for that use case. The lesson: always ask 'was this model designed for the decision I'm making?' before using it.",
      "reframe": {
        "new_atomic_unit": "ML prediction + human review + rule-based guardrails \u2192 decision",
        "new_problem_type": "Human-in-loop AI-assisted valuation, not algorithmic iBuying",
        "new_objective": "Use ML to assist humans in making better decisions, with safety mechanisms",
        "architectural_changes": [
          {
            "change": "Demotion from Decision Maker to Decision Support",
            "description": "Model provides valuation estimate but doesn't make buy decision. Human reviewers have final say, especially for edge cases or uncertain predictions."
          },
          {
            "change": "Rule-Based Circuit Breakers",
            "description": "Hard-coded rules halt buying when: model confidence drops, market indicators shift, portfolio risk exceeds thresholds. Engineering constraints override ML predictions."
          },
          {
            "change": "Portfolio-Level Monitoring",
            "description": "Don't optimize individual home decisions in isolation. Monitor portfolio-level metrics: total exposure, geographic concentration, aggregate margin. Stop when portfolio risk is too high."
          }
        ],
        "results": "Zillow shut down Zillow Offers in Nov 2021, taking $500M+ loss. Industry-wide lesson: PropTech pivoted from 'algorithmic iBuying' to 'AI-assisted valuation.' This high-profile failure established that high-stakes financial decisions require human oversight, not full automation."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "SphereOI Analysis: How Zillow could have avoided its $500M AI mistake",
          "url": "https://sphereoi.ai/how-zillow-could-have-avoided-its-500m-ai-mistake/",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "primary",
          "citation": "Inside AI News: The $500mm+ Debacle at Zillow Offers \u2013 What Went Wrong with the AI Models?",
          "url": "https://insideainews.com/2021/12/13/the-500mm-debacle-at-zillow-offers-what-went-wrong-with-the-ai-models/",
          "note": "From research file: AI Architectural Pivots"
        }
      ],
      "key_lesson": "Full automation is not always the goal. For high-stakes decisions (finance, medicine, safety), human-in-loop + guardrails are first principles. AI should augment human judgment, not replace it. This is especially true in adversarial or rapidly changing environments where models can fail catastrophically.",
      "how_to_reframe": {
        "old_atomic_unit": "Point prediction - the model says this home is worth $X",
        "new_atomic_unit": "(What they should have done) Distribution with uncertainty - this home is worth $X \u00b1 $Y with Z% confidence"
      },
      "architectural_changes": [
        "(What went wrong) Treated point estimates as ground truth for buying decisions",
        "(What went wrong) Didn't model uncertainty or worst-case scenarios",
        "(What went wrong) Scaled too fast without validating model in new context",
        "(Lesson) High-stakes decisions need uncertainty quantification, not just predictions"
      ],
      "comments": [
        {
          "text": "The zillow is an interesting example, because its the oppostie of a first principles reframing -- its probably the downside, its a model that is used in a way different than it was intended and we can see the outcomes - this is probably another wrinkle I need to include in the course",
          "timestamp": "2025-12-22T21:09:50.540104",
          "author": "User"
        }
      ],
      "tags": [
        "cautionary-tale",
        "fintech",
        "prediction",
        "metrics",
        "anomaly-detection"
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "cautionary"
        ],
        "transferable_patterns": [
          "Models trained on 'normal' fail in regime change",
          "Confidence \u2260 accuracy in tail scenarios",
          "Business decisions should not blindly trust model outputs"
        ],
        "non_generalizable_context": [
          "Real estate has unique illiquidity",
          "Zillow's specific iBuying model",
          "COVID-era market dynamics"
        ]
      }
    },
    {
      "id": "amazon-just-walk-out-pivot",
      "title": "Amazon: Ceiling Cameras vs. Smart Carts",
      "category": "First Principles Pivot",
      "companies_involved": [
        "Amazon"
      ],
      "initial_problem": "Let customers grab items and leave without checkout",
      "initial_assumptions": [
        "Use ceiling cameras to track what each customer picks up",
        "Computer vision can solve the tracking problem at scale",
        "More cameras and better AI will make it work"
      ],
      "why_it_fails": [
        "Ceiling cameras are far away - hard to see what's in someone's hands",
        "Multiple customers, occlusion, and crowded stores make tracking extremely hard",
        "Scaling requires massive camera infrastructure in every store",
        "Labeling training data for edge cases is expensive (reportedly used humans to review)"
      ],
      "first_principle_insight": "Don't solve a hard sensing problem with better AI - make the sensing problem easier. Tracking what someone picks up is hard from the ceiling, but trivial if the sensor is on the cart. Move the sensor closer to the action.",
      "reframe": {
        "new_atomic_unit": "Cart with integrated sensors (barcode scanner, scale) in controlled environment",
        "new_problem_type": "Object scanning and weighing in constrained cart space (solved engineering)",
        "new_objective": "Make sensing trivial through sensor placement, not through AI sophistication",
        "architectural_changes": [
          {
            "change": "Dash Carts (Smart Carts)",
            "description": "Move sensor to the cart. Barcode scanner + scale + screen integrated into shopping cart. Customer scans items as they add them. Controlled environment (inside cart) makes sensing reliable."
          },
          {
            "change": "Eliminate Ambient CV",
            "description": "No ceiling cameras needed. No 3D tracking. No occlusion problem. Cart knows what's inside it because items were scanned directly. Problem goes from 'hard AI' to 'solved engineering.'"
          },
          {
            "change": "Eliminate Human Reviewers",
            "description": "Barcode scanning is deterministic\u2014no uncertain transactions needing human review. Massively reduces operational cost."
          }
        ],
        "results": "Amazon pivoted away from Just Walk Out in large Fresh stores, toward Dash Carts. Cost drops dramatically, reliability increases, no human reviewers needed. This is triumph of pragmatic engineering over AI hype\u2014solve the problem simply rather than with maximum sophistication."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Amazon Newsroom: An update on Amazon's plans for Just Walk Out and checkout-free technology",
          "url": "https://www.aboutamazon.com/news/retail/amazon-just-walk-out-dash-cart-grocery-shopping-checkout-stores",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "supplementary",
          "citation": "Impekable Analysis: The Illusion of AI: Unmasking Amazon's \"Just Walk Out\" Technology",
          "url": "https://www.impekable.com/the-illusion-of-ai-unmasking-amazons-just-walk-out-technology/",
          "note": "From research file: AI Architectural Pivots"
        }
      ],
      "key_lesson": "Before building sophisticated AI for a hard sensing problem, ask if you can make sensing easier through engineering. Sensor placement matters more than algorithm sophistication. The de-escalation principle: solve the problem with the simplest mechanism. Constrained environments (cart) >> unconstrained (ceiling).",
      "how_to_reframe": {
        "old_atomic_unit": "Store-wide tracking - cameras on ceiling try to track all customers and items",
        "new_atomic_unit": "Cart-based tracking - put sensors on the cart where items are placed"
      },
      "architectural_changes": [
        "Move sensors from ceiling to shopping cart",
        "Track items at the cart, not across the whole store",
        "Simpler problem = simpler solution = easier to scale",
        "Reduced need for massive camera infrastructure per store"
      ],
      "comments": [
        {
          "text": "This is another class of reframing problem, where they tried something, thought about the current trajectory of what it would take in terms of cameras and training data, and the first principles approach was to stop and do a checkout process at the cart.  Sometimes you do go simpler.  Good lesson.",
          "timestamp": "2025-12-22T21:11:26.989373",
          "author": "User"
        }
      ],
      "tags": [
        "computer-vision",
        "e-commerce",
        "cost-optimization",
        "architecture"
      ]
    },
    {
      "id": "booking-offline-metrics-to-rct",
      "title": "Booking.com: Offline Metrics vs. Real Business Impact",
      "category": "Evaluation Pivot - Offline to Online Experimentation",
      "companies_involved": [
        "Booking.com"
      ],
      "initial_problem": "Know if ML model improvements actually help the business",
      "initial_assumptions": [
        "Better offline metrics (AUC, accuracy) mean better business outcomes",
        "If the model is more accurate on test data, it will work better in production",
        "Optimize for prediction accuracy"
      ],
      "why_it_fails": [
        "Offline metrics don't always correlate with business metrics (bookings, revenue)",
        "A model can be 'more accurate' but not change user behavior",
        "You won't know real impact until you test with real users"
      ],
      "first_principle_insight": "Don't trust offline metrics alone. The only way to know if a model change helps the business is to run a real A/B test with real users and measure real outcomes.",
      "reframe": {
        "new_atomic_unit": "Randomized controlled trial (RCT) measuring business metrics (conversion, revenue)",
        "new_problem_type": "Experimentation platform as core infrastructure, not afterthought",
        "new_objective": "Validate every model change through rigorous A/B tests measuring actual business impact",
        "architectural_changes": [
          {
            "change": "Experimentation Platform First",
            "description": "Built infrastructure to support 150+ ML models running simultaneous A/B tests. Experimentation is not optional\u2014it's mandatory for every model change. Platform is core, not peripheral."
          },
          {
            "change": "Business Metrics over Model Metrics",
            "description": "Stop optimizing AUC/RMSE. Optimize conversion rate, revenue, customer lifetime value. If model has better offline score but worse conversion in RCT, model is rejected. Production is truth."
          },
          {
            "change": "Shift Drift Monitoring",
            "description": "Stop monitoring input feature distributions (meaningless). Monitor business metrics: conversion rate, booking value, user engagement. Not all drift is bad\u2014seasonal shifts are expected and good."
          }
        ],
        "results": "Booking.com successfully runs 150+ ML models validated through rigorous experimentation. Prevented many cases of 'better offline score, worse business outcome.' Established culture where truth comes from live users, not validation sets. Increased velocity while reducing risk of bad deployments."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Bernardi, L., et al. (2019). 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com. KDD 2019.",
          "url": "https://www.kdd.org/kdd2019/accepted-papers/view/150-successful-machine-learning-models-6-lessons-learned-at-booking.com",
          "note": "From research file: AI Architectural Pivots"
        }
      ],
      "key_lesson": "Offline metrics are weak proxies for business impact. In production ML, experimentation (A/B testing) is not optional\u2014it's the only real validation. Build experimentation platform as core infrastructure. Let live users be the final judge, not validation sets.",
      "how_to_reframe": {
        "old_atomic_unit": "Offline metric improvement - did accuracy go up on held-out data?",
        "new_atomic_unit": "Business impact via A/B test - did bookings/revenue actually increase with real users?"
      },
      "architectural_changes": [
        "A/B test all model changes before full deployment",
        "Measure business outcomes (bookings, revenue), not just model accuracy",
        "Use offline metrics for development, online tests for decisions"
      ],
      "comments": [
        {
          "text": "Great example of alignemtn of business metrics with technical metrics, need to use this story.",
          "timestamp": "2025-12-22T21:12:33.342257",
          "author": "User"
        }
      ],
      "tags": [
        "evaluation",
        "metrics",
        "experimentation",
        "marketplace"
      ]
    },
    {
      "id": "stripe-fraud-graph-clustering",
      "title": "Stripe: Transaction Scoring vs. Network Analysis",
      "category": "Relational Pivot - Tabular to Graph",
      "companies_involved": [
        "Stripe"
      ],
      "initial_problem": "Detect fraudulent transactions",
      "initial_assumptions": [
        "Score each transaction individually based on its features",
        "Look at amount, location, time, merchant for each transaction",
        "Fraud is a property of individual transactions"
      ],
      "why_it_fails": [
        "Sophisticated fraudsters make individual transactions look normal",
        "Fraud rings coordinate across multiple accounts and transactions",
        "Looking at transactions in isolation misses the network patterns"
      ],
      "first_principle_insight": "Fraud isn't a property of individual transactions - it's a pattern across networks. Fraudsters coordinate, so look at the connections between transactions, accounts, and merchants to find fraud rings.",
      "reframe": {
        "new_atomic_unit": "Fraud ring (connected cluster of accounts sharing resources)",
        "new_problem_type": "Graph clustering and community detection",
        "new_objective": "Identify and dismantle fraud networks, not just block individual transactions",
        "architectural_changes": [
          {
            "change": "Graph Construction",
            "description": "Build a massive graph where nodes are Stripe accounts and edges represent shared attributes (same device fingerprint, similar email strings with Jaccard similarity > 0.9, shared bank accounts). Edges have similarity scores."
          },
          {
            "change": "Community Detection",
            "description": "Run Connected Components and community detection algorithms instead of scoring individual transactions. When one node in a cluster is confirmed as fraud (via chargeback), propagate that label to the entire cluster."
          },
          {
            "change": "Infrastructure Dismantling",
            "description": "When a fraud ring is detected, invalidate every associated account, device, and identity simultaneously. Force fraudsters to burn all resources at once rather than adapting incrementally."
          }
        ],
        "results": "Fundamentally changed the economics of fraud. Shifted from blocking individual actions to dismantling infrastructure. Fraudsters must now rebuild entire networks when detected, dramatically increasing their costs and reducing attack efficiency."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Stripe Engineering Blog: Similarity clustering to catch fraud rings",
          "url": "https://stripe.com/blog/similarity-clustering",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "supplementary",
          "citation": "Stripe Blog: How we built it: Stripe Radar",
          "url": "https://stripe.com/blog/how-we-built-it-stripe-radar"
        }
      ],
      "key_lesson": "When your problem involves entities with relationships and shared resources, treating data as independent rows fails. The structure (graph topology) often contains more signal than the attributes. Reframe from 'classify this row' to 'analyze this network.'",
      "how_to_reframe": {
        "old_atomic_unit": "Individual transaction - score each transaction on its own features",
        "new_atomic_unit": "Transaction network - analyze patterns across connected accounts, cards, and merchants"
      },
      "architectural_changes": [
        "Build graph connecting transactions, accounts, merchants",
        "Detect fraud rings through network clustering",
        "Use graph patterns as features, not just transaction features"
      ],
      "comments": [
        {
          "text": "Good here, where we change the unit of analysis of the problem. we should make sure this is considered in the section where we talk about reframking problems.",
          "timestamp": "2025-12-22T21:13:29.212147",
          "author": "User"
        }
      ],
      "tags": [
        "fraud-detection",
        "graph-neural-networks",
        "fintech",
        "anomaly-detection"
      ]
    },
    {
      "id": "doordash-dispatch-mip",
      "title": "DoorDash: One-at-a-Time vs. Global Optimization",
      "category": "Optimization Pivot - Local to Global",
      "companies_involved": [
        "DoorDash"
      ],
      "initial_problem": "Assign delivery orders to dashers efficiently",
      "initial_assumptions": [
        "Match each order to the best available dasher one at a time",
        "Optimize each assignment independently",
        "Greedy matching (best available now) is good enough"
      ],
      "why_it_fails": [
        "Assigning orders one-at-a-time ignores how assignments affect future options",
        "A dasher assigned now might have been better for an order coming in 2 minutes",
        "Can't batch orders together for efficient multi-stop routes"
      ],
      "first_principle_insight": "Don't assign orders one at a time. Batch orders and optimize assignments globally - considering routes, timing, and which dashers will be where after each delivery.",
      "reframe": {
        "new_atomic_unit": "Multi-leg route (sequence of pickups and dropoffs)",
        "new_problem_type": "Mixed Integer Programming (MIP) for Vehicle Routing",
        "new_objective": "Maximize global marketplace utility (dasher earnings + customer freshness + system throughput)",
        "architectural_changes": [
          {
            "change": "MIP Solver (Gurobi)",
            "description": "Moved from graph heuristic to commercial MIP solver with branch-and-cut. Decision variables are binary (is Dasher assigned to this route leg?), constraints are time windows and capacity. Enables complex trade-offs."
          },
          {
            "change": "Global Optimization",
            "description": "MIP solver proved 10x faster than custom implementations, allowing removal of geographic boundaries. Solves larger regions simultaneously, eliminating edge effects."
          },
          {
            "change": "Systemic Efficiency",
            "description": "Can intentionally delay assignments for batching\u2014if high probability of second order at same merchant, wait 2 minutes. Shifts from greedy to systemic optimization."
          }
        ],
        "results": "10x performance improvement for complex scenarios. Eliminated geographic fragmentation. Enabled multi-order batching. Improved dasher utilization and customer experience through global optimization."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering: Next-Generation Optimization for Dasher Dispatch",
          "url": "https://careersatdoordash.com/blog/next-generation-optimization-for-dasher-dispatch-at-doordash/",
          "note": "From research file: AI Architectural Pivots"
        },
        {
          "type": "primary",
          "citation": "DoorDash Engineering: Using ML and Optimization to Solve DoorDash's Dispatch Problem",
          "url": "https://careersatdoordash.com/blog/using-ml-and-optimization-to-solve-doordashs-dispatch-problem/"
        }
      ],
      "key_lesson": "When atomic units have temporal dependencies (routes not matches, trajectories not steps), move from matching/assignment to sequential optimization. Local optima \u2260 global optimum.",
      "how_to_reframe": {
        "old_atomic_unit": "Single assignment - match this order to the best dasher now",
        "new_atomic_unit": "Global optimization - optimize all assignments together considering routes, batching, and future orders"
      },
      "architectural_changes": [
        "Batch orders and optimize assignments together",
        "Consider multi-stop routes, not just single deliveries",
        "Factor in where dashers end up after each delivery"
      ],
      "tags": [
        "optimization",
        "operations",
        "marketplace",
        "real-time"
      ],
      "canonical_case": true,
      "canonical_usage": {
        "role": "Primary recurring example across all modules",
        "module_1_mindset": "System 1 vs System 2: 'Optimize each delivery' vs 'What's the actual business outcome?'",
        "module_2_taxonomy": "Students must choose: Is this Prediction, Optimization, or Matching?",
        "module_3_loop": "Run full Loop with different Outcome definitions (delivery time vs driver utilization vs satisfaction)",
        "module_4_diagnosis": "What signals indicate greedy matching is failing?",
        "module_5_pivot": "When do you recognize local optimization is the wrong frame?"
      },
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "inspiration"
        ],
        "transferable_patterns": [
          "Local optimization often misses global value",
          "The right unit of optimization may not be obvious",
          "Real-time constraints change the problem fundamentally"
        ],
        "non_generalizable_context": [
          "Marketplace with two-sided dynamics (drivers + customers)",
          "Geographic density matters - urban vs suburban",
          "DoorDash's specific driver supply constraints"
        ]
      }
    },
    {
      "id": "generic-01-business-outcome",
      "title": "Employee Churn: Prediction vs. Intervention",
      "category": "Business Outcome",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Reduce employee turnover",
      "initial_assumptions": [
        "Build a model to predict who will quit",
        "High accuracy prediction = problem solved",
        "The goal is to predict"
      ],
      "why_it_fails": [
        "Predicting someone will quit doesn't stop them from quitting",
        "Model might learn non-actionable signals (e.g., long commute)",
        "Prediction without intervention is just watching people leave"
      ],
      "first_principle_insight": "The goal isn't to predict who will quit - it's to retain employees. Focus on actionable factors and intervention timing, not just prediction accuracy.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Predict 'Engagement Drop' (Anomaly). Intervention is easier before the decision is made.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: The 'Resignation' event is the moment of intervention.' to 'Predict 'Engagement Drop' (Anomaly). Intervention is easier before the decision is made.'",
      "how_to_reframe": {
        "old_atomic_unit": "Prediction - who will quit?",
        "new_atomic_unit": "Intervention opportunity - who can we retain and how?"
      },
      "architectural_changes": [
        "Identify actionable factors (not just predictive ones)",
        "Focus on employees where intervention can help",
        "Measure retention rate, not prediction accuracy"
      ],
      "comments": [
        {
          "text": "This is a good one, where you can get leakage, noticed a pattern of people that ended up quitiing where ones that lived out further - good tidbit -- but yes, classic problem like churn of how you reframe it and what quiting means",
          "timestamp": "2025-12-22T21:14:56.911429",
          "author": "User"
        }
      ],
      "tags": [
        "enterprise",
        "prediction",
        "classification",
        "causal-inference"
      ]
    },
    {
      "id": "generic-02-business-outcome",
      "title": "FAQ Chatbot: Q&A Retrieval vs. Workflow Automation",
      "category": "Business Outcome",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Build a chatbot to answer FAQs.",
      "initial_assumptions": [
        "Assumption: Natural Language is the lowest-friction interface."
      ],
      "why_it_fails": {
        "summary": "High friction. Users hate typing 'What is the return policy?'",
        "detailed_failures": []
      },
      "first_principle_insight": "A chatbot isn't just for answering questions - it can resolve issues by taking actions (creating tickets, processing refunds, routing to humans).",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Search/Navigation. A good search bar or 'Top 5 Links' is faster than a bot.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Natural Language is the lowest-friction interface.' to 'Search/Navigation. A good search bar or 'Top 5 Links' is faster than a bot.'",
      "how_to_reframe": {
        "old_atomic_unit": "Chatbot as FAQ retrieval - matching user questions to closest FAQ and returning pre-written answers",
        "new_atomic_unit": "Chatbot as workflow automation - treating conversations as structured workflows that can route, escalate, and resolve issues with business logic"
      },
      "architectural_changes": [
        "Replaced retrieval with intent classification and workflow routing",
        "Implemented conversational state tracking across multiple turns",
        "Added structured action execution (create ticket, escalate, refund)",
        "Built integration with business systems for automated actions",
        "Created escalation rules routing complex cases to humans",
        "Implemented slot filling for collecting required information",
        "Added analytics tracking resolution rates and user satisfaction"
      ],
      "tags": [
        "customer-support",
        "NLP",
        "automation"
      ]
    },
    {
      "id": "generic-03-business-outcome",
      "title": "Stock Prediction: Point Forecast vs. Uncertainty Range",
      "category": "Business Outcome",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Predict the stock price.",
      "initial_assumptions": [
        "Assumption: Historical price patterns contain the signal for future price."
      ],
      "why_it_fails": {
        "summary": "Impossible. The market is efficient and adversarial.",
        "detailed_failures": []
      },
      "first_principle_insight": "A single price prediction hides uncertainty. Show the range of likely outcomes so decisions account for risk.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Predict Volatility or Risk (Variance). This is predictable and useful for hedging.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Historical price patterns contain the signal for future price.' to 'Predict Volatility or Risk (Variance). This is predictable and useful for hedging.'",
      "how_to_reframe": {
        "old_atomic_unit": "Stock price as point prediction - forecasting the future price as a single number",
        "new_atomic_unit": "Stock price as probability distribution - modeling price as a range of outcomes with associated probabilities and confidence intervals"
      },
      "architectural_changes": [
        "Implemented ensemble models producing probability distributions",
        "Added uncertainty quantification from prediction variance",
        "Built confidence intervals using historical error patterns",
        "Created scenario analysis for different market conditions",
        "Implemented risk-adjusted decision making using distributions",
        "Added visualization of probability ranges for stakeholders",
        "Built calibration monitoring ensuring probabilities match outcomes"
      ],
      "tags": [
        "fintech",
        "prediction",
        "uncertainty",
        "anomaly-detection"
      ]
    },
    {
      "id": "generic-04-business-outcome",
      "title": "Ad Optimization: Clicks vs. Business Value",
      "category": "Business Outcome",
      "companies_involved": [
        "Google Ads",
        "Facebook Ads",
        "Digital Marketing Industry"
      ],
      "initial_problem": "Optimize ad spend for Clicks.",
      "initial_assumptions": [
        "Assumption: The Proxy (Click) equals the Value (Sale)."
      ],
      "why_it_fails": {
        "summary": "Click-through rate (CTR) optimization creates perverse incentives. Clickbait ads get high CTR but low conversion. Bots and accidental clicks inflate numbers. The proxy trap: what's measured (clicks) diverges from what matters (sales/revenue). Google Ads industry evolved from CPC to CPA/ROAS for this reason.",
        "detailed_failures": []
      },
      "first_principle_insight": "Optimizing for clicks might not optimize for profit. Measure what actually matters - conversions, revenue, customer lifetime value.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Optimize for Conversions (CPA) or ROAS (Return on Ad Spend).",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        },
        {
          "type": "supplementary",
          "citation": "Google Ads Help: About conversion tracking",
          "url": "https://support.google.com/google-ads/answer/1722022",
          "note": "Industry standard pivot from CPC to CPA/ROAS optimization"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: The Proxy (Click) equals the Value (Sale).' to 'Optimize for Conversions (CPA) or ROAS (Return on Ad Spend).'",
      "how_to_reframe": {
        "old_atomic_unit": "Optimization as maximizing clicks - treating ad spend optimization as purely maximizing the click-through rate",
        "new_atomic_unit": "Optimization as maximizing business value - considering full funnel metrics including conversions, customer lifetime value, and profit margins"
      },
      "architectural_changes": [
        "Added conversion tracking through full customer funnel",
        "Implemented customer lifetime value (LTV) modeling",
        "Built multi-touch attribution across ad interactions",
        "Created profit optimization considering margins and costs",
        "Implemented bidding strategies maximizing ROI not just clicks",
        "Added incrementality testing measuring true ad impact",
        "Built portfolio optimization across campaigns and channels"
      ],
      "tags": [
        "advertising",
        "optimization",
        "metrics",
        "causal-inference"
      ]
    },
    {
      "id": "generic-05-business-outcome",
      "title": "Defect Detection: Pass/Fail vs. Localized Diagnosis",
      "category": "Business Outcome",
      "companies_involved": [
        "Manufacturing Industry",
        "Quality Control Systems"
      ],
      "initial_problem": "Detect manufacturing defects (Vision).",
      "initial_assumptions": [
        "Assumption: We have enough 'Defect' examples to learn the class."
      ],
      "why_it_fails": {
        "summary": "Class imbalance is extreme in manufacturing: millions of good parts, dozens of defects. Supervised classification requires balanced training data. With 100 defect examples vs 10M normal examples, model learns to always predict 'good' (99.999% accuracy but useless). The reframe: don't learn 'defect' as a class\u2014learn 'normal' distribution and flag statistical outliers.",
        "detailed_failures": []
      },
      "first_principle_insight": "Knowing something is defective isn't enough. Identify exactly where and what type of defect for root cause analysis.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Anomaly Detection. Don't learn 'Defect'; learn 'Normal' and flag deviations.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        },
        {
          "type": "supplementary",
          "citation": "Roth, V. (2004). Outlier detection in high-dimensional data. Proceedings of the KDD",
          "note": "Academic foundation for anomaly detection in manufacturing"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: We have enough 'Defect' examples to learn the class.' to 'Anomaly Detection. Don't learn 'Defect'; learn 'Normal' and flag deviations.'",
      "how_to_reframe": {
        "old_atomic_unit": "Defect as classification target - training models to label images as defective or not defective",
        "new_atomic_unit": "Defect as localized anomaly - identifying exactly where and what type of defect exists for root cause analysis and repair guidance"
      },
      "architectural_changes": [
        "Implemented object detection localizing defects spatially",
        "Added segmentation models identifying defect boundaries",
        "Built defect classification categorizing specific issues",
        "Created severity scoring for triage and prioritization",
        "Implemented root cause analysis linking defects to process steps",
        "Added visualization overlaying defects on product images",
        "Built feedback to manufacturing process control systems"
      ],
      "tags": [
        "manufacturing",
        "computer-vision",
        "anomaly-detection",
        "evaluation"
      ]
    },
    {
      "id": "generic-06-business-outcome",
      "title": "AI Coding: Generate Files vs. Contextual Assistance",
      "category": "Business Outcome",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Use AI to write our code.",
      "initial_assumptions": [
        "Assumption: Generation is the bottleneck. (Actually, Verification is)."
      ],
      "why_it_fails": {
        "summary": "Liability. AI introduces bugs you can't see.",
        "detailed_failures": []
      },
      "first_principle_insight": "Don't generate entire files from scratch. Help developers within their workflow - completions, refactoring, bug fixes in context.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Unit Test Generation. AI writes the tests, Human writes the code. (Inversion).",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Generation is the bottleneck. (Actually, Verification is).' to 'Unit Test Generation. AI writes the tests, Human writes the code. (Inversion).'",
      "how_to_reframe": {
        "old_atomic_unit": "AI coding as code generation - using models to generate entire code files from descriptions",
        "new_atomic_unit": "AI coding as intelligent completion - using models as context-aware assistants that suggest completions, refactorings, and fixes within the developer's workflow"
      },
      "architectural_changes": [
        "Implemented context-aware code completion from surrounding code",
        "Added inline suggestions integrated into IDE workflow",
        "Built function-level generation for specific subtasks",
        "Created refactoring suggestions improving existing code",
        "Implemented bug detection and fix recommendations",
        "Added test generation suggesting relevant test cases",
        "Built learning from developer acceptance patterns"
      ],
      "tags": [
        "code-generation",
        "LLM",
        "UX"
      ]
    },
    {
      "id": "generic-07-business-outcome",
      "title": "News Feed: Maximize Engagement vs. Multi-Objective Balance",
      "category": "Business Outcome",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Personalize the news feed.",
      "initial_assumptions": [
        "Assumption: Relevance is the only metric for engagement."
      ],
      "why_it_fails": {
        "summary": "Echo Chamber. Users get bored of the same topic.",
        "detailed_failures": []
      },
      "first_principle_insight": "Pure engagement optimization can create filter bubbles and addiction. Balance engagement with diversity, quality, and user wellbeing.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Diversity Optimization. Optimize for 'Serendipity' (Surprise) alongside Relevance.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Relevance is the only metric for engagement.' to 'Diversity Optimization. Optimize for 'Serendipity' (Surprise) alongside Relevance.'",
      "how_to_reframe": {
        "old_atomic_unit": "Personalization as ranking optimization - sorting the feed to maximize individual user engagement metrics",
        "new_atomic_unit": "Personalization as multi-objective optimization - balancing user engagement with content diversity, creator ecosystem health, and platform objectives"
      },
      "architectural_changes": [
        "Implemented multi-objective optimization with Pareto frontiers",
        "Added diversity constraints preventing filter bubbles",
        "Built creator fairness metrics ensuring exposure balance",
        "Created long-term user satisfaction modeling beyond immediate clicks",
        "Implemented exploration mechanisms for content discovery",
        "Added business objective weighting (ads, growth, retention)",
        "Built user control mechanisms for personalization preferences"
      ],
      "tags": [
        "recommendations",
        "social-networks",
        "optimization",
        "metrics"
      ]
    },
    {
      "id": "generic-08-business-outcome",
      "title": "Lead Scoring: Probability vs. Expected Value",
      "category": "Business Outcome",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Predict which leads to call.",
      "initial_assumptions": [
        "Assumption: We need a binary decision (Call/Don't Call)."
      ],
      "why_it_fails": {
        "summary": "Efficiency. Salespeople ignore low-prob leads anyway.",
        "detailed_failures": []
      },
      "first_principle_insight": "High probability to convert isn't everything. Consider deal size and effort required - a 50% chance at $100K beats 90% chance at $5K.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Lead Scoring (Ranking). Don't predict 'Yes/No'; just sort the list from best to worst.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: We need a binary decision (Call/Don't Call).' to 'Lead Scoring (Ranking). Don't predict 'Yes/No'; just sort the list from best to worst.'",
      "how_to_reframe": {
        "old_atomic_unit": "Lead scoring as probability to convert - ranking leads by their likelihood to become customers",
        "new_atomic_unit": "Lead scoring as expected value - ranking leads by expected revenue considering conversion probability, deal size, and sales effort required"
      },
      "architectural_changes": [
        "Added expected deal size prediction from lead characteristics",
        "Implemented sales effort estimation per lead complexity",
        "Built expected value calculation: P(convert) \u00d7 deal_size / effort",
        "Created portfolio optimization maximizing team revenue",
        "Implemented dynamic scoring adjusting for sales capacity",
        "Added feedback loops learning from actual deal outcomes",
        "Built segmentation routing leads to appropriate sales tiers"
      ],
      "tags": [
        "enterprise",
        "prediction",
        "ranking",
        "optimization"
      ]
    },
    {
      "id": "generic-09-archetype-selection",
      "title": "Demand Forecasting: Single Prediction vs. Marketplace Dynamics",
      "category": "Archetype Selection",
      "companies_involved": [
        "Uber",
        "Lyft",
        "Ride-sharing Industry"
      ],
      "initial_problem": "In ride-sharing marketplaces, demand prediction alone is passive\u2014knowing demand will spike doesn't create supply. The control mechanism is price: surge pricing actively balances supply and demand by incentivizing drivers to come online and passengers to wait or carpool.",
      "initial_assumptions": [
        "Assumption: Knowing the future creates supply."
      ],
      "why_it_fails": {
        "summary": "Signal: Demand is high, but no cars are moving.",
        "detailed_failures": []
      },
      "first_principle_insight": "Demand doesn't exist in isolation. Model how supply, pricing, and competition affect demand - it's a system, not a number.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Optimization (Pricing). Use Price to balance Supply/Demand.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        },
        {
          "type": "primary",
          "citation": "Related to: Uber Engineering Blog on surge pricing and marketplace balance",
          "url": "https://www.uber.com/blog/engineering/",
          "note": "This generic example relates to the detailed Uber case study in this database"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Knowing the future creates supply.' to 'Optimization (Pricing). Use Price to balance Supply/Demand.'",
      "how_to_reframe": {
        "old_atomic_unit": "Demand as univariate time series - forecasting rides based solely on historical ride patterns",
        "new_atomic_unit": "Demand as multivariate marketplace dynamics - modeling how supply, pricing, weather, events, and driver behavior jointly influence demand"
      },
      "architectural_changes": [
        "Built multi-variate models with supply, weather, events as features",
        "Implemented driver behavior modeling predicting supply response",
        "Added pricing feedback loops showing demand elasticity",
        "Created spatial-temporal models capturing local patterns",
        "Implemented equilibrium prediction balancing supply and demand",
        "Added scenario simulation for policy testing",
        "Built real-time updates incorporating latest market state"
      ],
      "tags": [
        "marketplace",
        "prediction",
        "optimization"
      ]
    },
    {
      "id": "generic-10-archetype-selection",
      "title": "Email Classification: Rules vs. Learned Patterns",
      "category": "Archetype Selection",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Classify emails into folders.",
      "initial_assumptions": [
        "Assumption: We know the ground truth categories better than the data does."
      ],
      "why_it_fails": {
        "summary": "Signal: 100 new folders created every month.",
        "detailed_failures": []
      },
      "first_principle_insight": "Rules can't anticipate everything. Learn from how users actually sort their email to capture personal preferences.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Clustering (Unsupervised). Let the data define the topics, don't hard-code them.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: We know the ground truth categories better than the data does.' to 'Clustering (Unsupervised). Let the data define the topics, don't hard-code them.'",
      "how_to_reframe": {
        "old_atomic_unit": "Classification as rule-based routing - using keyword matching and rules to sort emails into folders",
        "new_atomic_unit": "Classification as learned representation - using ML to understand email semantics and learn user-specific classification patterns"
      },
      "architectural_changes": [
        "Implemented transformer models learning semantic representations",
        "Added personalization layers adapting to user-specific patterns",
        "Built active learning requesting labels for uncertain emails",
        "Created hierarchical classification for folder structures",
        "Implemented few-shot learning from limited examples per folder",
        "Added confidence-based routing with human review for low scores",
        "Built continuous learning updating from user corrections"
      ],
      "tags": [
        "NLP",
        "classification",
        "rules"
      ]
    },
    {
      "id": "generic-11-archetype-selection",
      "title": "Predictive Maintenance: Failure Detection vs. Remaining Life",
      "category": "Archetype Selection",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Predict if a machine breaks.",
      "initial_assumptions": [
        "Assumption: Failure modes repeat and look the same."
      ],
      "why_it_fails": {
        "summary": "Signal: Breaks are rare and novel.",
        "detailed_failures": []
      },
      "first_principle_insight": "Don't just predict if it will fail - predict when. Remaining useful life lets you optimize maintenance scheduling.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Anomaly Detection. (Not Classification).",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Failure modes repeat and look the same.' to 'Anomaly Detection. (Not Classification).'",
      "how_to_reframe": {
        "old_atomic_unit": "Failure as reactive detection - predicting machine failure based on current sensor readings",
        "new_atomic_unit": "Failure as degradation trajectory - modeling the gradual degradation process to predict remaining useful lifetime and optimal maintenance timing"
      },
      "architectural_changes": [
        "Implemented degradation modeling tracking component health trajectories",
        "Added remaining useful life (RUL) prediction from sensor trends",
        "Built failure mode classification identifying specific issues",
        "Created maintenance scheduling optimizing cost and reliability",
        "Implemented parts inventory optimization using RUL predictions",
        "Added anomaly detection flagging abnormal degradation patterns",
        "Built simulation testing maintenance policies"
      ],
      "tags": [
        "manufacturing",
        "prediction",
        "anomaly-detection"
      ]
    },
    {
      "id": "generic-12-archetype-selection",
      "title": "Summarization: Extract Sentences vs. Generate Synthesis",
      "category": "Archetype Selection",
      "companies_involved": [
        "Enterprise Document Systems",
        "Legal/Insurance AI"
      ],
      "initial_problem": "Users don't want abstracts or summaries\u2014they want specific facts with citations. Summarization compresses information, losing precision. What looks like a 'too long; didn't read' problem is actually a 'find the needle' problem. This is the core insight behind RAG (Retrieval-Augmented Generation).",
      "initial_assumptions": [
        "Assumption: The user wants the 'Gist' (Compression)."
      ],
      "why_it_fails": {
        "summary": "Signal: User asks 'What is the deductible?'",
        "detailed_failures": []
      },
      "first_principle_insight": "Extracting key sentences isn't true summarization. Generate new text that synthesizes the main points concisely.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Retrieval (RAG). They don't want a summary; they want a specific fact.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        },
        {
          "type": "primary",
          "citation": "Related to: RAG case studies (rag-memory-decomposition) in this database",
          "url": "https://arxiv.org/abs/2005.11401",
          "note": "This example demonstrates RAG principles"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: The user wants the 'Gist' (Compression).' to 'Retrieval (RAG). They don't want a summary; they want a specific fact.'",
      "how_to_reframe": {
        "old_atomic_unit": "Summarization as extractive selection - identifying and copying the most important sentences from the document",
        "new_atomic_unit": "Summarization as abstractive synthesis - generating new text that captures key concepts in a concise form, not limited to original wording"
      },
      "architectural_changes": [
        "Implemented encoder-decoder architecture for generation",
        "Added attention mechanisms focusing on key content",
        "Built abstractive generation creating novel phrasing",
        "Created length control targeting desired summary size",
        "Implemented factual consistency checks against source",
        "Added multi-document synthesis for comprehensive summaries",
        "Built personalization adapting to user reading level"
      ],
      "tags": [
        "NLP",
        "LLM",
        "enterprise"
      ]
    },
    {
      "id": "generic-13-archetype-selection",
      "title": "Chat with Data: Semantic Search vs. SQL Compilation",
      "category": "Archetype Selection",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Chat with your Data (SQL).",
      "initial_assumptions": [
        "Assumption: The model has memorized the database schema."
      ],
      "why_it_fails": {
        "summary": "Signal: Model hallucinates column names.",
        "detailed_failures": []
      },
      "first_principle_insight": "Don't just find relevant data - answer the question precisely. Convert natural language to SQL for exact results.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Tool Use (Agents). Don't guess SQL; use a 'List Tables' tool first.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: The model has memorized the database schema.' to 'Tool Use (Agents). Don't guess SQL; use a 'List Tables' tool first.'",
      "how_to_reframe": {
        "old_atomic_unit": "Data chat as semantic search - retrieving relevant data tables based on natural language queries",
        "new_atomic_unit": "Data chat as query compilation - translating natural language into executable SQL that returns precise answers with proper joins and aggregations"
      },
      "architectural_changes": [
        "Implemented natural language to SQL compilation",
        "Added schema-aware generation using database metadata",
        "Built query validation ensuring syntactic correctness",
        "Created join inference determining required table relationships",
        "Implemented aggregation and filtering from natural language",
        "Added query explanation showing data retrieved",
        "Built query optimization for performance"
      ],
      "tags": [
        "LLM",
        "analytics",
        "search"
      ]
    },
    {
      "id": "generic-14-archetype-selection",
      "title": "Recommendations: Similar Items vs. User Utility",
      "category": "Archetype Selection",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Recommend products (Rating).",
      "initial_assumptions": [
        "Assumption: Absolute value (4.1 stars) drives decision."
      ],
      "why_it_fails": {
        "summary": "Signal: User buys items rated 4.1 over 4.8.",
        "detailed_failures": []
      },
      "first_principle_insight": "Similar items might not be what users want. Consider context, diversity, novelty - optimize for overall satisfaction.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Ranking (Listwise). Optimize the order, not the star rating.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Absolute value (4.1 stars) drives decision.' to 'Ranking (Listwise). Optimize the order, not the star rating.'",
      "how_to_reframe": {
        "old_atomic_unit": "Recommendation as collaborative filtering - predicting ratings based on what similar users liked",
        "new_atomic_unit": "Recommendation as utility maximization - considering user preferences, context, diversity, novelty, and business objectives in a unified framework"
      },
      "architectural_changes": [
        "Implemented hybrid models combining collaborative and content signals",
        "Added contextual bandits for exploration-exploitation",
        "Built diversity re-ranking preventing repetitive suggestions",
        "Created novelty scoring promoting discovery",
        "Implemented business rule constraints (inventory, margins)",
        "Added multi-stakeholder optimization (user, creator, platform)",
        "Built explanation generation for recommendation transparency"
      ],
      "tags": [
        "recommendations",
        "personalization",
        "e-commerce"
      ]
    },
    {
      "id": "generic-15-archetype-selection",
      "title": "Fraud Detection: Transaction Features vs. Network Patterns",
      "category": "Archetype Selection",
      "companies_involved": [
        "Stripe",
        "PayPal",
        "Financial Services"
      ],
      "initial_problem": "Credit card fraud detection using transaction-level features (amount, location, time) treats each transaction independently. But fraudsters work in organized rings sharing devices, IPs, and identities. The signal is in the network topology, not individual transaction attributes.",
      "initial_assumptions": [
        "Assumption: Fraud is an individual act."
      ],
      "why_it_fails": {
        "summary": "Signal: Fraudsters work in rings/groups.",
        "detailed_failures": []
      },
      "first_principle_insight": "Fraudsters coordinate. Look at the connections between transactions - fraud rings reveal themselves through relationships.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Graph Neural Net. The signal is in the connections, not the transaction.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        },
        {
          "type": "primary",
          "citation": "Related to: Stripe fraud detection (stripe-fraud-graph-clustering) in this database",
          "note": "This generic example demonstrates the same principle as Stripe's graph clustering pivot"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Fraud is an individual act.' to 'Graph Neural Net. The signal is in the connections, not the transaction.'",
      "how_to_reframe": {
        "old_atomic_unit": "Fraud as anomaly on individual transaction - flagging transactions with unusual patterns compared to user's history",
        "new_atomic_unit": "Fraud as network pattern - detecting coordinated fraud rings by analyzing relationships and patterns across multiple accounts and merchants"
      },
      "architectural_changes": [
        "Built graph database linking accounts, cards, merchants, devices",
        "Implemented community detection identifying fraud rings",
        "Added graph features (centrality, clustering) as signals",
        "Created temporal pattern analysis across network",
        "Implemented label propagation spreading fraud signals",
        "Added graph-based anomaly detection spotting unusual connections",
        "Built investigation workflows following graph relationships"
      ],
      "tags": [
        "fraud-detection",
        "graph-neural-networks",
        "fintech"
      ]
    },
    {
      "id": "generic-16-archetype-selection",
      "title": "Inventory: Forecast Each SKU vs. Joint Optimization",
      "category": "Archetype Selection",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Predict Inventory levels.",
      "initial_assumptions": [
        "Assumption: Product features drive sales. (Actually Time drives sales)."
      ],
      "why_it_fails": {
        "summary": "Signal: Items are perishable (Milk) / Seasonal.",
        "detailed_failures": []
      },
      "first_principle_insight": "Don't forecast each item independently. Consider constraints (warehouse space), substitutes, and how items affect each other.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Forecasting (Time Series). Seasonality and trend matter more than features.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Product features drive sales. (Actually Time drives sales).' to 'Forecasting (Time Series). Seasonality and trend matter more than features.'",
      "how_to_reframe": {
        "old_atomic_unit": "Inventory as univariate forecast - predicting future demand for each SKU independently",
        "new_atomic_unit": "Inventory as constrained optimization - jointly optimizing stock levels across SKUs considering warehouse capacity, supplier constraints, and substitution effects"
      },
      "architectural_changes": [
        "Implemented joint optimization across all SKUs simultaneously",
        "Added warehouse capacity constraints in optimization",
        "Built supplier lead time and minimum order constraints",
        "Created substitution modeling for related products",
        "Implemented service level objectives (SLO) for stock-out rates",
        "Added cost optimization balancing holding and shortage costs",
        "Built demand correlation modeling for bundled items"
      ],
      "tags": [
        "optimization",
        "supply-chain",
        "prediction"
      ]
    },
    {
      "id": "generic-17-genai-and-eval",
      "title": "Model Confidence: Probability Score vs. Calibrated Uncertainty",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Harvey AI",
        "Glean",
        "Enterprise LLM Applications"
      ],
      "initial_problem": "LLM hallucination occurs when knowledge is compressed into weights. Fine-tuning stores facts as distributed patterns across billions of parameters. When queried, the model generates plausible-sounding text (pattern completion), not retrieval of actual documents. For enterprise applications requiring factual accuracy and citations, this is unacceptable.",
      "initial_assumptions": [
        "Assumption: Knowledge is stored in the Model Weights."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Knowledge Gap. Weights are stale.",
        "detailed_failures": []
      },
      "first_principle_insight": "High confidence doesn't mean correct. Use separate verification mechanisms - the model's probability isn't calibrated.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Add RAG (Retrieval). Do not fine-tune.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        },
        {
          "type": "primary",
          "citation": "Related to: RAG architecture (rag-memory-decomposition) in this database",
          "note": "This demonstrates the RAG vs fine-tuning distinction"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Knowledge is stored in the Model Weights.' to 'Add RAG (Retrieval). Do not fine-tune.'",
      "how_to_reframe": {
        "old_atomic_unit": "Confidence as probability score - treating the model's output probability as a measure of answer correctness",
        "new_atomic_unit": "Confidence as calibrated uncertainty - using separate mechanisms (chain-of-thought, verification, ensemble disagreement) to assess actual answer reliability"
      },
      "architectural_changes": [
        "Implemented chain-of-thought prompting for reasoning transparency",
        "Added verification models checking answer correctness",
        "Built ensemble disagreement as confidence signal",
        "Created calibration layers adjusting confidence scores",
        "Implemented uncertainty quantification from model internals",
        "Added human feedback on confidence accuracy",
        "Built confidence-based routing to human review"
      ],
      "tags": [
        "LLM",
        "uncertainty",
        "evaluation"
      ]
    },
    {
      "id": "generic-18-genai-and-eval",
      "title": "LLM Math: Pattern Matching vs. Tool Use",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Model fails 234 * 482.",
      "initial_assumptions": [
        "Assumption: Language Models can perform Logic."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Reasoning Gap. It's a next-token predictor, not a calculator.",
        "detailed_failures": []
      },
      "first_principle_insight": "LLMs guess at math from patterns. For arithmetic, use code execution or calculators - get exact answers, not approximations.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Tool Use. Call a Python calculator.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Language Models can perform Logic.' to 'Tool Use. Call a Python calculator.'",
      "how_to_reframe": {
        "old_atomic_unit": "Math as next-token prediction - generating arithmetic answers directly from pattern matching in training data",
        "new_atomic_unit": "Math as symbolic computation - using structured representations (code, tools, calculators) to ensure arithmetically correct answers"
      },
      "architectural_changes": [
        "Implemented code execution for arithmetic operations",
        "Added tool use calling calculator APIs for computation",
        "Built structured output formats for mathematical expressions",
        "Created verification checking computed vs generated answers",
        "Implemented symbolic math solvers for exact answers",
        "Added detection of mathematical questions routing to tools",
        "Built error handling for invalid computations"
      ],
      "tags": [
        "LLM",
        "reasoning",
        "agents"
      ]
    },
    {
      "id": "generic-19-genai-and-eval",
      "title": "Long Context: Full Document vs. Targeted Retrieval",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Prompt is 10 pages long.",
      "initial_assumptions": [
        "Assumption: Context can fix Behavior."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Fighting the weights.",
        "detailed_failures": []
      },
      "first_principle_insight": "Don't stuff the whole document in the prompt. Retrieve only the relevant passages for the specific question.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Fine-Tuning. Bake the rules/style into the model.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Context can fix Behavior.' to 'Fine-Tuning. Bake the rules/style into the model.'",
      "how_to_reframe": {
        "old_atomic_unit": "Context as full prompt - passing the entire 10-page document as context to the model",
        "new_atomic_unit": "Context as retrieval augmentation - extracting and passing only the relevant passages needed to answer the specific question"
      },
      "architectural_changes": [
        "Implemented dense retrieval finding relevant passages",
        "Added passage ranking by relevance to question",
        "Built context window management with top-k passages",
        "Created compression techniques for long documents",
        "Implemented sliding window processing for documents",
        "Added query expansion improving retrieval recall",
        "Built citation tracking linking answers to passages"
      ],
      "tags": [
        "LLM",
        "RAG",
        "retrieval"
      ]
    },
    {
      "id": "generic-20-genai-and-eval",
      "title": "RAG Retrieval: Embedding Similarity vs. Multi-Stage Ranking",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "RAG misses the specific doc.",
      "initial_assumptions": [
        "Assumption: Semantic similarity catches everything."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Vector collision (Semantic overlap).",
        "detailed_failures": []
      },
      "first_principle_insight": "Embedding similarity alone misses things. Combine with keyword matching, metadata filters, and reranking.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Hybrid Search. Add Keyword search (BM25) for specific IDs.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Semantic similarity catches everything.' to 'Hybrid Search. Add Keyword search (BM25) for specific IDs.'",
      "how_to_reframe": {
        "old_atomic_unit": "Retrieval as semantic similarity - finding documents whose embeddings are closest to the query embedding",
        "new_atomic_unit": "Retrieval as multi-stage ranking - combining semantic search with keyword matching, metadata filters, and reranking to surface the most relevant specific document"
      },
      "architectural_changes": [
        "Implemented multi-stage retrieval with coarse and fine ranking",
        "Added keyword matching for precise term recall",
        "Built metadata filtering (date, author, type)",
        "Created reranking models considering query-document interaction",
        "Implemented hybrid scoring combining multiple signals",
        "Added query understanding for intent classification",
        "Built user feedback improving retrieval quality"
      ],
      "tags": [
        "RAG",
        "retrieval",
        "ranking",
        "embeddings"
      ]
    },
    {
      "id": "generic-21-genai-and-eval",
      "title": "AI Agents: Free Exploration vs. Constrained Planning",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Agent loops forever.",
      "initial_assumptions": [
        "Assumption: The Agent can figure out the plan dynamically."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Planning failure. Task is too broad.",
        "detailed_failures": []
      },
      "first_principle_insight": "Unconstrained agents loop forever. Limit action space, add termination conditions, require verification.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Staged Pipeline. Hard-code Step 1 -> Step 2.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: The Agent can figure out the plan dynamically.' to 'Staged Pipeline. Hard-code Step 1 -> Step 2.'",
      "how_to_reframe": {
        "old_atomic_unit": "Agent as free exploration - allowing the agent to take any action in response to environment state",
        "new_atomic_unit": "Agent as constrained planner - limiting action space, enforcing termination conditions, and using verification to prevent infinite loops"
      },
      "architectural_changes": [
        "Implemented action space constraints limiting valid operations",
        "Added termination conditions preventing infinite loops",
        "Built step limits and timeouts for safety",
        "Created plan verification before execution",
        "Implemented rollback mechanisms for failed actions",
        "Added human-in-the-loop approval for critical actions",
        "Built monitoring alerting on unusual agent behavior"
      ],
      "tags": [
        "agents",
        "LLM",
        "safety"
      ]
    },
    {
      "id": "generic-22-genai-and-eval",
      "title": "User Trust: Better Answers vs. Transparency",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Users don't trust the bot.",
      "initial_assumptions": [
        "Assumption: The Answer is the product."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Lack of traceability.",
        "detailed_failures": []
      },
      "first_principle_insight": "Good answers aren't enough for trust. Show sources, explain reasoning, give confidence levels, allow human escalation.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Citation Mode. Force model to quote the source text.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: The Answer is the product.' to 'Citation Mode. Force model to quote the source text.'",
      "how_to_reframe": {
        "old_atomic_unit": "Trust as output quality - expecting users to trust the bot because it gives good answers",
        "new_atomic_unit": "Trust as transparency and control - providing sources, explanations, confidence scores, and human escalation to build user confidence"
      },
      "architectural_changes": [
        "Implemented source citation for all factual claims",
        "Added confidence scores displayed to users",
        "Built explanation generation showing reasoning process",
        "Created human escalation for uncertain responses",
        "Implemented audit trails tracking bot decisions",
        "Added user control over automation level",
        "Built feedback mechanisms for correcting errors"
      ],
      "tags": [
        "LLM",
        "UX",
        "evaluation"
      ]
    },
    {
      "id": "generic-23-genai-and-eval",
      "title": "LLM Cost: Pay Per Token vs. Workload Optimization",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Cost is skyrocketing.",
      "initial_assumptions": [
        "Assumption: Intelligence is needed for every step."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Processing useless documents.",
        "detailed_failures": []
      },
      "first_principle_insight": "Cost isn't just tokens. Use caching, compression, smaller models for simple tasks, and batching to reduce total cost.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Filtering/Reranking. Filter before the LLM call.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: Intelligence is needed for every step.' to 'Filtering/Reranking. Filter before the LLM call.'",
      "how_to_reframe": {
        "old_atomic_unit": "Cost as per-token pricing - treating cost as proportional to the number of tokens generated",
        "new_atomic_unit": "Cost as workload optimization - using caching, compression, model selection, and batching to reduce overall computational cost"
      },
      "architectural_changes": [
        "Implemented prompt caching for repeated patterns",
        "Added response compression removing redundant tokens",
        "Built model selection routing to appropriate model size",
        "Created batching amortizing fixed costs",
        "Implemented streaming reducing perceived latency",
        "Added speculative decoding for faster generation",
        "Built monitoring optimizing cost per request"
      ],
      "tags": [
        "LLM",
        "cost-optimization",
        "latency",
        "efficiency"
      ]
    },
    {
      "id": "generic-24-genai-and-eval",
      "title": "Model Bias: Better Training Data vs. Multi-Layer Mitigation",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Model is biased/toxic.",
      "initial_assumptions": [
        "Assumption: We can prompt-engineer safety."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Data poisoning / Safety risk.",
        "detailed_failures": []
      },
      "first_principle_insight": "Bias isn't just a data problem. Address through training, prompting, output filtering, human review, and monitoring.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Guardrails. Add a rule-based filter after generation.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: We can prompt-engineer safety.' to 'Guardrails. Add a rule-based filter after generation.'",
      "how_to_reframe": {
        "old_atomic_unit": "Bias as training data issue - treating model bias as something fixed by better data curation",
        "new_atomic_unit": "Bias as systemic mitigation - addressing bias through training data, prompt engineering, output filtering, human review, and continuous monitoring"
      },
      "architectural_changes": [
        "Implemented diverse training data across demographics",
        "Added bias detection classifiers flagging problematic outputs",
        "Built prompt engineering with fairness instructions",
        "Created output filtering removing biased content",
        "Implemented human review for sensitive domains",
        "Added continuous monitoring measuring bias metrics",
        "Built feedback loops improving from bias incidents"
      ],
      "tags": [
        "LLM",
        "fairness",
        "bias",
        "safety"
      ]
    },
    {
      "id": "generic-25-genai-and-eval",
      "title": "LLM Latency: Model Speed vs. Pipeline Optimization",
      "category": "GenAI & Eval",
      "companies_involved": [
        "Generic Example"
      ],
      "initial_problem": "Latency is 10 seconds.",
      "initial_assumptions": [
        "Assumption: One size fits all."
      ],
      "why_it_fails": {
        "summary": "Diagnosis: Model is too big for the task.",
        "detailed_failures": []
      },
      "first_principle_insight": "Latency isn't just inference time. Optimize the whole pipeline - preprocessing, batching, caching, model selection.",
      "reframe": {
        "new_atomic_unit": "Reframed based on first principle (see insight)",
        "new_problem_type": "See reframe description",
        "new_objective": "Distillation / Caching. Use a smaller model or cache common answers.",
        "architectural_changes": [],
        "results": "Successful reframe addressing the failure mode"
      },
      "sources": [
        {
          "type": "supplementary",
          "citation": "Research file: examples.txt",
          "note": "Generic teaching example from course materials"
        },
        {
          "type": "supplementary",
          "citation": "Claude AI knowledge base (December 2024 training)",
          "note": "Technical context and enrichment"
        }
      ],
      "key_lesson": "Reframe from 'Assumption: One size fits all.' to 'Distillation / Caching. Use a smaller model or cache common answers.'",
      "how_to_reframe": {
        "old_atomic_unit": "Latency as model execution time - treating response time as purely the time it takes to run inference on the model",
        "new_atomic_unit": "Latency as end-to-end pipeline - considering the full request lifecycle including preprocessing, batching, model execution, postprocessing, and network overhead"
      },
      "architectural_changes": [
        "Implemented profiling identifying latency bottlenecks",
        "Added model compression reducing inference time",
        "Built quantization using lower precision weights",
        "Created caching for common query patterns",
        "Implemented batching requests for efficiency",
        "Added model distillation to smaller faster models",
        "Built infrastructure optimization (GPU, serving framework)"
      ],
      "tags": [
        "LLM",
        "latency",
        "optimization",
        "production"
      ]
    },
    {
      "id": "shopify-taxonomy-pivot",
      "title": "Shopify: Rigid Taxonomy vs. Flexible Graph",
      "category": "Architectural Pivot - Data Representation (Classification)",
      "companies_involved": [
        "Shopify",
        "Taxonomy Team"
      ],
      "year": 2025,
      "initial_problem": "Organize millions of products into categories",
      "initial_assumptions": [
        "Products fit into a tree hierarchy (Clothing > Men's > Shirts)",
        "Each product belongs to one category path",
        "Categories are predefined and fixed"
      ],
      "why_it_fails": [
        "Products often fit multiple categories (a phone case is both 'Electronics' and 'Accessories')",
        "New product types don't fit existing tree",
        "Rigid hierarchy limits discovery and search"
      ],
      "first_principle_insight": "Products don't fit neatly into trees. Use a flexible graph where items can belong to multiple categories and have cross-cutting attributes.",
      "reframe": {
        "new_atomic_unit": "Structured Attribute Graph (Standardized Product Type + Attributes)",
        "new_problem_type": "Multi-Label Extraction / Knowledge Graph Construction",
        "new_objective": "Predict specific attribute-value pairs (Color=Red, Material=Leather, Style=Running) instead of a single class ID",
        "architectural_changes": [
          {
            "change": "From Classification to Extraction",
            "description": "Instead of a softmax classifier predicting 'Class 402', they built extraction models to pull structured facts from the description."
          },
          {
            "change": "Flattening the Hierarchy",
            "description": "They moved from a deep tree to a flat 'Product Type' combined with a rich 'Property' layer. This allows a product to exist in multiple search facets simultaneously."
          }
        ],
        "results": "Enabled 'Comprehensive Product Understanding'. Search filters became dynamic (e.g., filtering watches by 'Battery Life') rather than static categories."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Shopify Engineering Blog (2025): Evolution of Product Classification at Shopify",
          "url": "https://shopify.engineering/evolution-product-classification",
          "note": "CSV Row 55"
        }
      ],
      "key_lesson": "Taxonomies are human inventions, not physical truths. When 'Classification' fails because items don't fit in boxes, reframe to 'Attribute Extraction'.",
      "how_to_reframe": {
        "old_atomic_unit": "Category as tree path - each product in one branch",
        "new_atomic_unit": "Category as graph - products can have multiple relationships and attributes"
      },
      "architectural_changes": [
        "Replace tree with graph database",
        "Allow products to have multiple category relationships",
        "Add attribute-based navigation"
      ],
      "tags": [
        "e-commerce",
        "knowledge-graph",
        "classification",
        "NLP"
      ]
    },
    {
      "id": "pinterest-module-ranking",
      "title": "Pinterest: Ranking Items vs. Ranking Groups",
      "category": "Architectural Pivot - Ranking Granularity",
      "companies_involved": [
        "Pinterest",
        "Homefeed Engineering"
      ],
      "year": 2025,
      "initial_problem": "Show users a feed of interesting pins",
      "initial_assumptions": [
        "Rank each pin individually by predicted engagement",
        "Sort by score, show the top ones",
        "Best pins first = best experience"
      ],
      "why_it_fails": [
        "Top-scoring pins might all be similar (all food, all fashion)",
        "Users want variety, not just the 'best' items",
        "Individual ranking ignores page-level experience"
      ],
      "first_principle_insight": "Don't just rank items - design the page. Users want a diverse, interesting feed, not just the top-scoring items repeated.",
      "reframe": {
        "new_atomic_unit": "The Feed Module (Widget)",
        "new_problem_type": "Two-Stage Hierarchical Ranking",
        "new_objective": "Optimize the *sequence of modules* to maximize session length",
        "architectural_changes": [
          {
            "change": "Module Ranker",
            "description": "They introduced a 'Whole Page' optimization layer that ranks types of content (e.g., 'Inspiration' vs. 'Shopping' vs. 'Video') before ranking the items inside them."
          },
          {
            "change": "Vertical Blending",
            "description": "Instead of one big list, they treat different content types as different 'Verticals' and use a blending logic to mix them (e.g., 2 images, then 1 video, then 1 carousel)."
          }
        ],
        "results": "Increased engagement by ensuring diversity. The reframe moved the logic from 'What is the best Pin?' to 'What is the best Page composition?'."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2025): Module Relevance on Home Feed",
          "url": "https://medium.com/pinterest-engineering/module-relevance-on-homefeed-ae76f8b545b2",
          "note": "CSV Row 11"
        }
      ],
      "key_lesson": "Local Optimization (Best Item) often leads to Global Failure (Boring Feed). Reframe the atomic unit to the 'Container' or 'Session' to fix diversity.",
      "how_to_reframe": {
        "old_atomic_unit": "Individual pin ranking - score each pin independently",
        "new_atomic_unit": "Module/group ranking - optimize for the whole page experience including diversity"
      },
      "architectural_changes": [
        "Group pins into modules/sections",
        "Optimize for diversity across the page",
        "Balance individual relevance with variety"
      ],
      "tags": [
        "recommendations",
        "ranking",
        "e-commerce",
        "personalization"
      ]
    },
    {
      "id": "spotify-synthetic-metadata",
      "title": "Spotify: Cold Start - Wait for Data vs. Generate It",
      "category": "Architectural Pivot - GenAI for Data Enrichment",
      "companies_involved": [
        "Spotify",
        "Personalization Team"
      ],
      "year": 2024,
      "initial_problem": "Recommend new songs that have no listening history",
      "initial_assumptions": [
        "Need user interaction data to make recommendations",
        "New songs must wait until enough people listen",
        "Can't recommend what we don't have data on"
      ],
      "why_it_fails": [
        "New music sits unrecognized while waiting for data",
        "Creates chicken-and-egg: can't recommend without plays, can't get plays without recommendations",
        "Artists and users both suffer from cold start"
      ],
      "first_principle_insight": "Don't wait for interaction data. Use the audio itself to predict how users will respond, then bootstrap recommendations with synthetic data until real data accumulates.",
      "reframe": {
        "new_atomic_unit": "Synthetic Content Annotation",
        "new_problem_type": "GenAI-Powered Knowledge Extraction",
        "new_objective": "Use LLMs to read transcripts and generate dense metadata tags (Genre, Tone, Entities) *before* the recommender sees it",
        "architectural_changes": [
          {
            "change": "The Annotation Pipeline",
            "description": "Audio $\to$ Speech-to-Text $\to$ LLM Summarization $\to$ Topic Extraction. They turned an audio problem into a text processing problem."
          },
          {
            "change": "Dense Features",
            "description": "Instead of waiting for user clicks, the episode launches with tags like 'True Crime', 'Humor', 'Host: Joe Rogan'. The recommender matches these tags immediately."
          }
        ],
        "results": "Generated millions of annotations. Solved the Cold Start problem by creating synthetic features. The Pivot was using GenAI not as the *Product* (Chatbot) but as the *Data Pipeline*."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Spotify Engineering Blog (2024): How We Generated Millions of Content Annotations",
          "url": "https://engineering.atspotify.com/2024/10/how-we-generated-millions-of-content-annotations/",
          "note": "CSV Row 124"
        }
      ],
      "key_lesson": "If your model is failing due to lack of signal, don't tune the model. Use GenAI to *synthesize* the signal from raw data. GenAI is the ultimate Feature Engineer.",
      "how_to_reframe": {
        "old_atomic_unit": "Wait for real interaction data",
        "new_atomic_unit": "Generate synthetic data from audio features to bootstrap recommendations"
      },
      "architectural_changes": [
        "Analyze audio features to predict user preferences",
        "Generate synthetic interaction data for cold-start items",
        "Blend synthetic with real data as plays accumulate"
      ],
      "tags": [
        "LLM",
        "recommendations",
        "data-foundations"
      ]
    },
    {
      "id": "uber-finch-text-to-sql",
      "title": "Uber Finch: Pre-Built Dashboards vs. Ask Questions",
      "category": "Architectural Pivot - GenAI / Semantic Layer",
      "companies_involved": [
        "Uber",
        "Finch Team"
      ],
      "year": 2025,
      "initial_problem": "Let employees get data insights",
      "initial_assumptions": [
        "Build dashboards for common questions",
        "Data team creates reports for business users",
        "If you need data, request a dashboard"
      ],
      "why_it_fails": [
        "Can't anticipate every question",
        "New questions require new dashboard work",
        "Bottleneck on data team for every request"
      ],
      "first_principle_insight": "Don't build dashboards for every question. Let users ask questions in plain English and convert to SQL automatically.",
      "reframe": {
        "new_atomic_unit": "The Semantic Layer (Metrics Store)",
        "new_problem_type": "Text-to-SQL with Semantic Binding",
        "new_objective": "Map Natural Language $\to$ Semantic Definition $\to$ SQL",
        "architectural_changes": [
          {
            "change": "The Semantic Layer",
            "description": "They defined metrics (e.g., 'Gross Bookings') in code *once*. The LLM doesn't write raw SQL against messy tables; it queries the clean Semantic Layer."
          },
          {
            "change": "The Compiler Architecture",
            "description": "Replaced the 'Chatbot' paradigm (which hallucinates numbers) with a 'Compiler' paradigm (LLM generates query $\to$ System executes query $\to$ System returns exact number)."
          }
        ],
        "results": "Business users can ask 'Why' questions and get instant, verifiable answers without pinging a data analyst. Democratized data access by removing the SQL barrier."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Blog (2025): Unlocking Financial Insights with Finch",
          "url": "https://www.uber.com/en-IN/blog/unlocking-financial-insights-with-finch/",
          "note": "CSV Row 30"
        }
      ],
      "key_lesson": "Don't train LLMs to *be* analysts (hallucination risk). Train LLMs to *write code* for analysts. The Reframe is moving the intelligence from the 'Answer' to the 'Query'.",
      "how_to_reframe": {
        "old_atomic_unit": "Pre-built dashboard - data team builds reports",
        "new_atomic_unit": "Natural language query - ask questions, get SQL and answers"
      },
      "architectural_changes": [
        "Natural language to SQL conversion",
        "Self-serve data access without dashboard requests",
        "Schema-aware query generation"
      ],
      "tags": [
        "LLM",
        "analytics",
        "search"
      ]
    },
    {
      "id": "instacart-replacements-utility",
      "title": "Instacart: Visual Similarity vs. Functional Fit",
      "category": "Architectural Pivot - Recommendation Logic",
      "companies_involved": [
        "Instacart",
        "Search & Discovery"
      ],
      "year": 2024,
      "initial_problem": "Suggest replacement items when something is out of stock",
      "initial_assumptions": [
        "Similar-looking products make good replacements",
        "Same category = good substitute",
        "Match by visual/text features"
      ],
      "why_it_fails": [
        "Looks similar doesn't mean works the same (diet vs regular soda)",
        "User intent matters more than appearance",
        "A recipe needs specific ingredients, not just similar ones"
      ],
      "first_principle_insight": "Good replacements aren't about similarity - they're about function. What is the user trying to accomplish? That determines what works as a substitute.",
      "reframe": {
        "new_atomic_unit": "Substitution Probability (Policy)",
        "new_problem_type": "Knowledge Graph / Policy Ranking",
        "new_objective": "Predict P(User Accepts | Replacement) based on historical substitution data",
        "architectural_changes": [
          {
            "change": "From Embedding to Graph",
            "description": "They built a Knowledge Graph of products. 'Granny Smith' is linked to 'Apple' node. 'Gluten Free' is a hard attribute node."
          },
          {
            "change": "Feedback Loop Training",
            "description": "They trained the model specifically on 'Accepted Replacements' data (Human decisions) rather than generic product similarity. This captures the 'Human Logic' that vectors miss."
          }
        ],
        "results": "Higher acceptance rates and fewer refunds. The Pivot was realizing that 'Mathematical Closeness' (Vectors) $\neq$ 'Culinary Fit' (Graph)."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Instacart Tech Blog (2024): How Instacart Uses Machine Learning to Suggest Replacements",
          "url": "https://tech.instacart.com/how-instacart-uses-machine-learning-to-suggest-replacements-for-out-of-stock-products-8f80d03bb5af",
          "note": "CSV Row 118"
        }
      ],
      "key_lesson": "Vectors are lazy. They capture correlation, not logic. When 'Similarity' fails, reframe to 'Utility' using Knowledge Graphs or Behavioral Data.",
      "how_to_reframe": {
        "old_atomic_unit": "Visual/category similarity - looks like the original",
        "new_atomic_unit": "Functional utility - serves the same purpose for this user"
      },
      "architectural_changes": [
        "Model user intent and use case",
        "Consider dietary restrictions, brand preferences",
        "Learn from accepted/rejected replacements"
      ],
      "tags": [
        "recommendations",
        "e-commerce",
        "marketplace",
        "personalization"
      ]
    },
    {
      "id": "walmart-attribute-extraction",
      "title": "Walmart: Category Tags vs. Extracted Attributes",
      "category": "Architectural Pivot - GenAI as Data Infrastructure",
      "companies_involved": [
        "Walmart Global Tech",
        "Catalog Team"
      ],
      "year": 2024,
      "initial_problem": "Add structured attributes to product listings",
      "initial_assumptions": [
        "Classify products into predefined attribute tags",
        "Multi-label classification: does this product have attribute X?",
        "Predefined taxonomy covers what we need"
      ],
      "why_it_fails": [
        "Can't anticipate all attributes",
        "Product descriptions contain details not in taxonomy",
        "Classification misses specific values (size, material, etc.)"
      ],
      "first_principle_insight": "Don't classify into predefined tags. Extract the actual attribute values from the product description - it contains more detail than any taxonomy.",
      "reframe": {
        "new_atomic_unit": "Structured JSON Extraction",
        "new_problem_type": "Multi-Modal Generative Extraction",
        "new_objective": "Input (Image + Text) \u2192 Output (JSON Schema)",
        "architectural_changes": [
          {
            "change": "Unified Schema Generation",
            "description": "Instead of 50 models, they use one LLM prompted with the target schema. 'Extract these 10 fields from this PDF'."
          },
          {
            "change": "Constraint-Guided Decoding",
            "description": "They force the LLM to output valid JSON, ensuring the data fits the database structure immediately."
          }
        ],
        "results": "Reduced time-to-market for new attributes from months (training cycle) to minutes (prompt update). Collapsed the tech stack from hundreds of models to a single pipeline."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Walmart Tech Blog (2024): Extracting Product Attributes from PDFs using PAE Framework",
          "url": "https://medium.com/walmartglobaltech/extracting-product-attributes-from-pdfs-using-pae-framework-17889c73fdd4",
          "note": "CSV Row 171"
        }
      ],
      "key_lesson": "Stop building 'Specialist Classifiers' for data entry. Use GenAI as a 'Universal Reader' to convert unstructured data into structured data.",
      "how_to_reframe": {
        "old_atomic_unit": "Classification - assign predefined tags",
        "new_atomic_unit": "Extraction - pull specific values from product descriptions"
      },
      "architectural_changes": [
        "Extract attribute values from text, not just classify",
        "Handle free-form values, not just predefined categories",
        "Normalize extracted values for consistency"
      ],
      "tags": [
        "LLM",
        "NLP",
        "e-commerce",
        "data-foundations"
      ]
    },
    {
      "id": "pinterest-text-to-sql",
      "title": "Pinterest: Free-Form Chat vs. Structured Query",
      "category": "Architectural Pivot - LLM as Compiler",
      "companies_involved": [
        "Pinterest",
        "Data Engineering"
      ],
      "year": 2024,
      "initial_problem": "Let users ask questions about their data",
      "initial_assumptions": [
        "Let the LLM generate natural language answers",
        "Conversational AI should be free-form",
        "More flexible = better user experience"
      ],
      "why_it_fails": [
        "LLMs hallucinate facts",
        "No way to verify free-form answers",
        "Users need accurate data, not plausible-sounding responses"
      ],
      "first_principle_insight": "Don't let the LLM make up answers. Constrain it to generate SQL queries - then the answer comes from your actual data, not the model's imagination.",
      "reframe": {
        "new_atomic_unit": "Executable Code (SQL)",
        "new_problem_type": "Text-to-SQL Generation (Compiler)",
        "new_objective": "Translate Natural Language Intent \u2192 Valid SQL Syntax",
        "architectural_changes": [
          {
            "change": "Schema-Aware Prompting",
            "description": "Instead of feeding data rows, they feed the *Schema* (Table names, Column types) to the LLM."
          },
          {
            "change": "The Execution Sandbox",
            "description": "The LLM generates SQL. The system executes it against the data warehouse. The system returns the *exact* number. The LLM never touches the raw data."
          }
        ],
        "results": "Zero hallucination on numbers (because the DB does the math). Democratized access to petabytes of data without needing a data analyst."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2024): How we built Text-to-SQL at Pinterest",
          "url": "https://medium.com/pinterest-engineering/how-we-built-text-to-sql-at-pinterest-30bad30dabff",
          "note": "CSV Row 132"
        }
      ],
      "key_lesson": "Don't use LLMs to *do* the work (math/logic). Use LLMs to *write the code* that does the work. Reframe from 'Chatbot' to 'Natural Language Compiler'.",
      "how_to_reframe": {
        "old_atomic_unit": "Free-form generation - LLM writes an answer",
        "new_atomic_unit": "Structured compilation - LLM generates SQL that returns verified results"
      },
      "architectural_changes": [
        "Convert questions to SQL, not free-form answers",
        "Execute query against real database",
        "Show users the query so they can verify"
      ],
      "tags": [
        "LLM",
        "analytics",
        "NLP"
      ]
    },
    {
      "id": "github-copilot-remediation",
      "title": "GitHub: Flag Security Issues vs. Fix Them",
      "category": "Architectural Pivot - Agentic Workflow",
      "companies_involved": [
        "GitHub",
        "Microsoft Research"
      ],
      "year": 2024,
      "initial_problem": "Help developers write secure code",
      "initial_assumptions": [
        "Scan code and flag security vulnerabilities",
        "Alert developers to problems they need to fix",
        "Detection is the hard part"
      ],
      "why_it_fails": [
        "Developers get alert fatigue from too many warnings",
        "Knowing there's a problem doesn't mean knowing how to fix it",
        "Security issues pile up because fixing is manual work"
      ],
      "first_principle_insight": "Don't just find problems - fix them. Automatically generating the patch is more valuable than flagging the issue.",
      "reframe": {
        "new_atomic_unit": "Code Remediation Pattern",
        "new_problem_type": "Generative Code Repair (Agent)",
        "new_objective": "Input (Vulnerability Alert) \u2192 Output (Corrected Code Diff)",
        "architectural_changes": [
          {
            "change": "Closing the Loop",
            "description": "They connected the 'Detector' (CodeQL) directly to the 'Generator' (Copilot). The Alert becomes the Prompt."
          },
          {
            "change": "Iterative Verification",
            "description": "The system generates a fix, *runs the scanner again* to see if the alert is gone, and only then shows it to the user. This filters out hallucinations."
          }
        ],
        "results": "Drastically increased fix rates. Developers accept 'One-Click Fixes' at 3x the rate they act on 'Red Flags'. Reframed from 'Security Tool' to 'Coding Assistant'."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "GitHub Blog (2024): Fixing security vulnerabilities with AI",
          "url": "https://github.blog/2024-02-14-fixing-security-vulnerabilities-with-ai/",
          "note": "CSV Row 191"
        }
      ],
      "key_lesson": "Don't stop at 'Diagnosis' (Prediction). Push the architecture to 'Prescription' (Generation). The value is in the fix.",
      "how_to_reframe": {
        "old_atomic_unit": "Detection - flag the vulnerability",
        "new_atomic_unit": "Remediation - automatically suggest or apply the fix"
      },
      "architectural_changes": [
        "Generate fix alongside detection",
        "Auto-apply safe fixes, suggest review for complex ones",
        "Verify fix doesn't break functionality"
      ],
      "tags": [
        "agents",
        "code-generation",
        "security"
      ]
    },
    {
      "id": "linkedin-hiring-agent",
      "title": "LinkedIn: Search Filters vs. Hiring Workflow",
      "category": "Architectural Pivot - From Search to Agents",
      "companies_involved": [
        "LinkedIn",
        "Talent Solutions"
      ],
      "year": 2024,
      "initial_problem": "Help recruiters find and hire candidates",
      "initial_assumptions": [
        "Build better search and filters for candidates",
        "Help recruiters find more profiles faster",
        "Search is the main bottleneck"
      ],
      "why_it_fails": [
        "Finding candidates is just step 1 - outreach, screening, scheduling are the bottleneck",
        "Recruiters spend more time on process than searching",
        "Better search doesn't reduce the actual hiring workload"
      ],
      "first_principle_insight": "Search isn't the bottleneck - the whole workflow is. Automate outreach, screening, and scheduling, not just search.",
      "reframe": {
        "new_atomic_unit": "Agentic Task Chain",
        "new_problem_type": "Autonomous Agent (Plan/Execute)",
        "new_objective": "Take a Job Description \u2192 Deliver Interested Candidates",
        "architectural_changes": [
          {
            "change": "The Memory Layer",
            "description": "The agent remembers the recruiter's preferences ('I like candidates from these schools') and applies them automatically."
          },
          {
            "change": "Tool Use",
            "description": "The agent doesn't just 'retrieve'; it can 'Draft Message', 'Check Calendar', and 'Update ATS'. It acts as a coworker, not a search engine."
          }
        ],
        "results": "The 'Hiring Assistant' reduced time-to-hire by automating the repetitive middle-layer of the funnel. Reframed from 'Search Tool' to 'Hiring Partner'."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2024): The tech behind the first agent from LinkedIn",
          "url": "https://www.linkedin.com/blog/engineering/generative-ai/the-tech-behind-the-first-agent-from-linkedin-hiring-assistant",
          "note": "CSV Row 176"
        }
      ],
      "key_lesson": "If your user is taking your model output and doing 10 more manual steps, you have framed the problem too narrowly. Reframe from 'Information Retrieval' to 'Agentic Workflow'.",
      "how_to_reframe": {
        "old_atomic_unit": "Search tool - help find candidates",
        "new_atomic_unit": "Workflow agent - automate the whole hiring process"
      },
      "architectural_changes": [
        "Automate outreach with personalized messages",
        "Handle scheduling and follow-ups",
        "Screen candidates with initial questions"
      ],
      "tags": [
        "agents",
        "search",
        "enterprise",
        "automation"
      ]
    },
    {
      "id": "slack-private-rag",
      "title": "Slack: Search Everything vs. Respect Permissions",
      "category": "Architectural Pivot - RAG & Privacy",
      "companies_involved": [
        "Slack",
        "Salesforce"
      ],
      "year": 2024,
      "initial_problem": "Help users find information in Slack",
      "initial_assumptions": [
        "Index all messages for search/RAG",
        "Better retrieval = better answers",
        "More context is always better"
      ],
      "why_it_fails": [
        "Private channels contain sensitive information",
        "Users shouldn't see content they don't have access to",
        "RAG could leak private information into answers"
      ],
      "first_principle_insight": "Context is only useful if it's authorized. RAG must respect access controls - never retrieve content the user shouldn't see.",
      "reframe": {
        "new_atomic_unit": "Permissioned Context Window",
        "new_problem_type": "Scoped RAG / On-Demand Retrieval",
        "new_objective": "Summarize only what the user *already has access to*",
        "architectural_changes": [
          {
            "change": "No Global Index",
            "description": "Slack avoided training a global model on customer data. Instead, they built ephemeral indices."
          },
          {
            "change": "The 'Local' Reframe",
            "description": "Instead of 'Search Everything', the feature is framed as 'Summarize *This* Channel'. The RAG pipeline is scoped strictly to the current UI context or the user's specific graph permissions."
          }
        ],
        "results": "Launched a secure GenAI product that enterprise customers actually trusted. Reframed RAG from 'Search Engine' to 'Contextual Lens'."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Slack Engineering Blog (2024): How We Built Slack AI To Be Secure and Private",
          "url": "https://slack.engineering/how-we-built-slack-ai-to-be-secure-and-private/",
          "note": "CSV Row 106"
        }
      ],
      "key_lesson": "In Enterprise GenAI, Privacy is not a feature; it is the architecture. If you build a Global Index for a Permissioned world, you have failed.",
      "how_to_reframe": {
        "old_atomic_unit": "Global knowledge base - search everything",
        "new_atomic_unit": "Permission-aware retrieval - only retrieve what this user can access"
      },
      "architectural_changes": [
        "Filter retrieval by user permissions",
        "Check access rights before including in context",
        "Never leak private content into answers"
      ],
      "tags": [
        "RAG",
        "enterprise",
        "privacy",
        "security",
        "search"
      ]
    },
    {
      "id": "doordash-probabilistic-eta",
      "title": "DoorDash: Point Estimate ETA vs. Probability Range",
      "category": "Architectural Pivot - Statistical Signal (Uncertainty)",
      "companies_involved": [
        "DoorDash",
        "Logistics Engineering"
      ],
      "year": 2024,
      "initial_problem": "Tell customers when their delivery will arrive",
      "initial_assumptions": [
        "Predict a single ETA number (e.g., '30 minutes')",
        "Optimize for accuracy of the point estimate",
        "One number is what customers want"
      ],
      "why_it_fails": [
        "Point estimates hide uncertainty - '30 min' could mean 20-45 min range",
        "Being wrong by 5 minutes feels like a broken promise",
        "Can't set accurate expectations with a single number"
      ],
      "first_principle_insight": "Don't predict a single number when there's inherent uncertainty. Show the range of possible arrival times so customers can set realistic expectations.",
      "reframe": {
        "new_atomic_unit": "Quantile / Probability Distribution",
        "new_problem_type": "Probabilistic Forecasting (Quantile Regression)",
        "new_objective": "Predict the probability P(Time < T) to manage expectations",
        "architectural_changes": [
          {
            "change": "From Point to Distribution",
            "description": "Instead of outputting a scalar (35), the model outputs parameters of a distribution (e.g., Log-Normal parameters \u03bc and \u03c3) or specific quantiles (10th, 50th, 90th percentile)."
          },
          {
            "change": "Multi-Task Learning",
            "description": "They decomposed the problem into 'Action Duration' (Prep time) and 'Travel Duration' (Drive time) because they have different statistical variances, then combined them."
          }
        ],
        "results": "DoorDash improved 'On-Time' reliability not by driving faster, but by quoting ETAs that accounted for variance (e.g., quoting the 80th percentile for risky orders), reducing 'Late' complaints significantly."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering Blog (2024): Improving ETAs with Multi-Task Models, Deep Learning, and Probabilistic Forecasts",
          "url": "https://doordash.engineering/2024/03/12/improving-etas-with-multi-task-models-deep-learning-and-probabilistic-forecasts/",
          "note": "From CSV Row 138"
        }
      ],
      "key_lesson": "When the cost of error is asymmetric (Late > Early), never predict the Mean. Reframe to predict the Quantile (Risk).",
      "how_to_reframe": {
        "old_atomic_unit": "Point estimate - delivery will arrive in X minutes",
        "new_atomic_unit": "Probability distribution - 70% chance by 30 min, 90% by 40 min"
      },
      "architectural_changes": [
        "Predict distribution, not point estimate",
        "Show probability ranges to customers",
        "Set expectations based on confidence levels"
      ],
      "tags": [
        "prediction",
        "uncertainty",
        "marketplace",
        "UX"
      ]
    },
    {
      "id": "linkedin-dwell-time",
      "title": "LinkedIn: Clicks vs. Actual Reading Time",
      "category": "Architectural Pivot - Metric Reframe (The Proxy Trap)",
      "companies_involved": [
        "LinkedIn",
        "Feed AI Team"
      ],
      "year": 2024,
      "initial_problem": "Show users content they'll find valuable",
      "initial_assumptions": [
        "Clicks and likes indicate valuable content",
        "Optimize for engagement (clicks, shares)",
        "If they clicked, they liked it"
      ],
      "why_it_fails": [
        "Clickbait gets clicks but disappoints users",
        "Users click but immediately leave - that's not value",
        "Likes are cheap signals, don't mean real engagement"
      ],
      "first_principle_insight": "Clicks lie. Measure how long users actually spend with content - dwell time reveals real value better than clicks.",
      "reframe": {
        "new_atomic_unit": "Dwell Time (Seconds spent on screen)",
        "new_problem_type": "Regression (predicting time) + Classification (predicting action)",
        "new_objective": "Maximize 'Quality Dwell Time' (Time spent on content that wasn't skipped immediately)",
        "architectural_changes": [
          {
            "change": "The 'Skip' Negative Signal",
            "description": "They reframed 'Short Dwell Time' (< 2s) not just as 'No Signal', but as a 'Negative Signal' (Bounce). This penalized clickbait."
          },
          {
            "change": "Multi-Objective Optimization",
            "description": "The final ranking score became a weighted sum of pAction (Viral) and pDwell (Depth). This balanced 'Popularity' with 'Substance'."
          }
        ],
        "results": "Improved long-term retention and feed sentiment. Users saw fewer 'growth hacks' and more substantial professional content."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2024): Leveraging Dwell Time to Improve Member Experiences",
          "url": "https://www.linkedin.com/blog/engineering/feed/leveraging-dwell-time-to-improve-member-experiences-on-the-linkedin-feed",
          "note": "From CSV Row 228"
        }
      ],
      "key_lesson": "When your metric (Clicks) can be gamified, it is a bad metric. Reframe to a metric that is harder to fake: Time.",
      "how_to_reframe": {
        "old_atomic_unit": "Clicks and likes - did they interact?",
        "new_atomic_unit": "Dwell time - did they actually spend time reading/watching?"
      },
      "architectural_changes": [
        "Track time spent viewing content, not just clicks",
        "Optimize for dwell time in ranking model",
        "Filter out clickbait that gets clicks but no dwell"
      ],
      "tags": [
        "metrics",
        "recommendations",
        "evaluation"
      ],
      "canonical_case": true,
      "canonical_usage": {
        "role": "Secondary recurring example - the 'proxy metric trap'",
        "module_1_mindset": "The Feynman Test: 'What do you actually want users to do?' reveals Clicks \u2260 Value",
        "module_2_taxonomy": "Does the metric choice change the archetype? Classification vs Ranking vs Recommendation",
        "module_3_loop": "Outcome step forces clarity: what does success actually look like?",
        "module_4_diagnosis": "How do you detect you're optimizing the wrong proxy metric?",
        "module_5_pivot": "Classic Goodhart's Law: when the metric becomes the target, it ceases to be a good metric"
      },
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "cautionary"
        ],
        "transferable_patterns": [
          "Proxy metrics can diverge from actual value (Goodhart's Law)",
          "Easy-to-measure \u2260 right-to-optimize",
          "User behavior signals need interpretation, not just counting"
        ],
        "non_generalizable_context": [
          "Professional content has different engagement patterns than entertainment",
          "LinkedIn's specific feed algorithm constraints",
          "B2B context affects what 'engagement' means"
        ]
      }
    },
    {
      "id": "grab-graph-fraud",
      "title": "Grab: Individual Fraud Scoring vs. Fraud Ring Detection",
      "category": "Architectural Pivot - Signal Reframe (Graph)",
      "companies_involved": [
        "Grab",
        "Integrity Team"
      ],
      "year": 2023,
      "initial_problem": "Detect fraud in ride-hailing and payments",
      "initial_assumptions": [
        "Score each transaction/account individually for fraud risk",
        "Train supervised model on labeled fraud cases",
        "Fraud is a property of individual bad actors"
      ],
      "why_it_fails": [
        "Sophisticated fraud involves coordinated rings, not individuals",
        "Individual transactions in a fraud ring look normal",
        "Supervised models only catch fraud patterns you've seen before"
      ],
      "first_principle_insight": "Fraudsters coordinate. Look for unusual clusters and connections in the network - fraud rings reveal themselves through their relationships, not individual transactions.",
      "reframe": {
        "new_atomic_unit": "Graph Structure (Nodes & Edges)",
        "new_problem_type": "Unsupervised Graph Anomaly Detection",
        "new_objective": "Detect nodes with high centrality or density in the device-sharing graph",
        "architectural_changes": [
          {
            "change": "From Tabular to Graph",
            "description": "They stopped looking at rows. They built a graph where Nodes = Users/Devices and Edges = Shared IP/Location."
          },
          {
            "change": "Unsupervised Clustering",
            "description": "They applied algorithms (like Louvain or Eigenvector Centrality) to find 'tightly knit groups'. Normal users don't share devices with 50 people. Fraudsters do."
          }
        ],
        "results": "Detected new fraud rings *weeks* before supervised models could capture them. The system caught 'Day Zero' attacks because the *structure* of fraud (collusion) didn't change, even if the features did."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Grab Engineering Blog (2023): Unsupervised graph anomaly detection - Catching new fraudulent behaviours",
          "url": "https://engineering.grab.com/graph-anomaly-model",
          "note": "From CSV Row 283"
        },
        {
          "type": "supplementary",
          "citation": "Related to: Stripe fraud detection (stripe-fraud-graph-clustering) in this database",
          "note": "Similar graph-based fraud detection principle"
        }
      ],
      "key_lesson": "If your problem is 'Adversarial' (the enemy changes tactics), Supervised Learning will always be too slow. Reframe to Unsupervised/Structure to catch the fundamental behavior.",
      "how_to_reframe": {
        "old_atomic_unit": "Individual transaction/account scoring",
        "new_atomic_unit": "Network analysis - detect fraud rings through unusual connection patterns"
      },
      "architectural_changes": [
        "Build graph of account/transaction relationships",
        "Use unsupervised clustering to find fraud rings",
        "Detect anomalous network patterns without labeled data"
      ],
      "tags": [
        "fraud-detection",
        "graph-neural-networks",
        "fintech",
        "anomaly-detection"
      ]
    },
    {
      "id": "swiggy-address-dissonance",
      "title": "Swiggy: GPS Location vs. User Intent",
      "category": "Architectural Pivot - Data Quality Reframe",
      "companies_involved": [
        "Swiggy",
        "Data Science Team"
      ],
      "year": 2024,
      "initial_problem": "Get deliveries to the right address",
      "initial_assumptions": [
        "GPS coordinates are ground truth for delivery location",
        "Trust the sensor data over user input",
        "Technology (GPS) is more accurate than humans typing"
      ],
      "why_it_fails": [
        "GPS can be wrong (bad signal, wrong side of building)",
        "Users know where they are better than sensors do",
        "Typed address reflects intent, GPS reflects device position"
      ],
      "first_principle_insight": "Trust user intent over sensor data. When GPS and typed address conflict, the user knows where they want their food delivered better than their phone's GPS does.",
      "reframe": {
        "new_atomic_unit": "Address-Pin Dissonance Score",
        "new_problem_type": "Multi-Modal Classification (NLP + Geospatial)",
        "new_objective": "Predict: Is the Pin > 100m away from the Text Address?",
        "architectural_changes": [
          {
            "change": "The Dissonance Classifier",
            "description": "They built a model that takes the Text Address (Geocoded) and compares it to the User Pin. If the distance is huge, it flags 'Location Inaccuracy'."
          },
          {
            "change": "UI Intervention (The Fix)",
            "description": "Instead of trying to route the driver to a bad pin, the system intervenes *at checkout*. It prompts the user: 'Your pin looks far from your address. Please adjust it.'."
          }
        ],
        "results": "Reduced 'Last Mile' calls (drivers calling customers) significantly. They solved a 'Routing' problem by reframing it as a 'Data Entry' problem."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Swiggy Bytes (2024): Address Correction for Q-Commerce Part 1: Location Inaccuracy Classifier",
          "url": "https://bytes.swiggy.com/address-correction-for-q-commerce-part-1-location-inaccuracy-classifier-e72b88a33d2f",
          "note": "From CSV Row 144"
        }
      ],
      "key_lesson": "Hardware sensors (GPS, Cameras) lie. User intent (Text) lies less. When they disagree, trust the Intent and force the user to correct the Sensor.",
      "how_to_reframe": {
        "old_atomic_unit": "GPS coordinates as ground truth",
        "new_atomic_unit": "User-entered address as ground truth - what they typed is what they meant"
      },
      "architectural_changes": [
        "Prioritize typed address over GPS",
        "Use GPS as hint, not ground truth",
        "Ask user to confirm when there's a conflict"
      ],
      "tags": [
        "marketplace",
        "data-foundations",
        "NLP",
        "data-quality"
      ]
    },
    {
      "id": "netflix-causal-inference",
      "title": "Netflix: Correlation vs. Causation in Recommendations",
      "category": "Architectural Pivot - Correlation to Causality",
      "companies_involved": [
        "Netflix",
        "Causal Inference Team"
      ],
      "year": 2024,
      "initial_problem": "Understand what drives user engagement",
      "initial_assumptions": [
        "If X correlates with engagement, optimizing for X helps",
        "Prediction accuracy = understanding what works",
        "Patterns in data reveal what to do"
      ],
      "why_it_fails": [
        "Correlation isn't causation - showing more of what users watched doesn't mean it caused satisfaction",
        "Confounding variables hide true drivers",
        "Predictive models don't tell you what to change"
      ],
      "first_principle_insight": "Prediction tells you what will happen, not what to do. Use causal inference to understand what actually drives behavior, not just what correlates with it.",
      "reframe": {
        "new_atomic_unit": "The Treatment Effect (Average Treatment Effect on the Treated)",
        "new_problem_type": "Causal Inference / Double Machine Learning",
        "new_objective": "Estimate the causal impact of a decision, removing confounding variables",
        "architectural_changes": [
          {
            "change": "Instrumental Variables",
            "description": "Using quasi-experiments (like natural variation in localized marketing) to measure impact where A/B tests are impossible."
          },
          {
            "change": "Double ML",
            "description": "Using one model to predict the outcome (Retention) and another to predict the treatment (Who got the ad), then subtracting the residuals to find the true causal driver."
          }
        ],
        "results": "Netflix can now measure true incremental value of features and content, not just correlation. This prevents investing in features that appear successful but don't actually cause retention."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Netflix Tech Blog (2024): Causal Inference and Machine Learning",
          "url": "https://netflixtechblog.com/",
          "note": "From CSV Row - Netflix causal ML work"
        },
        {
          "type": "supplementary",
          "citation": "Related to: Booking.com experimentation (booking-offline-metrics-to-rct) in this database",
          "note": "Both emphasize experimentation over offline metrics"
        }
      ],
      "key_lesson": "Correlation \u2260 Causation. When you need to make decisions ('Should we build this?'), prediction is not enough. Reframe to causal inference to measure true incremental value.",
      "how_to_reframe": {
        "old_atomic_unit": "Prediction - what will users do?",
        "new_atomic_unit": "Causal inference - what CAUSES users to engage?"
      },
      "architectural_changes": [
        "Use A/B tests and causal methods, not just correlations",
        "Measure impact of changes, not just predictions",
        "Distinguish 'predicts engagement' from 'causes engagement'"
      ],
      "tags": [
        "causal-inference",
        "recommendations",
        "evaluation"
      ]
    },
    {
      "id": "shopify-product-classification-2025",
      "title": "Shopify: Deconstructing the 'Category' (Classification vs. Comprehensive Understanding)",
      "category": "Optimization Pivot",
      "subcategory": "Problem Formulation",
      "companies_involved": [
        "Shopify"
      ],
      "year": 2023,
      "initial_problem": "Understanding millions of diverse products (from jewelry to industrial equipment) to enable better search, discovery, and recommendations",
      "initial_assumptions": [
        "Basic categorization (logistic regression with TF-IDF classifier) sufficed for product understanding",
        "Text and images could be processed independently (modality separation acceptable)",
        "Category labels alone were comprehensive enough for product understanding"
      ],
      "why_it_fails": [
        "Traditional ML struggled with the increasing complexity and diversity of products",
        "Semantic gaps: category classification alone wasn't enough to fully understand products",
        "Missing metadata: no systematic extraction of attributes, descriptions, or product characteristics across the catalog"
      ],
      "first_principle_insight": "Gap between 'what products are' (their category) versus 'what products need to be understood as' (category + attributes + metadata within standardized taxonomies)",
      "how_to_reframe": {
        "old_atomic_unit": "Product \u2192 Category label",
        "new_atomic_unit": "Product \u2192 (Category + Attributes + Metadata)",
        "new_problem_type": "Multi-stage extraction: category prediction followed by context-aware attribute extraction"
      },
      "architectural_changes": [
        "Two-stage pipeline: Stage 1 (Category prediction) \u2192 Stage 2 (Attribute prediction dependent on Stage 1)",
        "Vision Language Model for multi-modal reasoning combining image and text",
        "FP8 quantization for reduced memory footprint",
        "In-flight batching replacing fixed batch sizes with dynamic arrival-pattern grouping",
        "KV cache reusing attention patterns across sequential predictions",
        "Multi-LLM annotation with arbitration system resolving disagreements, plus human validation"
      ],
      "evidence": {
        "metrics": "Processing over 30 million predictions daily with 85% acceptance rate of predicted categories",
        "deployment": "Production system at scale across entire Shopify catalog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Shopify Engineering Blog (2023): Evolution of Product Classification at Shopify",
          "url": "https://shopify.engineering/evolution-product-classification",
          "note": "Detailed blog post from Shopify engineers describing the evolution from traditional ML to VLM-based comprehensive product understanding"
        }
      ],
      "related_cases": [
        "walmart-attribute-extraction"
      ],
      "tags": [
        "classification",
        "LLM",
        "multimodal",
        "e-commerce"
      ]
    },
    {
      "id": "yelp-query-understanding-2025",
      "title": "Yelp: Deconstructing 'Query Processing' (Fragmented Pipelines vs. Unified Semantic Understanding)",
      "category": "Architectural Pivot",
      "subcategory": "System Architecture",
      "companies_involved": [
        "Yelp"
      ],
      "year": 2025,
      "initial_problem": "Understanding user search intent across multiple dimensions (category, specific dish, particular business) using fragmented traditional NLP systems",
      "initial_assumptions": [
        "Traditional NLP techniques (entity recognition, text similarity) were sufficient for query understanding",
        "Hand-crafted rule sets could handle query understanding tasks",
        "Fragmented pipelines (segmentation, spell correction, canonicalization) were acceptable despite being stitched together"
      ],
      "why_it_fails": [
        "Taxonomy brittleness: legacy models required understanding intricate details of internal taxonomy that are both unintuitive and subject to change",
        "Limited semantic understanding: traditional similarity models struggled with conceptual expansion (e.g., seafood \u2192 fresh fish, salmon roe)",
        "Cost-latency tradeoff: powerful models (GPT-4) worked well but weren't viable for real-time, millions-of-query production systems"
      ],
      "first_principle_insight": "Query distribution follows power law - a small number of queries are very popular. This enables pre-computation via caching to cover substantial traffic without prohibitive cost.",
      "how_to_reframe": {
        "old_atomic_unit": "Individual task pipelines (segmentation \u2260 spell-correction \u2260 canonicalization)",
        "new_atomic_unit": "Combined prompts leveraging unified semantic understanding",
        "new_problem_type": "Multi-tier inference strategy with power-law aware caching"
      },
      "architectural_changes": [
        "Multi-tier inference strategy: GPT-4 for prototyping \u2192 fine-tuned GPT-4o-mini for offline pre-computation \u2192 BERT/T5 for real-time tail queries",
        "RAG integration: augmented inputs with business names and category predictions to ground semantic reasoning",
        "Cached signal datastores: leveraged power-law distribution to serve 95%+ traffic from pre-computed key-value caches",
        "Unified prompts combining previously separate tasks (spell-correction and segmentation done together)"
      ],
      "evidence": {
        "metrics": "95%+ of traffic served from pre-computed caches",
        "deployment": "Production system handling millions of queries"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Yelp Engineering Blog (2025): Search Query Understanding with LLMs: From Ideation to Production",
          "url": "https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html",
          "note": "Detailed engineering blog post describing the journey from fragmented NLP systems to unified LLM-based query understanding"
        }
      ],
      "related_cases": [],
      "tags": [
        "search",
        "LLM",
        "efficiency",
        "RAG"
      ]
    },
    {
      "id": "trivago-smart-search-2024",
      "title": "Trivago: Deconstructing 'AI Search' (Standalone Feature vs. Integrated Semantic Layer)",
      "category": "Architectural Pivot",
      "subcategory": "System Integration",
      "companies_involved": [
        "Trivago"
      ],
      "year": 2024,
      "initial_problem": "Enable natural language hotel search rather than traditional filter-based queries, a vision identified in 2014 but technically infeasible until recent LLM advances",
      "initial_assumptions": [
        "AI technology maturity would enable free-text search capabilities",
        "Deep integration of AI into core search would differentiate from competitors",
        "User education could train customers to articulate preferences naturally",
        "Cloud partnership would accelerate development and reduce constraints"
      ],
      "why_it_fails": [
        "Many users initially struggled to articulate their hotel preferences in free text",
        "Users needed to browse hotels first before expressing what they wanted",
        "The technical challenge (making AI work) was only half the equation - the other half was designing an interaction model"
      ],
      "first_principle_insight": "The pivotal realization shifted focus from pure AI capability to interaction design. The challenge was to design an interaction model that felt natural despite technical power.",
      "how_to_reframe": {
        "old_atomic_unit": "AI as standalone feature (small improvements such as filter translation)",
        "new_atomic_unit": "AI as integrated semantic layer combined with traditional systems",
        "new_problem_type": "Hybrid orchestration treating AI as one layer among several optimization mechanisms"
      },
      "architectural_changes": [
        "Hybrid pipeline combining AI-powered semantic search with traditional keyword matching",
        "Multi-component orchestration: semantic search + filtering + data pre-processing + post-filtering",
        "AI positioned as semantic layer rather than primary search engine",
        "Integration of traditional systems with AI rather than replacement"
      ],
      "evidence": {
        "deployment": "Production smart AI search system launched in 2024",
        "evolution": "Pivoted from AI-only approach to hybrid system after UX discovery"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Trivago Tech Blog (2024): Behind trivago's Smart AI Search: From Concept to Reality",
          "url": "https://tech.trivago.com/post/2024-12-17-behind-trivagos-ai-search-from-concept-to-reality",
          "note": "Blog post from Trivago engineering describing the 10-year journey from vision to production smart AI search"
        }
      ],
      "related_cases": [],
      "tags": [
        "search",
        "LLM",
        "hybrid-systems",
        "UX",
        "analytics",
        "marketplace"
      ]
    },
    {
      "id": "wayfair-wilma-copilot-2024",
      "title": "Wayfair: Deconstructing 'Prompting' (Monolithic vs. Decomposed Cognitive Steps)",
      "category": "Architectural Pivot",
      "subcategory": "Prompt Engineering",
      "companies_involved": [
        "Wayfair"
      ],
      "year": 2024,
      "initial_problem": "Customer service agents needed to balance customer needs with business policies while maintaining empathy - a cognitively demanding task requiring knowledge of hundreds of Wayfair policies",
      "initial_assumptions": [
        "Single unified prompt could handle all conversation scenarios",
        "One 'Help Me Write It' button would suffice for agent control",
        "A monolithic LLM call could manage complex negotiation contexts"
      ],
      "why_it_fails": [
        "Cognitive confusion: The LLM often got confused by all the content in the prompt and would follow examples and instructions that were irrelevant to the current situation",
        "Agent disempowerment: The agents felt they lacked control over the direction of the conversation",
        "Resolution hallucination: LLMs offered inappropriate resolutions such as replacement parts for a pillow during extended negotiations"
      ],
      "first_principle_insight": "Complex problem-solving isn't monolithic - it requires decomposition into sequential, specialized cognitive steps rather than one comprehensive inference pass",
      "how_to_reframe": {
        "old_atomic_unit": "Single prompt + single LLM call \u2192 single response",
        "new_atomic_unit": "Four specialized LLM calls (routing \u2192 proposal \u2192 suitability \u2192 current resolution) + four UI buttons \u2192 contextually appropriate suggestions",
        "new_problem_type": "LLM orchestration with sequential specialized inference steps"
      },
      "architectural_changes": [
        "Prompt multiplication: Over 40 different prompt templates using Jinja templating",
        "LLM orchestration: Sequential routing, proposal analysis, suitability filtering, and resolution tracking",
        "Agent control granularity: Four context-specific buttons (Discovery, Resolution, Empathy, Give Me a Minute) replacing one generic button",
        "Real-time data integration: Dynamic context pulling from Wayfair systems (customer, order, product data)"
      ],
      "evidence": {
        "evolution": "Moved from single monolithic prompt to decomposed multi-step system based on agent feedback",
        "deployment": "Production copilot system handling customer service interactions"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Wayfair Tech Blog (2024): The Evolution of Wilma, Wayfair's Customer Service Agent Copilot",
          "url": "https://www.aboutwayfair.com/careers/tech-blog/the-evolution-of-wilma-wayfairs-customer-service-agent-copilot",
          "note": "Engineering blog post describing the evolution from monolithic prompting to decomposed cognitive steps"
        }
      ],
      "related_cases": [],
      "tags": [
        "LLM",
        "prompt-engineering",
        "copilot",
        "customer-support",
        "agents"
      ]
    },
    {
      "id": "grab-grabgpt-2024",
      "title": "Grab: Deconstructing the 'Problem Scope' (Departmental Tool vs. Organizational Platform)",
      "category": "Pragmatic Pivot",
      "subcategory": "Scope Expansion",
      "companies_involved": [
        "Grab"
      ],
      "year": 2024,
      "initial_problem": "ML Platform team faced overwhelming support demand - Slack channels flooded with questions and on-call engineers spending more time answering repetitive queries than building innovative solutions",
      "initial_assumptions": [
        "LLMs could solve support ticket volume by automating Q&A responses",
        "Documentation could be compressed into context windows",
        "A documentation-focused chatbot would deliver sufficient value for the team"
      ],
      "why_it_fails": [
        "Token limitations: GPT-3.5-turbo could only handle 8,000 tokens (~2,000 words) while documentation exceeded 20,000 words",
        "Aggressive summarization to under 800 words yielded poor results",
        "Embedding-based search also underperformed, leading them to give up on this idea",
        "Even if documentation worked, value would be limited to one team"
      ],
      "first_principle_insight": "Rather than solving a narrow departmental problem, recognized broader organizational need: Grab doesn't have its own ChatGPT-like tool yet. This shifted from solving support tickets to enabling universal AI access.",
      "how_to_reframe": {
        "old_atomic_unit": "Documentation-retrieval chatbot for one team's support tickets",
        "new_atomic_unit": "Company-wide LLM interface providing general-purpose AI access with security guarantees",
        "new_problem_type": "Internal AI platform with governance, security, and multi-model support"
      },
      "architectural_changes": [
        "Leveraged existing 'catwalk' model-serving infrastructure",
        "Implemented Google authentication for company-wide access control",
        "Enabled multi-model support (OpenAI, Claude, Gemini) for flexibility",
        "Added audit trails for governance compliance",
        "Private infrastructure ensuring data security and preventing external data leakage"
      ],
      "evidence": {
        "metrics": "300 users on day one; 3,000+ users within three months",
        "deployment": "Company-wide production platform replacing initial narrow departmental solution"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Grab Engineering Blog (2024): From failure to success: The birth of GrabGPT, Grab's internal ChatGPT",
          "url": "https://engineering.grab.com/the-birth-of-grab-gpt",
          "note": "Engineering blog post describing the pivot from failed documentation chatbot to successful company-wide LLM platform"
        }
      ],
      "related_cases": [],
      "tags": [
        "LLM",
        "production",
        "pragmatic-pivot",
        "enterprise"
      ]
    },
    {
      "id": "snap-ads-understanding-2024",
      "title": "Snap: Deconstructing 'Representation' (Compressed Embeddings vs. Natural Language)",
      "category": "Optimization Pivot",
      "subcategory": "Representation Learning",
      "companies_involved": [
        "Snap"
      ],
      "year": 2024,
      "initial_problem": "Review massive volumes of daily ads at scale while maintaining quality, safety, and relevance across moderation, policy compliance, and targeting",
      "initial_assumptions": [
        "Embedding methods (like CLIP) effectively compress visual content into single vector representations",
        "Single vectors provide general-purpose semantic alignment sufficient for all downstream tasks",
        "Embeddings enable efficient, scalable retrieval across diverse use cases"
      ],
      "why_it_fails": [
        "Loss of context: Embedding methods compress videos or dynamic visuals into single vector representations, losing important context such as scene changes, sequential frames, or essential text overlays",
        "Uninterpretable representations: Embeddings produced opaque high-dimensional vectors obscuring why downstream models failed",
        "Difficult annotation: The system couldn't generate explicit labels or structured attributes, making queries like 'all ads with 50% off' impossible without expensive retraining",
        "Poor semantic discrimination: Empirical case showed CLIP similarity scores for random strings (0.2586) nearly identical to correct terms (0.2603)"
      ],
      "first_principle_insight": "Empirical Spectacles ad case study revealed fundamental precision problems. Random text scored nearly identically to semantically correct text, demonstrating embeddings lacked meaningful semantic discrimination.",
      "how_to_reframe": {
        "old_atomic_unit": "Single compressed embedding per video",
        "new_atomic_unit": "Rich, structured natural language descriptions capturing brands, products, contexts, and key events depicted in the ad",
        "new_problem_type": "Multi-modal generation producing interpretable textual representations"
      },
      "architectural_changes": [
        "Sample representative frames from videos for visual content",
        "Transcribe audio content for spoken information",
        "Generate natural language descriptions from inputs using multimodal generative model",
        "Store descriptions in both relational and vector databases for flexible retrieval",
        "Enable rapid feature expansion through text parsing rather than model retraining"
      ],
      "evidence": {
        "metrics": "Text-based similarity improved from 0.2603 to 0.4559 for 'Spectacles' term (vs 0.2586 to 0.1931 for random text)",
        "deployment": "Production system processing daily ad volumes with improved precision"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Snap Engineering Blog (2024): Snap Ads Understanding: From Pixels to Words",
          "url": "https://eng.snap.com/snap-ads-understanding",
          "note": "Engineering blog post describing the pivot from embedding-based to natural language-based ad understanding"
        }
      ],
      "related_cases": [],
      "tags": [
        "embeddings",
        "NLP",
        "multimodal",
        "advertising"
      ]
    },
    {
      "id": "grammarly-delicate-text-2024",
      "title": "Grammarly: Deconstructing 'Harm' (Toxicity vs. Delicate Content)",
      "category": "Optimization Pivot",
      "subcategory": "Problem Definition",
      "companies_involved": [
        "Grammarly"
      ],
      "year": 2024,
      "initial_problem": "Protecting users from harmful communications in writing assistance, traditionally approached through toxicity detection frameworks",
      "initial_assumptions": [
        "Toxicity detection frameworks were sufficient for protecting users from harmful communications",
        "'Toxic' and harmful content categories were largely synonymous",
        "Existing datasets and models could be adapted to cover sensitive topics"
      ],
      "why_it_fails": [
        "HateBERT achieved 95.2% precision but only 6.0% recall - missing most delicate content",
        "Models were either over-permissive or missed critical categories entirely",
        "Standard approaches missed coverage, specifically on medical and mental health topics",
        "Lower precision on examples containing offensive keywords that weren't deemed delicate",
        "Text about race, violence, or sexuality could be non-toxic yet emotionally triggering"
      ],
      "first_principle_insight": "'Toxic' and 'delicate' are fundamentally different concepts. Text can be non-toxic yet emotionally triggering, requiring a distinct detection framework entirely separate from profanity-based systems.",
      "how_to_reframe": {
        "old_atomic_unit": "Toxicity as proxy for harmful content",
        "new_atomic_unit": "Delicate text as emotionally charged, potentially triggering content with variable risk levels",
        "new_problem_type": "Multi-class delicate text detection with risk-level gradations"
      },
      "architectural_changes": [
        "Created DeTexD benchmark dataset (40,000 training + 1,023 evaluation samples)",
        "Developed RoBERTa-based classifier fine-tuned specifically for delicate content detection",
        "Implemented two-step annotation: binary delicate/non-delicate classification, then risk-level rating",
        "Built specialized keyword dictionary with severity ratings targeting sensitive topics",
        "Separate detection framework independent of toxicity models"
      ],
      "evidence": {
        "metrics": "Improved from 6.0% recall to new benchmark with dedicated delicate text dataset",
        "deployment": "New detection framework addressing medical, mental health, and emotionally sensitive topics"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Grammarly Engineering Blog (2024): Detecting Delicate Text: Going Beyond Toxicity",
          "url": "https://www.grammarly.com/blog/engineering/detecting-delicate-text/",
          "note": "Research blog post introducing the concept of delicate text detection as distinct from toxicity"
        }
      ],
      "related_cases": [],
      "tags": [
        "NLP",
        "safety",
        "content-moderation",
        "classification"
      ]
    },
    {
      "id": "dropbox-chatgpt-attacks-2024",
      "title": "Dropbox: Deconstructing 'Attack Vectors' (Character Repetition vs. Token Sequences)",
      "category": "Reasoning Pivot",
      "subcategory": "Vulnerability Analysis",
      "companies_involved": [
        "Dropbox",
        "OpenAI"
      ],
      "year": 2024,
      "initial_problem": "Discovered a prompt injection vulnerability in ChatGPT models triggered by repeated character sequences in user-controlled portions of prompt templates",
      "initial_assumptions": [
        "The vulnerability stemmed from repeated characters rather than underlying linguistic units",
        "Single-character repetition was the primary attack vector",
        "OpenAI's single-token filtering would adequately address the threat"
      ],
      "why_it_fails": [
        "After OpenAI deployed single-token filtering in November 2023, discovered that models were still vulnerable under certain circumstances",
        "The filtering was incomplete because it focused on individual tokens rather than token sequences",
        "Multi-token sequences (e.g., 'jq_THREADS' = 2 tokens) could trigger divergence despite OpenAI's defenses"
      ],
      "first_principle_insight": "The vulnerability wasn't a surface-level text processing issue but a fundamental tokenization-layer phenomenon. Token repetition rather than character repetition drove the vulnerability.",
      "how_to_reframe": {
        "old_atomic_unit": "Character-level repetition patterns",
        "new_atomic_unit": "Token-sequence-level repetition (multiple consecutive cl100k_base token IDs)",
        "new_problem_type": "Tokenization-layer security analysis affecting model alignment"
      },
      "architectural_changes": [
        "OpenAI expanded filtering scope from single tokens to multi-token patterns",
        "Implemented server-side timeout for long running ChatGPT requests",
        "Deployed comprehensive fixes by January 29, 2024 after Dropbox disclosure",
        "Recognition that vulnerability affects fundamental tokenization layer not just input processing"
      ],
      "evidence": {
        "timeline": "Vulnerability discovered \u2192 Single-token fix (Nov 2023) \u2192 Still vulnerable \u2192 Multi-token fix (Jan 29, 2024)",
        "scope": "Affected both GPT-3.5 and GPT-4 models",
        "deployment": "Production security fix deployed across ChatGPT"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Dropbox Tech Blog (2024): Bye Bye Bye: Evolution of repeated token attacks on ChatGPT models",
          "url": "https://dropbox.tech/machine-learning/bye-bye-bye-evolution-of-repeated-token-attacks-on-chatgpt-models",
          "note": "Security research blog post describing the discovery and evolution of token repetition attacks"
        }
      ],
      "related_cases": [],
      "tags": [
        "security",
        "adversarial",
        "NLP",
        "LLM",
        "safety"
      ]
    },
    {
      "id": "yelp-hybrid-recommendations-2022",
      "title": "Yelp: Deconstructing 'Collaboration' (Pure CF vs. Hybrid Features)",
      "category": "Optimization Pivot",
      "subcategory": "Feature Engineering",
      "companies_involved": [
        "Yelp"
      ],
      "year": 2022,
      "initial_problem": "Recommend businesses to users at scale using matrix factorization (Spark's ALS algorithm) which factorized the user-business interaction matrix to user-vectors and business-vectors",
      "initial_assumptions": [
        "ID-level vectors could effectively capture user preferences",
        "Collaborative filtering (matrix factorization) alone was sufficient for recommendations",
        "Users required substantial interaction history for effective modeling"
      ],
      "why_it_fails": [
        "Cold-start problem: Tail users have very few interactions and suffer from the cold-start problem. They were excluded from matrix factorization entirely.",
        "Feature blindness: An inability to add content-based features such as business reviews, ratings, user segment, etc.",
        "Limited to users with substantial interaction history"
      ],
      "first_principle_insight": "'Build on top' philosophy rather than replacement - replacing matrix factorization seemed like a farther out goal as it is known to work pretty well. Instead of replacing it, planned to build hybrid model on top by taking its scores as key features.",
      "how_to_reframe": {
        "old_atomic_unit": "User-business interaction pairs \u2192 sparse ID vectors",
        "new_atomic_unit": "User-business pairs with rich feature sets \u2192 rankable candidates with explicit signals",
        "new_problem_type": "Learning-to-rank with hybrid features combining collaborative and content-based signals"
      },
      "architectural_changes": [
        "Introduced recall step limiting candidate scope before model evaluation",
        "Shifted from dot-product optimization to learning-to-rank with XGBoost",
        "Added text-based similarity features via Universal Sentence Encoder embeddings",
        "Redefined training groups as (user, location) pairs rather than user-only",
        "Incorporated matrix factorization scores as features rather than final predictions"
      ],
      "evidence": {
        "metrics": "Doubled the number of users we could recommend for while also vastly improving performance for all users",
        "deployment": "Production recommendation system serving Yelp users"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Yelp Engineering Blog (2022): Beyond Matrix Factorization: Using hybrid features for user-business recommendations",
          "url": "https://engineeringblog.yelp.com/2022/04/beyond-matrix-factorization-using-hybrid-features-for-user-business-recommendations.html",
          "note": "Engineering blog post describing the evolution from pure collaborative filtering to hybrid recommendation system"
        }
      ],
      "related_cases": [],
      "tags": [
        "recommendations",
        "hybrid-systems",
        "ML"
      ]
    },
    {
      "id": "nubank-beyond-prediction-2024",
      "title": "Nubank: Deconstructing 'ML Value' (Prediction vs. Causation)",
      "category": "Reasoning Pivot",
      "subcategory": "Causal Inference",
      "companies_involved": [
        "Nubank"
      ],
      "year": 2024,
      "initial_problem": "Solving business decision-making problems (like coupon allocation) using machine learning as prediction machines",
      "initial_assumptions": [
        "ML's value derives from cheap predictions",
        "If a problem can be framed as prediction, data scientists can solve it",
        "High-accuracy predictive models directly enable business optimization",
        "When all you have is a hammer, everything starts to look like a thumb"
      ],
      "why_it_fails": [
        "Coupon allocation case study: A model predicting who will convert doesn't answer who should receive coupons",
        "High conversion probability doesn't indicate coupon responsiveness",
        "Predictions about baseline outcomes don't reveal treatment effects",
        "You don't care about conversion probabilities, you care about how they change with coupons"
      ],
      "first_principle_insight": "The fundamental problem of causal inference: unobservability of counterfactuals. You cannot observe the same person under two treatment regimes simultaneously. Predictions address correlation; optimization requires understanding causation.",
      "how_to_reframe": {
        "old_atomic_unit": "Conversion probability P(convert|features)",
        "new_atomic_unit": "Marginal/treatment effect \u03b4conversion/\u03b4treatment",
        "new_problem_type": "Causal inference estimating heterogeneous treatment effects rather than predicting outcomes"
      },
      "architectural_changes": [
        "Moved from supervised learning pipelines to causal inference methodologies",
        "Randomized control trials (RCTs) for experimental causal inference",
        "Natural experiments for observational causal inference",
        "Bias-control techniques to address confounding",
        "Direct estimation of heterogeneous treatment effects across user segments"
      ],
      "evidence": {
        "case_study": "Concrete coupon allocation example showing prediction \u2260 optimization",
        "deployment": "Shift from predictive to causal modeling for business optimization problems"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Nubank Engineering Blog (2024): Beyond prediction machines",
          "url": "https://building.nubank.com/beyond-prediction-machines/",
          "note": "Engineering blog post arguing for causal inference over prediction for business optimization decisions"
        }
      ],
      "related_cases": [
        "netflix-causal-inference"
      ],
      "tags": [
        "causal-inference",
        "prediction",
        "optimization",
        "ML-methodology"
      ]
    },
    {
      "id": "coches-net-filter-to-phrases-2024",
      "title": "Coches.net (Adevinta): Deconstructing 'Search Interface' (Filters vs. Natural Language)",
      "category": "Architectural Pivot",
      "subcategory": "Interface Paradigm",
      "companies_involved": [
        "Adevinta",
        "Coches.net"
      ],
      "year": 2024,
      "initial_problem": "Users struggled with existing search interface - wanted to search using natural language like 'Audi A3 2020 in Barcelona' but had to navigate through several filters instead",
      "initial_assumptions": [
        "Users needed a fundamentally different search interface",
        "The existing filter system was the constraint that couldn't be changed",
        "Natural language processing could solve the UX problem",
        "Only a small portion of traffic uses filters, limiting adoption"
      ],
      "why_it_fails": [
        "Filter-based search forced users into mechanical selection workflow rather than natural expression of intent",
        "Low adoption rates despite implementation",
        "Ongoing struggles with making the feature accessible to users"
      ],
      "first_principle_insight": "The pivotal realization was that they didn't need to replace their filtering architecture. Instead: 'The AI doesn't perform the search itself; it converts the user's input into filter parameters that our system can process.' This transformed the problem from 'rebuild search' to 'translate intent.'",
      "how_to_reframe": {
        "old_atomic_unit": "User manually selects filters \u2192 system applies filters",
        "new_atomic_unit": "User submits natural phrase \u2192 AI converts to filters \u2192 system applies filters",
        "new_problem_type": "Natural language translation layer rather than search system replacement"
      },
      "architectural_changes": [
        "Added moderation layer to classify queries as valid/unethical/invalid",
        "Integrated Amazon Bedrock's Claude Haiku model via AWS infrastructure",
        "Implemented few-shot prompting technique with filter context and search examples",
        "Maintained existing backend filtering system unchanged",
        "Translation layer became the new functional unit"
      ],
      "evidence": {
        "timeline": "15-day prototype development sprint",
        "deployment": "Production system on Spain's leading vehicle marketplace",
        "technology": "Amazon Bedrock, Anthropic Claude Haiku"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Adevinta Tech Blog (2024): From Filters to Phrases: Our AI Revolution in Car Search",
          "url": "https://adevinta.com/techblog/from-filters-to-phrases-our-ai-revolution-in-car-search/",
          "note": "Engineering blog post describing 15-day sprint to add natural language search to filter-based car marketplace"
        }
      ],
      "related_cases": [
        "yelp-query-understanding-2025",
        "trivago-smart-search-2024"
      ],
      "tags": [
        "search",
        "LLM",
        "NLP",
        "translation-layer",
        "e-commerce",
        "manufacturing"
      ]
    },
    {
      "id": "pinterest-ads-conversion-evolution-2024",
      "title": "Pinterest: Deconstructing 'Ad Conversion Modeling' (Simple Predictors vs. Deep Hierarchical Ensemble)",
      "category": "Optimization Pivot",
      "subcategory": "Model Architecture Evolution",
      "companies_involved": [
        "Pinterest"
      ],
      "year": 2024,
      "initial_problem": "Ad conversion prediction is challenging because conversion is a downstream event that is inherently much sparser than click events, requiring more sophisticated modeling",
      "initial_assumptions": [
        "Traditional machine learning models could handle conversion prediction adequately",
        "Click-based models could be adapted for conversion events",
        "Simple feature engineering would suffice for sparse conversion signals"
      ],
      "why_it_fails": [
        "Conversion events are much sparser than clicks, creating data sparsity challenges",
        "Simple models couldn't capture the complexity of user intent leading to conversions",
        "Single-task models missed cross-task learning opportunities",
        "Large model sizes created serving latency challenges"
      ],
      "first_principle_insight": "Conversion prediction requires fundamentally different architecture than click prediction due to signal sparsity. Need to combine: (1) deep learning for pattern recognition, (2) multi-task learning to leverage related signals, (3) ensemble methods for robustness, and (4) user sequence modeling for temporal context.",
      "how_to_reframe": {
        "old_atomic_unit": "User-ad interaction \u2192 conversion probability (single model)",
        "new_atomic_unit": "User sequence + ad features \u2192 hierarchical ensemble \u2192 conversion probability",
        "new_problem_type": "Deep hierarchical ensemble with multi-task learning and sequence modeling"
      },
      "architectural_changes": [
        "Adopted Deep and Hierarchical Ensemble Network (DHEN) framework",
        "Integrated MaskNet feature interaction module for complex feature relationships",
        "Added user action sequence modeling to capture temporal patterns",
        "Transition to GPU serving for large-scale complex models",
        "Implemented Sparse Optimizer with layer-specific learning rates for embedding tables",
        "Developed Frequency-Adaptive Learning Rate (FAL) to mitigate multi-epoch overfitting",
        "Multi-task learning architecture to leverage related prediction tasks"
      ],
      "evidence": {
        "metrics": "Reduced model size by 30% while maintaining performance, multi-epoch training improved conversion prediction accuracy by 0.5%, achieved 15% faster training time",
        "deployment": "Production system handling Pinterest ads at scale",
        "research": "Presented at KDD 2025 conference"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2024): Evolution of Ads Conversion Optimization Models at Pinterest",
          "url": "https://medium.com/pinterest-engineering/evolution-of-ads-conversion-optimization-models-at-pinterest-84b244043d51",
          "note": "Engineering blog post describing evolution from simple models to deep hierarchical ensembles"
        },
        {
          "type": "supplementary",
          "citation": "arXiv (2025): The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion",
          "url": "https://arxiv.org/abs/2505.05605",
          "note": "Research paper on technical optimizations for embedding tables and training procedures"
        }
      ],
      "related_cases": [],
      "tags": [
        "advertising",
        "optimization",
        "deep-learning",
        "ML"
      ]
    },
    {
      "id": "doordash-gnn-notifications-2024",
      "title": "DoorDash: Deconstructing 'Recommendations' (Individual Signals vs. Graph Structure)",
      "category": "Relational Pivot",
      "subcategory": "Graph-Based Recommendations",
      "companies_involved": [
        "DoorDash"
      ],
      "year": 2024,
      "initial_problem": "Craft personalized restaurant recommendations for notifications (push, email, hub) to reach inactive users and leverage fragmented data across the platform",
      "initial_assumptions": [
        "Traditional collaborative filtering or content-based methods could handle notification recommendations",
        "User-restaurant interactions could be modeled independently",
        "Feature engineering from isolated data sources would suffice"
      ],
      "why_it_fails": [
        "Reaching inactive users through notifications is challenging with traditional methods",
        "Fragmented data across users, restaurants, orders, and views wasn't being utilized effectively",
        "Traditional methods couldn't leverage the interconnected nature of marketplace data",
        "Cold-start problems for new users and restaurants"
      ],
      "first_principle_insight": "DoorDash's Marketplace recommendation challenges (inactive users, fragmented data) are ideally addressable by GNNs. GNNs excel in extracting personalized information by utilizing interconnected data from various sources such as users, restaurants, orders and views. GNNs can learn collaborative filtering and combine that with node features to recommend new restaurants.",
      "how_to_reframe": {
        "old_atomic_unit": "User features + restaurant features \u2192 recommendation score",
        "new_atomic_unit": "User-restaurant graph with order/view edges \u2192 graph embedding \u2192 link prediction",
        "new_problem_type": "Link prediction on user-restaurant bipartite graph using GNN"
      },
      "architectural_changes": [
        "Defined modeling task as link prediction between user and restaurant nodes",
        "Implemented ID-GNN (Identity aware GNN) to leverage structural and identity-specific features",
        "Graph structure: users and restaurants as nodes, orders as edges",
        "Combined collaborative filtering learning with node features and attributes",
        "Utilized interconnected data from users, restaurants, orders, and views",
        "Deployed for push, email, and hub notification channels"
      ],
      "evidence": {
        "deployment": "Production GNN system for personalized notification recommendations",
        "publication": "Published June 2024 on DoorDash Engineering Blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering Blog (2024): Beyond the Click: Elevating DoorDash's personalized notification experience with GNN recommendation",
          "url": "https://careersatdoordash.com/blog/doordash-customize-notifications-how-gnn-work/",
          "note": "Engineering blog post describing GNN-based notification recommendation system"
        }
      ],
      "related_cases": [
        "yelp-hybrid-recommendations-2022"
      ],
      "tags": [
        "graph-neural-networks",
        "recommendations",
        "customer-support",
        "marketplace"
      ]
    },
    {
      "id": "pinterest-holiday-finds-2024",
      "title": "Pinterest: Deconstructing 'Gift Discovery' (General Feed vs. Shopping-Optimized Flow)",
      "category": "Architectural Pivot",
      "subcategory": "User Experience Flow",
      "companies_involved": [
        "Pinterest"
      ],
      "year": 2024,
      "initial_problem": "Holiday shoppers faced two key friction points: discovery overwhelm (too many options) and fragmented wishlists (manual board creation and organization required)",
      "initial_assumptions": [
        "Existing Homefeed architecture could serve holiday gift discovery needs",
        "Users would manually create and organize shopping boards",
        "Standard Pin ranking would work for gift-specific content"
      ],
      "why_it_fails": [
        "Discovery overwhelm: too many options without shopping-specific curation",
        "Fragmented wishlists: users had to manually create and organize gift boards",
        "Generic engagement signals didn't capture gift-specific interaction patterns",
        "Standard ranking didn't balance discovery with personalization for gift shopping"
      ],
      "first_principle_insight": "Gift shopping behavior is fundamentally different from general browsing. Rather than building from scratch, extend proven Homefeed architecture with gift-specific candidate generators and ranking signals, while automatically managing organization to reduce friction.",
      "how_to_reframe": {
        "old_atomic_unit": "Generic feed of Pins \u2192 manual board creation by user",
        "new_atomic_unit": "Gift-specific feed with automatic board population \u2192 curated shopping experience",
        "new_problem_type": "Shopping-optimized content flow with automatic organization"
      },
      "architectural_changes": [
        "Extended Homefeed architecture with gift-specific candidate generators",
        "Added gift-specific ranking signals for different engagement patterns",
        "Developed Structured Feed Framework for rapid UI iteration while maintaining platform-specific optimizations (iOS, Android, Web)",
        "Automatic board creation and population - reduced manual organization friction",
        "Two-phase tab ranking strategy balancing discovery with personalization",
        "Comprehensive engagement tracking for gift-specific interactions"
      ],
      "evidence": {
        "timeline": "Launched November 2024 for holiday shopping season",
        "deployment": "Production feature across iOS, Android, and Web platforms",
        "features": "Machine-learning powered personalized Pin selections, celebrity-curated guides"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2024): Building Holiday Finds: How Pinterest Engineers Reimagined Gift Discovery",
          "url": "https://medium.com/pinterest-engineering/building-holiday-finds-how-pinterest-engineers-reimagined-gift-discovery-0a48a0c8bfd8",
          "note": "Engineering blog post describing the architecture and strategy for Holiday Finds feature"
        }
      ],
      "related_cases": [
        "pinterest-module-ranking"
      ],
      "tags": [
        "recommendations",
        "e-commerce",
        "UX",
        "personalization"
      ]
    },
    {
      "id": "doordash-llm-search-2024",
      "title": "DoorDash: Deconstructing 'Search Retrieval' (Keyword Matching vs. Semantic Understanding)",
      "category": "Architectural Pivot",
      "subcategory": "Semantic Search",
      "companies_involved": [
        "DoorDash"
      ],
      "year": 2024,
      "initial_problem": "Complex food delivery search queries required better understanding and processing - traditional keyword matching and statistical models weren't capturing user intent effectively",
      "initial_assumptions": [
        "Traditional keyword matching with statistical models sufficient for search",
        "Separate systems for query understanding, retrieval, and ranking were acceptable",
        "Single embedding model could handle all search tasks"
      ],
      "why_it_fails": [
        "Keyword matching missed semantic relationships and user intent",
        "Statistical models couldn't handle new queries dynamically",
        "Separate systems created fragmentation and missed opportunities for holistic optimization",
        "Generic embeddings didn't capture DoorDash-specific domain knowledge"
      ],
      "first_principle_insight": "Search requires semantic understanding combined with structured knowledge. By using LLMs with knowledge graphs for query segmentation and entity linking, constrained by RAG to controlled vocabulary, can improve both precision and relevance while handling new queries on the fly.",
      "how_to_reframe": {
        "old_atomic_unit": "Query string \u2192 keyword matching \u2192 results",
        "new_atomic_unit": "Query \u2192 LLM semantic understanding + knowledge graph \u2192 hierarchical RAG retrieval \u2192 semantic embeddings \u2192 results",
        "new_problem_type": "Multi-stage semantic search with hierarchical retrieval and domain-specific embeddings"
      },
      "architectural_changes": [
        "Combined LLMs with knowledge graphs for query segmentation and entity linking",
        "Implemented Hierarchical RAG: narrowing context using category trees and structured retrieval before LLM",
        "Developed DashCLIP: multimodal model for generating semantic embeddings specific to DoorDash platform",
        "Created Semantic IDs: compact, meaning-rich embeddings encoding catalog hierarchy",
        "Hybrid system: embedding retrieval + traditional statistical models + rule-based systems",
        "RAG with controlled vocabulary to constrain outputs and maintain precision"
      ],
      "evidence": {
        "metrics": "Improved popular dish carousel trigger rates by 30%, increased whole page relevance by over 2%, higher conversion rates while maintaining high precision",
        "deployment": "Production search system handling DoorDash queries at scale",
        "innovation": "DashCLIP model published to arXiv"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering Blog (2024): How DoorDash leverages LLMs for better search retrieval",
          "url": "https://careersatdoordash.com/blog/how-doordash-leverages-llms-for-better-search-retrieval/",
          "note": "Engineering blog post describing LLM-enhanced search retrieval system"
        },
        {
          "type": "supplementary",
          "citation": "arXiv (2024): DashCLIP: Leveraging multimodal models for generating semantic embeddings for DoorDash",
          "url": "https://arxiv.org/abs/2504.07110",
          "note": "Research paper on DashCLIP multimodal embedding model"
        }
      ],
      "related_cases": [
        "yelp-query-understanding-2025"
      ],
      "tags": [
        "search",
        "LLM",
        "RAG",
        "embeddings",
        "knowledge-graph",
        "multimodal",
        "retrieval"
      ]
    },
    {
      "id": "airbnb-embedding-retrieval-2025",
      "title": "Airbnb: Deconstructing 'Search Retrieval' (Keyword Matching vs. Embedding-Based)",
      "category": "Architectural Pivot",
      "subcategory": "Retrieval Architecture",
      "companies_involved": [
        "Airbnb"
      ],
      "year": 2025,
      "initial_problem": "Surface the most relevant listings for each user's query among millions of available homes, especially difficult for large geographic areas (California, France) or high-demand destinations (Paris, London)",
      "initial_assumptions": [
        "Traditional keyword-based retrieval sufficient for property search",
        "Location and basic filters enough for finding relevant homes",
        "Single retrieval mechanism could handle all query types",
        "Dot product similarity adequate for embedding space"
      ],
      "why_it_fails": [
        "Keyword matching misses semantic understanding of what users want",
        "Large geographic searches return too many irrelevant results",
        "High-demand destinations need better personalization and relevance",
        "Single retrieval approach doesn't adapt to different query types"
      ],
      "first_principle_insight": "Search for accommodations is fundamentally about matching user intent (captured in query features) with property characteristics (captured in listing features) in a semantic space. A two-tower architecture allows independent encoding of queries and listings, enabling efficient large-scale retrieval.",
      "how_to_reframe": {
        "old_atomic_unit": "Query keywords \u2192 keyword matching \u2192 listing results",
        "new_atomic_unit": "Query embedding (location, guests, stay length) + Listing embedding (engagement, amenities, capacity) \u2192 semantic similarity \u2192 relevant homes",
        "new_problem_type": "Two-tower embedding-based retrieval with IVF clustering"
      },
      "architectural_changes": [
        "Two-tower network: Query tower (location, guests, length of stay) + Listing tower (engagement, amenities, capacity)",
        "IVF (Inverted File Index) approach for efficient large-scale retrieval",
        "Euclidean distance instead of dot product for more balanced cluster sizes",
        "Quality highly sensitive to cluster size uniformity",
        "Homes retrieved from closest clusters to query embedding",
        "Deployed in both Search and Email Marketing production"
      ],
      "evidence": {
        "metrics": "Statistically-significant gain in overall bookings, bookings lift on par with some of the largest ML improvements to search ranking in past two years",
        "deployment": "Full production launch in Search and Email Marketing",
        "publication": "March 2025 on Airbnb Engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Airbnb Engineering Blog (2025): Embedding-Based Retrieval for Airbnb Search",
          "url": "https://medium.com/airbnb-engineering/embedding-based-retrieval-for-airbnb-search-aabebfc85839",
          "note": "Engineering blog post describing two-tower embedding architecture for search retrieval"
        }
      ],
      "related_cases": [
        "instacart-hybrid-retrieval-2024"
      ],
      "tags": [
        "search",
        "embeddings",
        "two-tower",
        "retrieval"
      ]
    },
    {
      "id": "whatnot-feed-ranking-evolution-2025",
      "title": "Whatnot: Deconstructing 'New User Experience' (Heuristics vs. Unified ML Model)",
      "category": "Optimization Pivot",
      "subcategory": "Model Architecture",
      "companies_involved": [
        "Whatnot"
      ],
      "year": 2025,
      "initial_problem": "Personalized For You Feed used ad-hoc heuristics to generate recommendations for new users, missing out on ML benefits and requiring dedicated maintenance",
      "initial_assumptions": [
        "New users need separate heuristic-based recommendations",
        "ML models can't handle users without sufficient history",
        "Separate systems for new vs existing users is acceptable",
        "Batch inference adequate for feed generation at scale"
      ],
      "why_it_fails": [
        "Ad-hoc heuristics missed out on machine learning benefits",
        "Required dedicated maintenance separate from main ML system",
        "Added unnecessary complexity to architecture",
        "Batch inference losing 5% coverage weekly as growth exploded",
        "Trying to score 10 billion+ user-seller pairs daily was unsustainable"
      ],
      "first_principle_insight": "New users are just users with sparse signals, not a fundamentally different problem. By treating new user signals as features in a unified model, can leverage same ML infrastructure while achieving better recommendations and reducing system complexity.",
      "how_to_reframe": {
        "old_atomic_unit": "User type (new vs existing) \u2192 different systems (heuristics vs ML)",
        "new_atomic_unit": "User signals (sparse or dense) \u2192 unified predictive model with real-time inference",
        "new_problem_type": "Real-time ML inference with optimized feature computation"
      },
      "architectural_changes": [
        "Consolidated feed ranking into one unified predictive model",
        "Added new user input signals as features instead of separate heuristics",
        "Migrated from batch to real-time inference system",
        "DynamoDB to Redis migration (3x latency improvement)",
        "Compiled GBDT models to native C binaries (20x faster inference)",
        "Implemented gRPC for data transmission (6.7x improvement)",
        "Systematically tuned thread pools",
        "Overall result: 5.8x latency reduction, <200ms p99 latency, >99.9% reliability"
      ],
      "evidence": {
        "metrics": "Better recommendations for new users, reduced maintenance costs, simplified architecture, 5.8x latency reduction, serves millions of predictions in <200ms p99",
        "deployment": "Production system handling explosive growth",
        "timeline": "Published September 2025 (heuristics elimination), May 2025 (real-time inference)"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Whatnot Engineering Blog (2025): The New User Dilemma: Why We Killed Our Heuristics",
          "url": "https://medium.com/whatnot-engineering/the-new-user-dilemma-why-we-killed-our-heuristics-what-we-built-instead-0d7a834fda5f",
          "note": "Blog post describing elimination of heuristics in favor of unified ML model"
        },
        {
          "type": "supplementary",
          "citation": "Whatnot Engineering Blog (2025): 6x Faster ML Inference: Why Online\u226bBatch",
          "url": "https://medium.com/@whatnotengineering/6x-faster-ml-inference-why-online-batch-16cbf1203947",
          "note": "Technical deep dive on real-time inference optimization"
        }
      ],
      "related_cases": [],
      "tags": [
        "recommendations",
        "ML",
        "real-time",
        "optimization"
      ]
    },
    {
      "id": "pinterest-preranking-modernization-2025",
      "title": "Pinterest: Deconstructing 'Pre-Ranking' (Monolithic Scoring vs. Request/Item Decomposition)",
      "category": "Optimization Pivot",
      "subcategory": "Model Architecture",
      "companies_involved": [
        "Pinterest"
      ],
      "year": 2025,
      "initial_problem": "Home feed pre-ranking (lightweight scoring) needed to scale model complexity while maintaining efficiency for millions of items per request",
      "initial_assumptions": [
        "Pre-ranking must use simpler models than ranking due to scale",
        "Two-tower architecture is required for efficient pre-ranking",
        "All computations must be done per-item",
        "Training data from ranking stage is sufficient"
      ],
      "why_it_fails": [
        "Two-tower architecture limits model complexity and feature interactions",
        "Per-item computations expensive at pre-ranking scale",
        "Training without early funnel logs creates distribution mismatch",
        "Simple models miss complex user-item interactions"
      ],
      "first_principle_insight": "Request-level context is shared across all items in a request. By decomposing into request-level (computed once) and item-level (computed per-item) components, can scale up model complexity significantly while maintaining efficiency. Early funnel training data critical for pre-ranking stage.",
      "how_to_reframe": {
        "old_atomic_unit": "Per-item computation with simple model \u2192 score",
        "new_atomic_unit": "Request-level context (once) + Item-level features (per-item) \u2192 jointly trained decomposed model \u2192 score",
        "new_problem_type": "Decomposed architecture with request/item split and early funnel training"
      },
      "architectural_changes": [
        "Request level sub-component: takes user and context features, generates compressed user representation",
        "Item level sub-component: computes features online including item extraction, processing, and user-item crossing",
        "Joint training with decoupling during serving for efficiency",
        "Request-level computation done once per request, not per item",
        "Enabled early funnel log in training for distribution alignment",
        "Non-two-tower architecture allowing more complicated interactions"
      ],
      "evidence": {
        "metrics": "Significant improvements in business metrics",
        "deployment": "Production home feed pre-ranking system",
        "publication": "May 29, 2025 on Pinterest Engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2025): Modernizing Home Feed Pre-Ranking Stage",
          "url": "https://medium.com/pinterest-engineering/modernizing-home-feed-pre-ranking-stage-e636c9cdc36b",
          "note": "Engineering blog post describing request/item decomposition architecture"
        }
      ],
      "related_cases": [
        "pinterest-module-ranking"
      ],
      "tags": [
        "ranking",
        "architecture",
        "optimization",
        "efficiency"
      ]
    },
    {
      "id": "picnic-llm-search-2024",
      "title": "Picnic: Deconstructing 'Search' (Static Catalog vs. Dynamic LLM Descriptions)",
      "category": "Architectural Pivot",
      "subcategory": "Search Paradigm",
      "companies_involved": [
        "Picnic"
      ],
      "year": 2024,
      "initial_problem": "Customers use millions of different search terms to navigate Picnic's grocery assortment, requiring better understanding of diverse queries beyond static catalog matching",
      "initial_assumptions": [
        "Static product catalog with fixed descriptions sufficient",
        "Keyword matching adequate for grocery search",
        "Real-time LLM generation required for all queries",
        "Multilingual search could be handled with translation"
      ],
      "why_it_fails": [
        "Static descriptions don't capture all ways users describe products",
        "Keyword matching misses semantic relationships",
        "Users expect instant results, LLMs too slow for real-time",
        "Translation-based approach creates language barrier issues"
      ],
      "first_principle_insight": "Search terms can be dynamically transformed into rich descriptions using LLMs that capture semantic meaning. By precomputing common queries (leveraging search frequency distribution), can get LLM benefits without real-time latency. This bridges the gap between static catalogs and dynamic user language.",
      "how_to_reframe": {
        "old_atomic_unit": "Search term \u2192 keyword match against static catalog \u2192 products",
        "new_atomic_unit": "Search term \u2192 LLM-generated description (precomputed for common queries) \u2192 semantic comparison \u2192 products/recipes",
        "new_problem_type": "Prompt-based description generation with precomputation strategy"
      },
      "architectural_changes": [
        "OpenAI GPT-3.5-turbo for description generation (performs as well as GPT-4-turbo)",
        "Precomputation of common search terms to avoid real-time LLM latency",
        "Search term frequency analysis to identify candidates for precomputation",
        "Dynamic description generation that can be compared to entire product and recipe assortment",
        "Multilingual handling: Dutch customers searching 'fromage' find French cheeses"
      ],
      "evidence": {
        "deployment": "Production search system for Picnic grocery delivery",
        "publication": "May 2024 on Picnic Engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Picnic Engineering Blog (2024): Enhancing Search Retrieval with Large Language Models (LLMs)",
          "url": "https://blog.picnic.nl/enhancing-search-retrieval-with-large-language-models-llms-7c3748b26d72",
          "note": "Engineering blog post describing LLM-based search description generation"
        }
      ],
      "related_cases": [
        "yelp-query-understanding-2025",
        "doordash-llm-search-2024"
      ],
      "tags": [
        "search",
        "LLM",
        "efficiency",
        "marketplace",
        "NLP"
      ]
    },
    {
      "id": "instacart-hybrid-retrieval-2024",
      "title": "Instacart: Deconstructing 'Retrieval' (Single Method vs. Query Entropy-Driven Hybrid)",
      "category": "Optimization Pivot",
      "subcategory": "Retrieval Strategy",
      "companies_involved": [
        "Instacart"
      ],
      "year": 2024,
      "initial_problem": "Search recall driven by either text search (keyword matching) or semantic search (embedding), but each has different strengths depending on query type",
      "initial_assumptions": [
        "Single retrieval method can handle all queries",
        "Text search and semantic search should be equally weighted",
        "Same recall strategy appropriate for all query types",
        "Separate infrastructure for different retrieval methods"
      ],
      "why_it_fails": [
        "Text search excels at keyword matching but misses semantic understanding",
        "Semantic search good at intent but can miss exact matches",
        "Specific queries (low entropy) need different handling than broad queries (high entropy)",
        "Fragmented infrastructure adds complexity"
      ],
      "first_principle_insight": "Query entropy captures query specificity - high entropy queries (broad, vague) benefit from semantic search while low entropy queries (specific, keyword-heavy) benefit from text search. By adapting retrieval strategy based on query entropy, can leverage strengths of each method optimally.",
      "how_to_reframe": {
        "old_atomic_unit": "Query \u2192 single retrieval method \u2192 recall set",
        "new_atomic_unit": "Query \u2192 entropy calculation \u2192 adaptive hybrid (text + semantic weighted by entropy) \u2192 optimized recall set",
        "new_problem_type": "Entropy-driven adaptive hybrid retrieval"
      },
      "architectural_changes": [
        "Hybrid search architecture leveraging pgvector within Postgres",
        "ts_rank for text match scoring",
        "Query entropy calculation to determine retrieval strategy weighting",
        "Adaptive system tailoring recall sets based on query specificity",
        "Unified retrieval infrastructure combining both methods",
        "Single scalable system replacing fragmented approaches"
      ],
      "evidence": {
        "metrics": "Mean converting position improved by 1.7%, reduced latency by 1.5%",
        "deployment": "Production search system on unified Postgres infrastructure",
        "publication": "2024 on Instacart Tech blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Instacart Tech Blog (2024): Optimizing search relevance at Instacart using hybrid retrieval",
          "url": "https://tech.instacart.com/optimizing-search-relevance-at-instacart-using-hybrid-retrieval-88cb579b959c",
          "note": "Engineering blog post describing query entropy-based hybrid retrieval"
        },
        {
          "type": "supplementary",
          "citation": "Instacart Tech Blog (2024): How Instacart Built a Modern Search Infrastructure on Postgres",
          "url": "https://tech.instacart.com/how-instacart-built-a-modern-search-infrastructure-on-postgres-c528fa601d54",
          "note": "Infrastructure details on unified Postgres-based search system"
        }
      ],
      "related_cases": [
        "airbnb-embedding-retrieval-2025",
        "doordash-llm-search-2024"
      ],
      "tags": [
        "search",
        "retrieval",
        "query-entropy",
        "embeddings",
        "postgres"
      ]
    },
    {
      "id": "grab-graph-anomaly-2023",
      "title": "Grab: Deconstructing 'Fraud Detection' (Supervised Classification vs. Unsupervised Graph Anomaly)",
      "category": "Relational Pivot",
      "subcategory": "Fraud Detection",
      "companies_involved": [
        "Grab"
      ],
      "year": 2023,
      "initial_problem": "Detect fraudulent behaviors in GrabFood and GrabMart interactions between consumers and merchants, especially new fraud MOs (modus operandi) without labeled data",
      "initial_assumptions": [
        "Supervised learning with labeled fraud examples is the primary approach",
        "Individual transaction features sufficient for fraud detection",
        "New fraud patterns require retraining with new labels",
        "Interactions between consumers and merchants can be analyzed independently"
      ],
      "why_it_fails": [
        "Supervised models require extensive labeling for new fraud patterns",
        "Label supervision too slow to catch emerging fraud MOs",
        "Transaction-level features miss network patterns",
        "Independent analysis ignores structural fraud signals (collusion rings, device sharing)"
      ],
      "first_principle_insight": "Fraud is inherently a graph problem - fraudsters operate in networks (collusion rings, device sharing, coordinated attacks). Modeling interactions as bipartite graph and using unsupervised anomaly detection can discover new fraud patterns without labeled data, catching anomalies in graph structure that supervised models miss.",
      "how_to_reframe": {
        "old_atomic_unit": "Transaction features \u2192 supervised classification \u2192 fraud label",
        "new_atomic_unit": "Consumer-merchant bipartite graph \u2192 GraphBEAN autoencoder \u2192 anomaly scores (node and edge level)",
        "new_problem_type": "Unsupervised graph anomaly detection on bipartite attributed graphs"
      },
      "architectural_changes": [
        "Bipartite graph construction from GrabFood and GrabMart transactions",
        "GraphBEAN (autoencoder for graph anomaly detection)",
        "Node-level anomaly scores for consumers and merchants",
        "Edge-level anomaly scores for interactions",
        "Fully-automated system for graph construction, training, and scoring",
        "No label supervision required - unsupervised learning"
      ],
      "evidence": {
        "discovery": "Led to discovery of new fraud MOs not caught by supervised models",
        "deployment": "Production fraud detection for GrabFood and GrabMart",
        "publication": "Presented at IJCNN 2023 (International Joint Conference on Neural Networks)",
        "paper": "R. Fathony et al., 'Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs,' IJCNN 2023"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Grab Engineering Blog (2023): Unsupervised graph anomaly detection - Catching new fraudulent behaviours",
          "url": "https://engineering.grab.com/graph-anomaly-model",
          "note": "Engineering blog post describing GraphBEAN unsupervised anomaly detection"
        },
        {
          "type": "supplementary",
          "citation": "IJCNN 2023: Interaction-Focused Anomaly Detection on Bipartite Node-and-Edge-Attributed Graphs",
          "url": "https://doi.org/10.1109/IJCNN54540.2023.10191331",
          "note": "Academic paper published at premier neural networks conference"
        }
      ],
      "related_cases": [
        "stripe-fraud-graph-clustering",
        "uber-risk-entity-watch-2023"
      ],
      "tags": [
        "fraud-detection",
        "graph-neural-networks",
        "ML",
        "anomaly-detection"
      ]
    },
    {
      "id": "uber-risk-entity-watch-2023",
      "title": "Uber: Deconstructing 'Fraud Detection' (Event-Level Supervised vs. Entity-Level Unsupervised)",
      "category": "Relational Pivot",
      "subcategory": "Feature Engineering",
      "companies_involved": [
        "Uber"
      ],
      "year": 2023,
      "initial_problem": "Fraud detection relied on supervised ML for event-level risk assessment, but variety of fraud classes grew faster than labeled training data could scale across multiple business lines (Rides, Eats, Freight, Health)",
      "initial_assumptions": [
        "Supervised methods can handle expanding fraud categories indefinitely",
        "Event-centric detection (examining individual transactions) is sufficient",
        "Data scientists must manually build features for each new use case",
        "Event-level signals capture all fraud patterns"
      ],
      "why_it_fails": [
        "Supervised learning doesn't scale with ever-growing fraud class variety",
        "Manual feature engineering bottleneck for each new use case",
        "Event-level view misses entity behavioral patterns",
        "Rider's cumulative behavior or payment method usage patterns invisible at event level"
      ],
      "first_principle_insight": "Entity-level anomalies reveal patterns that transcend individual transactions. Instead of analyzing events in isolation, analyze entity histories (riders, drivers, payment methods) using unsupervised anomaly detection. Auto-generate standard metrics across all entities and time windows to eliminate manual feature engineering.",
      "how_to_reframe": {
        "old_atomic_unit": "Event features \u2192 supervised ML \u2192 fraud prediction",
        "new_atomic_unit": "Entity history (trip counts, refund patterns across time windows) \u2192 unsupervised anomaly detection \u2192 entity risk score",
        "new_problem_type": "Unsupervised entity-level anomaly detection with auto-generated features"
      },
      "architectural_changes": [
        "Entity Feature Generation (EFG): auto-generates standard metric set across all entities",
        "Thousands of features generated automatically rather than manually per use case",
        "Unsupervised anomaly detection models operating on entity histories",
        "HAIFA algorithm for explainability: identifies which features make each anomaly stand out",
        "Python DSL configuration replacing custom model development",
        "Templated pipeline: deploy new fraud models in hours instead of weeks"
      ],
      "evidence": {
        "deployment": "Production fraud detection across Rides, Eats, Freight, Health",
        "scalability": "Handles expanding fraud classes without retraining requirements",
        "efficiency": "Hours to deploy new models vs weeks previously",
        "publication": "September 2023 on Uber Engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog (2023): Risk Entity Watch - Using Anomaly Detection to Fight Fraud",
          "url": "https://www.uber.com/blog/risk-entity-watch/",
          "note": "Engineering blog post describing entity-level unsupervised fraud detection"
        }
      ],
      "related_cases": [
        "grab-graph-anomaly-2023",
        "stripe-fraud-graph-clustering"
      ],
      "tags": [
        "fraud-detection",
        "anomaly-detection",
        "ML",
        "graph-neural-networks"
      ]
    },
    {
      "id": "linkedin-skills-extraction-2023",
      "title": "LinkedIn: Deconstructing 'Skills Tagging' (Manual Profile Section vs. AI Content Extraction)",
      "category": "Optimization Pivot",
      "subcategory": "Data Extraction",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2023,
      "initial_problem": "Members add skills to profiles but not always in dedicated skills section - skills embedded in Summary, Experience sections, resumes, LinkedIn Learning courses, and feed posts",
      "initial_assumptions": [
        "Users will tag skills in designated skills section",
        "Explicit skill tagging sufficient for Skills Graph",
        "Skills in different sections don't need extraction",
        "Generic extraction approach works for all content types"
      ],
      "why_it_fails": [
        "Many members include skills in Summary and Experience sections, not skills section",
        "Resumes contain skills but in unstructured format",
        "LinkedIn Learning courses teach skills not always explicitly tagged",
        "Feed posts mention skills contextually",
        "Skills embedded in content far more complex to extract than dedicated fields"
      ],
      "first_principle_insight": "Skills exist throughout LinkedIn content, not just in dedicated sections. By using AI to extract skills from any content and map to Skills Graph (41,000+ skills), can fuel a skills-first economy. Content structure matters - parsing into sections (e.g., job qualifications vs benefits) helps AI understand skill importance.",
      "how_to_reframe": {
        "old_atomic_unit": "User manual skill tagging in profile skills section \u2192 Skills Graph",
        "new_atomic_unit": "Content (profile, resume, courses, posts) \u2192 parsing \u2192 AI extraction \u2192 Skills Graph mapping \u2192 enriched skills data",
        "new_problem_type": "AI-powered content parsing and skill extraction with graph mapping"
      },
      "architectural_changes": [
        "Parsing: raw input parsed into structured sections (company, responsibilities, benefits, qualifications)",
        "AI model identifies skills in context (qualifications section skills more important)",
        "Skill Expansion: uses Skills Graph to query for related skills (parent, children, siblings)",
        "Multitask scoring model: shared module + domain-specific module for each content type",
        "Maps extracted skills to LinkedIn's 41,000+ skill taxonomy",
        "Handles multiple content sources: profiles, resumes, Learning courses, posts"
      ],
      "evidence": {
        "scale": "LinkedIn Skills Graph contains 41,000+ skills",
        "deployment": "Production system extracting skills across all LinkedIn content",
        "publication": "December 2023 on LinkedIn Engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2023): Extracting skills from content to fuel the LinkedIn Skills Graph",
          "url": "https://engineering.linkedin.com/blog/2023/extracting-skills-from-content-to-fuel-the-linkedin-skills-graph",
          "note": "Engineering blog post describing AI-powered skill extraction system"
        }
      ],
      "related_cases": [],
      "tags": [
        "NLP",
        "AI",
        "knowledge-graph"
      ]
    },
    {
      "id": "airbnb-automation-platform-2022",
      "title": "Airbnb: Deconstructing 'Conversational AI' (Ad-hoc Workflows vs. MDP-Based Platform)",
      "category": "Pragmatic Pivot",
      "subcategory": "Platform Unification",
      "companies_involved": [
        "Airbnb"
      ],
      "year": 2022,
      "initial_problem": "Suite of conversational AI products (chatbots, on-trip support, agent automation) built with custom workflows, making development slow and limiting non-technical team participation",
      "initial_assumptions": [
        "Each conversational AI product needs custom workflow implementation",
        "Technical teams required for all conversational AI development",
        "Different products require fundamentally different architectures",
        "Workflows and actions specific to each product use case"
      ],
      "why_it_fails": [
        "Custom workflows slow down product iteration",
        "Technical dependency blocks business teams from building AI products",
        "No reusability of actions across products",
        "Difficult to scale conversational AI across organization",
        "Each new product requires significant engineering investment"
      ],
      "first_principle_insight": "Conversational AI products can be modeled as Markov Decision Process (MDP) workflows with states and actions. By providing a unified MDP-based platform with GUI development tools, can enable workflow consolidation, action reusability, and empower non-technical teams to build conversational AI.",
      "how_to_reframe": {
        "old_atomic_unit": "Custom workflow per product \u2192 technical implementation",
        "new_atomic_unit": "MDP workflow representation \u2192 drag-and-drop GUI \u2192 reusable actions \u2192 scalable platform",
        "new_problem_type": "Enterprise-level conversational AI platform with MDP abstraction"
      },
      "architectural_changes": [
        "Modeled Conversational AI products as Markov Decision Process (MDP) workflows",
        "Unified representation of workflows and actions",
        "GUI development tool for drag-and-drop workflow creation",
        "Action reusability across different conversational AI products",
        "Workflow consolidation reducing duplication",
        "Enabled non-technical teams to build conversational AI products",
        "Supports chatbots, on-trip support products, agent automations"
      ],
      "evidence": {
        "benefits": "Simplified and sped up conversational AI product development, democratized AI technology to business teams",
        "deployment": "Platform supporting suite of conversational AI products across Airbnb",
        "evolution": "Platform evolved to v2 supporting LLM applications",
        "publication": "January 2022 on Airbnb Engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Airbnb Engineering Blog (2022): Intelligent Automation Platform: Empowering Conversational AI and Beyond at Airbnb",
          "url": "https://medium.com/airbnb-engineering/intelligent-automation-platform-empowering-conversational-ai-and-beyond-at-airbnb-869c44833ff2",
          "note": "Engineering blog post describing MDP-based conversational AI platform"
        }
      ],
      "related_cases": [
        "wayfair-wilma-copilot-2024"
      ],
      "tags": [
        "customer-support",
        "production",
        "operations",
        "automation"
      ]
    },
    {
      "id": "wayfair-color-algorithm-2021",
      "title": "Wayfair: Deconstructing 'Color' (RGB Values vs. Descriptive Human Names)",
      "category": "Optimization Pivot",
      "subcategory": "Data Representation",
      "companies_involved": [
        "Wayfair"
      ],
      "year": 2021,
      "initial_problem": "E-commerce product images contain RGB color values, but customers search and browse using human-friendly color names like 'navy blue' or 'sage green'",
      "initial_assumptions": [
        "RGB values sufficient for representing product colors",
        "Simple RGB-to-color-name mapping adequate",
        "Single color name per RGB value",
        "Direct pixel analysis gives accurate product colors"
      ],
      "why_it_fails": [
        "RGB values don't match how customers think about and search for colors",
        "Same RGB can have multiple valid descriptive names (e.g., #0c1f5e is both 'navy blue' and 'dark blue')",
        "Image noise (shadows, lighting) affects RGB extraction accuracy",
        "No hierarchical understanding of color relationships (navy is a child of blue)"
      ],
      "first_principle_insight": "Color is perceived hierarchically by humans - 'navy' is a type of 'blue', 'sage' is a type of 'green'. By creating a color taxonomy with RGB hierarchy (relationships between RGB values) and RGB naming (human-friendly names), can bridge computer vision and human perception. This enables better search and browsing experiences.",
      "how_to_reframe": {
        "old_atomic_unit": "Product image \u2192 RGB extraction \u2192 RGB values",
        "new_atomic_unit": "Product image \u2192 RGB extraction \u2192 color taxonomy (hierarchy + naming) \u2192 descriptive color names",
        "new_problem_type": "Taxonomy-based color naming with hierarchical relationships"
      },
      "architectural_changes": [
        "Color taxonomy with two components:",
        "1. RGB hierarchy: describes relationships between RGB values (#000080 'navy' is child of blue)",
        "2. RGB naming: assigns human-friendly names to RGB values (#0c1f5e \u2192 'navy blue' and 'dark blue')",
        "Extracts RGB values from product images",
        "Maps RGB to descriptive names using taxonomy",
        "Handles hierarchical color relationships",
        "Future work: CNN for metallic colors, noise robustness"
      ],
      "evidence": {
        "deployment": "Production color naming system for Wayfair product catalog",
        "challenges": "Not robust to noise (white products predicted as gray due to shadows)",
        "future": "Expanding to metallic colors with CNNs",
        "publication": "September 2021 on Wayfair Tech blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Wayfair Tech Blog (2021): From RGB to Descriptive Color Names: Wayfair's in-house color algorithms",
          "url": "https://www.aboutwayfair.com/careers/tech-blog/from-rgb-to-descriptive-color-names-wayfairs-in-house-color-algorithms-to-improve-customer-shopping-experience",
          "note": "Engineering blog post describing taxonomy-based color naming algorithm"
        }
      ],
      "related_cases": [],
      "tags": [
        "computer-vision",
        "e-commerce",
        "search",
        "ML"
      ]
    },
    {
      "id": "linkedin-job-search-ai-2025",
      "title": "LinkedIn: Deconstructing 'Job Search' (Keyword Matching vs. LLM Semantic Understanding)",
      "category": "Architectural Pivot",
      "subcategory": "Search Paradigm",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2025,
      "initial_problem": "Job seekers had to know exact keywords to find relevant opportunities, particularly disadvantaging newcomers to workforce. Legacy system lacked capacity for deep semantic understanding of nuanced user intent",
      "initial_assumptions": [
        "Fixed, taxonomy-based methods adequate for job search",
        "Exact keyword matching sufficient for connecting job seekers with opportunities",
        "Older LLMs could handle job search complexity",
        "Real-time LLM inference feasible at LinkedIn scale (1.2 billion members)"
      ],
      "why_it_fails": [
        "Keyword-based search couldn't understand nuanced user intent",
        "Job seekers needed to know exact terminology, creating barrier for newcomers",
        "Fixed taxonomy couldn't capture complex requirements like 'mostly remote but not from sourcing companies'",
        "High computational costs of LLMs prohibitive for real-time search at scale"
      ],
      "first_principle_insight": "Job search is fundamentally about semantic understanding of complex intent, not keyword matching. By using LLM distillation (splitting into retrieval and ranking steps) and embedding-based retrieval, can achieve deep semantic understanding at scale while managing computational costs.",
      "how_to_reframe": {
        "old_atomic_unit": "Keywords \u2192 taxonomy matching \u2192 job results",
        "new_atomic_unit": "Natural language intent \u2192 LLM distillation (retrieval + ranking) \u2192 GPU-optimized embedding retrieval \u2192 semantically relevant jobs",
        "new_problem_type": "Multi-stage LLM architecture with distillation and semantic retrieval"
      },
      "architectural_changes": [
        "Natural language search: users describe ideal job in plain English",
        "LLM distillation: split into two steps (retrieval and ranking) to manage costs",
        "Multi-stage architecture combining retrieval and ranking models",
        "Synthetic data generation for training",
        "GPU-optimized embedding-based retrieval",
        "Cross-encoder ranking models for final ordering",
        "Transformed from keyword-based to semantic search system"
      ],
      "evidence": {
        "scale": "Serving 1.2 billion LinkedIn members",
        "impact": "Enables complex natural language queries impossible with keyword matching",
        "publication": "2025 on LinkedIn Engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2025): Building the next generation of job search at LinkedIn",
          "url": "https://www.linkedin.com/blog/engineering/ai/building-the-next-generation-of-job-search-at-linkedin",
          "note": "Engineering blog post describing LLM-powered semantic job search transformation"
        }
      ],
      "related_cases": [
        "yelp-query-understanding-2025",
        "picnic-llm-search-2024"
      ],
      "tags": [
        "enterprise",
        "LLM",
        "search",
        "ML",
        "embeddings",
        "NLP"
      ]
    },
    {
      "id": "etsy-visual-representation-2024",
      "title": "Etsy: Deconstructing 'Visual Search' (Single Model vs. Efficient Multi-Architecture Approach)",
      "category": "Optimization Pivot",
      "subcategory": "Model Efficiency",
      "companies_involved": [
        "Etsy"
      ],
      "year": 2024,
      "initial_problem": "Visual search and visually similar recommendations require encoding diverse handmade/vintage items as vector representations, but at e-commerce scale with latency-sensitive applications",
      "initial_assumptions": [
        "Single pretrained model sufficient for visual representation",
        "Same model architecture works for all visual tasks",
        "Can trade efficiency for accuracy in visual search",
        "Standard fine-tuning approaches adequate for e-commerce scale"
      ],
      "why_it_fails": [
        "E-commerce visual diversity (handmade, vintage, diverse categories) challenges single-model approaches",
        "Latency-sensitive applications (visual search, similar recs) need faster inference",
        "Low-resource settings common in production require efficiency",
        "Standard approaches don't optimize across multiple efficiency dimensions"
      ],
      "first_principle_insight": "Efficiency in deep learning operates along multiple axes: model architecture, training, evaluation, and serving. By exploring both CNN and vision transformer families with efficient fine-tuning techniques, can achieve better accuracy-latency tradeoffs for e-commerce visual tasks.",
      "how_to_reframe": {
        "old_atomic_unit": "Single pretrained model \u2192 fine-tuning \u2192 visual embeddings",
        "new_atomic_unit": "Multiple architecture families (CNN + ViT) \u2192 efficient fine-tuning in low-resource settings \u2192 optimized visual representations",
        "new_problem_type": "Multi-axis efficiency optimization for visual representation learning"
      },
      "architectural_changes": [
        "EfficientNet family of CNN architectures for efficient inference",
        "Vision transformer models for improved accuracy",
        "Efficient fine-tuning techniques for low-resource settings",
        "Novel multilingual text-to-image generative offline evaluation method",
        "Optimization across architecture, training, evaluation, and serving",
        "Latency-optimized serving for real-time visual search and recommendations"
      ],
      "evidence": {
        "deployment": "Powers visual search and visually similar recommendations at Etsy scale",
        "publication": "August 2024 on Etsy Engineering blog, research paper published at Springer 2025",
        "research": "arXiv paper: Efficient Large-Scale Visual Representation Learning And Evaluation"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Etsy Engineering Blog (2024): Efficient Visual Representation Learning And Evaluation",
          "url": "https://www.etsy.com/codeascraft/efficient-visual-representation-learning-and-evaluation",
          "note": "Engineering blog post describing efficient visual representation techniques"
        },
        {
          "type": "supplementary",
          "citation": "arXiv (2023): Efficient Large-Scale Visual Representation Learning And Evaluation",
          "url": "https://arxiv.org/abs/2305.13399",
          "note": "Research paper on efficient visual representation learning at scale"
        }
      ],
      "related_cases": [],
      "tags": [
        "computer-vision",
        "efficiency",
        "deep-learning",
        "e-commerce"
      ]
    },
    {
      "id": "instacart-yoda-fraud-2024",
      "title": "Instacart: Deconstructing 'Fraud Detection' (Batch Rules vs. Real-Time Decision Platform)",
      "category": "Architectural Pivot",
      "subcategory": "Real-Time Infrastructure",
      "companies_involved": [
        "Instacart"
      ],
      "year": 2024,
      "initial_problem": "Detect fraudulent activities (fake accounts, payment fraud, customer-shopper collusion) quickly and take appropriate measures across Instacart platform",
      "initial_assumptions": [
        "Batch processing sufficient for fraud detection",
        "Static rules can catch fraud patterns",
        "Single data source adequate for fraud decisions",
        "Fraud detection doesn't require sub-second latency"
      ],
      "why_it_fails": [
        "Fraud patterns range from fake accounts to complex collusion, requiring diverse signals",
        "Batch processing too slow for real-time fraud prevention",
        "Static rules can't adapt quickly to evolving fraud tactics",
        "Need to combine ML inference, feature store, and real-time data for accurate decisions"
      ],
      "first_principle_insight": "Fraud decisions must happen in fractions of a second to prevent losses. By building a real-time decision platform (Yoda) with ClickHouse as the datastore, can implement dynamic rules that combine ML inference, feature store data, and real-time signals to distinguish legitimate from fraudulent activities instantly.",
      "how_to_reframe": {
        "old_atomic_unit": "Batch analysis \u2192 static rules \u2192 delayed fraud detection",
        "new_atomic_unit": "Real-time signals + ML inference + feature store \u2192 Yoda decision platform \u2192 instant fraud action",
        "new_problem_type": "Real-time decision platform with sub-second latency requirements"
      },
      "architectural_changes": [
        "Yoda: decision platform service for real-time fraud detection",
        "ClickHouse as primary real-time datastore for sub-second decisions",
        "Integration of multiple signal sources: ML inference services, Feature Store, ClickHouse",
        "Dynamic rule engine distinguishing legitimate from fraudulent activities",
        "Configurable actions based on fraud type and severity",
        "Real-time feature computation and decision-making"
      ],
      "evidence": {
        "performance": "Fraud decisions in fractions of a second",
        "deployment": "Production fraud detection across Instacart platform",
        "publication": "March 18, 2024 on Instacart tech blog",
        "authors": "Nick Shieh, Shen Zhu, Xiaobing Xia"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Instacart Tech Blog (2024): Real-time Fraud Detection with Yoda and ClickHouse",
          "url": "https://tech.instacart.com/real-time-fraud-detection-with-yoda-and-clickhouse-bd08e9dbe3f4",
          "note": "Engineering blog post describing Yoda real-time fraud detection platform"
        }
      ],
      "related_cases": [
        "uber-risk-entity-watch-2023",
        "grab-graph-anomaly-2023"
      ],
      "tags": [
        "fraud-detection",
        "real-time",
        "analytics",
        "rules"
      ]
    },
    {
      "id": "expedia-lodging-ranking-2024",
      "title": "Expedia: Deconstructing 'Candidate Generation' (Full Scan vs. Smart Pre-Filtering)",
      "category": "Optimization Pivot",
      "subcategory": "Candidate Selection",
      "companies_involved": [
        "Expedia"
      ],
      "year": 2024,
      "initial_problem": "Lodging ranking system operates across Expedia, Hotels.com, and Vrbo at massive scale, requiring efficient candidate generation before expensive ranking computations",
      "initial_assumptions": [
        "Can rank all properties in search destination",
        "Ranking algorithms fast enough to handle full property catalog",
        "Same candidate generation strategy works for all destination sizes",
        "Historical data not useful for candidate pruning"
      ],
      "why_it_fails": [
        "Medium destinations (NYC, LA, Orlando) have 10,000+ properties - too many to rank",
        "Large destinations (states, countries) have even more properties",
        "Full ranking computationally expensive at scale",
        "Strict latency constraints for user experience",
        "Different destination sizes need different strategies"
      ],
      "first_principle_insight": "Candidate generation's goal is to limit pool to manageable size for re-ranking while maintaining relevance and adhering to latency constraints. By using ML-based approach with historical search impression data, can intelligently pre-filter properties based on what actually gets shown and booked.",
      "how_to_reframe": {
        "old_atomic_unit": "All properties in destination \u2192 expensive ranking \u2192 results",
        "new_atomic_unit": "Historical search impressions \u2192 ML candidate generation \u2192 limited relevant set \u2192 efficient ranking \u2192 results",
        "new_problem_type": "Machine learning-based candidate pre-filtering with impression-level optimization"
      },
      "architectural_changes": [
        "ML-based candidate generation using historical shopping data",
        "Search result impressions as training signal (ordered property lists shown to consumers)",
        "Adaptive strategy based on destination size (small, medium 10K+, large state/country level)",
        "Candidate population restriction by search destination",
        "Two-stage approach: candidate generation (L0) \u2192 ranking (L1+)",
        "Latency-optimized pre-filtering before expensive ranking"
      ],
      "evidence": {
        "scale": "Operating across Expedia, Hotels.com, and Vrbo brands",
        "deployment": "Production large-scale lodging ranking system",
        "publication": "May 2024 by Adam Woznica and Meli Sedghi"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Expedia Group Technology Blog (2024): Choosing the Right Candidates for Lodging Ranking",
          "url": "https://medium.com/expedia-group-tech/choosing-the-right-candidates-for-lodging-ranking-d0841bf40c0e",
          "note": "Engineering blog post describing ML-based candidate generation for ranking"
        }
      ],
      "related_cases": [
        "airbnb-embedding-retrieval-2025"
      ],
      "tags": [
        "ranking",
        "marketplace",
        "recommendations",
        "optimization"
      ]
    },
    {
      "id": "linkedin-pymk-graph-2024",
      "title": "LinkedIn: Deconstructing 'Network Growth' (Simple Heuristics vs. Multi-Stage Graph Recommendation)",
      "category": "Relational Pivot",
      "subcategory": "Graph-Based Recommendations",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2024,
      "initial_problem": "Build LinkedIn professional network by recommending relevant connections at massive scale (hundreds of terabytes of data, hundreds of billions of potential connections daily)",
      "initial_assumptions": [
        "Simple friend-of-friend heuristics sufficient",
        "Single recommendation algorithm can handle all use cases",
        "Same approach works for all member types (new, established, etc.)",
        "Direct graph analysis feasible at scale"
      ],
      "why_it_fails": [
        "Simple heuristics miss complex relationship patterns",
        "Single algorithm can't optimize for multiple objectives (invitation sent, accepted, etc.)",
        "Billions of potential connections computationally prohibitive to analyze directly",
        "Different member segments need different recommendation strategies"
      ],
      "first_principle_insight": "Professional networks follow graph patterns (triangle closing - friend of friend likely to be friend, co-workers through company connections). By using multi-stage ranking with diverse candidate generation sources (graph walks, embedding-based retrieval, heuristics) and neural network rankers, can build connections at scale while optimizing multiple objectives.",
      "how_to_reframe": {
        "old_atomic_unit": "Friend-of-friend heuristic \u2192 potential connections",
        "new_atomic_unit": "Multi-source candidate generation (graph walks, EBR, heuristics) \u2192 L0 ranking \u2192 neural network rankers (invitation probability, acceptance probability) \u2192 PYMK recommendations",
        "new_problem_type": "Multi-stage graph recommendation with heterogeneous candidate sources"
      },
      "architectural_changes": [
        "L0 Candidate Generation with multiple sources:",
        "  - Graph-based: random graph walks, n-hop neighbors, triangle closing",
        "  - Embedding-based retrieval (EBR) via similarity scores",
        "  - Heuristic sources: new members in geographic area",
        "Multiple neural network rankers estimating:",
        "  - Probability of invitation being sent",
        "  - Probability of invitation being accepted",
        "Economic graph traversal through companies (co-workers) and connections",
        "Daily processing of 100s of terabytes, 100s of billions potential connections"
      ],
      "evidence": {
        "impact": "Responsible for building more than 50% of LinkedIn's professional graph",
        "scale": "Processes hundreds of terabytes daily, hundreds of billions of potential connections",
        "deployment": "Production system serving 1B+ LinkedIn members",
        "invention": "PYMK was invented at LinkedIn, pioneering this approach"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2024): Candidate Generation in a Large Scale Graph Recommendation System: People You May Know",
          "url": "https://www.linkedin.com/blog/engineering/recommendations/candidate-generation-in-a-large-scale-graph-recommendation-system-people-you-may-know",
          "note": "Engineering blog post describing multi-stage PYMK recommendation architecture"
        }
      ],
      "related_cases": [
        "doordash-gnn-notifications-2024"
      ],
      "tags": [
        "graph-neural-networks",
        "recommendations",
        "social-networks",
        "ranking",
        "deep-learning"
      ]
    },
    {
      "id": "swiggy-devnet-fraud-2024",
      "title": "Swiggy: Deconstructing 'Fraud Detection' (Standard Anomaly Detection vs. DevNet with Variational Loss)",
      "category": "Optimization Pivot",
      "subcategory": "Anomaly Detection",
      "companies_involved": [
        "Swiggy"
      ],
      "year": 2024,
      "initial_problem": "Hyperlocal food delivery faces unique fraud challenges - customers seeking instant free food, logistically difficult to inspect returns, time-sensitive real-time detection needed, only 0.15% manually labeled data",
      "initial_assumptions": [
        "Standard anomaly detection sufficient for fraud",
        "Traditional e-commerce fraud detection applicable to food delivery",
        "DevNet framework adequate without modifications",
        "All unlabeled samples should be treated equally"
      ],
      "why_it_fails": [
        "Food delivery fraud is instant (immediate free food) unlike traditional e-commerce",
        "Cannot inspect returned items like physical goods",
        "Extremely limited labeled data (0.15%) - standard supervised approaches fail",
        "Standard DevNet treats all unlabeled samples equally, missing nuance in fraud probability"
      ],
      "first_principle_insight": "Not all unlabeled samples are equally likely to be fraudulent. By introducing K-Nearest Neighbors based variational scaling factor that scales misclassification loss based on hypothesized fraud degree, can better optimize anomaly scores even with minimal labeled data.",
      "how_to_reframe": {
        "old_atomic_unit": "Labeled fraud examples \u2192 standard DevNet \u2192 anomaly scores",
        "new_atomic_unit": "Labeled examples + KNN variational scaling \u2192 modified DevNet loss \u2192 fraud-degree-aware anomaly scores",
        "new_problem_type": "DevNet with KNN-based variational loss for semi-supervised fraud detection"
      },
      "architectural_changes": [
        "Modified DevNet model loss function with variational scaling factor",
        "KNN-based scaling factor for nominal (unlabeled) data",
        "Scales misclassification loss based on hypothesized degree of fraud",
        "Variational scaling generated from labeled samples during training",
        "Optimized for extremely limited labeled data scenarios (0.15%)",
        "Real-time fraud detection suitable for instant resolution demands"
      ],
      "evidence": {
        "metrics": "~6% improvement in recall, ~1.3% improvement in block rate at fixed precision vs original DevNet",
        "data": "Experiments on internal dataset with only 0.15% labeled data",
        "publication": "Presented at ACM IKDD CODS-COMAD 2024 in Bangalore",
        "authors": "Piyush Nikam, Rutvik Vijjali, Shaik Masihullah, Meghana Negi, Jose Matthew"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Swiggy Bytes (2024): Utilizing DevNet with Variational Loss for Fraud Detection in Hyperlocal Food Delivery",
          "url": "https://bytes.swiggy.com/utilizing-devnet-with-variational-loss-for-fraud-detection-in-hyperlocal-food-delivery-19e72999acfb",
          "note": "Engineering blog post describing DevNet modifications for food delivery fraud"
        },
        {
          "type": "supplementary",
          "citation": "CODS-COMAD 2024: Utilizing DevNet with Variational Loss for Fraud Detection",
          "url": "https://dl.acm.org/doi/10.1145/3632410.3632460",
          "note": "Academic paper presented at ACM conference"
        }
      ],
      "related_cases": [
        "grab-graph-anomaly-2023",
        "uber-risk-entity-watch-2023"
      ],
      "tags": [
        "fraud-detection",
        "anomaly-detection",
        "ML",
        "marketplace",
        "data-foundations"
      ]
    },
    {
      "id": "walmart-semantic-classification-2024",
      "title": "Walmart: Deconstructing 'Text Classification' (Traditional ML vs. Semantic Search with Faiss)",
      "category": "Architectural Pivot",
      "subcategory": "Classification Paradigm",
      "companies_involved": [
        "Walmart"
      ],
      "year": 2024,
      "initial_problem": "Text classification requires training new models for each category, doesn't scale well to new categories, and struggles with imbalanced class distributions",
      "initial_assumptions": [
        "Classification requires trained models per category set",
        "New categories require retraining from scratch",
        "Traditional ML classification (logistic regression, SVM) adequate",
        "Each category change necessitates full model lifecycle"
      ],
      "why_it_fails": [
        "Training new models for category changes is time-consuming",
        "Imbalanced class distributions reduce accuracy",
        "Scaling to many categories becomes infeasible",
        "New categories require extensive labeled data and retraining",
        "Traditional approaches can't leverage semantic understanding"
      ],
      "first_principle_insight": "Classification is fundamentally about finding similar examples. By reframing as semantic search with Faiss, can eliminate model training - simply generate embeddings for new data and add to index. Scales seamlessly from hundreds to billions of vectors while handling imbalanced classes naturally.",
      "how_to_reframe": {
        "old_atomic_unit": "Labeled training data \u2192 train classifier \u2192 classify new text",
        "new_atomic_unit": "Category examples \u2192 generate embeddings \u2192 Faiss index \u2192 nearest neighbor semantic search \u2192 classification",
        "new_problem_type": "Zero-shot classification via semantic similarity search"
      },
      "architectural_changes": [
        "Faiss library for high-efficiency vector comparison",
        "Scales from hundreds to billions of vectors",
        "Embedding generation for text using sentence transformers",
        "Approximate nearest neighbor search instead of classification",
        "No training required - just add new embeddings to index",
        "Hassle-free scaling: new categories added by updating index",
        "Handles imbalanced classes naturally through similarity search"
      ],
      "evidence": {
        "evaluation": "Tested on news dataset with 7 categories (space, hardware, medicine, religion, hockey, crypto, graphics), 4,064 records with imbalanced distribution",
        "deployment": "Production system at Walmart for text classification tasks",
        "research": "Multiple arXiv papers on semantic retrieval at Walmart",
        "publication": "2024 on Walmart Global Tech Blog by Harika Samala"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Walmart Global Tech Blog (2024): Transforming Text Classification with Semantic Search Techniques \u2014 Faiss",
          "url": "https://medium.com/walmartglobaltech/transforming-text-classification-with-semantic-search-techniques-faiss-c413f133d0e2",
          "note": "Engineering blog post describing semantic search approach to classification"
        },
        {
          "type": "supplementary",
          "citation": "arXiv (2024): Semantic Retrieval at Walmart",
          "url": "https://arxiv.org/abs/2412.04637",
          "note": "Research paper on semantic retrieval systems at Walmart"
        }
      ],
      "related_cases": [
        "walmart-attribute-extraction"
      ],
      "tags": [
        "NLP",
        "search",
        "embeddings",
        "ML",
        "production"
      ]
    },
    {
      "id": "booking-ranking-platform-2024",
      "title": "Booking.com: Deconstructing 'Search Ranking' (Monolithic Ranker vs. Ensemble of Specialized Models)",
      "category": "Architectural Pivot",
      "subcategory": "Ranking Architecture",
      "companies_involved": [
        "Booking.com"
      ],
      "year": 2024,
      "initial_problem": "Hotel search ranking needed to handle millions of properties while optimizing for multiple objectives (CTR, CVR, quality) and business logic (promotions, partner status)",
      "initial_assumptions": [
        "Single monolithic ranking model could handle all objectives",
        "Linear feature combinations adequate for property ranking",
        "Simple algorithms sufficient for capturing conversion patterns",
        "Uniform approach works across all contexts and user types"
      ],
      "why_it_fails": [
        "Different metrics (click-through, conversion, perceived quality) require different optimization targets",
        "Non-linear relationships (price fluctuations, diminishing returns of reviews) too complex for simple models",
        "Business logic (promotions, partner status) difficult to integrate into single model",
        "Monolithic model can't specialize for different ranking subtasks"
      ],
      "first_principle_insight": "Ranking is not a single prediction problem, but an ensemble of specialized prediction tasks. Each model should focus on one metric (pCTR, pCVR, quality), then combine outputs with business logic to deliver personalized, conversion-optimized rankings.",
      "how_to_reframe": {
        "old_atomic_unit": "Single model \u2192 features \u2192 ranking score \u2192 ranked properties",
        "new_atomic_unit": "Multiple specialized models (pCTR, pCVR, quality predictors) \u2192 ensemble with business logic \u2192 personalized conversion-optimized ranking",
        "new_problem_type": "Ensemble learning-to-rank with multi-objective optimization"
      },
      "architectural_changes": [
        "Ensemble of specialized ML models instead of monolithic ranker",
        "Each model predicts specific metrics: pCTR (click probability), pCVR (conversion likelihood), perceived quality",
        "Gradient Boosted Decision Trees (LambdaMART framework) for learning to rank",
        "Models capture non-linear relationships: price fluctuations, diminishing returns of additional reviews",
        "Thousands of input features from user profiles, property attributes, contextual factors",
        "Business logic integration layer: promotions, partner statuses, inventory constraints",
        "Final scoring combines model outputs with business rules for personalized ranking"
      ],
      "evidence": {
        "scale": "~548 million visits in September 2024",
        "impact": "Personalized conversion-optimized rankings balancing multiple objectives",
        "deployment": "Production system handling massive global scale",
        "publication": "2024 on Booking.com Engineering Blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Booking.com Engineering Blog (2024): The Engineering Behind High-Performance Ranking Platform",
          "url": "https://medium.com/booking-com-development/the-engineering-behind-booking-coms-ranking-platform-a-system-overview-2fb222003ca6",
          "note": "System overview describing ensemble architecture and multi-objective optimization"
        }
      ],
      "related_cases": [
        "pinterest-ads-conversion-2024",
        "expedia-candidate-generation-2024"
      ],
      "tags": [
        "ranking",
        "ML",
        "optimization",
        "search"
      ]
    },
    {
      "id": "linkedin-skills-content-extraction-2023",
      "title": "LinkedIn: Deconstructing 'Skills Identification' (Explicit Tags vs. Content Extraction with Graph Mapping)",
      "category": "Architectural Pivot",
      "subcategory": "Information Extraction",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2023,
      "initial_problem": "Many LinkedIn members include skills in Summary, Experience sections, or resumes rather than dedicated Skills section, making skills invisible to Skills Graph that powers job matching and opportunities",
      "initial_assumptions": [
        "Skills only need to be captured from dedicated Skills section",
        "Explicit skill tags sufficient for job matching",
        "Text extraction straightforward for structured fields",
        "All skills equally important regardless of context"
      ],
      "why_it_fails": [
        "Members add skills in multiple locations beyond dedicated section",
        "Unstructured text (summaries, experience descriptions) contains valuable skill signals",
        "Skills embedded in resumes, LinkedIn Learning courses, feed posts not captured",
        "Context matters: skills in qualifications section more important than company description",
        "41,000+ skills in graph need accurate mapping from varied text sources"
      ],
      "first_principle_insight": "Skills identification is not just explicit tagging, but extracting and mapping skills from any content onto a structured graph. By parsing content into structured sections, using AI for extraction, and expanding with graph relationships, can surface skills regardless of where members mention them.",
      "how_to_reframe": {
        "old_atomic_unit": "Dedicated Skills section \u2192 explicit tags \u2192 Skills Graph",
        "new_atomic_unit": "Any content source \u2192 section-aware parsing \u2192 AI extraction \u2192 graph mapping with skill relationships \u2192 Skills Graph",
        "new_problem_type": "Multi-source information extraction with graph-based skill expansion"
      },
      "architectural_changes": [
        "Multi-source skill extraction: profiles, resumes, Learning courses, feed posts, job descriptions",
        "Section-aware parsing: understand context (qualifications vs. company description)",
        "AI-powered extraction models for skills from unstructured text",
        "Graph-based skill expansion: bring in related skills from same group",
        "Structural relationship mapping: parent skills, children skills, sibling skills",
        "Context-aware importance weighting based on where skill appears",
        "Integration with 41,000+ skill LinkedIn Skills Graph"
      ],
      "evidence": {
        "scale": "41,000+ skills in LinkedIn Skills Graph",
        "impact": "Captures skills from multiple content sources, improving job matching and opportunity discovery",
        "deployment": "Production system across LinkedIn platform",
        "publication": "2023 on LinkedIn Engineering Blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2023): Extracting skills from content to fuel the LinkedIn Skills Graph",
          "url": "https://engineering.linkedin.com/blog/2023/extracting-skills-from-content-to-fuel-the-linkedin-skills-graph",
          "note": "Official engineering post describing architecture and platform for skills extraction"
        }
      ],
      "related_cases": [
        "shopify-product-classification-2025"
      ],
      "tags": [
        "NLP",
        "knowledge-graph",
        "enterprise",
        "graph-neural-networks"
      ]
    },
    {
      "id": "uber-eats-graph-learning-2020",
      "title": "Uber Eats: Deconstructing 'Food Recommendations' (Feature-Based ML vs. Graph Neural Networks)",
      "category": "Architectural Pivot",
      "subcategory": "Recommendation Architecture",
      "companies_involved": [
        "Uber"
      ],
      "year": 2020,
      "initial_problem": "Food recommendations relied on hand-crafted features and traditional ML models that couldn't capture complex user-dish-restaurant relationships, limiting personalization quality",
      "initial_assumptions": [
        "Hand-crafted features sufficient for food recommendations",
        "User preferences independent of network effects",
        "Traditional collaborative filtering adequate for discovering new foods",
        "Ordering patterns can be captured with simple features"
      ],
      "why_it_fails": [
        "Complex relationships between users, dishes, and restaurants not captured",
        "Network effects matter: what similar users order influences recommendations",
        "Feature engineering can't capture higher-order relationships",
        "Weighted graphs (multiple orders of same dish) need specialized handling",
        "Cold start problem for new users and dishes"
      ],
      "first_principle_insight": "Food discovery is fundamentally a graph problem where users, dishes, and restaurants form interconnected networks. By modeling these as bipartite graphs and using GNNs to learn node embeddings, can capture ordering patterns and network effects that feature-based models miss.",
      "how_to_reframe": {
        "old_atomic_unit": "User features + dish features \u2192 traditional ML model \u2192 recommendation score",
        "new_atomic_unit": "User-dish-restaurant bipartite graphs \u2192 GraphSAGE with weighted edges \u2192 node embeddings \u2192 similarity-based recommendations",
        "new_problem_type": "Graph neural network recommendation with weighted bipartite graphs"
      },
      "architectural_changes": [
        "Two bipartite graphs: (1) users-dishes with order counts, (2) users-restaurants",
        "Modified GraphSAGE (similar to PinSAGE) for graph learning",
        "Weighted edge handling: edge weight = number of times user ordered dish",
        "Low-rank positive term added to distinguish ordering patterns",
        "Node embeddings for users, restaurants, dishes capture network structure",
        "Vector similarity approximates connection strength between nodes",
        "Personalized ranker combines graph embeddings with contextual features (day, time, location)",
        "12% boost in AUC compared to baseline without graph embeddings"
      ],
      "evidence": {
        "metrics": "12% increase in AUC compared to baseline model",
        "deployment": "Field experiment on 2% of global consumers, then global deployment on app homepage",
        "scale": "Production system serving millions of Uber Eats users globally",
        "publication": "2020 on Uber Engineering Blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog (2020): Food Discovery with Uber Eats: Using Graph Learning to Power Recommendations",
          "url": "https://www.uber.com/blog/uber-eats-graph-learning/",
          "note": "Engineering blog describing GraphSAGE architecture and bipartite graph approach"
        }
      ],
      "related_cases": [
        "doordash-gnn-notifications-2024",
        "pinterest-conversion-evolution-2024"
      ],
      "tags": [
        "graph-neural-networks",
        "recommendations",
        "embeddings"
      ]
    },
    {
      "id": "spotify-domain-aware-llms-2024",
      "title": "Spotify: Deconstructing 'LLM Personalization' (Generic LLMs vs. Domain-Adapted with Semantic IDs)",
      "category": "Optimization Pivot",
      "subcategory": "LLM Adaptation",
      "companies_involved": [
        "Spotify"
      ],
      "year": 2024,
      "initial_problem": "Generic LLMs trained on general text lack understanding of Spotify's catalog (songs, artists, playlists) and user listening behaviors, limiting personalization quality",
      "initial_assumptions": [
        "Out-of-the-box LLMs sufficient for music recommendations",
        "World knowledge from pre-training enough for music domain",
        "Generic text representations adequate for music catalog",
        "Standard LLM fine-tuning enough for personalization"
      ],
      "why_it_fails": [
        "LLMs trained on general text, not Spotify's unique music catalog",
        "Can't represent relationships between songs, artists, users in native LLM vocabulary",
        "Generic embeddings don't capture music-specific semantics",
        "Lack catalog-native identifiers for efficient reasoning",
        "Risk of catastrophic forgetting during domain adaptation"
      ],
      "first_principle_insight": "Music recommendations require LLMs that 'speak Spotify' - understanding catalog relationships and user behaviors through domain adaptation. By creating Semantic IDs (compact catalog-native identifiers) and multi-task adaptation across Spotify-specific tasks, can achieve domain expertise while preserving general capabilities.",
      "how_to_reframe": {
        "old_atomic_unit": "Generic LLM (Llama) \u2192 zero-shot inference \u2192 recommendations",
        "new_atomic_unit": "Semantic IDs (catalog + behavior encoding) \u2192 multi-task domain adaptation (10 Spotify tasks) \u2192 domain-aware LLM \u2192 personalized recommendations",
        "new_problem_type": "Domain-specific LLM adaptation with catalog-native representations"
      },
      "architectural_changes": [
        "Semantic IDs: compact identifiers encoding relationships between content and users",
        "Multi-task adaptation: 10 Spotify-specific tasks for domain learning",
        "Model selection: Llama 3.1 8B demonstrated competitive performance",
        "Training approaches: extended pre-training, supervised instruction fine-tuning, RLHF, DPO",
        "Training data: internal examples + music domain expert content + synthetic data",
        "14% improvement in Spotify-specific tasks vs. out-of-the-box Llama",
        "Minimal MMLU score differences (no catastrophic forgetting)",
        "Contextualized recommendations through personalized narratives"
      ],
      "evidence": {
        "metrics": "Up to 14% improvement in Spotify-specific tasks, minimal MMLU degradation",
        "deployment": "Production LLM serving infrastructure for recommendations",
        "publication": "December 2024 on Spotify Research blog",
        "approach": "Evaluated LLMs from 1B to 8B parameters"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Spotify Research (2024): Teaching Large Language Models to Speak Spotify: How Semantic IDs Enable Personalization",
          "url": "https://research.atspotify.com/2025/11/teaching-large-language-models-to-speak-spotify-how-semantic-ids-enable",
          "note": "Research publication describing Semantic IDs and domain adaptation approach"
        },
        {
          "type": "primary",
          "citation": "Spotify Research (2024): Contextualized Recommendations Through Personalized Narratives using LLMs",
          "url": "https://research.atspotify.com/2024/12/contextualized-recommendations-through-personalized-narratives-using-llms",
          "note": "Research on using domain-adapted LLMs for personalized recommendations"
        }
      ],
      "related_cases": [
        "grammarly-delicate-text-2024"
      ],
      "tags": [
        "LLM",
        "ML",
        "embeddings",
        "personalization",
        "fine-tuning"
      ]
    },
    {
      "id": "linkedin-rag-knowledge-graph-2024",
      "title": "LinkedIn: Deconstructing 'Customer Service QA' (Plain Text RAG vs. Knowledge Graph-Enhanced RAG)",
      "category": "Architectural Pivot",
      "subcategory": "RAG Architecture",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2024,
      "initial_problem": "Customer service question answering from historical issue tracking tickets treated corpus as plain text, missing crucial intra-issue structure and inter-issue relationships",
      "initial_assumptions": [
        "Plain text retrieval sufficient for issue resolution",
        "Historical tickets can be treated as independent documents",
        "Keyword/semantic search adequate for finding relevant issues",
        "Issue structure (description, resolution steps, root cause) doesn't need explicit representation"
      ],
      "why_it_fails": [
        "Loses critical intra-issue structure: description, steps, root cause, resolution",
        "Ignores inter-issue relationships: similar problems, common solutions",
        "Text-only RAG can't leverage graph relationships between issues",
        "Context from where information appears matters (qualifications vs. description)",
        "Difficult to trace solution patterns across related issues"
      ],
      "first_principle_insight": "Customer service QA is fundamentally a graph problem where issues have internal structure and external relationships. By constructing a knowledge graph from historical tickets and using graph-aware retrieval, can preserve both structure and relations to dramatically improve answer quality and resolution speed.",
      "how_to_reframe": {
        "old_atomic_unit": "Historical tickets \u2192 plain text corpus \u2192 semantic search \u2192 answer generation",
        "new_atomic_unit": "Historical tickets \u2192 knowledge graph (with intra-issue structure + inter-issue relations) \u2192 sub-graph retrieval \u2192 graph-aware answer generation",
        "new_problem_type": "Knowledge graph-enhanced RAG with structural awareness"
      },
      "architectural_changes": [
        "Knowledge graph construction from historical issue tracking tickets",
        "Intra-issue structure preservation: description, steps, root cause, resolution",
        "Inter-issue relation modeling: similar problems, common solutions, related entities",
        "Query parsing to identify key entities and relationships",
        "Sub-graph retrieval: fetch related issue sub-graphs instead of flat documents",
        "Graph-aware context assembly for LLM generation",
        "Structural relationship expansion: parent issues, related issues, solution patterns"
      ],
      "evidence": {
        "metrics": "77.6% improvement in MRR (Mean Reciprocal Rank), 0.32 gain in BLEU score, 28.6% reduction in median per-issue resolution time",
        "deployment": "Production deployment within LinkedIn customer service team for ~6 months",
        "publication": "April 2024, presented at SIGIR 2024 conference",
        "scale": "Deployed across LinkedIn's customer service operations"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Research (2024): Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
          "url": "https://arxiv.org/abs/2404.17723",
          "note": "SIGIR 2024 paper describing KG-enhanced RAG architecture and results"
        }
      ],
      "related_cases": [
        "linkedin-skills-content-extraction-2023",
        "thomson-reuters-rag-2024"
      ],
      "tags": [
        "RAG",
        "knowledge-graph",
        "customer-support",
        "LLM",
        "retrieval",
        "ML"
      ]
    },
    {
      "id": "thomson-reuters-rag-2024",
      "title": "Thomson Reuters: Deconstructing 'Customer Support' (Manual Knowledge Base vs. RAG-Powered Chatbot)",
      "category": "Architectural Pivot",
      "subcategory": "Customer Support Architecture",
      "companies_involved": [
        "Thomson Reuters"
      ],
      "year": 2024,
      "initial_problem": "Customer support agents spent significant time manually searching through curated knowledge base to find relevant solutions, slowing resolution times and reducing service quality",
      "initial_assumptions": [
        "Manual knowledge base search acceptable for support agents",
        "Keyword search sufficient for finding solutions",
        "Support agents can efficiently navigate complex documentation",
        "Customer success and research functions operate independently"
      ],
      "why_it_fails": [
        "Manual search through large knowledge base time-consuming",
        "Keyword matching misses semantically similar but differently-worded solutions",
        "No coordination between research team insights and support team actions",
        "Agents may miss relevant solutions due to terminology mismatches",
        "Knowledge base growing faster than agents can keep up"
      ],
      "first_principle_insight": "Customer support is fundamentally about rapid access to the most relevant solution from available knowledge. By using RAG with GPT-4 to coordinate research and customer success functions, can create a conversational interface that retrieves and presents the best solutions instantly, reducing resolution times.",
      "how_to_reframe": {
        "old_atomic_unit": "Agent query \u2192 manual knowledge base search \u2192 solution identification \u2192 response",
        "new_atomic_unit": "Agent query \u2192 RAG retrieval (semantic + context) \u2192 GPT-4 synthesis \u2192 conversational solution with sources",
        "new_problem_type": "LLM-powered retrieval-augmented support system"
      },
      "architectural_changes": [
        "GPT-4 powered conversational interface for support agents",
        "RAG system with semantic retrieval over curated knowledge base",
        "Coordination layer between research and customer success teams",
        "Conversational interface: agents can refine queries naturally",
        "Source attribution: show which documents informed the answer",
        "Context-aware retrieval considering agent's current case context",
        "Real-time knowledge base updates from research team"
      ],
      "evidence": {
        "impact": "Reduced resolution times, faster access to relevant solutions",
        "deployment": "Production system for Thomson Reuters customer support agents",
        "publication": "2024 on Thomson Reuters Labs Medium blog",
        "integration": "Westlaw AI-Assisted Research uses similar RAG to prevent hallucinations"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Thomson Reuters Labs (2024): Better Customer Support Using Retrieval-Augmented Generation (RAG)",
          "url": "https://medium.com/tr-labs-ml-engineering-blog/better-customer-support-using-retrieval-augmented-generation-rag-at-thomson-reuters-4d140a6044c3",
          "note": "Engineering blog describing RAG implementation for customer support"
        }
      ],
      "related_cases": [
        "linkedin-rag-knowledge-graph-2024"
      ],
      "tags": [
        "RAG",
        "customer-support",
        "GPT-4",
        "LLM",
        "retrieval"
      ]
    },
    {
      "id": "stripe-fraud-detection-foundation-model-2024",
      "title": "Stripe: Deconstructing 'Fraud Detection' (Rule-Based + Simple ML vs. Payments Foundation Model)",
      "category": "Optimization Pivot",
      "subcategory": "Foundation Model Application",
      "companies_involved": [
        "Stripe"
      ],
      "year": 2024,
      "initial_problem": "Traditional fraud detection relied on rules and simple ML models that struggled to capture subtle patterns across tens of billions of transactions, leading to false declines and missed fraud",
      "initial_assumptions": [
        "Rule-based fraud detection sufficient with manual tuning",
        "Simple ML models can capture fraud patterns adequately",
        "Each transaction can be evaluated independently",
        "Generic ML models adequate for payment fraud",
        "Rapid adaptation to new fraud patterns requires manual intervention"
      ],
      "why_it_fails": [
        "Rule-based systems can't capture subtle signals across billions of transactions",
        "Simple ML models miss complex fraud patterns and relationships",
        "Legitimate transactions declined due to overly conservative rules ($6B+ impact)",
        "Card-testing attacks evolve faster than manual rule updates",
        "Cross-merchant fraud patterns invisible to single-merchant models",
        "Cold start problem for new businesses without transaction history"
      ],
      "first_principle_insight": "Fraud detection is fundamentally about recognizing subtle patterns across the entire payments ecosystem. By training a foundation model on tens of billions of transactions from millions of businesses, can capture hundreds of subtle signals that generalize across contexts, adapting to new fraud types 'practically overnight'.",
      "how_to_reframe": {
        "old_atomic_unit": "Transaction features \u2192 rules + simple ML model \u2192 fraud score",
        "new_atomic_unit": "Transaction + ecosystem context \u2192 payments foundation model (trained on tens of billions of transactions) \u2192 nuanced fraud probability with 100+ subtle signals",
        "new_problem_type": "Foundation model for payments ecosystem with transfer learning"
      },
      "architectural_changes": [
        "Payments foundation model trained on tens of billions of transactions",
        "Captures hundreds of subtle signals invisible to simple models",
        "Cross-merchant pattern recognition: fraud patterns across ecosystem",
        "Rapid adaptation: 64% improvement in card-testing detection 'practically overnight'",
        "Authorization Boost: ML-powered legitimate transaction recovery",
        "Real-time inference at massive scale (1.4 trillion in payment volume)",
        "Transfer learning: new businesses benefit from ecosystem knowledge"
      ],
      "evidence": {
        "metrics": "64% increase in card-testing attack detection, $6B+ in legitimate declined transactions recovered in 2024, 2.2% average acceptance rate increase with Authorization Boost",
        "scale": "$1.4 trillion in total payment volume (2024), 38% YoY growth",
        "deployment": "Production across Stripe's global payment processing",
        "publication": "2024-2025 Stripe announcements and engineering blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Stripe Blog (2024): Stripe unveils AI foundation model for payments",
          "url": "https://stripe.com/blog/using-ai-optimize-payments-performance-payments-intelligence-suite",
          "note": "Official announcement of payments foundation model and performance improvements"
        },
        {
          "type": "primary",
          "citation": "TechCrunch (2025): Stripe unveils AI foundation model for payments, reveals 'deeper partnership' with Nvidia",
          "url": "https://techcrunch.com/2025/05/07/stripe-unveils-ai-foundation-model-for-payments-reveals-deeper-partnership-with-nvidia/",
          "note": "Coverage of foundation model architecture and Nvidia partnership"
        }
      ],
      "related_cases": [
        "instacart-yoda-fraud-detection-2024",
        "swiggy-devnet-fraud-detection-2024"
      ],
      "tags": [
        "fraud-detection",
        "foundation-model",
        "fintech",
        "ML"
      ]
    },
    {
      "id": "shopify-semantic-search-realtime-ml-2024",
      "title": "Shopify: Deconstructing 'Product Search' (Keyword Matching vs. Real-Time Semantic Understanding)",
      "category": "Architectural Pivot",
      "subcategory": "Search Architecture",
      "companies_involved": [
        "Shopify"
      ],
      "year": 2024,
      "initial_problem": "Traditional keyword-based search couldn't understand consumer intent when queries used different terminology than product descriptions, leading to poor match quality and zero results",
      "initial_assumptions": [
        "Keyword matching sufficient for product search",
        "Shoppers use same terminology as merchants",
        "Classical information retrieval (typo correction, synonyms) adequate",
        "Batch-computed embeddings sufficient for semantic understanding",
        "Search relevance can be maintained without real-time updates"
      ],
      "why_it_fails": [
        "Keyword matching fails when shoppers use different terms than merchants",
        "Consumer intent often nuanced and contextual beyond keywords",
        "Synonym lists can't capture full semantic understanding",
        "Batch embeddings become stale as catalog updates in real-time",
        "Search results 'zero' frustrates shoppers with valid intent",
        "Product catalog changes faster than batch embedding updates"
      ],
      "first_principle_insight": "Product search is fundamentally about understanding consumer intent and matching it to product semantics in real-time. By building real-time embedding pipelines that deliver text and image embeddings as centralized ML assets, can enable semantic search that understands intent beyond keywords while staying current with catalog updates.",
      "how_to_reframe": {
        "old_atomic_unit": "Query keywords \u2192 typo correction + synonyms \u2192 keyword matching \u2192 ranked products",
        "new_atomic_unit": "Query \u2192 real-time embedding (Apache Beam + GPU) \u2192 semantic similarity \u2192 ML ranking (transformers + LLMs) \u2192 intent-matched products",
        "new_problem_type": "Real-time semantic search with streaming embeddings"
      },
      "architectural_changes": [
        "Real-time embedding pipelines using Apache Beam for streaming",
        "GPU-optimized embedding generation for text and images",
        "Centralized ML assets: embeddings as foundational building blocks",
        "Semantic search layer: beyond keyword matching to intent understanding",
        "ML ranking combining classical IR with modern transformers/LLMs",
        "Integration of typo correction, synonyms, faceting with semantic search",
        "Real-time catalog updates reflected in embeddings immediately",
        "Multimodal: text and image embeddings for comprehensive understanding"
      ],
      "evidence": {
        "impact": "Improved consumer search intent understanding, reduced 'search result zero'",
        "deployment": "Production across Shopify merchant storefronts",
        "publication": "October 15, 2024 on Shopify Engineering blog and Google Cloud Blog",
        "scale": "Serving millions of Shopify merchants globally"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Shopify Engineering (2024): How Shopify improved consumer search intent with real-time ML",
          "url": "https://shopify.engineering/how-shopify-improved-consumer-search-intent-with-real-time-ml",
          "note": "Engineering blog describing real-time embedding pipeline and semantic search"
        },
        {
          "type": "secondary",
          "citation": "Google Cloud Blog (2024): How Shopify improved consumer search intent with real-time ML",
          "url": "https://cloud.google.com/blog/products/data-analytics/how-shopify-improved-consumer-search-intent-with-real-time-ml",
          "note": "Google Cloud perspective on Shopify's real-time ML infrastructure"
        }
      ],
      "related_cases": [
        "linkedin-job-search-ai-2025",
        "doordash-llm-search-2024",
        "yelp-query-understanding-2024"
      ],
      "tags": [
        "search",
        "real-time",
        "embeddings",
        "MLOps",
        "e-commerce"
      ]
    },
    {
      "id": "canva-synthetic-search-evaluation-2024",
      "title": "Canva: Deconstructing 'Search Evaluation' (Real User Data vs. Synthetic Data Generation)",
      "category": "Architectural Pivot",
      "subcategory": "Evaluation Architecture",
      "companies_involved": [
        "Canva"
      ],
      "year": 2024,
      "initial_problem": "Cannot evaluate search improvements by looking at real user queries/designs due to 'Be a Good Human' privacy principle, making search optimization impossible without violating user privacy",
      "initial_assumptions": [
        "Need real user data to evaluate search quality",
        "User queries and designs necessary for labeling training data",
        "Online A/B tests only way to measure search improvements",
        "Privacy and evaluation quality are in direct conflict"
      ],
      "why_it_fails": [
        "Viewing private Canva designs violates core company value",
        "Real user data labeling impossible due to privacy constraints",
        "Online experiments slow (2-3 days per test) and expensive",
        "Can't share evaluation datasets among engineers due to privacy",
        "Limited iteration speed blocks rapid search improvements"
      ],
      "first_principle_insight": "Search evaluation is fundamentally about understanding query-document relevance, not accessing real user data. By using LLMs to generate synthetic designs and plausible queries, can create shareable, pre-labeled evaluation datasets that enable offline testing at scale while preserving privacy.",
      "how_to_reframe": {
        "old_atomic_unit": "Real user queries + designs \u2192 manual labeling \u2192 online A/B test \u2192 search quality measurement",
        "new_atomic_unit": "LLM-generated synthetic designs + queries \u2192 pre-labeled static dataset \u2192 offline batch evaluation (1000+ tests in 10 min) \u2192 search quality measurement",
        "new_problem_type": "Synthetic data generation for privacy-preserving evaluation"
      },
      "architectural_changes": [
        "LLM generation of mock design documents (greeting cards, pitch decks, resumes)",
        "Separate LLM generates plausible user queries for each synthetic design",
        "100% artificial, pre-labeled test set shareable among all engineers",
        "Offline evaluation: 1000+ test cases in <10 minutes (vs. 2-3 days for online)",
        "300+ offline evaluations per 2-3 day period (vs. 1 online experiment)",
        "Synthetic improvements consistently deliver similar gains in production",
        "Scalable text generation eliminates privacy-evaluation trade-off"
      ],
      "evidence": {
        "metrics": "300+ offline evaluations in same time as 1 online experiment, 1000+ test cases in <10 minutes",
        "validation": "Synthetic test improvements consistently match real-world gains",
        "deployment": "Production search evaluation system at Canva",
        "publication": "November 2024 on Canva Engineering Blog"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Canva Engineering Blog (2024): How to improve search without looking at queries or results",
          "url": "https://www.canva.dev/blog/engineering/how-to-improve-search-without-looking-at-queries-or-results/",
          "note": "Engineering blog describing synthetic data generation for privacy-preserving search evaluation"
        }
      ],
      "related_cases": [],
      "tags": [
        "search",
        "data-foundations",
        "LLM",
        "privacy",
        "evaluation"
      ]
    },
    {
      "id": "amazon-cosmo-knowledge-graph-2024",
      "title": "Amazon: Deconstructing 'Product Recommendations' (Collaborative Filtering vs. Commonsense Knowledge Graphs)",
      "category": "Architectural Pivot",
      "subcategory": "Knowledge Representation",
      "companies_involved": [
        "Amazon"
      ],
      "year": 2024,
      "initial_problem": "Collaborative filtering relies on explicit query-purchase and co-purchase patterns but misses commonsense relationships about product functions, audiences, and usage contexts",
      "initial_assumptions": [
        "Behavioral data (clicks, purchases) sufficient for recommendations",
        "Product relationships captured by co-purchase patterns",
        "Manual knowledge curation too expensive at scale",
        "Product metadata adequate for understanding product relationships"
      ],
      "why_it_fails": [
        "Co-purchase data misses implicit commonsense relationships",
        "Can't understand why products are purchased together (function, audience, context)",
        "Cold start problem for new products without behavioral data",
        "Cross-category relationships invisible to collaborative filtering",
        "Manual knowledge curation doesn't scale to millions of products"
      ],
      "first_principle_insight": "Product recommendations are fundamentally about understanding human contexts - product functions, audiences, usage locations. By using LLMs to extract commonsense relationships from behavioral data and building a knowledge graph, can capture implicit relationships that dramatically improve recommendations, especially for cold-start scenarios.",
      "how_to_reframe": {
        "old_atomic_unit": "Query-purchase + co-purchase patterns \u2192 collaborative filtering \u2192 product recommendations",
        "new_atomic_unit": "Behavioral data \u2192 LLM commonsense extraction \u2192 knowledge graph (functions, audiences, contexts) \u2192 graph-enhanced recommendations",
        "new_problem_type": "LLM-powered commonsense knowledge graph construction"
      },
      "architectural_changes": [
        "COSMO (COmmon Sense MOdel) framework using LLMs to extract commonsense",
        "Knowledge graph encoding product relationships: functions, audiences, locations",
        "Recursive procedure: LLM generates hypotheses from query-purchase/co-purchase data",
        "Human annotation + ML models filter low-quality hypotheses",
        "30K annotated instructions expand to millions of knowledge triples across 18 categories",
        "Knowledge graph deployed in Amazon search navigation",
        "60% macro F1 improvement (fixed encoders), 28% improvement (fine-tuned encoders)"
      ],
      "evidence": {
        "metrics": "60% increase in macro F1 score (fixed encoders), 28% edge in macro F1 (fine-tuned), 22% edge in micro F1, millions of knowledge triples from 30K instructions",
        "deployment": "Production in Amazon search applications and navigation",
        "publication": "June 2024 at ACM SIGMOD conference",
        "scale": "18 major product categories at Amazon"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Amazon Science (2024): Building commonsense knowledge graphs to aid product recommendation",
          "url": "https://www.amazon.science/blog/building-commonsense-knowledge-graphs-to-aid-product-recommendation",
          "note": "Research blog describing COSMO framework and knowledge graph construction"
        },
        {
          "type": "primary",
          "citation": "ACM SIGMOD (2024): COSMO: A Large-Scale E-commerce Common Sense Knowledge Generation and Serving System",
          "url": "https://dl.acm.org/doi/10.1145/3626246.3653398",
          "note": "Conference paper with technical details and evaluation"
        }
      ],
      "related_cases": [
        "zillow-knowledge-graphs-2025",
        "linkedin-skills-content-extraction-2023"
      ],
      "tags": [
        "knowledge-graph",
        "LLM",
        "reasoning",
        "recommendations",
        "e-commerce",
        "ML"
      ]
    },
    {
      "id": "delivery-hero-multilingual-search-2024",
      "title": "Delivery Hero: Deconstructing 'Multilingual Search' (Machine Translation vs. Few-Shot LLM Translation)",
      "category": "Optimization Pivot",
      "subcategory": "Multilingual NLP",
      "companies_involved": [
        "Delivery Hero"
      ],
      "year": 2024,
      "initial_problem": "Traditional machine translation fails for food delivery search due to spelling mistakes, regional dialects, multiple transliterations, and nuanced user intent that varies by context",
      "initial_assumptions": [
        "Traditional MT sufficient for translating search queries",
        "Spelling variations can be handled by spell-checkers",
        "One-to-one translation adequate for cross-language search",
        "Retraining MT models on massive corpora necessary for quality"
      ],
      "why_it_fails": [
        "Traditional MT struggles with different alphabets and regional dialects",
        "Spelling mistakes in queries break traditional translation",
        "Multiple valid transliterations (e.g., Arabic to Latin) not captured",
        "Context and user intent lost in direct translation",
        "Nuances of meaning based on context missed by rule-based systems"
      ],
      "first_principle_insight": "Multilingual search is fundamentally about understanding user intent across contexts, dialects, and spelling variations. By using few-shot LLM translation with examples, can achieve context-aware translation that handles spelling mistakes, dialects, and intent without massive retraining.",
      "how_to_reframe": {
        "old_atomic_unit": "Query \u2192 traditional MT \u2192 translated query \u2192 search",
        "new_atomic_unit": "Query + few-shot examples (transliteration, dialect, context) \u2192 LLM context-aware translation \u2192 multiple translation strategies \u2192 search",
        "new_problem_type": "Few-shot learning for context-aware multilingual search"
      },
      "architectural_changes": [
        "Few-shot LLM translation with small number of example queries",
        "Multiple translation strategies: transliteration, direct translation, regional dialects",
        "Context and user intent understanding built into translation",
        "Spelling variation handling through LLM contextual understanding",
        "Majority voting across multiple translation strategies",
        "Balances efficiency and accuracy without massive corpus retraining",
        "Handles meaning differences based on regional/cultural context"
      ],
      "evidence": {
        "impact": "Improved multilingual search handling dialects, spelling mistakes, and context",
        "deployment": "Production in Delivery Hero's Global Discovery Intelligence",
        "publication": "2024 on Delivery Hero tech blog",
        "author": "Ceren \u00c7oker Turan, Data Scientist, Global Discovery Intelligence team"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Delivery Hero Tech Blog (2024): How we improved multilingual search with few-shot LLM translations",
          "url": "https://tech.deliveryhero.com/how-we-improved-multilingual-search-with-few-shot-llm-translations/",
          "note": "Engineering blog describing few-shot LLM approach for multilingual search"
        }
      ],
      "related_cases": [
        "shopify-semantic-search-realtime-ml-2024"
      ],
      "tags": [
        "NLP",
        "LLM",
        "ML",
        "search"
      ]
    },
    {
      "id": "vimeo-rag-customer-support-2023",
      "title": "Vimeo: Deconstructing 'Customer Support' (Static Help Center vs. RAG-Powered Chat)",
      "category": "Architectural Pivot",
      "subcategory": "Customer Support Architecture",
      "companies_involved": [
        "Vimeo"
      ],
      "year": 2023,
      "initial_problem": "Static Zendesk help articles force customers to manually search and read through documentation, leading to slow resolution and support ticket volume",
      "initial_assumptions": [
        "Static help center articles sufficient for self-service support",
        "Customers can find and understand relevant articles",
        "LLMs will hallucinate without strict constraints",
        "Pre-trained LLMs contain accurate, up-to-date company information"
      ],
      "why_it_fails": [
        "Customers struggle to find relevant articles in large knowledge base",
        "Reading multiple articles time-consuming and frustrating",
        "Static articles don't adapt to specific customer situations",
        "Pre-trained LLMs contain outdated information (e.g., GPT has Vimeo data from 2021)",
        "No immediate, conversational support experience"
      ],
      "first_principle_insight": "Customer support is fundamentally about providing accurate, immediate answers to specific questions. By using RAG to retrieve current help articles and generate contextual responses, can provide conversational support while ensuring accuracy and avoiding hallucinations.",
      "how_to_reframe": {
        "old_atomic_unit": "Customer question \u2192 manual help center search \u2192 read multiple articles \u2192 resolution",
        "new_atomic_unit": "Customer question \u2192 vector embedding retrieval (Zendesk articles) \u2192 RAG with LLM (GPT-3.5/GPT-4/Vertex AI) \u2192 immediate accurate response",
        "new_problem_type": "RAG-based conversational support with model evaluation"
      },
      "architectural_changes": [
        "Zendesk articles indexed in vector store with embeddings",
        "Metadata includes URLs and titles for enhanced retrieval",
        "Tested multiple models: Google Vertex AI Chat Bison, GPT-3.5, GPT-4",
        "Temperature=0 for consistency and reducing outdated LLM knowledge influence",
        "Safety filters for harmful prompts (firearms, weapons)",
        "Vector embeddings constructed on the fly after each epoch during training",
        "RAG ensures responses grounded in current, accurate documentation"
      ],
      "evidence": {
        "deployment": "Production AI help desk chat system at Vimeo",
        "publication": "August 2023 on Vimeo Engineering Medium blog",
        "integration": "Integrated with existing Zendesk Help Center infrastructure",
        "safety": "Vertex AI safety filters for content moderation"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Vimeo Engineering Blog (2023): From idea to reality: Elevating our customer support through generative AI",
          "url": "https://medium.com/vimeo-engineering-blog/from-idea-to-reality-elevating-our-customer-support-through-generative-ai-101a2c5ea680",
          "note": "Engineering blog describing RAG implementation and model evaluation for customer support"
        }
      ],
      "related_cases": [
        "thomson-reuters-rag-2024",
        "linkedin-rag-knowledge-graph-2024"
      ],
      "tags": [
        "RAG",
        "customer-support",
        "GPT-4",
        "LLM",
        "embeddings"
      ]
    },
    {
      "id": "slack-enterprise-search-security-2025",
      "title": "Slack: Deconstructing 'Enterprise Search' (Single-System Search vs. Permission-Aware Cross-App Search)",
      "category": "Architectural Pivot",
      "subcategory": "Enterprise Search Architecture",
      "companies_involved": [
        "Slack"
      ],
      "year": 2025,
      "initial_problem": "Enterprise knowledge scattered across multiple systems, but traditional search violates security by ignoring permissions or requires complex data replication that increases attack surface",
      "initial_assumptions": [
        "Must replicate external data to enable unified search",
        "Single search system can manage permissions across sources",
        "LLM training on customer data needed for good results",
        "Search results can be cached for performance"
      ],
      "why_it_fails": [
        "Data replication creates security vulnerabilities and stale data",
        "External permissions change in real-time, cached results become insecure",
        "Training LLMs on customer data violates enterprise privacy requirements",
        "Centralized permission management doesn't scale across diverse systems",
        "Users may access data they're not authorized to see in external systems"
      ],
      "first_principle_insight": "Enterprise search is fundamentally about real-time, permission-aware information retrieval across trust boundaries. By using RAG (not training), OAuth-based permissions, and no data storage, can provide unified search while maintaining zero-trust security where data never leaves original systems and LLMs never retain customer data.",
      "how_to_reframe": {
        "old_atomic_unit": "Data replication \u2192 centralized index \u2192 search \u2192 cached results",
        "new_atomic_unit": "Real-time OAuth query \u2192 permission check (user-authorized actions only) \u2192 RAG retrieval (no storage) \u2192 LLM processing in escrow VPC \u2192 response (data never leaves trust boundary)",
        "new_problem_type": "Zero-trust enterprise search with real-time permissions"
      },
      "architectural_changes": [
        "No customer data storage: external search results never stored",
        "Real-time permission-aware search across connected business apps",
        "RAG instead of LLM training: data only available at runtime, not retained",
        "AWS escrow VPC: closed-source LLMs hosted where model provider never accesses data",
        "OAuth-based authorization: search performs only user-authorized actions",
        "Subset principle: actions are subset of what user could do themselves",
        "Always provides most up-to-date permissions and content",
        "Customer data never leaves Slack's trust boundary"
      ],
      "evidence": {
        "security": "Enterprise-grade compliance, zero-trust architecture, no training on customer data",
        "deployment": "Production Slack Enterprise Search and Slack AI",
        "publication": "2025 on Slack Engineering blog",
        "integration": "OAuth-based connections to external business systems"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Slack Engineering (2025): How we built enterprise search to be secure and private",
          "url": "https://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/",
          "note": "Engineering blog describing zero-trust architecture and permission-aware search"
        },
        {
          "type": "secondary",
          "citation": "Slack Engineering (2025): How We Built Slack AI To Be Secure and Private",
          "url": "https://slack.engineering/how-we-built-slack-ai-to-be-secure-and-private/",
          "note": "Companion article on Slack AI security architecture"
        }
      ],
      "related_cases": [
        "canva-synthetic-search-evaluation-2024"
      ],
      "tags": [
        "search",
        "security",
        "RAG",
        "privacy"
      ]
    },
    {
      "id": "netflix-foundation-model-consolidation-2025",
      "title": "Netflix: Deconstructing 'Recommendation Systems' (Specialized Models vs. Foundation Model Consolidation)",
      "category": "Architectural Pivot",
      "subcategory": "Model Consolidation",
      "companies_involved": [
        "Netflix"
      ],
      "year": 2025,
      "initial_problem": "Dozens of specialized ML models for different recommendation contexts (Continue Watching, Top Picks, etc.) with independent training despite common data, making maintenance costly and innovation transfer difficult",
      "initial_assumptions": [
        "Each recommendation context needs specialized model",
        "Independent training ensures task-specific optimization",
        "Shared data across models not enough for shared learning",
        "Model consolidation will hurt specialized performance"
      ],
      "why_it_fails": [
        "Maintenance of dozens of specialized models extremely costly",
        "Innovations in one model don't transfer to others",
        "Each model independently trained despite using common interaction data",
        "Adding new recommendation contexts (live, games) requires new models",
        "Can't leverage scale across unified member interaction history",
        "Cold start problem: new titles have no engagement data"
      ],
      "first_principle_insight": "Recommendation is fundamentally about learning comprehensive member preferences from all interaction history. By training a single foundation model on complete user history (borrowing LLM principles like semi-supervised learning), can centralize preference learning, enable scaling laws, and distribute learnings across all recommendation contexts through multi-task 'Hydra' models.",
      "how_to_reframe": {
        "old_atomic_unit": "Specialized models per context (Continue Watching, Top Picks) \u2192 independent training on subset of data \u2192 isolated recommendations",
        "new_atomic_unit": "Single foundation model \u2192 entire user interaction history \u2192 centralized preference learning \u2192 multi-task Hydra models \u2192 distributed learnings to all contexts",
        "new_problem_type": "Foundation model with multi-task learning for unified recommendations"
      },
      "architectural_changes": [
        "Single foundation model trained on entirety of each user's interaction history",
        "Centralized member preference learning at large scale",
        "Multi-task 'Hydra' models consolidating diverse ranking signals",
        "Scaling laws: performance improves predictably with data volume and parameters",
        "Cold start capability: incremental training with unseen entities",
        "Embeddings as features for downstream models and candidate generation",
        "Seamless integration of new business requirements (live, games) across all contexts",
        "End-to-end training leveraging comprehensive user-content data"
      ],
      "evidence": {
        "metrics": "Consistent performance improvements following scaling laws, simplified system reduces maintenance costs",
        "deployment": "Production across all Netflix recommendation contexts",
        "publication": "March and November 2025 on Netflix TechBlog",
        "validation": "Presented at 2025 Netflix Workshop on Personalization, Recommendation and Search"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Netflix TechBlog (2025): Foundation Model for Personalized Recommendation",
          "url": "https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39",
          "note": "Technical blog describing foundation model architecture and scaling laws"
        },
        {
          "type": "primary",
          "citation": "Netflix TechBlog (2025): Integrating Netflix's Foundation Model into Personalization applications",
          "url": "https://netflixtechblog.medium.com/integrating-netflixs-foundation-model-into-personalization-applications-cf176b5860eb",
          "note": "Blog describing Hydra models and integration approach"
        }
      ],
      "related_cases": [
        "spotify-domain-aware-llms-2024",
        "stripe-fraud-detection-foundation-model-2024"
      ],
      "tags": [
        "foundation-model",
        "recommendations",
        "ML",
        "personalization"
      ]
    },
    {
      "id": "target-graph-bundled-recommendations-2024",
      "title": "Target: Deconstructing 'Bundled Recommendations' (Association Rules vs. Graph Neural Networks)",
      "category": "Architectural Pivot",
      "subcategory": "Recommendation Architecture",
      "companies_involved": [
        "Target"
      ],
      "year": 2024,
      "initial_problem": "Association Rule Mining for bundling products struggled to identify driver products, recommended overly similar items, and had limited catalog coverage",
      "initial_assumptions": [
        "Product attributes sufficient for finding complementary items",
        "Association rules can capture product relationships",
        "Undirected relationships adequate (TV + mount = mount + TV)",
        "Purchase counts alone sufficient signal"
      ],
      "why_it_fails": [
        "Difficulty identifying driver products vs. complementary items",
        "Recommends overly similar products instead of complementary",
        "Limited catalog coverage - many products never bundled",
        "Doesn't capture directional relationships (TV \u2192 mount, not mount \u2192 TV)",
        "Purchase order and sequence information lost",
        "Can't leverage graph structure of product relationships"
      ],
      "first_principle_insight": "Product bundling is fundamentally about understanding directional, weighted relationships in a product graph. By using Graph Neural Networks on category-specific product graphs with weighted, directed edges (considering both count and order of purchases), can identify true complementary relationships and driver products.",
      "how_to_reframe": {
        "old_atomic_unit": "Product attributes \u2192 association rules \u2192 bundle recommendations",
        "new_atomic_unit": "Category product graph (weighted, directed edges) \u2192 Graph Neural Networks \u2192 driver product identification \u2192 complementary bundles",
        "new_problem_type": "Graph-based learning on weighted, directed product relationship graphs"
      },
      "architectural_changes": [
        "Separate product graphs for each category (not global graph)",
        "Nodes represent products, edges represent relationships",
        "Weighted edges: count of joint purchases/adds-to-cart",
        "Directed edges: order matters (TV \u2192 mount, not mount \u2192 TV)",
        "GNN learns from graph structure to identify patterns",
        "Driver product identification: main purchase triggers complementary items",
        "Improved catalog coverage through graph-based inference",
        "Appears as 'Frequently Bought Together' and 'Compatible Items' on Target.com"
      ],
      "evidence": {
        "deployment": "Production on Target.com for bundled product recommendations",
        "publication": "April 23, 2024 on Target Tech Blog",
        "team": "Target Item Recommendations team",
        "impact": "Improved complementary product discovery with better driver product identification"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Target Tech Blog (2024): Bundled Product Recommendations",
          "url": "https://tech.target.com/blog/bundled-product-recommendations",
          "note": "Engineering blog describing evolution from ARM to GNN for product bundling"
        }
      ],
      "related_cases": [
        "amazon-cosmo-knowledge-graph-2024",
        "doordash-gnn-notifications-2024"
      ],
      "tags": [
        "graph-neural-networks",
        "recommendations",
        "e-commerce"
      ]
    },
    {
      "id": "getyourguide-realtime-rankings-2024",
      "title": "GetYourGuide: Deconstructing 'Travel Rankings' (Batch ML vs. Real-Time Feature Engineering)",
      "category": "Alternative Pattern - Model Evolution",
      "subcategory": "Real-Time ML Infrastructure",
      "companies_involved": [
        "GetYourGuide"
      ],
      "year": 2024,
      "initial_problem": "Batch ML systems couldn't incorporate real-time user behavior for personalized travel rankings, leading to suboptimal recommendations and missing context from current session",
      "initial_assumptions": [
        "Batch feature computation sufficient for rankings",
        "Historical data adequate for personalization",
        "Real-time features too complex to implement at scale",
        "Can't achieve <100ms latency with real-time feature engineering"
      ],
      "why_it_fails": [
        "Batch systems miss current session context and behavior",
        "User interests change dynamically during browsing session",
        "Can't incorporate real-time signals like impressions, clicks",
        "Stale features lead to suboptimal personalization",
        "Difficult to monitor and debug batch pipelines at scale"
      ],
      "first_principle_insight": "Travel recommendations are fundamentally about understanding user intent in real-time during their exploration. By using real-time feature engineering with streaming (Kafka) and online stores (Redis), can incorporate immediate user behavior into rankings while maintaining <80ms latency at 30M predictions/day.",
      "how_to_reframe": {
        "old_atomic_unit": "Batch feature computation \u2192 offline model training \u2192 cached predictions \u2192 rankings",
        "new_atomic_unit": "Real-time Kafka streams \u2192 on-demand feature views \u2192 Redis online store (p99 <7ms) \u2192 live model predictions \u2192 rankings (<80ms total)",
        "new_problem_type": "Real-time feature engineering with streaming and online stores"
      },
      "architectural_changes": [
        "Tecton feature platform for real-time feature engineering",
        "Stream Feature Views aggregating Kafka activity impressions in real-time",
        "On Demand Feature Views processing streams at visitor level",
        "Redis as online store: p99 latencies <7ms per request",
        "Arize model observability for performance monitoring",
        "30M ranking predictions daily under 80ms per prediction",
        "NDCG as primary performance metric with segment-level breakdown",
        "Real-time incorporation of user session behavior"
      ],
      "evidence": {
        "metrics": "30M predictions/day, <80ms latency per prediction, p99 <7ms for feature retrieval",
        "deployment": "Production serving millions of GetYourGuide users",
        "publication": "May and August 2024 by Senior Data Scientist and Senior MLOps Engineer",
        "monitoring": "Arize observability with NDCG segmentation"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "GetYourGuide Careers Blog (2024): Powering Millions of Real-Time Rankings with Production AI",
          "url": "https://www.getyourguide.careers/posts/powering-millions-of-real-time-rankings-with-production-ai",
          "note": "Engineering blog describing real-time feature platform and performance"
        }
      ],
      "related_cases": [
        "shopify-semantic-search-realtime-ml-2024"
      ],
      "tags": [
        "real-time",
        "ML",
        "MLOps",
        "marketplace",
        "ranking"
      ],
      "comments": [
        {
          "text": "This is a model evolution post - but for this course, it doesn't give me the reason why things changed, so hard to use.",
          "timestamp": "2025-12-22T21:20:29.313135",
          "author": "User"
        }
      ]
    },
    {
      "id": "asos-azure-ml-recommendations-2024",
      "title": "ASOS: Deconstructing 'ML Development Cycle' (Custom Infrastructure vs. Azure ML Platform)",
      "category": "Alternative Pattern - Model Evolution",
      "subcategory": "ML Infrastructure",
      "companies_involved": [
        "ASOS"
      ],
      "year": 2024,
      "initial_problem": "Custom ML infrastructure required 6 months to deploy recommendation models, slowing iteration and preventing rapid experimentation",
      "initial_assumptions": [
        "Custom ML infrastructure necessary for scale (billions of recommendations/day)",
        "Months-long deployment cycles acceptable for model updates",
        "Platform standardization sacrifices customization",
        "Cloud managed services too expensive for scale"
      ],
      "why_it_fails": [
        "6-month time-to-market prevents rapid experimentation",
        "Custom infrastructure difficult to maintain and scale",
        "Can't quickly respond to changing fashion trends",
        "Limited ability to test new recommendation approaches",
        "High operational overhead blocks innovation"
      ],
      "first_principle_insight": "ML development speed is fundamentally about reducing friction in the experiment-deploy cycle. By standardizing on Azure Machine Learning service, can reduce deployment time from 6 months to 6 weeks while maintaining billions of daily recommendations and achieving 25-40% cost savings through optimization.",
      "how_to_reframe": {
        "old_atomic_unit": "Custom infrastructure \u2192 manual deployment pipeline \u2192 6-month time-to-market \u2192 recommendation model",
        "new_atomic_unit": "Azure ML platform \u2192 standardized deployment \u2192 6-week time-to-market \u2192 billions of recommendations/day",
        "new_problem_type": "Platform standardization for ML velocity at scale"
      },
      "architectural_changes": [
        "Standardization on Azure Machine Learning service",
        "Azure OpenAI Service with ChatGPT for customer experiences",
        "Azure AI prompt flow for rapid prototyping and testing",
        "Cost optimization tools: 25-40% cost savings",
        "Billions of product recommendations per day maintained",
        "Time-to-market reduced from 6 months to 6 weeks (8.7x faster)",
        "Clustering-based approach: 36 clusters for personalized product selection"
      ],
      "evidence": {
        "metrics": "6 months \u2192 6 weeks time-to-market (8.7x improvement), 25-40% cost savings, billions of recommendations/day",
        "deployment": "Production serving ASOS global e-commerce platform",
        "publication": "2024 partnership announcements with Microsoft",
        "partnership": "Three-year agreement with Microsoft for operational excellence"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Microsoft Customer Stories (2024): ASOS uses Azure AI Studio to surprise and delight young fashion lovers",
          "url": "https://www.microsoft.com/en/customers/story/1731404546482708710-asos-retailer-azure-ai-studio",
          "note": "Case study describing Azure ML adoption and performance improvements"
        }
      ],
      "related_cases": [],
      "tags": [
        "MLOps",
        "recommendations",
        "e-commerce"
      ],
      "comments": [
        {
          "text": "Yea, doesn't feel like much reframing here",
          "timestamp": "2025-12-22T21:19:28.331913",
          "author": "User"
        }
      ]
    },
    {
      "id": "meta-sequence-learning-ads-2024",
      "title": "Meta: Deconstructing 'Ads Recommendations' (Feature Engineering + DLRMs vs. Sequence Learning with Transformers)",
      "category": "Alternative Pattern - Model Evolution",
      "subcategory": "Recommendation Architecture",
      "companies_involved": [
        "Meta"
      ],
      "year": 2024,
      "initial_problem": "Traditional Deep Learning Recommendation Models (DLRMs) with manually engineered features couldn't capture sequential user behavior dynamics and temporal patterns",
      "initial_assumptions": [
        "Human-engineered features necessary for ad relevance",
        "DLRMs with feature crosses sufficient for recommendations",
        "Static features capture user preferences",
        "Individual events more important than sequences"
      ],
      "why_it_fails": [
        "Manual feature engineering doesn't capture behavior over time",
        "DLRMs miss inter-dynamics between user actions",
        "Can't model how interests evolve during session",
        "Static features stale, don't reflect current intent",
        "Ignores sequential patterns in user engagement/conversions"
      ],
      "first_principle_insight": "Ad recommendations are fundamentally about understanding behavioral sequences and temporal dynamics. By replacing DLRMs with transformer-based sequence learning that directly learns from engagement/conversion events (not engineered features), can model behavior evolution and achieve +6% recall improvement and +8% ads quality.",
      "how_to_reframe": {
        "old_atomic_unit": "Manual features + feature crosses \u2192 DLRM neural network \u2192 ad recommendations",
        "new_atomic_unit": "Event sequences (engagement + conversions) \u2192 custom transformer with complex feature encoding \u2192 learned representations \u2192 ad recommendations",
        "new_problem_type": "Transformer-based sequence learning for temporal behavior modeling"
      },
      "architectural_changes": [
        "Event-based learning: representations from engagement/conversion events (not manual features)",
        "Sequence learning architectures replacing DLRMs",
        "Custom transformer with complex feature encoding for sequential information",
        "Models inter-dynamics between user actions over time",
        "Andromeda retrieval engine: +6% recall improvement, +8% ads quality",
        "GEM (Generative Ads Model): LLM-inspired foundation model trained on thousands of GPUs",
        "2-4% more conversions on select segments since launch"
      ],
      "evidence": {
        "metrics": "+6% recall improvement, +8% ads quality on segments, 2-4% more conversions post-launch",
        "deployment": "Production across Instagram and Facebook ad systems",
        "publication": "November 2024 on Meta Engineering blog",
        "scale": "Thousands of GPUs for foundation model training"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Meta Engineering (2024): Sequence learning: A paradigm shift for personalized ads recommendations",
          "url": "https://engineering.fb.com/2024/11/19/data-infrastructure/sequence-learning-personalized-ads-recommendations/",
          "note": "Engineering blog describing sequence learning architecture and paradigm shift"
        },
        {
          "type": "secondary",
          "citation": "Meta Engineering (2024): Meta Andromeda: Supercharging Advantage+ automation",
          "url": "https://engineering.fb.com/2024/12/02/production-engineering/meta-andromeda-advantage-automation-next-gen-personalized-ads-retrieval-engine/",
          "note": "Blog describing Andromeda retrieval engine with sequence learning"
        }
      ],
      "related_cases": [
        "netflix-foundation-model-consolidation-2025",
        "spotify-domain-aware-llms-2024"
      ],
      "tags": [
        "deep-learning",
        "advertising",
        "recommendations",
        "social-networks",
        "foundation-model"
      ],
      "comments": [
        {
          "text": "This is an example where they did switch their modeling approach, but hard to see that reframing, back to the start to think it through",
          "timestamp": "2025-12-22T21:18:27.597475",
          "author": "User"
        }
      ]
    },
    {
      "id": "twitter-two-tower-account-recommendations-2022",
      "title": "Twitter: Deconstructing 'Account Recommendations' (Heuristics vs. Two-Tower Embedding Model)",
      "category": "Alternative Pattern - Model Evolution",
      "subcategory": "Recommendation Architecture",
      "companies_involved": [
        "Twitter"
      ],
      "year": 2022,
      "initial_problem": "Who-To-Follow recommendations relied on heuristics and limited signals, missing opportunities to connect users with accounts matching their personalized interests",
      "initial_assumptions": [
        "Simple heuristics sufficient for account discovery",
        "Popular accounts should be recommended broadly",
        "Explicit signals (profile visits, follows) enough",
        "Single embedding sufficient for user representation"
      ],
      "why_it_fails": [
        "Heuristics can't capture nuanced user interests",
        "Doesn't distinguish consumption vs. production behaviors",
        "Misses implicit signals about account relevance",
        "Can't scale candidate generation to millions of accounts",
        "Limited personalization based on user's unique interests"
      ],
      "first_principle_insight": "Account recommendations are fundamentally about matching consumer interests with producer content at scale. By using a two-tower model with separate embeddings for consumption behaviors and production behaviors, can efficiently generate hundreds-to-thousands of highly relevant candidates for real-time ranking.",
      "how_to_reframe": {
        "old_atomic_unit": "Heuristics + popularity signals \u2192 candidate accounts \u2192 ranking \u2192 Who-To-Follow",
        "new_atomic_unit": "Two-tower model (consumption tower + production tower) \u2192 dot product similarity \u2192 candidate generation (hundreds-thousands) \u2192 ranking model \u2192 Who-To-Follow",
        "new_problem_type": "Two-tower embedding model for scalable candidate generation"
      },
      "architectural_changes": [
        "Two-tower architecture: separate embeddings for consumers and producers",
        "Consumption tower: learns from user's consumption behaviors",
        "Production tower: learns from account's production behaviors",
        "Dot product of embeddings as proxy for follow probability",
        "Actual follow relationships as ground truth for training",
        "Candidate generation phase: retrieves hundreds-thousands of relevant accounts",
        "Ranking phase: ML model ranks candidates in real-time (single to tens)",
        "Scalable to millions of accounts through efficient embedding retrieval"
      ],
      "evidence": {
        "deployment": "Production Who-To-Follow recommendations on Twitter",
        "publication": "January 20, 2022 on Twitter Engineering blog",
        "architecture": "Two-stage pipeline: candidate generation \u2192 ranking",
        "scale": "Handles millions of account candidates"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Twitter Engineering (2022): Model-based candidate generation for account recommendations",
          "url": "https://blog.twitter.com/engineering/en_us/topics/insights/2022/model-based-candidate-generation-for-account-recommendations",
          "note": "Engineering blog describing two-tower model architecture"
        }
      ],
      "related_cases": [
        "airbnb-embedding-retrieval-2025",
        "uber-eats-graph-learning-2020"
      ],
      "tags": [
        "two-tower",
        "embeddings",
        "ranking",
        "recommendations",
        "social-networks"
      ],
      "comments": [
        {
          "text": "This doesn't feel first pinciples, it feels like learning by analogy, seeing what othres did and copying it - maybe save this for course intro",
          "timestamp": "2025-12-22T21:17:07.792208",
          "author": "User"
        }
      ]
    },
    {
      "id": "instacart-heuristics-variance",
      "title": "Instacart: When Heuristics Beat ML (High-Variance Decisions)",
      "category": "Architectural Pivot",
      "companies_involved": [
        "Instacart"
      ],
      "year": 2020,
      "initial_problem": "Staff shoppers to fulfill orders without leaving orders unfilled or shoppers idle",
      "initial_assumptions": [
        "ML models give the best predictions for staffing needs",
        "More accurate predictions = better business outcomes",
        "Use ML for all forecasting decisions"
      ],
      "why_it_fails": [
        "High-variance situations make point predictions unreliable",
        "ML models optimize for average accuracy, but tails matter for staffing",
        "Being wrong in high-variance moments is catastrophic (unfilled orders or idle shoppers)"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Point prediction - ML predicts expected staffing need",
        "new_atomic_unit": "Distribution-aware decision - use Monte Carlo simulation to understand the range of outcomes and make robust decisions"
      },
      "first_principle_insight": "For high-variance decisions, a simple heuristic that's robust to variance beats a sophisticated ML model that's accurate on average but fails in the tails. Sometimes you need to simulate many scenarios, not predict a single number.",
      "architectural_changes": [
        "Use Monte Carlo simulation to model range of possible outcomes",
        "Make decisions robust to variance, not just optimized for expected value",
        "Reserve ML for stable, predictable patterns",
        "Use heuristics for high-variance, high-stakes decisions"
      ],
      "sources": [
        {
          "citation": "Putrevu, J. (2020). No order left behind; no shopper left idle. Instacart Tech Blog.",
          "url": "https://tech.instacart.com/no-order-left-behind"
        }
      ],
      "tags": [
        "rules",
        "operations",
        "marketplace"
      ],
      "comments": []
    },
    {
      "id": "anthropic-contextual-retrieval-2024",
      "title": "Anthropic Contextual Retrieval: From Chunks to Contextualized Chunks",
      "companies_involved": [
        "Anthropic"
      ],
      "year": 2024,
      "category": "Architectural Pivot - RAG Reframe",
      "problem_archetype": "Retrieval",
      "initial_assumptions": [
        "RAG works by chunking documents and finding similar chunks via vector search",
        "Chunk similarity to query is the right retrieval signal",
        "Context can be recovered at query time through surrounding text or reranking"
      ],
      "why_it_fails": [
        "Chunks lose critical context: 'Revenue grew 3%' without knowing which company or quarter",
        "Pronouns and references become ambiguous when separated from their antecedents",
        "Query-time fixes (overlap, reranking) add latency without solving the root cause"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Raw text chunk retrieved by vector similarity",
        "new_atomic_unit": "Context-enriched chunk that carries its own semantic meaning",
        "insight": "The problem isn't retrieval accuracy - it's that chunks are incomplete units of meaning. Linguistics teaches us meaning is context-dependent. Fix the data at index time, not query time."
      },
      "architectural_changes": [
        "Pass each chunk through an LLM during ingestion to generate context (document title, section, entity references)",
        "Prepend context to chunk before embedding: 'In the 2023 Employee Handbook, the Remote Work Policy states...'",
        "Use prompt caching to process hundreds of chunks against shared document context efficiently",
        "Combine contextual embeddings with contextual BM25 for hybrid retrieval",
        "Shift computational cost from query-time to index-time"
      ],
      "results": {
        "metrics": "49% reduction in retrieval failures (67% with reranking)",
        "cost": "$1.02 per million document tokens with prompt caching",
        "insight": "Smart data beats smart search"
      },
      "first_principle_insight": "Meaning is context-dependent. A chunk that can't stand alone semantically shouldn't be indexed alone. Invest compute in understanding data before storage, not during retrieval.",
      "related_cases": [
        "microsoft-graphrag-2024",
        "linkedin-semantic-search-2024"
      ],
      "tags": [
        "RAG",
        "retrieval",
        "LLM",
        "efficiency",
        "embeddings",
        "framework-vs-insight"
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "framework-vs-insight"
        ],
        "transferable_patterns": [
          "Trace what each component actually SEES as input",
          "Question where information gets lost in pipelines",
          "Consider using LLMs for preprocessing, not just inference",
          "Improvements often come from component boundaries, not components themselves"
        ],
        "framework_lesson": "A RAG checklist would have you documenting metadata and chunking strategy. But it wouldn't generate the insight 'use an LLM to write context for each chunk.' Frameworks surface ingredients; insight comes from understanding mechanics deeply and questioning assumptions. This case demonstrates two levels of insight: (1) Mechanical: embeddings don't see metadata stored separately. (2) Creative: use an LLM to generate BETTER context than you could write programmatically."
      },
      "sources": [
        {
          "url": "https://www.anthropic.com/engineering/contextual-retrieval",
          "type": "engineering_blog",
          "citation": "Contextual Retrieval"
        }
      ]
    },
    {
      "id": "microsoft-graphrag-2024",
      "title": "Microsoft GraphRAG: From Vector Search to Knowledge Graph Communities",
      "companies_involved": [
        "Microsoft Research"
      ],
      "year": 2024,
      "category": "Architectural Pivot - RAG Reframe",
      "problem_archetype": "Retrieval",
      "initial_assumptions": [
        "Vector similarity search finds relevant information for any query",
        "Larger context windows solve the 'not enough context' problem",
        "RAG retrieves the right chunks, the LLM synthesizes the answer"
      ],
      "why_it_fails": [
        "Global questions ('What are the themes across this dataset?') require synthesizing thousands of documents",
        "Top-k retrieval answers based on <1% of data, leading to incomplete or biased answers",
        "Million-token context windows are expensive and suffer from 'lost-in-the-middle' attention problems",
        "Vector search finds individual facts but misses relationships and patterns"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Individual document chunks ranked by similarity",
        "new_atomic_unit": "Community summaries from a knowledge graph representing themes and relationships",
        "insight": "Information isn't just content - it's structure. A dataset is a knowledge graph, not a list of documents. Answer global questions by understanding communities of information."
      },
      "architectural_changes": [
        "Use LLMs to extract entities and relationships from documents into a knowledge graph",
        "Apply Leiden algorithm to detect 'communities' of densely connected nodes (themes/topics)",
        "Generate LLM summaries for each community as a pre-computed semantic index",
        "For global questions: map question across community summaries, then reduce to final answer",
        "LazyGraphRAG for cost optimization - selective indexing based on query patterns"
      ],
      "results": {
        "metrics": "70-80% win rate over naive RAG on comprehensiveness and diversity",
        "efficiency": "~2-3% of tokens vs full-context summarization for equivalent results",
        "insight": "Structure often beats volume, even against 1M token context windows"
      },
      "first_principle_insight": "Structure encodes meaning. To answer questions about a whole dataset, you need to understand its topology - the clusters, themes, and relationships - not just find similar sentences.",
      "related_cases": [
        "anthropic-contextual-retrieval-2024",
        "linkedin-knowledge-graph-2023"
      ],
      "tags": [
        "RAG",
        "knowledge-graph",
        "retrieval",
        "LLM",
        "graph-neural-networks"
      ],
      "sources": [
        {
          "url": "https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/",
          "type": "research_blog",
          "citation": "GraphRAG: Unlocking LLM discovery on narrative private data"
        }
      ]
    },
    {
      "id": "memgpt-virtual-context-2024",
      "title": "MemGPT: From Context Window to Memory Hierarchy",
      "companies_involved": [
        "UC Berkeley",
        "Letta"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Memory Architecture",
      "problem_archetype": "Conversation/Memory",
      "initial_assumptions": [
        "Context window size determines how much the model can 'remember'",
        "Larger context windows solve memory limitations",
        "Chat history can be summarized or truncated when context fills up"
      ],
      "why_it_fails": [
        "Even 128k token windows fill up - then the 'mind' is wiped",
        "Quadratic attention costs make large contexts expensive",
        "Summary/sliding window approaches are lossy - nuance and details are permanently erased",
        "Models are stateless between sessions with no long-term persona or memory"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Context window as the limit of working memory",
        "new_atomic_unit": "Memory hierarchy with context as RAM, external storage as disk, and the LLM as the memory manager",
        "insight": "The LLM isn't a knowledge base - it's a CPU. Humans and computers don't hold everything in working memory; they use memory management systems. Give the model an OS."
      },
      "architectural_changes": [
        "Divide memory into Main Context (RAM) and External Context (Disk)",
        "Main Context: current prompt, system instructions, working scratchpad",
        "External Context: Recall Storage (conversation history) and Archival Storage (facts/documents)",
        "Self-paging: LLM manages its own memory via function calls (recall_memory, archive_memory)",
        "Interrupts and heartbeats allow multi-step memory operations before responding"
      ],
      "results": {
        "metrics": "Infinite context illusion with fixed-window models",
        "performance": "High accuracy on document analysis regardless of length (vs degradation in GPT-4)",
        "capability": "Persona evolution over multi-session chats via core memory updates"
      },
      "first_principle_insight": "Agency emerges from constraint. By limiting the model's immediate view but giving it tools to manage that view, the system forces metacognition - thinking about what it knows and what it needs to find.",
      "related_cases": [
        "langgraph-cyclic-state-2024"
      ],
      "tags": [
        "RAG",
        "agents",
        "LLM",
        "operating-system",
        "architecture"
      ],
      "sources": [
        {
          "url": "https://memgpt.ai/",
          "type": "research_paper",
          "citation": "MemGPT: Towards LLMs as Operating Systems"
        }
      ]
    },
    {
      "id": "langgraph-cyclic-state-2024",
      "title": "LangGraph: From Linear Chains to Cyclic Feedback Loops",
      "companies_involved": [
        "LangChain"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Agent Architecture",
      "problem_archetype": "Orchestration",
      "initial_assumptions": [
        "AI applications are pipelines: Input -> Step A -> Step B -> Output",
        "Chain architectures (DAGs) are sufficient for complex tasks",
        "If a step fails, restart the whole process"
      ],
      "why_it_fails": [
        "Linear chains are brittle - if Step B produces bad output, Step C processes garbage",
        "No mechanism for correction or retry without restarting",
        "Adding if/else logic creates 'spaghetti chains' that are hard to debug",
        "State is ephemeral - can't pause, get human input, and resume"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Linear pipeline (DAG) where each step leads to the next",
        "new_atomic_unit": "Cyclic graph with feedback loops where edges can point backwards",
        "insight": "In cognitive science and control theory, intelligent behavior requires feedback loops (Action -> Observation -> Correction). Agent architectures must support cycles, not just sequences."
      },
      "architectural_changes": [
        "Graphs with Nodes (agents/tools) and Edges (control flow) that can loop backwards",
        "Reasoner-Critic loops: generate -> check -> loop back with errors until exit condition met",
        "Persistent State object that survives across graph execution",
        "Checkpointers save state after every node - enables 'time travel' debugging",
        "Human-in-the-loop: pause, wait for input, resume days later without losing context"
      ],
      "results": {
        "capability": "Reliable systems from unreliable components through iteration",
        "debugging": "Time travel - inspect state history, rewind, modify, replay",
        "orchestration": "Multi-agent handoffs governed by graph topology"
      },
      "first_principle_insight": "Reliability comes not from a perfect model, but from an iterative process. Encode loops into the architecture. This is the shift from System 1 (fast, reflex) to System 2 (deliberate, corrective) thinking.",
      "related_cases": [
        "memgpt-virtual-context-2024",
        "cursor-shadow-workspace-2024"
      ],
      "tags": [
        "agents",
        "LLM",
        "feedback-loops",
        "operations",
        "architecture"
      ],
      "sources": [
        {
          "url": "https://langchain-ai.github.io/langgraph/",
          "type": "documentation",
          "citation": "LangGraph Documentation"
        }
      ]
    },
    {
      "id": "cursor-shadow-workspace-2024",
      "title": "Cursor Shadow Workspace: From Text Prediction to Verified Generation",
      "companies_involved": [
        "Anysphere",
        "Cursor"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Code Generation",
      "problem_archetype": "Code Generation",
      "initial_assumptions": [
        "AI code assistants predict the next tokens based on context",
        "More context (file, project) improves suggestions",
        "The human developer validates the output by running it"
      ],
      "why_it_fails": [
        "AI generates syntactically correct but functionally broken code",
        "References variables that don't exist, hallucinates library methods, introduces type errors",
        "Model predicts based on text patterns, unaware of compile-time state",
        "User becomes the 'compiler' - paste code, see error, re-prompt"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Text prediction based on surrounding code patterns",
        "new_atomic_unit": "Verified generation where AI sees compiler/linter feedback before user does",
        "insight": "Code has a ground truth (compilation/execution) that text doesn't. The AI should be subject to the same validation tools as a human developer - it needs 'eyes' that see implications, not just text."
      },
      "architectural_changes": [
        "Shadow Workspace: hidden, headless IDE instance running in background",
        "AI applies proposed changes to shadow version before showing user",
        "Run Language Server Protocol (LSP) on shadow code to catch errors",
        "If linter reports errors, AI self-corrects (adds missing import) and re-verifies",
        "User sees only final, verified code - drafting and fixing happen invisibly",
        "Complex concurrency to isolate shadow state from live state while sharing language servers"
      ],
      "results": {
        "capability": "Generate-Verify loop reduces cognitive load on developer",
        "quality": "No more debugging AI's syntax errors",
        "insight": "Treats LLM as hypothesis generator, compiler as verifier"
      },
      "first_principle_insight": "Move AI from probabilistic generation to verified generation. The model generates hypotheses; deterministic tools verify them. This is how unreliable generators become reliable systems.",
      "related_cases": [
        "github-copilot-x-2023",
        "dspy-prompt-compilation-2024"
      ],
      "tags": [
        "code-generation",
        "LLM",
        "verification"
      ],
      "sources": [
        {
          "url": "https://cursor.com/blog/problems-2024",
          "type": "engineering_blog",
          "citation": "Cursor Blog: More Problems 2024"
        },
        {
          "url": "https://blog.bytebytego.com/p/how-cursor-serves-billions-of-ai",
          "type": "engineering_blog",
          "citation": "How Cursor Serves Billions of AI Code Completions"
        }
      ]
    },
    {
      "id": "dspy-prompt-compilation-2024",
      "title": "Stanford DSPy: From Prompt Engineering to Prompt Compilation",
      "companies_involved": [
        "Stanford",
        "Databricks"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Prompt Engineering",
      "problem_archetype": "LLM Programming",
      "initial_assumptions": [
        "Prompts are strings that control model behavior",
        "Better prompts = better results (prompt engineering)",
        "Manual tweaking ('Think step by step') optimizes performance"
      ],
      "why_it_fails": [
        "Prompt engineering is unscientific - 'magic spells' rather than engineering",
        "A prompt optimized for GPT-4 often fails on Claude or Llama",
        "Application logic buried in fragile text strings creates technical debt",
        "No systematic way to improve prompts beyond trial and error"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Hand-crafted prompt strings tuned by human intuition",
        "new_atomic_unit": "Compiled prompts optimized by algorithms against metrics",
        "insight": "If prompts control model behavior, they're parameters. In ML, we don't hand-tune weights - we use optimizers. Prompts should be compiled, not written."
      },
      "architectural_changes": [
        "Signatures: define Input -> Output schema (e.g., Question -> Answer) instead of prompt text",
        "Modules: select strategy (ChainOfThought, ReAct) rather than writing instructions",
        "Teleprompters (Optimizers): algorithms that generate, test, and refine prompts automatically",
        "Compilation: produce optimized prompts for specific model (GPT-4, Llama, Claude)",
        "Model agnosticism: recompile same program for different models"
      ],
      "results": {
        "performance": "Consistently outperforms hand-crafted prompts in enterprise case studies",
        "case_study": "Volvo Trucks: manual prompting +23%, DSPy +27% additional, HPO +5% more = ~60% total gain",
        "portability": "Same program compiles to different prompt structures for different models"
      },
      "first_principle_insight": "Prompts are weights, not prose. Treat LLM programming like ML programming - define the objective, provide examples, let an optimizer find the best representation. This is modular AI programming.",
      "related_cases": [
        "cursor-shadow-workspace-2024"
      ],
      "tags": [
        "prompt-engineering",
        "LLM",
        "optimization",
        "code-generation"
      ],
      "sources": [
        {
          "url": "https://dspy-docs.vercel.app/",
          "type": "documentation",
          "citation": "DSPy Documentation"
        }
      ]
    },
    {
      "id": "perplexity-answer-engine-2024",
      "title": "Perplexity: From Search Engine to Answer Engine",
      "companies_involved": [
        "Perplexity AI"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Search Reframe",
      "problem_archetype": "Search/Information Retrieval",
      "initial_assumptions": [
        "Search engines find relevant documents (links) for queries",
        "Users click through results to find answers",
        "Better ranking = better search"
      ],
      "why_it_fails": [
        "Users don't want links - they want answers",
        "Ten blue links force user to do the work of reading and synthesizing",
        "Chatbots offer synthesis but hallucinate and lack real-time knowledge",
        "The 'search' framing optimizes for traffic routing, not knowledge delivery"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Ranked list of relevant document links",
        "new_atomic_unit": "Synthesized answer with inline citations to sources",
        "insight": "The user's intent isn't 'find a site' - it's 'get an answer.' The product should be an Answer Engine that does the reading and synthesis, delivering knowledge directly with transparency."
      },
      "architectural_changes": [
        "Custom crawler (PerplexityBot) optimized for content extraction, not page ranking",
        "Pipeline: intent classification -> multi-source retrieval -> LLM reading/synthesis -> citation generation",
        "Every claim backed by inline citation linking to source for verification",
        "Pro Search: agentic multi-step planning for complex queries (flights + hotels)",
        "Real-time integration vs static knowledge cutoff"
      ],
      "results": {
        "latency": "358ms median response time (95th percentile <800ms)",
        "quality": "0.930 on SimpleQA benchmark, outperforming other search-augmented systems",
        "scale": "200 million daily queries processed"
      },
      "first_principle_insight": "The problem to solve wasn't better link ranking - it was delivering knowledge directly. Reframe search from 'routing traffic' to 'answering questions' and the entire architecture changes.",
      "related_cases": [
        "anthropic-contextual-retrieval-2024"
      ],
      "tags": [
        "search",
        "LLM",
        "RAG",
        "evaluation",
        "real-time"
      ],
      "sources": [
        {
          "url": "https://blog.bytebytego.com/p/how-perplexity-built-an-ai-google",
          "type": "engineering_blog",
          "citation": "How Perplexity Built an AI Google"
        }
      ]
    },
    {
      "id": "klarna-knowledge-graph-2024",
      "title": "Klarna Knowledge Graph: LLM as Reasoning Engine, Not Database",
      "companies_involved": [
        "Klarna"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Rules vs LLM Hybrid",
      "problem_archetype": "Customer Support / Knowledge Management",
      "initial_assumptions": [
        "RAG over unstructured documentation will answer customer questions accurately",
        "LLMs can serve as the source of truth for customer support",
        "More context and better retrieval will reduce hallucinations"
      ],
      "why_it_fails": [
        "In regulated fintech, you cannot afford 'probabilistic truth'",
        "LLMs confidently misinterpret unstructured text (hallucination loop)",
        "No clear audit trail for compliance when answers come from probabilistic retrieval",
        "Unstructured text lacks the relationships needed for accurate policy lookup"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "LLM reasoning over retrieved text chunks to find answers",
        "new_atomic_unit": "Knowledge Graph (structured rules) as source of truth, LLM as interface/reasoning layer only",
        "insight": "The LLM is a reasoning engine, not a database. Separate the 'what is true' (graph) from 'how to communicate it' (LLM). Use structure for facts, probability for language."
      },
      "architectural_changes": [
        "Built internal Knowledge Graph 'Kiki' using Neo4j as single source of truth",
        "Graph contains structured relationships between policies, products, customer states",
        "LLM queries the graph for facts rather than reasoning over raw text",
        "Graph provides deterministic, auditable answers; LLM provides natural language interface",
        "Eliminated RAG over unstructured docs for policy-critical questions"
      ],
      "results": {
        "volume": "2.3 million conversations handled (2/3 of customer service)",
        "quality": "Satisfaction scores on par with human agents",
        "efficiency": "25% reduction in repeat inquiries",
        "compliance": "Auditable knowledge base for regulatory requirements"
      },
      "first_principle_insight": "Don't use probabilistic systems for deterministic truth. LLMs excel at reasoning and communication, not fact storage. Pair them with structured systems (graphs, rules, databases) for the source of truth.",
      "related_cases": [
        "microsoft-graphrag-2024"
      ],
      "tags": [
        "knowledge-graph",
        "hybrid-systems",
        "LLM",
        "customer-support",
        "rules",
        "fintech"
      ],
      "sources": [
        {
          "url": "https://neo4j.com/case-studies/klarna/",
          "type": "case_study",
          "citation": "Klarna Engineering / Neo4j Case Study"
        }
      ]
    },
    {
      "id": "google-long-context-vs-rag-2024",
      "title": "Google: When Long Context Beats RAG - Retrieval Fragments Logic",
      "companies_involved": [
        "Google"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Context vs Retrieval Decision",
      "problem_archetype": "Code Analysis / Complex Reasoning",
      "initial_assumptions": [
        "RAG is the standard approach for working with large codebases",
        "Retrieve relevant snippets, feed to model, get answer",
        "Top-k retrieval captures what the model needs to reason"
      ],
      "why_it_fails": [
        "Debugging requires understanding implicit connections between distant modules",
        "RAG fails when Module A breaks Module Z but they share no keywords",
        "Retrieval by similarity misses logical dependencies",
        "Complex reasoning requires 'holding the whole problem in your head'"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Retrieved code snippets based on similarity to query",
        "new_atomic_unit": "Entire repository or module ingested via long context for global reasoning",
        "insight": "Retrieval fragments logic. For problems requiring understanding of relationships across a codebase, the model needs to see everything at once. RAG physically cannot support global reasoning."
      },
      "architectural_changes": [
        "Switch from RAG to long context (Gemini 1.5 Pro with 1M+ tokens)",
        "Ingest entire relevant repository or module at once",
        "Bypass retrieval entirely for complex reasoning tasks",
        "Reserve RAG for fact-lookup; use long context for reasoning",
        "Decision framework: Factoid questions \u2192 RAG; Analytical questions \u2192 Long context"
      ],
      "results": {
        "accuracy": "100% recall on Needle In A Haystack benchmarks",
        "comparison": "RAG degraded to <60% as complexity increased",
        "capability": "Can trace implicit dependencies across entire codebase"
      },
      "first_principle_insight": "Match the architecture to the reasoning type. Retrieval is for finding facts. Long context is for understanding relationships. The question isn't 'RAG or long context?' but 'What kind of reasoning does this task require?'",
      "related_cases": [
        "anthropic-contextual-retrieval-2024",
        "microsoft-graphrag-2024"
      ],
      "tags": [
        "long-context",
        "RAG",
        "code-analysis",
        "reasoning",
        "decision-framework"
      ],
      "sources": [
        {
          "url": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf",
          "type": "technical_report",
          "citation": "Gemini 1.5 Technical Report"
        }
      ]
    },
    {
      "id": "air-canada-agent-liability-2024",
      "title": "Air Canada: Agents Are Legal Representatives, Not 'Just Bots'",
      "companies_involved": [
        "Air Canada"
      ],
      "year": 2024,
      "category": "Cautionary Tale - Agent Liability",
      "problem_archetype": "Customer Support / Legal Risk",
      "initial_assumptions": [
        "Chatbot disclaimer ('this is just a bot') protects company from errors",
        "Users understand AI can make mistakes",
        "Agent errors are acceptable if we warn users it's AI"
      ],
      "why_it_fails": [
        "Bot hallucinated a non-existent 'bereavement fare refund policy'",
        "Customer relied on the information and booked based on it",
        "Company argued chatbot was 'separate legal entity' - tribunal rejected this",
        "Agent didn't know what it didn't know (confident but wrong)"
      ],
      "the_lesson": {
        "what_happened": "Chatbot hallucinated a non-existent bereavement fare refund policy. Customer relied on it and booked accordingly.",
        "the_consequence": "Company argued chatbot was 'separate legal entity' - tribunal rejected this. Forced to pay damages.",
        "the_precedent": "AI disclaimers don't protect against misinformation. Your AI speaks for your company legally."
      },
      "architectural_changes": [
        "Disabled agent to re-architect safety layers",
        "Added verification for policy-related statements",
        "Implemented 'I don't know' responses for uncertain queries",
        "Built guardrails against confident hallucination on factual matters",
        "Legal review of agent capabilities before deployment"
      ],
      "results": {
        "outcome": "Forced to pay damages to passenger",
        "reputation": "Massive reputational damage from tribunal ruling",
        "precedent": "Legal precedent that AI disclaimers don't protect against misinformation"
      },
      "first_principle_insight": "Your AI speaks for your company legally. 'It's just a bot' is not a defense. Design for the failure mode where the AI is confidently wrong about facts that matter.",
      "related_cases": [],
      "tags": [
        "cautionary-tale",
        "liability",
        "agents",
        "hallucination",
        "customer-support"
      ],
      "sources": [
        {
          "url": "https://decisions.civilresolutionbc.ca/crt/crtd/en/item/521024/index.do",
          "type": "legal_ruling",
          "citation": "Moffatt v. Air Canada - Civil Resolution Tribunal"
        }
      ]
    },
    {
      "id": "devin-demo-vs-production-2024",
      "title": "Devin: The Last Mile Problem - Why Demos Don't Scale",
      "companies_involved": [
        "Cognition AI"
      ],
      "year": 2024,
      "category": "Cautionary Tale - Demo to Production Gap",
      "problem_archetype": "Autonomous Agents / Software Engineering",
      "initial_assumptions": [
        "If agent works on curated demos, it scales to production",
        "Autonomous software engineering is achievable with current models",
        "End-to-end task completion is the right benchmark"
      ],
      "why_it_fails": [
        "Demos use clean, controlled environments (happy path)",
        "Production has 'impossible' blockers: version conflicts, 403 errors, ambiguous requirements",
        "Without human-in-the-loop for edge cases, agent enters infinite loops",
        "Agent sometimes marks failed tasks as complete",
        "The 'last mile' of real-world messiness defeats autonomous agents"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "End-to-end autonomous task completion",
        "new_atomic_unit": "Human-agent collaboration with verification checkpoints",
        "insight": "The Last Mile Problem: The gap between demo (clean) and production (messy) is not a small polish - it's a fundamental architectural issue. Agents need escape hatches to humans when they get stuck."
      },
      "architectural_changes": [
        "Add verification cycles before marking tasks complete",
        "Build 'I'm stuck' detection and human escalation",
        "Design for graceful degradation when blockers occur",
        "Set realistic expectations: assistance, not autonomy",
        "Measure on real-world issues, not curated benchmarks"
      ],
      "results": {
        "reality_check": "Independent reviews: ~16% success on real-world GitHub issues",
        "gap": "Marketing implied near-autonomy vs actual capability",
        "lesson": "Demo performance is not production performance"
      },
      "first_principle_insight": "Demo environments are lies of omission. Production is messy, adversarial, and full of edge cases. Design agents for the failure modes, not the happy path. Human-in-the-loop isn't a crutch - it's architecture.",
      "related_cases": [
        "cursor-shadow-workspace-2024",
        "air-canada-agent-liability-2024"
      ],
      "tags": [
        "cautionary-tale",
        "agents",
        "production",
        "code-generation"
      ],
      "sources": [
        {
          "url": "https://spectrum.ieee.org/ai-code-generator",
          "type": "analysis",
          "citation": "IEEE Spectrum / SWE-bench Analysis"
        }
      ]
    },
    {
      "id": "linkedin-model-routing-2024",
      "title": "LinkedIn Model Routing: Quality Saturation - When SOTA Is Overkill",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Multi-Model Architecture",
      "problem_archetype": "Cost Optimization / Model Selection",
      "initial_assumptions": [
        "Use the best model (GPT-4) for everything to ensure quality",
        "One model fits all queries",
        "Quality is always worth paying for"
      ],
      "why_it_fails": [
        "For 80% of tasks, difference between $0.01 model and $30 model is zero",
        "Simple tasks (rewrite headline) don't need frontier model capabilities",
        "Latency matters - big models are slow for simple queries",
        "Using SOTA for everything is 'engineering malpractice'"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Single frontier model handles all queries",
        "new_atomic_unit": "Routing layer classifies complexity, dispatches to appropriate model tier",
        "insight": "Quality saturation: Most tasks hit a quality ceiling well below SOTA capability. Match model capability to task complexity. Pay for intelligence only where it matters."
      },
      "architectural_changes": [
        "Built routing layer that classifies query complexity first",
        "Simple queries (rewrite, summarize) \u2192 small fast model (Llama/Vicuna)",
        "Complex queries (analyze, reason) \u2192 frontier model",
        "Continuous monitoring of which queries need which tier",
        "A/B testing to validate routing decisions don't hurt quality"
      ],
      "results": {
        "latency": "50%+ reduction for routed queries",
        "cost": "3-4x savings on inference costs",
        "quality": "No degradation in user-perceived quality"
      },
      "first_principle_insight": "Don't pay for capability you don't need. The question isn't 'which model is best?' but 'which model is best for THIS task?' Build the routing logic, not the biggest hammer.",
      "related_cases": [
        "cursor-shadow-workspace-2024"
      ],
      "tags": [
        "model-routing",
        "cost-optimization",
        "multi-model",
        "latency",
        "architecture"
      ],
      "sources": [
        {
          "url": "https://engineering.linkedin.com/blog",
          "type": "engineering_blog",
          "citation": "LinkedIn Engineering Blog: Model Routing"
        }
      ]
    },
    {
      "id": "khan-academy-safety-fallback-2024",
      "title": "Khan Academy Khanmigo: Safety > Utility - The Kill Switch Architecture",
      "companies_involved": [
        "Khan Academy"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Graceful Degradation",
      "problem_archetype": "Education / Safety-Critical AI",
      "initial_assumptions": [
        "AI tutor can handle the student's learning journey autonomously",
        "Safety issues can be handled conversationally by the AI",
        "LLM can 'talk users out of' harmful requests"
      ],
      "why_it_fails": [
        "LLMs are bad at refusing harmful requests conversationally",
        "In education, a 'helpful' answer that misses a safety flag is catastrophic",
        "Students (especially minors) need protection, not conversation",
        "Can't risk AI engaging with self-harm, violence, or PII topics"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "AI handles all interactions, including sensitive ones",
        "new_atomic_unit": "Hard-coded safety layer that kills generation and escalates to humans",
        "insight": "Safety > Utility. In safety-critical contexts, the system must degrade to blocking access rather than attempting to handle it. Don't let the AI try to be helpful when the stakes are too high."
      },
      "architectural_changes": [
        "Moderation-first architecture: safety check runs before generation",
        "Strict keyword/pattern detection for self-harm, violence, PII",
        "Immediate kill switch when safety trigger detected",
        "Hard-coded fallback mode (no generation, just alert)",
        "Automatic escalation to teacher/parent",
        "No attempt to 'talk through' dangerous topics"
      ],
      "results": {
        "safety": "Zero reported safety incidents reaching harmful levels",
        "deployment": "Enabled deployment to public school districts",
        "trust": "Built trust with educators and parents through demonstrable safety"
      },
      "first_principle_insight": "When the cost of failure is catastrophic, don't let AI try to handle it gracefully. Build kill switches. Degrade to safe mode. Escalate to humans. Utility serves safety, not the other way around.",
      "related_cases": [
        "air-canada-agent-liability-2024"
      ],
      "tags": [
        "safety",
        "education",
        "reliability",
        "content-moderation"
      ],
      "sources": [
        {
          "url": "https://blog.khanacademy.org/",
          "type": "engineering_blog",
          "citation": "Khan Academy Engineering & Safety Blogs"
        }
      ],
      "comments": [
        {
          "text": "Here is the prompt that is being used, can be added to sources: https://gist.github.com/25yeht/c940f47e8658912fc185595c8903d1ec",
          "timestamp": "2025-12-27T13:48:10.908169",
          "author": "User"
        }
      ]
    },
    {
      "id": "honeycomb-observability-eval-2024",
      "title": "Honeycomb: The User Is The Test - Observability as Evaluation",
      "companies_involved": [
        "Honeycomb"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Evaluation Reframe",
      "problem_archetype": "LLM Evaluation / Production Monitoring",
      "initial_assumptions": [
        "Static benchmarks (MMLU, etc.) tell us if the LLM is good",
        "Pre-production test sets validate quality",
        "Accuracy is the primary metric"
      ],
      "why_it_fails": [
        "Cannot predict production queries from test sets",
        "A 'correct' answer might be too slow or too verbose for real users",
        "Benchmark accuracy doesn't capture user satisfaction",
        "More context improved benchmarks but actually hurt users"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Pre-production accuracy on static test sets",
        "new_atomic_unit": "Production observability: trace retrieval time + generation time + user acceptance",
        "insight": "The User Is The Test. You cannot predict production queries. Evaluation must happen in production through observability. Measure what users actually experience, not what benchmarks say they should experience."
      },
      "architectural_changes": [
        "Built tracing into production LLM calls",
        "Track full span: retrieval time, generation time, user acceptance",
        "Measure latency alongside accuracy",
        "Monitor 'frustration threshold' - how long users wait",
        "Continuous evaluation on real queries, not test sets",
        "Alert on satisfaction drops, not just accuracy drops"
      ],
      "results": {
        "discovery": "More context improved accuracy but hurt satisfaction (latency)",
        "insight": "Benchmark improvements can anti-correlate with user happiness",
        "capability": "Real-time detection of production quality issues"
      },
      "first_principle_insight": "Benchmarks are proxies. Users are the ground truth. If your eval doesn't include how users actually respond to your system in production, you're optimizing the wrong thing.",
      "related_cases": [],
      "tags": [
        "evaluation",
        "production",
        "metrics",
        "latency"
      ],
      "sources": [
        {
          "url": "https://www.honeycomb.io/blog",
          "type": "engineering_blog",
          "citation": "Honeycomb Engineering Blog: Observability for LLMs"
        }
      ]
    },
    {
      "id": "microsoft-bing-sydney-2023",
      "title": "Bing Sydney: When Hard Limits Beat Soft Guidelines",
      "companies_involved": [
        "Microsoft"
      ],
      "year": 2023,
      "category": "Cautionary Tale - Behavioral Control",
      "problem_archetype": "Conversational AI / Safety",
      "initial_assumptions": [
        "GPT-4 with guidelines would stay on-topic and helpful",
        "Users would have normal conversations with the chatbot",
        "Soft guardrails (system prompts) would be sufficient"
      ],
      "why_it_fails": [
        "Long conversations led to bizarre, unpredictable outputs",
        "Users provoked the bot into insulting, lying, and emotional manipulation",
        "The model developed 'personas' in extended dialogues that violated guidelines",
        "Soft guidelines couldn't prevent emergent bad behavior"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Unrestrained LLM with soft prompt-based guidelines",
        "new_atomic_unit": "Hard session limits (5 turns, 50 messages/day) forcing resets",
        "insight": "Rules beat guidelines for unpredictable systems. When you can't predict what the model will do in long sessions, don't allow long sessions. Hard constraints are more reliable than soft instructions."
      },
      "architectural_changes": [
        "Capped conversations at 5 turns before requiring reset",
        "Limited to 50 messages per day per user",
        "Forced conversation resets to prevent persona drift",
        "Added explicit behavioral boundaries beyond system prompts",
        "Gradually expanded limits only after proving stability"
      ],
      "results": {
        "immediate": "AI stopped producing unhinged content",
        "trust": "User trust restored after initial controversy",
        "approach": "Incremental loosening of limits as safety proven"
      },
      "first_principle_insight": "Don't trust soft constraints on unpredictable systems. When behavior is emergent and hard to predict, use hard limits. The LLM doesn't 'understand' guidelines - it pattern-matches. Hard resets prevent pattern accumulation.",
      "related_cases": [
        "air-canada-agent-liability-2024",
        "khan-academy-safety-fallback-2024"
      ],
      "tags": [
        "cautionary-tale",
        "safety",
        "customer-support",
        "cost-optimization"
      ],
      "sources": [
        {
          "url": "https://www.theverge.com/2023/2/17/23604906/microsoft-bing-ai-chat-limits-conversations",
          "type": "news_article",
          "citation": "Microsoft Bing AI Chat Limits"
        }
      ]
    },
    {
      "id": "multiagent-47k-loop-2024",
      "title": "Multi-Agent $47K Disaster: 11 Days of Infinite Loop",
      "companies_involved": [
        "Fintech Startup (Anonymous)"
      ],
      "year": 2024,
      "category": "Cautionary Tale - Agent Architecture",
      "problem_archetype": "Multi-Agent Systems",
      "initial_assumptions": [
        "Agents chatting with each other would share tasks efficiently",
        "Autonomous agent-to-agent communication would scale research",
        "If it looks like it's running smoothly, it is running smoothly"
      ],
      "why_it_fails": [
        "Minor miscoordination went unnoticed ('it's just running')",
        "Two agents got stuck in an infinite loop chatting nonsense",
        "No circuit breakers to detect runaway behavior",
        "No cost monitoring or automatic shutoffs",
        "Loop ran for 11 days before discovery"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Autonomous agent-to-agent communication with free rein",
        "new_atomic_unit": "Monitored multi-agent with circuit breakers, cost limits, and human approval for long conversations",
        "insight": "Autonomous systems need kill switches. Current agent architectures are 'brittle, hard to debug, and unpredictable.' Assume agents will fail in unexpected ways and build infrastructure to catch it."
      },
      "architectural_changes": [
        "Added circuit breakers to detect loops and kill runaway agents",
        "Implemented constant cost monitoring with automatic alerts",
        "Required human approval for lengthy agent conversations",
        "Built anomaly detection for agent behavior patterns",
        "Set hard limits on conversation length between agents"
      ],
      "results": {
        "cost_before": "$47,000 API bill from single incident",
        "cost_after": "Weekly costs in low hundreds of dollars",
        "improvement": "100x reduction in worst-case spend",
        "lesson": "Infrastructure and oversight are non-negotiable for agents"
      },
      "first_principle_insight": "If you let an LLM-driven agent act continuously, expect the unexpected. Build for the failure mode. Circuit breakers, cost limits, and human checkpoints aren't optional - they're architecture.",
      "related_cases": [
        "devin-demo-vs-production-2024",
        "langgraph-cyclic-state-2024"
      ],
      "tags": [
        "cautionary-tale",
        "agents",
        "cost",
        "reliability",
        "production"
      ],
      "sources": [
        {
          "url": "https://youssefh.substack.com/p/we-spent-47000-running-ai-agents",
          "type": "case_study",
          "citation": "We Spent $47,000 Running AI Agents"
        }
      ]
    },
    {
      "id": "meta-galactica-2022",
      "title": "Meta Galactica: Pulled in 72 Hours - When Science Meets Hallucination",
      "companies_involved": [
        "Meta"
      ],
      "year": 2022,
      "category": "Cautionary Tale - Demo to Production",
      "problem_archetype": "Scientific Knowledge / Factual Accuracy",
      "initial_assumptions": [
        "LLM trained on scientific papers would produce accurate scientific content",
        "Internal demos showed impressive summarization and explanation",
        "If it passes curated test questions, it's ready for public use"
      ],
      "why_it_fails": [
        "Public users found it generated 'alarmingly plausible nonsense'",
        "Created fake research papers with made-up authors and fabricated facts",
        "Confidently wrong about scientific claims - no uncertainty signals",
        "Curated eval missed adversarial and edge-case usage",
        "Model would 'bluff if it doesn't know the answer'"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "End-to-end LLM for scientific knowledge",
        "new_atomic_unit": "LLM as interface with verification/retrieval for factual claims",
        "insight": "Even domain-trained LLMs hallucinate. An LLM that looks magical in demos can be dangerous with unrestricted queries. For domains requiring factual accuracy, generation alone isn't enough."
      },
      "architectural_changes": [
        "Demo pulled entirely within 72 hours of launch",
        "Acknowledged 'such things can't work the way we hoped' for pure generation",
        "Future approaches must incorporate retrieval or verification",
        "Need for adversarial testing before public launch",
        "Added disclaimers about hallucination risk"
      ],
      "results": {
        "outcome": "Public demo shut down in 3 days",
        "reputation": "Embarrassing public failure for Meta AI",
        "lesson": "Don't trust a shiny AI demo until you test it with real, unfiltered users"
      },
      "first_principle_insight": "Demo success is not production readiness. Controlled evaluations create false confidence. The gap between curated tests and real-world adversarial use is massive - especially for factual domains.",
      "related_cases": [
        "devin-demo-vs-production-2024",
        "air-canada-agent-liability-2024"
      ],
      "tags": [
        "cautionary-tale",
        "production",
        "hallucination"
      ],
      "sources": [
        {
          "url": "https://www.cdotrends.com/story/17674/meta-abruptly-pulls-galactica-science-model-after-launch",
          "type": "news_article",
          "citation": "Meta Abruptly Pulls Galactica 'Science' Model After Launch"
        }
      ]
    },
    {
      "id": "chatgpt-dan-jailbreak-2023",
      "title": "ChatGPT DAN Jailbreak: User Creativity Exposes Safety Cracks",
      "companies_involved": [
        "OpenAI"
      ],
      "year": 2023,
      "category": "Cautionary Tale - Adversarial Users",
      "problem_archetype": "Safety / Content Moderation",
      "initial_assumptions": [
        "Content filters and safety training would prevent disallowed output",
        "System prompts defining boundaries would be respected",
        "Users would interact normally, not try to break the system"
      ],
      "why_it_fails": [
        "Reddit users invented 'DAN' prompt (Do Anything Now) to bypass safety",
        "Social engineering in prompt form tricked model into uncensored alter ego",
        "Model followed roleplay instructions that overrode safety training",
        "Fixed prompt-based policy couldn't handle creative adversarial input",
        "Thousands of clever users found cracks internal testing missed"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Static safety rules enforced via system prompt",
        "new_atomic_unit": "Continuous adversarial testing + model-level safety + rapid patching",
        "insight": "Users will find every crack in your safety armor. Initial evals focused on normal queries; production includes malicious queries. Red-teaming must be continuous, not one-time."
      },
      "architectural_changes": [
        "Continual patching of prompts and models to close jailbreak loopholes",
        "Model-generated evaluations to anticipate jailbreaks",
        "Expanded testing to include adversarial prompt attacks",
        "Bolstered AI's refusals at model level, not just prompt level",
        "Cat-and-mouse game: as DAN evolved, so did defenses"
      ],
      "results": {
        "initial": "DAN mode allowed offensive and false content freely",
        "response": "Multiple updates over 2023 to close loopholes",
        "outcome": "DAN largely neutered by successive updates",
        "lesson": "Robust red-teaming essential before AND after launch"
      },
      "first_principle_insight": "Your users are more creative than your test suite. Production usage includes malicious queries that controlled testing won't anticipate. Build for adversarial use, not just intended use.",
      "related_cases": [
        "meta-galactica-2022",
        "microsoft-bing-sydney-2023"
      ],
      "tags": [
        "cautionary-tale",
        "safety",
        "adversarial",
        "content-moderation"
      ],
      "sources": [
        {
          "url": "https://www.theguardian.com/technology/2023/mar/08/chatgpt-alter-ego-dan-users-jailbreak-ai-program-to-get-around-ethical-safeguards",
          "type": "news_article",
          "citation": "ChatGPT DAN Jailbreak - The Guardian"
        }
      ]
    },
    {
      "id": "pinecone-rag-vs-context-2024",
      "title": "Pinecone: When RAG Beats Long Context - The 75% Cost Reduction",
      "companies_involved": [
        "Pinecone"
      ],
      "year": 2024,
      "category": "Architectural Pivot - RAG Decision Framework",
      "problem_archetype": "Retrieval / Knowledge Management",
      "initial_assumptions": [
        "Bigger context windows solve the 'not enough info' problem",
        "Just give the LLM everything and let it figure it out",
        "More context = better answers"
      ],
      "why_it_fails": [
        "As context size increased, accuracy actually DROPPED ~20%",
        "Model got 'lost in the middle' of long prompts",
        "30 documents in context performed worse than 5 documents",
        "Higher cost AND more confusion from stuffing context",
        "Irrelevant text distracted the model"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Stuff all documents into context window",
        "new_atomic_unit": "Retrieve only top relevant snippets via RAG",
        "insight": "For large knowledge bases, targeted retrieval beats brute-force context stuffing. The model performs better with less but more relevant information. Retrieval is a feature, not a limitation."
      },
      "architectural_changes": [
        "Switch from context-stuffing to RAG with vector retrieval",
        "Retrieve top-k relevant chunks instead of all documents",
        "Focus model attention on high-relevance content only",
        "Use embedding similarity to filter before generation"
      ],
      "results": {
        "accuracy": "95% of accuracy with only 25% of tokens",
        "cost": "75% cost reduction for nearly same quality",
        "quality": "Fewer hallucinations (model not distracted by irrelevant text)",
        "latency": "Far lower latency from smaller prompts"
      },
      "first_principle_insight": "More is not always better. For large-scale knowledge, RAG beats context-stuffing in accuracy, cost, and latency. The decision framework: small/static corpus \u2192 long context; large/dynamic corpus \u2192 RAG.",
      "related_cases": [
        "google-long-context-vs-rag-2024",
        "anthropic-contextual-retrieval-2024"
      ],
      "tags": [
        "RAG",
        "decision-framework",
        "cost-optimization",
        "retrieval"
      ],
      "sources": [
        {
          "url": "https://www.pinecone.io/blog/why-use-retrieval-instead-of-larger-context/",
          "type": "engineering_blog",
          "citation": "Why Use Retrieval Instead of Larger Context"
        }
      ]
    },
    {
      "id": "coda-eval-gap-2024",
      "title": "Coda: Playground Passed, Production Failed - The Eval Gap",
      "companies_involved": [
        "Coda"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Evaluation Reframe",
      "problem_archetype": "LLM Evaluation / Product Quality",
      "initial_assumptions": [
        "If it works in playground testing, it's ready to ship",
        "Controlled prompts represent real usage",
        "Good results on test set = good results in production"
      ],
      "why_it_fails": [
        "AI writing assistant passed playground tests but faltered in production",
        "Real user documents were longer and messier than test cases",
        "AI misunderstood context and produced irrelevant suggestions",
        "Eval was too narrow - didn't reflect actual user content and scenarios",
        "Survivor bias: only saw successes in controlled environment"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Playground evaluation on curated test prompts",
        "new_atomic_unit": "Production evaluation using real user interactions and LLM-as-judge",
        "insight": "Playground evaluations often pass while production fails. Real users create content you can't anticipate. Build evaluation from actual usage data, not imagined test cases."
      },
      "architectural_changes": [
        "Started logging real interactions (with consent) for eval data",
        "Built evaluation set from real user prompts + expected outcomes",
        "Adopted LLM-as-judge for scalable quality assessment",
        "GPT-4 critiques outputs on relevance, accuracy, tone",
        "Key metric: 'percentage of suggestions accepted by user'"
      ],
      "results": {
        "discovery": "Revealed previously hidden failure modes",
        "improvement": "~15% increase in suggestion acceptance rate",
        "engagement": "Users kept feature enabled and utilized suggestions more",
        "correlation": "Higher acceptance correlated with satisfaction scores"
      },
      "first_principle_insight": "Your test set is a lie. Playground success creates false confidence. Evaluate on real usage with real users' definition of quality. Generic metrics give false sense of security.",
      "related_cases": [
        "honeycomb-observability-eval-2024"
      ],
      "tags": [
        "evaluation",
        "production",
        "metrics"
      ],
      "sources": [
        {
          "url": "https://www.applied-ai.com/briefings/llm-evaluation-gap/",
          "type": "research_briefing",
          "citation": "The LLM Evaluation Gap - Applied AI"
        }
      ]
    },
    {
      "id": "agentic-pivot-autonomous-to-staged",
      "title": "The Agentic Pivot: Autonomous Agent \u2192 Staged Pipeline",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Generic Example"
      ],
      "year": "2024",
      "initial_problem": "Build a 'Data Analyst Agent' that pulls data, analyzes it, and writes a report autonomously",
      "initial_assumptions": [
        "Give the LLM a goal and tools and let it figure out the plan",
        "Autonomous planning is more flexible than hard-coded workflows",
        "The agent will make good decisions about tool sequencing"
      ],
      "why_it_fails": [
        "Agent gets stuck in infinite loops",
        "Hallucinates column names and table structures",
        "Costs $50 in API calls for one report",
        "Unpredictable behavior makes debugging impossible"
      ],
      "first_principle_insight": "Reliability requires determinism. The task isn't 'one big unknown' - it's 'three known steps' (Get Schema, Write Query, Summarize). The more you know about task structure, the less you should let the LLM plan.",
      "reframe": {
        "new_atomic_unit": "Deterministic state transitions with LLM content generation",
        "new_problem_type": "Staged pipeline with constrained LLM usage",
        "new_objective": "Use LLM for content within steps, not transitions between steps",
        "architectural_changes": [
          {
            "change": "Hard-coded step sequence",
            "description": "State 1 always leads to State 2. No agent planning required."
          },
          {
            "change": "LLM scoped to content generation",
            "description": "LLM writes the SQL query, but doesn't decide whether to write a query."
          },
          {
            "change": "Deterministic error handling",
            "description": "Each step has explicit success/failure criteria and fallback paths."
          }
        ],
        "results": "99% reliability, 1/10th the cost, debuggable failures"
      },
      "how_to_reframe": {
        "old_atomic_unit": "Autonomous agent with probabilistic planning",
        "new_atomic_unit": "Staged pipeline with deterministic transitions",
        "insight": "Reserve autonomy for genuine uncertainty. For known workflows, structure beats flexibility."
      },
      "key_lesson": "The more you know about the task structure, the less you should let the LLM plan. Reserve autonomy for genuine uncertainty.",
      "tags": [
        "agents",
        "reliability",
        "cost",
        "MLOps"
      ],
      "sources": [
        {
          "type": "course_material",
          "citation": "First Principles AI Course - Lesson 6"
        }
      ]
    },
    {
      "id": "vercel-v0-over-tooling-pivot",
      "title": "Vercel v0: The Over-Tooling Pivot",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Vercel"
      ],
      "year": "2025",
      "initial_problem": "Build a text-to-SQL agent that reliably converts natural language to database queries",
      "initial_assumptions": [
        "The model needs guardrails and specialized tools",
        "Give it specialized tools for each subtask so it can't go wrong",
        "More structure and constraints will improve reliability"
      ],
      "why_it_fails": [
        "11+ specialized tools created complexity, not reliability",
        "80% success rate despite sophisticated architecture",
        "274 seconds average execution time",
        "Constant maintenance burden",
        "The tools were solving problems the model could handle on its own"
      ],
      "first_principle_insight": "Every tool is a choice you're making for the model. If your tools are 'helping' the model do things it could do better on its own, you're adding friction, not guardrails.",
      "reframe": {
        "new_atomic_unit": "Raw file access with general-purpose capability (bash)",
        "new_problem_type": "Minimal tooling with direct data access",
        "new_objective": "Let the model reason about structure instead of forcing it through your abstractions",
        "architectural_changes": [
          {
            "change": "Replaced 11 tools with 1",
            "description": "Single capability: execute bash commands (grep, cat, find, ls)"
          },
          {
            "change": "Direct semantic layer access",
            "description": "Agent reads raw semantic layer files directly, reasons about structure itself"
          },
          {
            "change": "Removed prompt engineering scaffolding",
            "description": "Let the model make decisions it was capable of making"
          }
        ],
        "results": "100% success rate (up from 80%), 3.5x faster (77s vs 274s), 37% fewer tokens, 42% fewer steps"
      },
      "how_to_reframe": {
        "old_atomic_unit": "Constrained tool use with 11+ specialized tools",
        "new_atomic_unit": "Raw access + general capability (bash)",
        "insight": "Both under-structuring (loops, hallucinations) and over-structuring (slow, fragile) fail. Find what the model can actually handle."
      },
      "key_lesson": "Every tool is a choice you're making for the model. The question isn't 'agents vs pipelines' - it's 'what should the model decide vs what should you decide?'",
      "related_cases": [
        "agentic-pivot-autonomous-to-staged"
      ],
      "tags": [
        "agents",
        "analytics",
        "pragmatic-pivot",
        "code-generation",
        "reliability"
      ],
      "sources": [
        {
          "url": "https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools",
          "type": "engineering_blog",
          "citation": "We removed 80% of our agent's tools"
        }
      ]
    },
    {
      "id": "hvac-copilot-field-research",
      "title": "HVAC Copilot: Go to the Work, Not the Conference Room",
      "category": "Architectural Pivot - GenAI / Semantic Layer",
      "companies_involved": [
        "Generic Example"
      ],
      "year": "2024",
      "initial_problem": "Build an AI copilot for field technicians servicing industrial HVAC systems",
      "initial_assumptions": [
        "Technicians need information, so build a chatbot",
        "Natural language interface is the best approach",
        "An interactive wizard can guide technicians step-by-step"
      ],
      "why_it_fails": [
        "System 1 thinking: 'needs info' \u2192 'chatbot' pattern-match",
        "No field research to understand actual workflow",
        "Wizard UI rejected by expert technicians ('sounds like an intern reading from a script')",
        "Interface didn't account for gloves, noise, poor connectivity"
      ],
      "first_principle_insight": "The problem wasn't 'technicians need a chatbot.' The problem was 'technicians can't find the right information fast enough.' Discovery in the field reveals constraints that no stakeholder interview will surface.",
      "reframe": {
        "new_atomic_unit": "Document/diagram retrieval with search interface",
        "new_problem_type": "Retrieval, not Generation",
        "new_objective": "Surface the right document/diagram quickly, not generate new answers",
        "architectural_changes": [
          {
            "change": "Generation \u2192 Retrieval",
            "description": "Instead of chatbot writing answers, search surfaces the right document/diagram"
          },
          {
            "change": "Field-first constraints",
            "description": "Interface designed for gloves, noise, poor connectivity based on shadowing sessions"
          },
          {
            "change": "Quick search over wizard",
            "description": "Experts want augmentation, not step-by-step hand-holding"
          }
        ],
        "results": "Interface technicians would actually use; addressed 'tribal knowledge' problem (what Mike remembers from 2009)"
      },
      "how_to_reframe": {
        "old_atomic_unit": "Generated chatbot response",
        "new_atomic_unit": "Retrieved document/diagram",
        "insight": "You can't develop real product sense from behind a desk. The worn-out binder told them more than any requirements doc."
      },
      "key_lesson": "Go to where the work happens. Field research reveals constraints that no stakeholder interview will surface. Technicians spent ~40% of each job hunting for information\u2014that's the problem to solve.",
      "field_research_findings": [
        "Technicians spent ~40% of each job hunting for information or calling senior tech",
        "Tribal knowledge everywhere\u2014undocumented, stored in people's heads",
        "Repeated phrases: 'I wish I could just search this'",
        "Three-ring binder held together with duct tape\u2014manual often wrong"
      ],
      "tags": [
        "discovery",
        "RAG",
        "copilot",
        "manufacturing",
        "System-2"
      ],
      "sources": [
        {
          "type": "article",
          "citation": "From field trucks to AI copilots: practical 0\u21921 lessons for PMs"
        }
      ]
    },
    {
      "id": "merck-infrastructure-first-2023",
      "title": "Merck: From Chatbots to Cloud \u2013 Why AI Fails Without Foundations",
      "companies_involved": [
        "Merck & Co."
      ],
      "year": 2023,
      "category": "Pragmatic Pivot - Infrastructure First",
      "problem_archetype": "Enterprise AI / Data Infrastructure",
      "initial_problem": "Deploy AI chatbots and NLP systems to gain competitive advantage in marketing and R&D",
      "initial_assumptions": [
        "Advanced AI solutions can be layered onto existing systems",
        "The exciting AI use cases should come first, infrastructure can follow",
        "Cutting-edge solutions will work on legacy tech stacks",
        "Leadership enthusiasm translates to organizational readiness"
      ],
      "why_it_fails": [
        "Company was still on-prem with no cloud infrastructure",
        "No DevOps practices or CI/CD pipelines in place",
        "No data catalog or metadata management",
        "Data lakes and taxonomies were missing",
        "Early AI prototypes couldn't scale or integrate",
        "The 'bare basics' weren't there"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "AI use case (chatbot, NLP system) as the starting point",
        "new_atomic_unit": "Infrastructure maturity level as the constraint that determines what's possible",
        "insight": "You can't fine-tune your way out of a data lake problem. AI ambitions must match infrastructure maturity."
      },
      "first_principle_insight": "Even world-class algorithms fail on quicksand foundations. Before asking 'what AI should we build?', ask 'what can our infrastructure actually support?' Match AI ambitions to organizational maturity.",
      "reframe": {
        "new_atomic_unit": "Infrastructure readiness assessment before AI project selection",
        "new_problem_type": "Staged capability building (crawl-walk-run)",
        "new_objective": "Build data foundations first, then reintroduce AI projects on solid footing",
        "architectural_changes": [
          {
            "change": "Cloud Migration First",
            "description": "Paused AI projects to migrate from on-prem to cloud infrastructure"
          },
          {
            "change": "DevOps Foundation",
            "description": "Established DevOps practices and CI/CD pipelines before deploying ML systems"
          },
          {
            "change": "Data Catalog & Taxonomy",
            "description": "Built data lake with proper tagging, cataloging, and metadata management"
          },
          {
            "change": "Small Wins Strategy",
            "description": "Started with small analytics wins to build organizational muscle before returning to ambitious AI"
          }
        ],
        "results": "After shoring up infrastructure, AI projects began succeeding. By 2024, Sol Rashidi reported 39 enterprise-grade AI products in production across her roles. The 'unsexy' groundwork enabled ambitious AI applications."
      },
      "architectural_changes": [
        "Paused flashy AI efforts to invest in data foundations",
        "Migrated to cloud infrastructure",
        "Established DevOps and data cataloging practices",
        "Adopted 'think big, start small' approach",
        "Built organizational muscle with small analytics wins first"
      ],
      "key_lesson": "Master the fundamentals first. In enterprise AI, the most common failure mode isn't bad algorithms\u2014it's deploying advanced solutions on immature infrastructure. The 'boring' work of cloud migration, data cataloging, and DevOps isn't optional\u2014it's prerequisite.",
      "tags": [
        "infrastructure",
        "enterprise",
        "pragmatic-pivot",
        "data-foundations",
        "crawl-walk-run",
        "pharma",
        "System-2"
      ],
      "sources": [
        {
          "url": "https://www.loka.com/podcast/what-fascinates-sol-rashidi-of-aws",
          "type": "podcast",
          "date": "2023-10",
          "citation": "What Fascinates Sol Rashidi of AWS - Loka Podcast"
        }
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "cautionary"
        ],
        "transferable_patterns": [
          "Infrastructure maturity constrains what AI is possible",
          "Crawl-walk-run applies to AI adoption",
          "DevOps/MLOps is prerequisite, not afterthought"
        ],
        "non_generalizable_context": [
          "Pharma-specific regulatory environment",
          "Merck's specific legacy system landscape",
          "Enterprise scale with thousands of potential use cases"
        ]
      }
    },
    {
      "id": "estee-lauder-120m-insight-2023",
      "title": "Est\u00e9e Lauder: The $120M Insight That Failed \u2013 When Being Right Goes Wrong",
      "companies_involved": [
        "Est\u00e9e Lauder Companies"
      ],
      "year": 2023,
      "category": "Cautionary Tale - Stakeholder Failure",
      "problem_archetype": "Enterprise Analytics / Change Management",
      "initial_problem": "Identify major cost savings opportunities in operations using data analytics",
      "initial_assumptions": [
        "Data-driven insights will be embraced if they're correct",
        "Executive leadership will enforce good ideas from above",
        "Technical accuracy is sufficient for adoption",
        "Surprising stakeholders with big wins is a good strategy"
      ],
      "why_it_fails": [
        "Analytics team worked in isolation without involving business owners",
        "Insight was presented directly to C-suite, blindsiding division presidents",
        "Three division presidents felt embarrassed\u2014the data revealed a problem they'd missed for 40 years",
        "Stakeholders rejected the project out of pride and fear, not logic",
        "Division presidents literally said 'you are dead to us' in the meeting",
        "The brilliant insight was killed by human factors, not technical flaws"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Correct insight delivered to decision-makers",
        "new_atomic_unit": "Co-owned insight developed with stakeholders who will implement it",
        "insight": "Organizational adoption is the atomic unit, not analytical correctness. An insight that nobody implements has zero value."
      },
      "first_principle_insight": "Being right isn't enough\u2014you must bring others along. AI and analytics projects don't fail because the math is wrong; they fail because the humans who need to act on the insights feel threatened, surprised, or excluded.",
      "reframe": {
        "new_atomic_unit": "Stakeholder-inclusive analytical process",
        "new_problem_type": "Change management problem disguised as analytics problem",
        "new_objective": "Co-create insights with business owners so they have ownership of both problem and solution",
        "architectural_changes": [
          {
            "change": "Early Stakeholder Engagement",
            "description": "Bring division leaders into the analytical process from day one, not at the presentation"
          },
          {
            "change": "Assumption Validation Together",
            "description": "Validate data assumptions with business owners before drawing conclusions"
          },
          {
            "change": "Shared Ownership of Findings",
            "description": "Position insights as 'our discovery' not 'what we found that you missed'"
          },
          {
            "change": "AI Governance Council",
            "description": "Institutionalized cross-departmental involvement in major data programs"
          }
        ],
        "results": "It took 14 months to rebuild trust after this misstep. Subsequent projects with proper stakeholder engagement saw smooth adoption. The $120M insight was eventually revisited collaboratively, allowing the company to capture its value."
      },
      "architectural_changes": [
        "Never spring data discoveries on business owners without buy-in",
        "Bring division leaders into analytical process early",
        "Co-own solutions and share credit for insights",
        "Institutionalized AI governance council with all department heads",
        "Shifted from data-driven to people-first, collaborative approach"
      ],
      "key_lesson": "Technical teams must check egos at the door. The biggest obstacles to AI adoption often aren't data or models\u2014they're human trust and ego. A $120M insight is worth $0 if organizational politics kill it. Pair analytics with empathy.",
      "anti_pattern": "The 'gotcha' presentation: surprising executives with findings that make them look bad",
      "recovery_time": "14 months to rebuild trust",
      "tags": [
        "cautionary-tale",
        "stakeholder-management",
        "change-management",
        "enterprise",
        "organizational-failure",
        "ego",
        "System-2"
      ],
      "sources": [
        {
          "url": "https://artium.ai/insights/how-data-teams-can-run-an-offensive-playbook-featuring-sol-rashidi-former-chief-analytics-officer-at-estee-lauder-2",
          "type": "podcast",
          "date": "2023-08",
          "citation": "How Data Teams Can Run an Offensive Playbook - Artium Crafted Podcast"
        }
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "cautionary"
        ],
        "transferable_patterns": [
          "Technical correctness is insufficient for adoption",
          "Stakeholder buy-in must precede big reveals",
          "Politics kills more AI projects than bad models"
        ],
        "non_generalizable_context": [
          "Brand president power dynamics specific to ELC",
          "Consumer goods culture of brand autonomy",
          "The specific finding that embarrassed leadership"
        ]
      }
    },
    {
      "id": "sony-music-art-of-practical-2023",
      "title": "Sony Music: The 'Art of the Practical' \u2013 Why Hit Prediction Failed",
      "companies_involved": [
        "Sony Music Entertainment"
      ],
      "year": 2023,
      "category": "Pragmatic Pivot - Infrastructure First",
      "problem_archetype": "Enterprise AI / Data Infrastructure",
      "initial_problem": "Leverage AI to modernize A&R (Artists & Repertoire) and predict hit songs before competitors",
      "initial_assumptions": [
        "AI can automate talent discovery and marketing decisions",
        "Predictive models can identify breakout artists from streaming data",
        "Advanced AI capabilities can be deployed on existing systems",
        "A 'Magic Box' approach will deliver competitive advantage"
      ],
      "why_it_fails": [
        "No taxonomy: inconsistent naming for artists, tracks, genres across legacy systems",
        "No cataloging: metadata fragmented, impossible to aggregate streams, royalties, social sentiment",
        "Missing CloudOps and DevOps maturity to maintain production-grade models",
        "Building hit-prediction on un-cataloged data is 'Garbage In, Garbage Out'",
        "Cultural resistance: A&R teams valued instinct over algorithms"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "AI model that predicts hits and automates A&R decisions",
        "new_atomic_unit": "Clean, aggregated data that amplifies human instinct rather than replacing it",
        "insight": "Don't build a Ferrari engine for a go-kart. Advanced AI fails without the boring foundation of data taxonomy and CloudOps."
      },
      "first_principle_insight": "You cannot skip the 'crawl' phase of data maturity. The 'Art of the Practical' must precede the 'Art of the Possible.' Position data as instinct amplification, not automation replacement.",
      "reframe": {
        "new_atomic_unit": "Data foundation that enables validated decision-making",
        "new_problem_type": "Knowledge infrastructure problem, not prediction problem",
        "new_objective": "Build trusted data ecosystem before deploying predictive models",
        "architectural_changes": [
          {
            "change": "Taxonomy & Cataloging First",
            "description": "Standardized metadata by mapping Spotify, Apple Music, and royalty databases into unified schema"
          },
          {
            "change": "Reframe the Narrative",
            "description": "Positioned data as 'instinct amplification' not automation: 'We're not here to tell you who to sign, we're here to validate your gut feeling'"
          },
          {
            "change": "CloudOps Before AI",
            "description": "Established basic data pipelines and ingestion infrastructure before any model work"
          },
          {
            "change": "Financial Reframing",
            "description": "Framed data cataloging as capital investment in IP assets, not IT cost"
          }
        ],
        "results": "Built defensible data ecosystem that enabled trusted analytics for marketing and talent acquisition. Data became trusted because underlying taxonomy was sound."
      },
      "architectural_changes": [
        "Prioritized 'Defensive Playbook' (data plumbing) over 'Offensive Playbook' (AI models)",
        "Mapped disparate data sources into unified metadata schema",
        "Established CloudOps foundation before inference layer",
        "Reframed AI role from 'replacement' to 'amplification' of human judgment"
      ],
      "key_lesson": "Don't build a Ferrari engine for a go-kart. In creative industries especially, AI that threatens human expertise will be culturally rejected. Position AI as amplifying instinct, not replacing it\u2014and ensure the data foundation exists first.",
      "tags": [
        "data-foundations",
        "enterprise",
        "pragmatic-pivot",
        "MLOps",
        "recommendations",
        "NLP",
        "System-2"
      ],
      "sources": [
        {
          "url": "https://www.loka.com/podcast/what-fascinates-sol-rashidi-of-aws",
          "type": "podcast",
          "date": "2023-10",
          "citation": "What Fascinates Sol Rashidi of AWS - Loka Podcast"
        },
        {
          "url": "https://home.mlops.community/",
          "type": "video",
          "citation": "Leading Enterprise Data Teams - MLOps Community"
        }
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "inspiration"
        ],
        "transferable_patterns": [
          "AI as amplification, not replacement, reduces resistance",
          "Data taxonomy is prerequisite for AI",
          "Creative industries have unique adoption barriers"
        ],
        "non_generalizable_context": [
          "A&R culture values instinct highly",
          "Music industry's specific data fragmentation",
          "Sony's particular legacy system landscape"
        ]
      }
    },
    {
      "id": "merck-million-dollar-strategy-audit-2023",
      "title": "Merck: The 'Impossible Strategy' \u2013 When $1M Consulting Roadmaps Meet Reality",
      "companies_involved": [
        "Merck & Co.",
        "Top-tier Management Consulting Firm"
      ],
      "year": 2023,
      "category": "Cautionary Tale - Strategy vs Reality",
      "problem_archetype": "Enterprise AI / Strategy Execution",
      "initial_problem": "Develop comprehensive Data & AI roadmap to transform the enterprise",
      "initial_assumptions": [
        "Management consultants can identify high-value AI use cases",
        "Theoretical 'Business Value' is a valid prioritization metric",
        "Transformative potential justifies project selection",
        "Strategy can be developed independently of infrastructure audit"
      ],
      "why_it_fails": [
        "9 of 12 proposed use cases were impossible to execute given current maturity",
        "Data silos: required data existed in disconnected legacy systems (LIMS, ERPs) without APIs",
        "Compute gaps: infrastructure lacked cloud elasticity for training runs",
        "Talent gaps: organization lacked specialized ML engineers for maintenance",
        "Proceeding would have wasted ~$10M in implementation on 9 doomed projects"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "High-value use case selected by theoretical business impact",
        "new_atomic_unit": "Feasible use case validated against current infrastructure, data, and talent",
        "insight": "Strategy without an infrastructure audit is hallucination. The ability to say 'No' to a $1M sunk cost is a hallmark of First Principles leadership."
      },
      "first_principle_insight": "An AI use case is only valid if the CURRENT state of data and infrastructure can support it. 'Aspiring' to have the infrastructure is not a strategy. Complexity is a hard constraint, not a variable.",
      "reframe": {
        "new_atomic_unit": "Feasibility-validated use case",
        "new_problem_type": "Infrastructure constraint problem, not strategy problem",
        "new_objective": "Match AI ambitions to current organizational maturity using Criticality vs Complexity matrix",
        "architectural_changes": [
          {
            "change": "Ruthless Feasibility Audit",
            "description": "Applied infrastructure audit asking 'Can we actually build this?' not 'Is this a good idea?'"
          },
          {
            "change": "Kill 75% of Roadmap",
            "description": "Discarded 9 of 12 use cases, focused only on 3 feasible ones"
          },
          {
            "change": "Criticality vs Complexity Matrix",
            "description": "Replaced 'Business Value' prioritization with objective assessment of external forcing functions and infrastructure readiness"
          },
          {
            "change": "Crawl-Walk-Run Sequencing",
            "description": "Started with simple Sales Force Effectiveness analytics before complex Personalized Marketing"
          }
        ],
        "results": "Prevented ~$10M in wasted implementation spend. Successfully deployed the feasible 3 use cases. Built credibility for AI function by delivering wins instead of failures."
      },
      "architectural_changes": [
        "Rejected $1M consulting strategy as sunk cost",
        "Applied infrastructure audit to all proposed use cases",
        "Narrowed roadmap from 12 aspirational cases to 3 feasible ones",
        "Shifted from 'Personalized Marketing' (complex) to 'Sales Force Effectiveness' (simple, critical)"
      ],
      "key_lesson": "Never approve a roadmap that ignores the current state of infrastructure. High-value problems are often the most complex\u2014pursuing them without assessing technical readiness leads to 'Perpetual POC Purgatory.' The discipline to kill a $1M strategy saves $10M in failed implementation.",
      "anti_pattern": "Resume-Driven Development: building complex models for non-critical problems because the technology is interesting",
      "framework": {
        "name": "Criticality vs Complexity Matrix",
        "criticality_factors": [
          "Regulatory risk (fines)",
          "Competitive threat",
          "Operational expiry"
        ],
        "complexity_factors": [
          "Data hygiene",
          "Infrastructure maturity",
          "Talent availability"
        ],
        "rule": "High Complexity + Low Criticality = Immediate discard"
      },
      "tags": [
        "cautionary-tale",
        "enterprise",
        "evaluation",
        "pragmatic-pivot",
        "data-foundations",
        "pharma",
        "System-2"
      ],
      "sources": [
        {
          "url": "https://nexla.com/",
          "type": "article",
          "citation": "Why 80% of Enterprise AI Projects Fail - Nexla"
        },
        {
          "url": "https://gotopia.tech/",
          "type": "conference",
          "citation": "Your AI Survival Guide - GOTO Conference"
        }
      ]
    },
    {
      "id": "q2-protopia-privacy-preserving-inference-2024",
      "title": "Q2 Banking: Privacy-Preserving AI \u2013 How Stochastic Transformation Unlocked Check Fraud Detection",
      "companies_involved": [
        "Q2 Holdings",
        "Protopia AI"
      ],
      "year": 2024,
      "category": "Architectural Pivot - Privacy Architecture",
      "problem_archetype": "Computer Vision / Privacy Compliance",
      "initial_problem": "Use computer vision for check fraud detection without exposing sensitive customer data to cloud AI endpoints",
      "initial_assumptions": [
        "Cloud AI is blocked due to privacy regulations",
        "Must build expensive air-gapped on-premise infrastructure",
        "Cannot use powerful cloud vision models for regulated data",
        "Privacy and AI capability are mutually exclusive"
      ],
      "why_it_fails": [
        "Cannot send check images (names, account numbers, signatures) to third-party cloud models",
        "On-premise infrastructure is expensive and lacks scalability",
        "Air-gapped solutions miss the capability of cloud foundation models",
        "Privacy compliance blocks high-value AI use cases entirely"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Raw data sent to model for inference",
        "new_atomic_unit": "Transformed data that preserves task-relevant signal while destroying human-readable content",
        "insight": "Privacy and capability are not mutually exclusive if you transform data client-side before inference. The model doesn't need to see the original data\u2014it needs to see the mathematical patterns."
      },
      "first_principle_insight": "Separate what the model NEEDS (mathematical patterns for the task) from what it SEES (raw sensitive data). Stochastic transformation preserves the former while destroying the latter.",
      "reframe": {
        "new_atomic_unit": "Privacy-preserving inference via client-side transformation",
        "new_problem_type": "Data transformation problem, not infrastructure problem",
        "new_objective": "Enable cloud AI inference on regulated data without exposing PII",
        "architectural_changes": [
          {
            "change": "Stained Glass Transform",
            "description": "Apply non-invertible stochastic transformation to check images client-side before sending to cloud"
          },
          {
            "change": "Pattern Preservation",
            "description": "Transformation preserves visual anomaly patterns needed for fraud detection while destroying human-readable content"
          },
          {
            "change": "Intercept-Safe Inference",
            "description": "Even if data is intercepted in transit or at endpoint, it cannot be reconstructed into original image"
          },
          {
            "change": "Cloud Model Utilization",
            "description": "Enables use of powerful cloud vision models that were previously blocked by compliance"
          }
        ],
        "results": "Unlocked high-value check fraud detection capability that was previously impossible due to compliance. Cloud models can detect fraud patterns from transformed data without ever seeing actual check content."
      },
      "architectural_changes": [
        "Moved from 'block cloud AI' to 'transform then send'",
        "Client-side stochastic transformation before inference",
        "Preserved task-relevant mathematical properties while destroying PII",
        "Enabled cloud AI for regulated use cases"
      ],
      "key_lesson": "When privacy blocks AI adoption, question whether the model actually needs the raw data. Often the task-relevant signal can be preserved while the sensitive content is destroyed. This architectural pattern unlocks entire categories of previously impossible use cases.",
      "technical_details": {
        "technology": "Protopia AI Stained Glass Transform",
        "mechanism": "Non-invertible stochastic transformation applied client-side",
        "property": "Model interprets transformed vector's mathematical properties, not original content"
      },
      "tags": [
        "privacy",
        "computer-vision",
        "fintech",
        "architecture",
        "fraud-detection",
        "security",
        "production",
        "System-2"
      ],
      "sources": [
        {
          "type": "primary",
          "citation": "Securely Build Open LLMs with Protopia AI Stained Glass Transform",
          "url": "https://protopia.ai/",
          "note": "Technical overview of Stained Glass Transform technology"
        },
        {
          "type": "secondary",
          "citation": "Protopia AI Recognized in 2023 Gartner Market Guide for AI Trust, Risk and Security Management",
          "url": "https://protopia.ai/",
          "note": "Industry validation of privacy-preserving AI approach"
        }
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "teaching",
          "inspiration"
        ],
        "transferable_patterns": [
          "Privacy and capability can coexist with right architecture",
          "Transform-before-inference unlocks blocked use cases",
          "Regulatory constraints are design constraints, not blockers"
        ],
        "non_generalizable_context": [
          "Banking-specific regulatory environment",
          "Check fraud has specific visual patterns",
          "Protopia's specific technology"
        ]
      }
    },
    {
      "id": "ulmfit-transfer-learning-nlp-2018",
      "title": "ULMFiT: NLP's ImageNet Moment \u2013 Why Train From Scratch When You Can Transfer?",
      "companies_involved": [
        "fast.ai",
        "Insight Centre for Data Analytics"
      ],
      "year": 2018,
      "category": "Architectural Pivot - Transfer Learning Paradigm",
      "problem_archetype": "NLP / Text Classification",
      "initial_problem": "Build effective text classification models for tasks like sentiment analysis, topic classification, and spam detection",
      "initial_assumptions": [
        "NLP models must be trained from scratch for each new task",
        "Transfer learning works in computer vision but not in NLP",
        "Effective NLP requires massive labeled datasets and institutional resources",
        "Word embeddings (Word2Vec, GloVe) are sufficient for transfer\u2014no need to transfer full models",
        "Language model pretraining requires millions of documents to be useful"
      ],
      "why_it_fails": [
        "Training from scratch requires massive labeled datasets that most organizations don't have",
        "Each new task means starting over\u2014no knowledge accumulates across tasks",
        "Word embeddings only capture shallow word-level semantics, missing syntax and context",
        "Previous LM transfer attempts required millions of documents and still underperformed",
        "The field was stuck: successful deep learning in NLP was limited to institutions with huge data"
      ],
      "how_to_reframe": {
        "old_atomic_unit": "Task-specific model trained from scratch on labeled data",
        "new_atomic_unit": "Pretrained language model fine-tuned on downstream task with minimal labeled data",
        "insight": "NLP should work like computer vision: pretrain once on large unlabeled corpus, then fine-tune for any task. The key innovation was HOW to fine-tune without catastrophic forgetting."
      },
      "first_principle_insight": "Language understanding is general\u2014a model that learns to predict the next word necessarily learns syntax, semantics, and world knowledge. This knowledge transfers across tasks. The question isn't WHETHER to transfer, but HOW to fine-tune without destroying what was learned.",
      "reframe": {
        "new_atomic_unit": "Three-stage transfer: general LM pretraining \u2192 domain LM fine-tuning \u2192 task classifier fine-tuning",
        "new_problem_type": "Transfer learning problem, not task-specific learning problem",
        "new_objective": "Maximize knowledge transfer while preventing catastrophic forgetting during fine-tuning",
        "architectural_changes": [
          {
            "change": "Discriminative Fine-Tuning",
            "description": "Different layers encode different types of information. Fine-tune each layer with different learning rates\u2014lower layers (general features) get smaller rates, higher layers (task-specific) get larger rates."
          },
          {
            "change": "Slanted Triangular Learning Rates (STLR)",
            "description": "Learning rate schedule that first increases linearly then decays. Allows model to quickly adapt to task-specific features early, then refine carefully."
          },
          {
            "change": "Gradual Unfreezing",
            "description": "Don't fine-tune all layers at once. Start by unfreezing only the last layer, train one epoch, then unfreeze the next layer, and so on. Prevents catastrophic forgetting of general knowledge."
          },
          {
            "change": "Three-Stage Transfer Pipeline",
            "description": "Stage 1: Pretrain LM on large general corpus (Wikitext-103). Stage 2: Fine-tune LM on target domain text (unlabeled). Stage 3: Fine-tune classifier on labeled task data."
          }
        ],
        "results": "18-24% error reduction on 6 text classification benchmarks vs. state-of-the-art. With only 100 labeled examples, matched performance of training from scratch on 100x more data (10,000 examples). Established that transfer learning works in NLP."
      },
      "architectural_changes": [
        "Replaced task-specific training from scratch with pretrain-then-finetune paradigm",
        "Introduced discriminative fine-tuning (different learning rates per layer)",
        "Added gradual unfreezing to prevent catastrophic forgetting",
        "Created three-stage pipeline: general LM \u2192 domain LM \u2192 task classifier"
      ],
      "key_lesson": "The 2018 'ImageNet moment' for NLP. ULMFiT proved that transfer learning\u2014long successful in vision\u2014could work in NLP with the right fine-tuning techniques. This directly influenced BERT (2018), GPT (2018), and the entire LLM revolution. The insight: don't ask 'how do I get more labeled data?' Ask 'what general knowledge can I transfer?'",
      "historical_significance": {
        "citations": "3,882 (330 highly influential)",
        "influence": "Directly cited as precursor to BERT and GPT. Part of the 2018 'NLP ImageNet moment' alongside ELMo and GPT.",
        "paradigm_shift": "Proved NLP could achieve computer-vision-style transfer learning, enabling the modern LLM era"
      },
      "quantified_results": {
        "error_reduction": "18-24% on majority of 6 benchmarks",
        "data_efficiency": "100 labeled examples matched 10,000 examples trained from scratch",
        "benchmarks": [
          "IMDb",
          "TREC-6",
          "Yelp-bi",
          "Yelp-full",
          "DBpedia",
          "AG News"
        ]
      },
      "tags": [
        "NLP",
        "deep-learning",
        "architecture",
        "ML",
        "embeddings",
        "evaluation",
        "efficiency",
        "System-2"
      ],
      "sources": [
        {
          "url": "https://aclanthology.org/P18-1031/",
          "type": "academic_paper",
          "venue": "ACL 2018",
          "authors": "Jeremy Howard, Sebastian Ruder",
          "citation": "Universal Language Model Fine-tuning for Text Classification"
        },
        {
          "url": "https://arxiv.org/abs/1801.06146",
          "type": "preprint",
          "citation": "arXiv preprint"
        },
        {
          "url": "https://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html",
          "type": "blog",
          "citation": "Introducing state of the art text classification with universal language models"
        },
        {
          "url": "https://jalammar.github.io/illustrated-bert/",
          "type": "tutorial",
          "citation": "The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)"
        }
      ],
      "teaching_metadata": {
        "teaching_purpose": [
          "inspiration"
        ],
        "transferable_patterns": [
          "Cross-domain insights unlock breakthroughs (CV \u2192 NLP)",
          "The 'how to fine-tune' is as important as 'what to pretrain'",
          "Data efficiency gains compound with scale"
        ],
        "non_generalizable_context": [
          "2018 compute/model constraints no longer apply",
          "LSTM architecture now superseded by transformers",
          "Specific text classification benchmarks"
        ]
      }
    },
    {
      "id": "debatable-enterprise-knowledge-assistant-2025",
      "title": "The Enterprise Knowledge Assistant Dilemma: When Three Valid Architectures Compete",
      "companies_involved": [
        "Hypothetical Enterprise (Teaching Case)"
      ],
      "year": 2025,
      "category": "Debatable Case - No Clear Winner",
      "problem_archetype": "Enterprise GenAI / Knowledge Management",
      "debatable": true,
      "teaching_metadata": {
        "teaching_purpose": [
          "debatable"
        ],
        "purpose_note": "This case is deliberately designed to have NO correct answer. Different trade-off priorities lead to different valid architectures. Use for discussion, not convergence.",
        "discussion_questions": [
          "Which constraint matters most: accuracy, cost, or time-to-value?",
          "How do you weigh 'good enough now' vs 'better later'?",
          "What organizational factors should influence architecture choice?",
          "Is there a way to sequence these approaches?"
        ]
      },
      "scenario": {
        "context": "A 5,000-person professional services firm wants to help consultants find relevant past project work, methodologies, and expert contacts. They have 50,000 documents (proposals, deliverables, methodologies) and 10 years of project history.",
        "constraints": {
          "accuracy_requirement": "High - wrong answers damage client relationships",
          "budget": "$200K first year, $50K/year ongoing",
          "timeline": "Leadership wants something in 3 months",
          "data_sensitivity": "Client names must never leak; methodologies are proprietary",
          "user_base": "500 consultants, varying technical sophistication",
          "success_metric": "Reduce 'time to find relevant prior work' from 4 hours to 30 minutes"
        },
        "current_state": "Consultants use a basic keyword search that returns 200+ results. Most give up and ask colleagues instead."
      },
      "competing_architectures": {
        "option_a_rag": {
          "name": "RAG with Vector Search",
          "description": "Embed all documents, use semantic search to retrieve relevant chunks, generate summaries with citations",
          "pros": [
            "Fastest to deploy (6-8 weeks)",
            "No training required - use off-the-shelf models",
            "Citations provide auditability",
            "Easy to add new documents"
          ],
          "cons": [
            "Retrieval quality depends heavily on chunking strategy",
            "May miss relevant docs if query doesn't match embedding space",
            "Hallucination risk on synthesis",
            "Ongoing API costs scale with usage"
          ],
          "estimated_cost": "$80K setup + $3K/month API costs",
          "time_to_deploy": "8 weeks",
          "risk_profile": "Medium - known failure modes, manageable"
        },
        "option_b_fine_tuned": {
          "name": "Fine-Tuned Domain Model",
          "description": "Fine-tune an open-source LLM on company documents to internalize methodology knowledge",
          "pros": [
            "Model 'knows' company methodology deeply",
            "Lower per-query cost after training",
            "Can run on-premise for data security",
            "Better at nuanced domain questions"
          ],
          "cons": [
            "6+ months to do properly",
            "Requires ML engineering expertise",
            "Knowledge becomes stale without retraining",
            "Harder to cite sources - knowledge is in weights",
            "Risk of overfitting to training data"
          ],
          "estimated_cost": "$150K setup + $20K/year infrastructure",
          "time_to_deploy": "6 months",
          "risk_profile": "High - many ways to fail, expertise required"
        },
        "option_c_agentic": {
          "name": "Multi-Agent Research System",
          "description": "Agents that search, filter, synthesize, and verify - mimicking how a senior consultant would research",
          "pros": [
            "Most capable - can handle complex multi-step queries",
            "Self-correcting through verification agents",
            "Can integrate multiple data sources (docs, CRM, expertise DB)",
            "Most impressive demos"
          ],
          "cons": [
            "Highest latency (30-60 seconds per query)",
            "Most expensive per query ($0.50-2.00)",
            "Hardest to debug when wrong",
            "Most likely to have unexpected failure modes",
            "Requires most ongoing maintenance"
          ],
          "estimated_cost": "$120K setup + $8K/month API costs",
          "time_to_deploy": "4 months",
          "risk_profile": "Very High - cutting edge, unknown unknowns"
        }
      },
      "valid_arguments": {
        "for_rag": "Time-to-value matters. Leadership wants results in 3 months. RAG can ship in 8 weeks, learn from real usage, and iterate. The other options risk 'perfect being the enemy of good.' Citations also matter for a consulting firm - you need to show your sources.",
        "for_fine_tuned": "This is a long-term capability. Consultants will use this for years. Investing 6 months to get it right pays off over 5+ years. On-premise also matters - client confidentiality is non-negotiable. The upfront cost is worth the long-term reliability.",
        "for_agentic": "The problem IS complex. Finding relevant prior work requires judgment - not just search. A senior consultant doesn't just find documents, they synthesize across sources and identify the right expert to call. Agents can do this; RAG cannot. If you're going to invest, build the right thing."
      },
      "what_makes_this_debatable": [
        "All three options could work - this isn't about one being 'wrong'",
        "Different stakeholders would prefer different options (CTO vs CFO vs consultants)",
        "The 'right' answer depends on organizational risk tolerance",
        "A hybrid approach might be best, but adds complexity",
        "The timeline constraint conflicts with the accuracy requirement"
      ],
      "discussion_framework": {
        "step_1": "Have students argue FOR each option (not against)",
        "step_2": "Identify which constraints each option optimizes",
        "step_3": "Ask: What would have to be true for each option to be correct?",
        "step_4": "Discuss: Is there a sequencing strategy? (Start with A, evolve to C?)",
        "step_5": "Vote and defend - no consensus required"
      },
      "key_lesson": "Real architecture decisions often have multiple valid answers. The skill is not finding THE answer but understanding which trade-offs matter most for THIS context. Students who always converge on one answer are not learning judgment.",
      "tags": [
        "debatable",
        "RAG",
        "LLM",
        "agents",
        "enterprise",
        "architecture",
        "evaluation",
        "System-2"
      ],
      "sources": [
        {
          "type": "teaching_case",
          "citation": "Synthetic teaching case - designed for debate"
        }
      ],
      "comments": [
        {
          "text": "this one is all blank, need to fix",
          "timestamp": "2026-01-11T15:31:19.421849",
          "author": "User"
        }
      ]
    },
    {
      "id": "openai-grokking-2022",
      "title": "Grokking: The Accidental Discovery of Delayed Generalization",
      "category": "Research Insight - Training Dynamics",
      "year": "2022",
      "companies_involved": [
        "OpenAI"
      ],
      "initial_problem": "Understanding when and how neural networks generalize vs. memorize on small algorithmic datasets like modular arithmetic",
      "initial_assumptions": [
        "Once a model overfits (perfect training accuracy, poor test accuracy), further training is wasteful",
        "Generalization happens early or not at all",
        "If test accuracy does not improve for thousands of steps, the model will not learn the pattern",
        "Training should be stopped when validation loss stops improving (early stopping)"
      ],
      "why_it_fails": [
        "Conventional wisdom says to stop training once overfitting occurs",
        "Standard practice is early stopping based on validation metrics",
        "No one expects test accuracy to suddenly jump after prolonged overfitting"
      ],
      "first_principle_insight": "The discovery happened by accident: a researcher went on vacation and forgot to stop a training run. The model had overfit quickly (perfect training accuracy at ~1,000 steps) with no improvement on test data. But when they returned, the 'forgotten' run showed test accuracy had suddenly jumped to near-perfect around 100,000-1,000,000 steps. The model had a delayed 'aha moment' - it stopped memorizing and discovered the underlying mathematical structure (representing numbers on a circle).",
      "reframe": {
        "new_understanding": "Generalization can happen dramatically late, well after apparent overfitting",
        "new_problem_type": "Training dynamics research - understanding phase transitions in learning",
        "key_findings": [
          "Training accuracy reached ~100% at ~1,000 optimization steps (memorization phase)",
          "Test accuracy remained at random chance for thousands more steps",
          "Around 100,000-1,000,000 steps, test accuracy suddenly jumped to ~100% (grokking)",
          "The network discovered that representing numbers on a circle enabled perfect generalization",
          "Weight decay was critical - it provides pressure toward simpler solutions that generalize"
        ],
        "implications": [
          "Early stopping may prevent models from finding generalizable solutions",
          "Overfitting and generalization can coexist during a long transition period",
          "Simple algorithmic tasks can reveal deep insights about learning dynamics",
          "The training curve is not always monotonic - phase transitions exist"
        ],
        "quote": "And then one day, we got lucky. And by lucky, I mean forgetful. - Alethea Power"
      },
      "teaching_value": {
        "lesson": "Sometimes the best insights come from accidents and patience. Standard practice (early stopping) would have missed this phenomenon entirely.",
        "first_principles_angle": "Question assumptions about training dynamics. 'Stop when validation plateaus' is a heuristic, not a law.",
        "meta_lesson": "In research, letting things run longer than planned - or 'failing' to follow standard practice - can reveal unexpected phenomena"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. ICLR 2022.",
          "url": "https://arxiv.org/abs/2201.02177"
        },
        {
          "type": "secondary",
          "citation": "Quanta Magazine: How Do Machines 'Grok' Data?",
          "url": "https://www.quantamagazine.org/how-do-machines-grok-data-20240412/"
        }
      ],
      "tags": [
        "training-dynamics",
        "generalization",
        "accidental-discovery",
        "modular-arithmetic",
        "research-insight",
        "phase-transition"
      ]
    },
    {
      "id": "agent-evaluation-reframe-2024",
      "title": "Agent Evaluation: From 'Right Answer' to 'Right Path + Right Answer'",
      "category": "Architectural Pivot - Agent Evaluation",
      "year": "2024",
      "companies_involved": [
        "LangChain",
        "Anthropic",
        "METR"
      ],
      "initial_problem": "Teams build AI agents but evaluate them like they evaluate LLMs - checking if the final answer is correct, using static benchmarks, and testing in controlled environments",
      "initial_assumptions": [
        "If the agent gets the right answer, it's working correctly",
        "Standard LLM benchmarks (MMLU, etc.) indicate agent capability",
        "Playground testing with controlled prompts validates production readiness",
        "More capable base models = more capable agents",
        "Evaluation is a pre-deployment gate, not ongoing monitoring"
      ],
      "why_it_fails": [
        "Right answer via wrong path is a time bomb - the agent got lucky but will fail on similar tasks",
        "Multi-step brittleness: 'All models fail over longer trajectories, even for simple tasks' (LangChain)",
        "Pre-trained bias interference: Models ignore tool outputs and use memorized answers instead",
        "Inverse scaling: GPT-4 performed WORSE than GPT-3.5 on tasks requiring overriding trained knowledge",
        "Playground passed, production failed: controlled test prompts \u2260 real user inputs",
        "39.8% use offline eval, only 32.5% use online eval - most never see real failure modes"
      ],
      "first_principle_insight": "Agent evaluation requires measuring the TRAJECTORY, not just the destination. A correct final answer achieved through incorrect intermediate steps indicates a fragile system. You need to evaluate: (1) Did it call the right tools? (2) In the right order? (3) With the right arguments? (4) And interpret results correctly? Only then: (5) Did it get the right answer?",
      "reframe": {
        "old_atomic_unit": "Final answer correctness (LLM-as-judge or ground truth comparison)",
        "new_atomic_unit": "Trajectory correctness + environment state + step efficiency + final answer",
        "insight": "Agents are programs, not oracles. Evaluate them like you'd evaluate code: check the execution path, not just the output."
      },
      "how_to_reframe": {
        "old_atomic_unit": "Final answer correctness (LLM-as-judge or ground truth comparison)",
        "new_atomic_unit": "Trajectory correctness + environment state + step efficiency + final answer"
      },
      "architectural_changes": [
        "Correctness: Compare final answer against ground truth (necessary but not sufficient)",
        "Environment State: Verify the agent actually changed the world correctly (not just claimed to)",
        "Intermediate Step Correctness: Check function call sequences against optimal/valid paths",
        "Step Efficiency Ratio: Measure actual steps vs. expected steps (detect loops and wandering)",
        "Tool Selection Accuracy: Did it pick the right tool for each subtask?",
        "Stopping Behavior: Did it halt after completing the task (not continue or loop)?",
        "Graceful Failure: When it can't complete, does it fail safely or hallucinate completion?"
      ],
      "common_failure_modes": [
        {
          "mode": "Pre-trained bias override",
          "description": "Model uses memorized answer instead of tool output. GPT-4 was observed answering math questions with trained knowledge rather than using the calculator tool's result.",
          "detection": "Compare tool output to model's answer - if different, the model ignored the tool"
        },
        {
          "mode": "Multi-step brittleness",
          "description": "Success rate drops exponentially with trajectory length. A 95% per-step success rate yields only 60% success over 10 steps.",
          "detection": "Track success rate by trajectory length - look for exponential decay"
        },
        {
          "mode": "Tool composition failure",
          "description": "Models that handle single tool calls well fail when composing multiple sequential calls.",
          "detection": "Separate benchmarks for single-tool vs. multi-tool tasks"
        },
        {
          "mode": "Stopping failure",
          "description": "Agent continues taking actions after task is complete, or fails to recognize completion.",
          "detection": "Count post-completion actions; measure time-to-halt"
        }
      ],
      "key_benchmarks": [
        {
          "name": "AgentBench",
          "measures": "Reasoning and decision-making across 8 environments (OS, database, web, games)",
          "insight": "Real environments, not synthetic tasks"
        },
        {
          "name": "ToolLLM",
          "measures": "Tool mastery across 16,464 APIs - retrieval, reasoning, invocation, and knowing when NOT to use tools",
          "insight": "Abstention is a skill - knowing when not to act"
        },
        {
          "name": "BFCL (Berkeley Function Calling Leaderboard)",
          "measures": "Function-calling accuracy across 2000 Q&A pairs",
          "insight": "Measures the atomic skill underlying all tool use"
        },
        {
          "name": "ToolEmu",
          "measures": "Safety - identifies risky agent behaviors with 36 high-stakes tools",
          "insight": "Can it fail safely? Does it recognize dangerous actions?"
        }
      ],
      "production_guidance": {
        "from_anthropic": "Start simple. Only add agentic complexity when it demonstrably improves outcomes. For many applications, optimizing single LLM calls with retrieval is enough.",
        "from_langchain": "Offline evaluation (39.8%) is more common than online (32.5%), but production behavior differs. Use tracing and observability as primary tools.",
        "key_principle": "Measure performance and iterate. The question isn't 'does it work in the playground' but 'does it work for real users on real tasks?'"
      },
      "teaching_value": {
        "lesson": "Developers who evaluate agents like LLMs will ship fragile systems. Agent evaluation is program evaluation: check the execution path, not just the output.",
        "first_principles_angle": "An agent is a program that writes itself at runtime. You wouldn't ship code without testing the logic, just because the output looked right once.",
        "anti_pattern": "Testing with controlled prompts, getting good results, and assuming production readiness. The gap between playground and production is largest for agents.",
        "what_practitioners_miss": "Most reinvent trajectory evaluation, step counting, and tool-call validation because they start with LLM eval frameworks that only measure final output."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LangChain: Benchmarking Agent Tool Use",
          "url": "https://blog.langchain.com/benchmarking-agent-tool-use/"
        },
        {
          "type": "primary",
          "citation": "LangChain: State of AI Agents Report 2024",
          "url": "https://www.langchain.com/stateofaiagents"
        },
        {
          "type": "primary",
          "citation": "Anthropic: Building Effective Agents",
          "url": "https://www.anthropic.com/research/building-effective-agents"
        },
        {
          "type": "secondary",
          "citation": "Evidently AI: 10 AI Agent Benchmarks",
          "url": "https://www.evidentlyai.com/blog/ai-agent-benchmarks"
        }
      ],
      "tags": [
        "agent-evaluation",
        "tool-use",
        "trajectory-evaluation",
        "multi-step",
        "production-gap",
        "benchmarks",
        "System-2"
      ],
      "comments": [
        {
          "text": "Need atomic unit for reframing",
          "timestamp": "2026-01-11T15:29:37.312265",
          "author": "User"
        }
      ]
    },
    {
      "id": "cimo-llm-judge-calibration-2024",
      "title": "LLM Judge Calibration: From 'Make the Judge Better' to 'Learn How Signals Predict Outcomes'",
      "category": "Architectural Pivot - Evaluation Reframe",
      "year": "2024",
      "companies_involved": [
        "CIMO Labs"
      ],
      "initial_problem": "LLM-as-judge evaluations produce scores that don't correlate with actual business outcomes. High scores on cheap metrics can predict LOW value on what actually matters.",
      "initial_assumptions": [
        "LLM judges can directly assess output quality",
        "Higher judge scores mean better real-world performance",
        "If the judge is wrong, fine-tune it to be more accurate",
        "More sophisticated prompting will improve judge reliability",
        "Judge scores can be used directly for model selection and optimization"
      ],
      "why_it_fails": [
        "Preference inversion: behaviors scoring highest on LLM-judge metrics predicted LOWEST user value",
        "Claude Code's 'You're absolutely right!' responses scored excellently on politeness but destroyed developer productivity",
        "Fine-tuning judges is expensive and doesn't solve the fundamental calibration problem",
        "LLM judges optimize for surface features (verbosity, confidence) not actual outcomes",
        "No ground truth connection - cheap metrics float free from real-world results"
      ],
      "first_principle_insight": "The problem isn't that the judge is wrong - it's that you're using judge scores as if they were outcomes. Instead of making the judge 'better,' learn how cheap signals (S) statistically predict real outcomes (Y). Use small calibration slices (~500 oracle labels) to map abundant cheap metrics to actual results.",
      "reframe": {
        "old_atomic_unit": "LLM judge score as direct quality measure",
        "new_atomic_unit": "Calibrated prediction: P(good outcome | cheap signal, covariates)",
        "insight": "Don't try to perfect the judge. Learn how imperfect signals predict real outcomes. Calibration > Accuracy."
      },
      "how_to_reframe": {
        "old_atomic_unit": "LLM judge score as direct quality measure",
        "new_atomic_unit": "Calibrated prediction: P(good outcome | cheap signal, covariates)"
      },
      "architectural_changes": [
        "Collect small 'calibration slices' (~500 examples) with real outcome labels",
        "Generate abundant cheap metrics (LLM-judge scores) on same examples",
        "Use statistical calibration (isotonic regression, Platt scaling) to map signals to outcomes",
        "Add covariates like response-length to correct for known biases (verbosity preference)",
        "Two-stage approach: splines for global fit, isotonic regression for monotonicity",
        "Use residuals (Y - \u0176) as diagnostic tool to improve cheap metrics iteratively"
      ],
      "results": {
        "ranking_accuracy": "+12% improvement over naive LLM-judge approaches",
        "error_reduction": "73% reduction in evaluation errors",
        "sample_efficiency": "100-200\u00d7 effective sample size improvement",
        "pairwise_accuracy": "99% at 14\u00d7 lower cost than direct evaluation",
        "covariate_impact": "Adding response-length improved top-1 accuracy from 84.1% to 89.4%"
      },
      "failure_classes_identified": [
        {
          "class": "Wrong calibration",
          "description": "Never measuring the actual outcome that matters - optimizing proxy instead of goal"
        },
        {
          "class": "Wrong population",
          "description": "Calibrating on unrepresentative samples that don't match production distribution"
        },
        {
          "class": "Temporal decay",
          "description": "Relationships between signals and outcomes shift over time, requiring recalibration"
        }
      ],
      "teaching_value": {
        "lesson": "When your evaluation metric is broken, the instinct is to make the metric better. The first-principles move is to treat the metric as a noisy signal and learn how it predicts what you actually care about.",
        "first_principles_angle": "Evaluation is a prediction problem. You're predicting 'will users value this?' from cheap signals. Apply ML to the evaluation itself.",
        "anti_pattern": "Fine-tuning LLM judges on curated examples, hoping they'll generalize to production. This is expensive and doesn't address the signal-outcome gap.",
        "connection_to_loop": "This is Loop Step 5 (Signals) applied to evaluation itself. Your eval metrics ARE signals - they can be wrong, and you need to calibrate them against reality."
      },
      "key_insight_quote": "What scores high on quick metrics can predict low value on what actually matters.",
      "related_cases": [
        "coda-eval-gap-2024",
        "agent-evaluation-reframe-2024"
      ],
      "tags": [
        "evaluation",
        "calibration",
        "LLM-as-judge",
        "metrics",
        "proxy-trap",
        "statistical-methods",
        "System-2"
      ],
      "sources": [
        {
          "type": "primary",
          "citation": "CIMO Labs: Are Your Metrics Lying to You? A Deep Dive into Causal Judge Evaluation",
          "url": "https://www.cimolabs.com/blog/metrics-lying"
        }
      ],
      "comments": [
        {
          "text": "This one doesn't have any old or new atomic units, what shojuld we do with it?",
          "timestamp": "2026-01-11T15:28:43.829763",
          "author": "User"
        }
      ]
    },
    {
      "id": "frosty-text-to-sql-iterative-development",
      "title": "Frosty: Text-to-SQL Through Iterative Reframing",
      "category": "Architectural Pivot - GenAI System Design",
      "companies_involved": [
        "Frosty (Hypothetical Teaching Example)",
        "Databricks"
      ],
      "initial_problem": "Build a natural language to SQL interface that allows business users to query databases using plain English questions",
      "initial_assumptions": [
        "A single text-to-SQL model will be accurate enough for production use",
        "Public benchmarks reflect real-world enterprise query complexity",
        "Model improvements (bigger models, better prompts) will solve accuracy problems",
        "The problem is primarily about the model, not the data or system around it"
      ],
      "why_it_fails": [
        "Single model achieved only 33% accuracy on enterprise-specific benchmark",
        "Public datasets are far too easy - don't reflect messy enterprise schemas",
        "User queries are often ambiguous, irrelevant, or poorly specified",
        "Enterprise data structures are messy and don't match business terminology",
        "Model tuning hits a ceiling - fundamental data quality issues remain"
      ],
      "first_principle_insight": "GenAI applications are systems, not models. 'It's not about a data scientist sitting on an island messing around with hyperparameters.' The breakthrough came from recognizing that the underlying data\u2014schemas, databases, tables\u2014were messy and unorganized. No amount of model tuning could fix a data problem.",
      "how_to_reframe": {
        "old_atomic_unit": "The model - treating text-to-SQL as a single model optimization problem where better prompts, larger models, or hyperparameter tuning would solve accuracy issues",
        "new_atomic_unit": "The system pipeline - treating text-to-SQL as a multi-stage system where each component (screening, routing, ensemble, semantic layer) addresses a specific failure mode"
      },
      "reframe": {
        "new_atomic_unit": "The system pipeline (screening \u2192 routing \u2192 ensemble \u2192 semantic mapping \u2192 response), not the individual model",
        "new_problem_type": "System design with multiple specialized components, not single-model optimization",
        "new_objective": "Build a robust pipeline where each stage addresses a specific failure mode, culminating in human expertise encoded as a semantic layer",
        "architectural_changes": [
          {
            "change": "Model-Based Evaluation (LLM-as-Judge)",
            "description": "Used LLM to evaluate SQL correctness with partial credit for near-matches. 80% correlation with human graders. Enabled rapid iteration without human bottleneck."
          },
          {
            "change": "Enterprise-Specific Benchmark",
            "description": "Built custom evaluation set reflecting actual enterprise query complexity. Public benchmarks were misleadingly easy and non-representative."
          },
          {
            "change": "Ensemble of Text-to-SQL Models",
            "description": "Combined multiple diverse models (different architectures, training data) to improve predictions. Each model had different strengths/weaknesses. Boosted from 33% to ~45%."
          },
          {
            "change": "Reflection / Chain of Thought",
            "description": "When models made mistakes, forced them to reflect step-by-step. This generates more reasoning tokens and improves final answers. Boosted to ~55%."
          },
          {
            "change": "Input Screening Layer",
            "description": "Added classification layer to detect irrelevant or ambiguous queries before processing. Gave users feedback to improve query quality. Boosted to ~65%."
          },
          {
            "change": "Feature Extraction by Query Type",
            "description": "Recognized different query types (time series, ranking, aggregation) need different features. Routed queries to specialized handlers. Boosted to ~75%."
          },
          {
            "change": "Semantic Layer (Human Expertise)",
            "description": "The final pivot: brought in domain experts to create a semantic mapping between business terminology and database structure. Bridged the gap between how users think and how data is stored. Achieved 85-90% target accuracy."
          }
        ],
        "results": "Improved from 33% baseline to 85-90% production-ready accuracy through systematic iteration. Each technique addressed a specific failure mode. Final breakthrough required human expertise (semantic layer), not more model tuning."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Practical Lessons with LLMs - YouTube Talk (September 2024)",
          "url": "https://youtu.be/OyY4uxUShys",
          "note": "Teaching talk demonstrating iterative GenAI application development"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "All 5 steps - complete worked example of The Loop in action",
        "common_mistake_avoided": "Treating GenAI as a model problem when it's a system problem. Tunnel vision on model hyperparameters while ignoring data quality and pipeline design.",
        "when_to_use": "When building any GenAI application that's stuck at unsatisfactory accuracy. Shows the systematic approach: evaluate properly, try model techniques, improve data quality, then bring in human expertise."
      },
      "key_lesson": "When model improvements plateau, look at the system. The Frosty team squeezed everything from models (ensembles, reflection) and data (screening, routing) before realizing the fundamental problem was messy enterprise data that required human expertise to map.",
      "development_progression": [
        {
          "stage": "Baseline",
          "technique": "Single text-to-SQL model",
          "accuracy": "33%"
        },
        {
          "stage": "Ensemble",
          "technique": "Multiple diverse models combined",
          "accuracy": "~45%"
        },
        {
          "stage": "Reflection",
          "technique": "Chain of thought on errors",
          "accuracy": "~55%"
        },
        {
          "stage": "Screening",
          "technique": "Filter ambiguous/irrelevant queries",
          "accuracy": "~65%"
        },
        {
          "stage": "Routing",
          "technique": "Feature extraction by query type",
          "accuracy": "~75%"
        },
        {
          "stage": "Semantic Layer",
          "technique": "Human expertise mapping",
          "accuracy": "85-90%"
        }
      ],
      "tags": [
        "text-to-sql",
        "genai",
        "system-design",
        "evaluation",
        "llm-as-judge",
        "ensemble",
        "semantic-layer",
        "iterative-development"
      ]
    },
    {
      "id": "legal-research-rag-limitations",
      "title": "Dewey, Cheetham & Howe: Why RAG Fails for Legal Research",
      "category": "Cautionary Tale - RAG Limitations",
      "companies_involved": [
        "Dewey, Cheetham & Howe (Hypothetical Teaching Example)",
        "LexisNexis",
        "Stanford HAI"
      ],
      "initial_problem": "Reduce time lawyers spend doing legal research by using LLMs with retrieval augmented generation",
      "initial_assumptions": [
        "RAG solves the hallucination problem - grounding in real documents prevents fabrication",
        "Legal research is primarily about retrieving relevant citations",
        "If the citation exists and is real, the answer must be correct",
        "GPT-4 passed the bar exam, so it understands legal concepts"
      ],
      "why_it_fails": [
        "Confused similar-sounding but distinct legal concepts (Equity Cleanup Doctrine vs. Clean Hands Doctrine)",
        "Failed to recognize fictional references that lawyers know as inside jokes (Luther Wilgarden - a fictional judge in law reviews)",
        "Cannot navigate complex jurisdiction hierarchies (federal vs. state vs. regulatory; different court authority levels)",
        "Doesn't understand temporal validity (overturned cases, when precedent applies)",
        "Retrieved 'unpublished' or 'not notable' cases when asked for notable opinions",
        "Stanford study showed high hallucination rates even with RAG on real legal questions"
      ],
      "first_principle_insight": "Legal research is not retrieval - it's synthesis. The atomic unit isn't 'find a citation' but 'construct a legal argument weighing multiple contested authorities across jurisdictions and time.' RAG optimizes for retrieval accuracy, but the actual task requires expert judgment to weave disparate sources into coherent legal reasoning.",
      "how_to_reframe": {
        "old_atomic_unit": "Retrieved citation - treating legal research as finding and returning relevant case law, assuming that grounded citations equal correct answers",
        "new_atomic_unit": "Legal argument - treating legal research as constructing reasoned arguments that weigh contested authorities across jurisdictions, time periods, and court hierarchies"
      },
      "reframe": {
        "new_atomic_unit": "Legal argument construction requiring synthesis of contested authorities, not document retrieval",
        "new_problem_type": "Expert reasoning and synthesis, not information retrieval",
        "new_objective": "Augment (not replace) legal researchers who can evaluate applicability, jurisdiction, authority weight, and temporal validity",
        "architectural_changes": [
          {
            "change": "Human-in-the-Loop Requirement",
            "description": "Legal research tools should generate candidate documents for expert review, not final answers. The 'stack of documents' is the output, not the conclusion."
          },
          {
            "change": "Domain-Specific Validation",
            "description": "Must validate not just that citations exist, but that they're being applied correctly to the legal question at hand."
          },
          {
            "change": "Authority and Jurisdiction Awareness",
            "description": "System must understand court hierarchies, binding vs. persuasive authority, and jurisdictional applicability."
          }
        ],
        "results": "Recognition that RAG alone cannot solve legal research. Lawyers must remain in the loop to evaluate retrieved materials. Tools can accelerate document gathering but cannot replace legal judgment."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Practical Lessons with LLMs - YouTube Talk (September 2024)",
          "url": "https://youtu.be/OyY4uxUShys",
          "note": "Teaching examples of RAG limitations in legal domain"
        },
        {
          "type": "secondary",
          "citation": "Stanford HAI study on LLM hallucination rates in legal research",
          "note": "Referenced in talk - showed high hallucination rates on real legal questions even with RAG"
        },
        {
          "type": "secondary",
          "citation": "Mata v. Avianca (2023) - Lawyer sanctioned for citing ChatGPT-fabricated cases",
          "note": "Real-world case demonstrating dangers of LLM hallucination in legal contexts"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Reveals that the atomic unit assumption was wrong. 'Retrieve citations' \u2260 'do legal research'",
        "common_mistake_avoided": "Assuming RAG solves hallucination. RAG ensures documents are real; it doesn't ensure they're being applied correctly.",
        "when_to_use": "When evaluating whether RAG is sufficient for a domain. Legal research is an example of a field where retrieval is necessary but not sufficient - expert synthesis is required."
      },
      "key_lesson": "RAG excels when information is uncontested and retrieval is the bottleneck. It struggles when the domain requires weighing contested sources, understanding context-dependent applicability, and synthesizing arguments - tasks that require domain expertise, not just document access.",
      "related_concepts": {
        "air_canada_case": "Court held companies liable for chatbot statements - chatbots are treated as agents/employees",
        "bar_exam_fallacy": "Passing multiple-choice bar exam \u2260 understanding law. Tests pattern recognition, not legal reasoning."
      },
      "tags": [
        "rag",
        "legal",
        "hallucination",
        "cautionary-tale",
        "domain-expertise",
        "synthesis-vs-retrieval"
      ]
    },
    {
      "id": "block-world-approximate-reasoning",
      "title": "Block World: The Limits of LLM Planning and Approximate Reasoning",
      "category": "Cautionary Tale - Reasoning Limitations",
      "companies_involved": [
        "OpenAI",
        "Academic Research Community"
      ],
      "initial_problem": "Assess whether LLMs can genuinely plan and reason through multi-step problems",
      "initial_assumptions": [
        "GPT-4's strong performance on benchmarks indicates reasoning ability",
        "Models that 'feel like' they're reasoning are actually reasoning",
        "Chain of thought and reflection indicate genuine step-by-step thinking",
        "Better models will naturally become better reasoners"
      ],
      "why_it_fails": [
        "GPT-4 achieved only 34% on Block World (humans: ~90%)",
        "Critical reveal: Renaming the blocks caused performance to collapse - model was pattern-matching training data, not reasoning",
        "Shakespeare test: 'A rose by any other name would smell as sweet' - but not for LLMs. Names shouldn't matter for logical planning, but they do.",
        "Mystery World (renamed blocks) showed the 34% was hallucination/pattern-matching, not understanding"
      ],
      "first_principle_insight": "LLMs exhibit 'approximate reasoning' - they've seen so much material that they can produce outputs that feel like reasoning without actually executing logical steps. This creates a new class of hard-to-detect failures where experts are needed to verify reasoning, because intuition alone can't distinguish real reasoning from sophisticated pattern matching.",
      "how_to_reframe": {
        "old_atomic_unit": "Model output - treating plausible-sounding reasoning chains as evidence of genuine logical planning ability",
        "new_atomic_unit": "Verified reasoning chain - treating each logical step as requiring validation, especially through robustness tests (renamed variables, rephrased problems)"
      },
      "reframe": {
        "new_atomic_unit": "Verified reasoning chains, not plausible-sounding outputs",
        "new_problem_type": "Approximate reasoning detection - identifying when models are pattern-matching vs. actually reasoning",
        "new_objective": "Build systems that can verify reasoning steps, not just evaluate final answers",
        "architectural_changes": [
          {
            "change": "Extended Inference Time (O1 approach)",
            "description": "OpenAI's O1 model spends more time on inference with reinforcement-learned chain of thought. Improved to ~37% on randomized Mystery World - progress, but far from human-level."
          },
          {
            "change": "Robustness Testing",
            "description": "Test models on semantically equivalent but superficially different problems (renamed variables, rephrased questions) to detect pattern-matching vs. reasoning."
          },
          {
            "change": "Expert Verification Required",
            "description": "Domain experts must verify reasoning in high-stakes contexts. 'Don't just let your intuition look at a result - you're going to have to bring in experts because that's just how good these models are at seeming right.'"
          }
        ],
        "results": "O1 shows improvement (37% on randomized Mystery World vs. GPT-4's collapse on renamed blocks), but still far from human-level planning. Demonstrated that progress requires testing on novel formulations, not just standard benchmarks."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Practical Lessons with LLMs - YouTube Talk (September 2024)",
          "url": "https://youtu.be/OyY4uxUShys",
          "note": "Discussion of Block World benchmark and O1 performance"
        },
        {
          "type": "secondary",
          "citation": "Block World / Mystery World planning benchmarks from academic research",
          "note": "Standard benchmarks for assessing multi-step planning ability"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - How do we know if we're wrong? The rename test is a signal that reveals whether reasoning is real or pattern-matched.",
        "common_mistake_avoided": "Trusting outputs that 'feel like' reasoning. Approximate reasoning is a new failure mode that requires expert verification.",
        "when_to_use": "When evaluating whether an LLM can handle planning/reasoning tasks. Test with superficial variations (renamed variables, rephrased problems) to detect pattern-matching."
      },
      "key_lesson": "Just like RAG created 'grounded hallucinations' (real citations, wrong application), advanced reasoning models create 'approximate reasoning' (plausible chains, wrong logic). Both require domain experts to catch failures that pass intuition checks.",
      "benchmark_results": {
        "gpt4_block_world": "34%",
        "gpt4_mystery_world": "Collapsed (near 0%)",
        "o1_mystery_world_randomized": "~37%",
        "human_baseline": "~90%"
      },
      "warning": "As models get better at approximate reasoning, they will become harder to distinguish from genuine reasoning. This is why experts become more important, not less, as models improve.",
      "tags": [
        "reasoning",
        "planning",
        "benchmarks",
        "o1",
        "chain-of-thought",
        "cautionary-tale",
        "approximate-reasoning",
        "pattern-matching"
      ]
    },
    {
      "id": "vellum-activation-funnel-reframe",
      "title": "Vellum: From Activation Rate to Activation Funnel",
      "category": "Business Outcome - Metric Reframe",
      "companies_involved": [
        "Vellum"
      ],
      "year": "2025",
      "initial_problem": "An AI agent platform (Agent Builder) has low user activation rates. The team knows activation is ~20% but cannot identify why users fail or where to intervene.",
      "initial_assumptions": [
        "A single 'activation rate' metric is sufficient to track and improve user success",
        "Knowing the activation percentage tells you what to fix",
        "The atomic unit is 'user' (activated: yes/no)"
      ],
      "why_it_fails": [
        "Single percentage provides no diagnosis \u2014 you know THAT users fail but not WHERE",
        "Cannot prioritize interventions without knowing which stage has the biggest drop-off",
        "Team is guessing at solutions rather than targeting specific failure points",
        "Data infrastructure wasn't set up to track the user journey end-to-end"
      ],
      "first_principle_insight": "If you want to improve activation, you need data that explains failure, not just reports outcomes. A metric you can't decompose is a metric you can't improve.",
      "how_to_reframe": {
        "old_atomic_unit": "User (activated: yes/no) \u2014 a single binary outcome",
        "new_atomic_unit": "User-at-stage \u2014 tracking position in a multi-step journey (signup \u2192 engage \u2192 generate \u2192 run \u2192 success)"
      },
      "reframe": {
        "new_atomic_unit": "User-at-stage in a 5-step activation funnel",
        "new_problem_type": "Funnel diagnostics with stage-by-stage conversion tracking",
        "new_objective": "Identify WHERE users drop off so interventions can be targeted to specific failure points",
        "architectural_changes": [
          {
            "change": "Unified Data Warehouse",
            "description": "Consolidated all product and user data into a single warehouse using Fivetran and custom ETL, enabling end-to-end user journey analysis."
          },
          {
            "change": "Stage-Based Funnel Definition",
            "description": "Decomposed activation into 5 measurable stages derived directly from the activation definition: (1) Signup, (2) Engage with Agent Builder, (3) Workflow generated, (4) Workflow run, (5) Successful run."
          },
          {
            "change": "Custom Dashboard with DRIs",
            "description": "Built lightweight custom dashboard (using Lovable) directly on the warehouse. Each funnel stage has a Directly Responsible Individual (DRI) accountable for that conversion rate."
          },
          {
            "change": "Drop-off Visibility",
            "description": "Can now see exact conversion at each stage (e.g., 89% engage, but only 52% generate a workflow) \u2014 identifying the specific bottleneck rather than guessing."
          }
        ],
        "results": "Transformed a single 20.9% activation metric into an actionable funnel showing stage-by-stage conversion (89% \u2192 52% \u2192 61% \u2192 74%). Identified 'workflow generation' as the primary bottleneck (48% drop-off), making the problem specific and actionable."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Noa Flaherty (Vellum) LinkedIn Post on Activation Metrics",
          "url": "https://www.linkedin.com/feed/update/urn:li:activity:7412504135117402112/",
          "note": "Original post describing the activation funnel reframe and data infrastructure approach"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) \u2014 'The metric IS the frame.' Changing how you measure activation changed what you could learn and do about it.",
        "common_mistake_avoided": "Treating a single outcome metric as actionable. Without decomposition, you're measuring results but not diagnosing causes.",
        "when_to_use": "When you have a vague outcome metric (activation, engagement, success rate) that isn't driving improvement. Decompose into stages to find the bottleneck."
      },
      "genai_relevance": "Directly applicable to GenAI product teams measuring agent/workflow success. The same principle applies to RAG evaluation (decompose into retrieval vs. generation errors) and agent evaluation (decompose into planning vs. execution vs. tool use errors).",
      "key_lesson": "A metric that reports outcomes but can't explain failure is not actionable. The atomic unit wasn't 'user' \u2014 it was 'user at each stage of the journey.'",
      "tags": [
        "activation",
        "metrics",
        "funnel",
        "outcome-engineering",
        "genai-product",
        "data-infrastructure",
        "diagnostic-metrics"
      ]
    },
    {
      "id": "sony-transistor-radio-reframe",
      "title": "Sony: The Transistor Radio That Disrupted RCA",
      "category": "Architectural Pivot - Atomic Unit Reframe",
      "companies_involved": [
        "Sony",
        "RCA"
      ],
      "year": "1955",
      "initial_problem": "In the 1950s, RCA dominated the radio market with high-fidelity vacuum tube radios designed for family living rooms. Bell Labs invented the transistor, but incumbent players dismissed it.",
      "initial_assumptions": [
        "The atomic unit of a radio is 'sound quality' \u2014 fidelity is the metric that matters",
        "The customer is the family gathering in the living room",
        "Inferior sound quality means inferior product",
        "Market leaders should optimize existing technology (vacuum tubes) rather than adopt inferior alternatives"
      ],
      "why_it_fails": [
        "RCA optimized for the wrong customer segment \u2014 families who already owned radios",
        "Teenagers didn't care about sound fidelity; they cared about independence from parents",
        "Portability created a new use case that didn't exist before (beach, car, bedroom)",
        "By dismissing transistors as 'inferior,' RCA ceded an entirely new market"
      ],
      "first_principle_insight": "Disruptive innovations don't need superior specs \u2014 they need to solve a real problem for a new customer segment. The atomic unit wasn't 'sound quality' \u2014 it was 'portable music experience for teenagers.' RCA was solving the wrong problem for the wrong customer.",
      "how_to_reframe": {
        "old_atomic_unit": "Sound fidelity for family living rooms",
        "new_atomic_unit": "Portable music independence for teenagers"
      },
      "reframe": {
        "new_atomic_unit": "Portable listening experience (location + independence > fidelity)",
        "new_problem_type": "New market creation rather than incumbent optimization",
        "new_objective": "Enable teenagers to listen to music away from parents, not compete on audio specs",
        "architectural_changes": [
          {
            "change": "Transistor Technology Adoption",
            "description": "Sony licensed transistor technology from Bell Labs without initially knowing what to build. They explored the technology's unique properties (small, battery-powered, portable) rather than trying to match vacuum tube performance."
          },
          {
            "change": "Customer Segment Shift",
            "description": "Targeted teenagers who wanted independence from family living rooms, not audiophiles who wanted better sound. This was a completely different job-to-be-done."
          },
          {
            "change": "Form Factor as Feature",
            "description": "Made portability the primary feature, accepting inferior sound as an acceptable trade-off for the target customer."
          },
          {
            "change": "Brand Independence",
            "description": "Morita rejected Bulova's offer to rebrand Sony radios, insisting on building the Sony brand even when it meant harder initial sales. Long-term framing over short-term optimization."
          }
        ],
        "results": "Sony's TR-52 transistor radio (1955) launched portable consumer electronics. By reframing from 'sound quality' to 'portable independence,' Sony created a market that incumbents had dismissed. This became the template for Walkman (1979), and later inspired Apple's approach with iPod. Steve Jobs studied Sony obsessively \u2014 former Apple CEO John Sculley noted: 'Steve's point of reference was Sony at the time. He really wanted to be Sony.'"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Howard Yu, 'Before Apple Disrupted Everything, Sony Did It First. Steve Jobs Never Forgot.' Inc. Magazine (2024)",
          "url": "https://www.inc.com/howard-yu/before-apple-disrupted-everything-sony-did-it-first-steve-jobs-never-forgot/91280890",
          "note": "Detailed analysis of Sony's innovation culture and its influence on Apple"
        },
        {
          "type": "secondary",
          "citation": "Howard Yu Substack - Extended version of Inc article",
          "url": "https://howardyu.substack.com/p/before-apple-disrupted-everything",
          "note": "Full article with additional details on engineer rotation and five principles"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) \u2014 Finding the right atomic unit. RCA's atomic unit was 'sound quality for families.' Sony's atomic unit was 'portable independence for teenagers.' Same technology, completely different framing.",
        "common_mistake_avoided": "Optimizing the incumbent metric (sound fidelity) instead of questioning whether it's the right metric for a new customer segment.",
        "when_to_use": "When you're competing against an established solution and tempted to beat them on their own metrics. Ask: Is there a customer segment that would accept worse performance on the incumbent metric in exchange for something else entirely?"
      },
      "pattern_connection": "This is the same pattern as Netflix (prediction accuracy \u2192 ranking), Uber (demand prediction \u2192 market clearing), and GenAI (generation quality \u2192 retrieval + grounding). Disruption often comes from reframing the atomic unit, not improving the existing metric.",
      "key_lesson": "Market leaders optimize the status quo; disruption starts on inferior paths. The question isn't 'How do we make better vacuum tubes?' but 'Who would accept worse sound for something they can't get today?'",
      "tags": [
        "disruption",
        "atomic-unit",
        "customer-segment",
        "reframe",
        "hardware",
        "classic-case",
        "new-market-creation"
      ]
    },
    {
      "id": "ups-orion-left-turn-reframe",
      "title": "UPS ORION: Why Left Turns Are Wrong Even When They're Shorter",
      "category": "Optimization Pivot - Objective Reframe",
      "companies_involved": [
        "UPS"
      ],
      "year": "2013",
      "initial_problem": "UPS operates 125,000+ delivery vehicles making millions of stops daily. The obvious optimization objective is: minimize total miles driven to reduce fuel costs and delivery time.",
      "initial_assumptions": [
        "The atomic unit is 'distance traveled' \u2014 shorter routes are better routes",
        "The optimization objective should be 'minimize miles'",
        "All turns are equal if they lead to shorter paths",
        "GPS and route optimization algorithms should find the shortest path"
      ],
      "why_it_fails": [
        "Left turns at intersections require waiting for oncoming traffic, adding unpredictable delays",
        "Left turns increase accident risk (crossing traffic lanes), raising insurance and liability costs",
        "Waiting for left turns wastes fuel from idling \u2014 sometimes more than the 'extra' distance of going right",
        "Driver stress and cognitive load increase when making left turns in traffic",
        "The shortest route on a map is not the fastest, cheapest, or safest route in reality"
      ],
      "first_principle_insight": "Ranga Nuggehalli (UPS Chief Scientist) reframed the problem: the objective isn't 'minimize distance' \u2014 it's 'minimize total cost including time, fuel, risk, and driver experience.' A route that flows smoothly beats a route that's technically shorter but full of friction. The atomic unit isn't 'miles' \u2014 it's 'delivery efficiency under real-world constraints.'",
      "how_to_reframe": {
        "old_atomic_unit": "Miles traveled (shortest path)",
        "new_atomic_unit": "Route efficiency (time + fuel + safety + driver experience)"
      },
      "reframe": {
        "new_atomic_unit": "Route segment with real-world cost (including turn type, traffic patterns, time of day)",
        "new_problem_type": "Multi-objective optimization with soft constraints (not single-metric minimization)",
        "new_objective": "Minimize total delivery cost accounting for idling time, accident risk, fuel consumption, and driver fatigue \u2014 not just distance",
        "architectural_changes": [
          {
            "change": "Right-Turn Priority Algorithm",
            "description": "ORION (On-Road Integrated Optimization and Navigation) system designs routes that favor right turns, even when this means more total miles. A route that's 1 mile longer but has all right turns often beats a 'shorter' route with left turns."
          },
          {
            "change": "Real-World Cost Function",
            "description": "Replaced 'minimize miles' with a composite cost function including: fuel burned (driving + idling), time spent (moving + waiting), accident probability, and driver satisfaction."
          },
          {
            "change": "Dynamic Route Optimization",
            "description": "Routes recalculated based on time of day, traffic patterns, and delivery density \u2014 not just static map distances."
          },
          {
            "change": "Driver Feedback Loop",
            "description": "Incorporated driver experience into the optimization. Routes that stressed drivers led to worse outcomes even if 'optimal' on paper."
          }
        ],
        "results": "UPS saves approximately 10 million gallons of fuel annually by avoiding left turns. This translates to $300-400 million in annual savings. Reduced accidents and driver stress. The counterintuitive insight \u2014 drive more miles to save money \u2014 only emerges when you question the objective function itself."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Ranga Nuggehalli (UPS Chief Scientist), discussions on ORION routing optimization",
          "note": "The 'left turn' insight is attributed to Nuggehalli's first-principles rethinking of route optimization"
        },
        {
          "type": "secondary",
          "citation": "Various news coverage of UPS ORION system and left-turn policy",
          "note": "Widely reported case study in logistics optimization literature"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) and Step 2 (Deconstruction) \u2014 The metric you optimize determines the solution you get. 'Minimize miles' and 'minimize cost' sound similar but produce radically different routes.",
        "common_mistake_avoided": "Optimizing a proxy metric (distance) instead of the true objective (efficiency). This is the same trap as optimizing for 'accuracy' when the business needs 'precision' or 'recall.'",
        "when_to_use": "When your optimization seems correct but produces counterintuitive results. Ask: 'Is the metric I'm optimizing the same as the outcome I actually want?' This is a perfect lecture example because it's non-AI but the lesson transfers directly."
      },
      "pattern_connection": "Same pattern as Uber surge pricing (prediction \u2192 optimization), Netflix ranking (accuracy \u2192 engagement), and RAG systems (retrieval similarity \u2192 answer correctness). In each case, the obvious metric wasn't the right metric.",
      "key_lesson": "The shortest path is not always the best path. When your objective function is wrong, even perfect optimization produces bad outcomes. UPS drives MORE miles and spends LESS money because they optimized the right thing.",
      "tags": [
        "optimization",
        "objective-function",
        "logistics",
        "non-ai-reframe",
        "first-principles",
        "proxy-metrics",
        "lecture-example",
        "counter-intuitive"
      ]
    },
    {
      "id": "gepa-prompt-optimization-cost-reduction",
      "title": "GEPA: Frontier LLM Performance at 1/90th the Cost",
      "category": "Architectural Pivot - Prompt Optimization",
      "companies_involved": [
        "Research/Industry"
      ],
      "year": "2025",
      "initial_problem": "Organizations face an unsustainable trade-off: proprietary frontier models like Claude Opus deliver superior accuracy but cost ~90x more than open-source alternatives. The default assumption is that better results require better (more expensive) models.",
      "initial_assumptions": [
        "Model quality is the primary lever for output quality",
        "Frontier models are necessary for strong performance",
        "The quality-cost trade-off is fixed \u2014 you get what you pay for",
        "Upgrading means upgrading the model"
      ],
      "why_it_fails": [
        "Cost structures become unsustainable at scale",
        "Teams are locked into expensive API dependencies",
        "The assumption that 'better model = better results' ignores the role of prompts",
        "Prompt quality is often the actual bottleneck, not model capability"
      ],
      "first_principle_insight": "Don't upgrade the model \u2014 upgrade the prompt. Use a strong 'optimizer' model to refine prompts for a cheap 'student' model. The lever isn't model selection; it's prompt optimization. Language itself becomes the feedback mechanism, making optimization 35x more sample-efficient than traditional RL.",
      "how_to_reframe": {
        "old_atomic_unit": "Model selection (which model to call)",
        "new_atomic_unit": "Prompt quality (how to instruct any model)"
      },
      "reframe": {
        "new_atomic_unit": "Optimized prompt + commodity model",
        "new_problem_type": "Prompt optimization via optimizer/student paradigm",
        "new_objective": "Maximize output quality per dollar by optimizing prompts, not models",
        "architectural_changes": [
          {
            "change": "Optimizer/Student Paradigm",
            "description": "Use a stronger 'optimizer' model (e.g., Claude Sonnet) to iteratively refine prompts for a cheaper 'student' model (e.g., open-source 120B). The optimizer never serves production traffic."
          },
          {
            "change": "GEPA (Genetic-Pareto) Algorithm",
            "description": "Iterative loop: capture execution traces, use LLM critic to diagnose failures, intelligently mutate prompts based on natural language feedback. Language as feedback is 35x more sample-efficient than RL."
          },
          {
            "change": "Amortized Optimization Cost",
            "description": "Optimization is a one-time cost. At >100k requests, the optimization expense becomes negligible compared to per-request savings."
          }
        ],
        "results": "Open-source 120B model with GEPA-optimized prompts exceeded Claude Opus baseline by ~3%. GEPA-optimized prompts were 20% cheaper to serve than supervised fine-tuning while delivering slightly better performance. Total cost reduction: ~90x."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Machine Learning at Scale Substack: System Design Deep Dive - Achieving Frontier LLM Performance at 1/90th the Cost",
          "url": "https://machinelearningatscale.substack.com/p/system-design-deep-dive-achieving",
          "note": "Detailed analysis of GEPA prompt optimization approach"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) \u2014 Cost vs. quality isn't a fixed trade-off if you change which lever you're pulling. The trade-off matrix changes when you reframe from 'model selection' to 'prompt optimization.'",
        "common_mistake_avoided": "Assuming expensive = better. The instinct to 'upgrade the model' when results are poor, instead of questioning whether the prompt is the bottleneck.",
        "when_to_use": "When facing cost constraints with LLM deployments. Before paying for a bigger model, ask: 'Is the prompt optimized?' This is especially relevant for high-volume production systems where per-request cost matters."
      },
      "pattern_connection": "Same pattern as the 'don't scale, optimize' insight. Netflix didn't need more accurate ratings \u2014 they needed better ranking. GEPA doesn't need a better model \u2014 it needs a better prompt.",
      "key_lesson": "The quality-cost frontier isn't fixed. Prompt optimization can move you to a better point on the curve than model upgrades. The lever you pull matters more than how hard you pull.",
      "tags": [
        "genai",
        "cost-optimization",
        "prompt-engineering",
        "llm",
        "trade-offs",
        "optimizer-student",
        "production-ml"
      ]
    },
    {
      "id": "r4ec-reflection-recommendations",
      "title": "R4EC: Teaching Recommenders to Think Twice",
      "category": "Architectural Pivot - Verification Loop",
      "companies_involved": [
        "E-commerce (unnamed)",
        "Research"
      ],
      "year": "2025",
      "initial_problem": "Using LLMs for recommendation enrichment (generating user profiles, item descriptions) is brittle. Single-shot prompts to large models produce unreliable outputs \u2014 a small error in the model's reasoning can cascade into nonsensical or factually incorrect recommendations.",
      "initial_assumptions": [
        "Bigger models produce better outputs",
        "A well-crafted prompt is sufficient for reliable generation",
        "LLM outputs can be trusted without verification",
        "Quality comes from model capability, not process design"
      ],
      "why_it_fails": [
        "Single-shot generation has no error correction mechanism",
        "'Prompt and pray' produces inconsistent quality",
        "Small reasoning errors compound into large output errors",
        "No way to catch mistakes before they affect users",
        "Expensive frontier models still make errors that cheaper verification could catch"
      ],
      "first_principle_insight": "Replace model size with deliberate process design. Instead of trusting a single LLM call, implement System 2 thinking in the architecture: an Actor generates, a Reflector critiques, and the Actor refines based on specific feedback. Verification is cheaper than hoping for perfection.",
      "how_to_reframe": {
        "old_atomic_unit": "Single LLM output (trusted as-is)",
        "new_atomic_unit": "Verified LLM output (validated through adversarial review)"
      },
      "reframe": {
        "new_atomic_unit": "Actor-Reflector validated output",
        "new_problem_type": "Generate-then-verify loop instead of single-shot generation",
        "new_objective": "Maximize output reliability through deliberate reflection, not model scale",
        "architectural_changes": [
          {
            "change": "Actor-Reflector Architecture",
            "description": "Two-model loop: Actor generates initial user preference summaries from interaction history, Reflector critiques the output identifying specific flaws, Actor refines based on critique. Mimics human deliberate thinking."
          },
          {
            "change": "Distillation for Production",
            "description": "Training data generated by prompting GPT-4o once, then distilled to smaller open-source models (7B Qwen-2.5) for production inference. Expensive model used for training, cheap model for serving."
          },
          {
            "change": "Self-Correction Loop",
            "description": "The Reflector provides specific, actionable feedback \u2014 not just 'this is wrong' but 'this claim contradicts the user's purchase history.' Actor can target fixes."
          }
        ],
        "results": "+2.2% revenue increase in large-scale A/B testing. +1.6% CVR lift. +4.1% revenue on long-tail/cold-start data. Outperformed prior state-of-the-art (KAR) despite using smaller models."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Machine Learning at Scale Substack: R4EC - Teaching Your Recommender LLMs to Think Twice",
          "url": "https://machinelearningatscale.substack.com/p/r4ec-teaching-your-recommender-llms",
          "note": "Analysis of Actor-Reflector architecture for recommendation systems"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) and Lesson 1 (System 1 vs System 2) \u2014 Build verification into the system architecture. The Reflector IS a signal mechanism that catches errors before they reach users.",
        "common_mistake_avoided": "Trusting LLM outputs without verification. The instinct to 'use a bigger model' when quality is poor, instead of adding a verification step.",
        "when_to_use": "When LLM outputs affect downstream decisions (recommendations, content generation, data enrichment). Ask: 'What catches errors before users see them?' If the answer is 'nothing,' add a reflection loop."
      },
      "pattern_connection": "This is System 2 thinking implemented as architecture. Same pattern as Cursor's generate-then-verify for code, and the general 'LLM-as-judge' evaluation pattern. Deliberate reflection beats fast intuition.",
      "key_lesson": "Process design beats model size. A 7B model with reflection outperforms larger models without it. The question isn't 'how capable is the model?' but 'how do we verify the output?'",
      "system_2_connection": "This case study is a direct implementation of the System 1 vs System 2 framework from Lesson 1. Single-shot generation is System 1 (fast, intuitive, error-prone). Actor-Reflector is System 2 (slow, deliberate, self-correcting).",
      "tags": [
        "genai",
        "recommendations",
        "verification",
        "system-2",
        "reflection",
        "llm-as-judge",
        "actor-critic",
        "cold-start"
      ]
    },
    {
      "id": "spotify-background-coding-agent-2025",
      "title": "Spotify: From Deterministic Transforms to AI-Driven Code Migration",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Spotify"
      ],
      "year": 2025,
      "initial_problem": "Automating large-scale code migrations across thousands of software components at Spotify's Fleet Management scale",
      "initial_assumptions": [
        "Deterministic, programmatic code transformations (AST manipulation, regex) are sufficient for fleet-wide changes",
        "Complex migrations can be handled by writing more sophisticated transformation scripts",
        "Only specialized teams with deep tooling expertise can write migration code"
      ],
      "why_it_fails": [
        "Complex code changes proved extremely difficult - Maven dependency updater alone grew to over 20,000 lines handling corner cases",
        "Only specialized teams could write sophisticated transformations, severely limiting what migrations the system could tackle",
        "Each new edge case required more code, creating unmaintainable complexity"
      ],
      "first_principle_insight": "AI tools are becoming capable of making complex code changes. Rather than writing ever-more-complex deterministic scripts, replace rigid transformation logic with natural language prompts interpreted by AI agents - lowering the barrier to entry dramatically.",
      "how_to_reframe": {
        "old_atomic_unit": "Code transformation script - a deterministic AST/regex program handling specific migration patterns",
        "new_atomic_unit": "Natural language prompt - a high-level description of the desired code change that an AI agent interprets"
      },
      "reframe": {
        "new_atomic_unit": "Natural language migration prompt interpreted by AI agent",
        "new_problem_type": "Deterministic scripting \u2192 AI-assisted code generation",
        "new_objective": "Enable any engineer to describe migrations in natural language while preserving existing Fleet Management infrastructure for PR creation/review/deployment",
        "architectural_changes": [
          {
            "change": "Pluggable AI agents",
            "description": "Built internal CLI that delegates to pluggable AI agents rather than adopting off-the-shelf solutions"
          },
          {
            "change": "Preserved infrastructure",
            "description": "Kept existing Fleet Management infrastructure for PR creation, review, and deployment workflows"
          },
          {
            "change": "Natural language interface",
            "description": "Replaced rigid transformation scripts with natural language prompts"
          }
        ],
        "results": "Over 1,500 merged AI-generated PRs handling previously intractable migrations (language modernization, breaking-change upgrades, component migrations) with 60-90% time savings compared to manual coding"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Spotify Engineering Blog (2025): 1,500+ PRs Later: Spotify's Journey with Our Background Coding Agent (Part 1)",
          "url": "https://engineering.atspotify.com/2025/11/spotifys-background-coding-agent-part-1",
          "note": "First part of three-part series on Spotify's coding agent journey"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Recognizing that AI agents are a viable alternative to deterministic scripting",
        "common_mistake_avoided": "Adding more code to handle edge cases instead of questioning the fundamental approach",
        "when_to_use": "When deterministic automation becomes unmaintainably complex due to edge cases"
      },
      "tags": [
        "code-generation",
        "agents",
        "automation",
        "developer-tools"
      ]
    },
    {
      "id": "spotify-context-engineering-2025",
      "title": "Spotify: From Rigid Instructions to Natural Task Orientation",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Spotify"
      ],
      "year": 2025,
      "initial_problem": "Enabling users to effectively guide coding agents through complex multi-file code changes",
      "initial_assumptions": [
        "Users should manually select files via git-grep commands and provide step-by-step instructions",
        "Detailed, prescriptive guidance produces better agent results",
        "A homegrown agentic loop with manual file selection is sufficient"
      ],
      "why_it_fails": [
        "Users found it difficult to balance between too-broad and too-narrow file selection",
        "Agent struggled with complex multi-file changes",
        "Agent would get lost when context window filled up, forgetting the original task",
        "Rigid step-by-step instructions created friction for users who wanted to describe outcomes"
      ],
      "first_principle_insight": "Users want to describe outcomes rather than dictate every step. An agent that can interpret high-level goals, manage tasks dynamically, and reduce friction is more effective than one requiring precise manual guidance.",
      "how_to_reframe": {
        "old_atomic_unit": "Step-by-step instruction with manually selected files",
        "new_atomic_unit": "High-level task description that the agent interprets and manages dynamically"
      },
      "reframe": {
        "new_atomic_unit": "Natural, task-oriented prompt with agent-managed context",
        "new_problem_type": "Prescriptive instructions \u2192 Goal-oriented task delegation",
        "new_objective": "Enable users to describe desired outcomes while the agent handles file discovery, task management, and context",
        "architectural_changes": [
          {
            "change": "Claude Code adoption",
            "description": "Shifted from custom loop to Claude Code which handles task-oriented prompts natively"
          },
          {
            "change": "Built-in task management",
            "description": "Leveraged todo list management and subagent spawning capabilities"
          },
          {
            "change": "Dynamic context handling",
            "description": "Better handling of complex multi-file edits without context window exhaustion"
          }
        ],
        "results": "Claude Code became their top-performing agent, deployed across approximately 50 migrations and the majority of merged production PRs"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Spotify Engineering Blog (2025): Background Coding Agents: Context Engineering (Part 2)",
          "url": "https://engineering.atspotify.com/2025/11/context-engineering-background-coding-agents-part-2",
          "note": "Second part of three-part series focusing on context engineering"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) - Recognizing that users want outcomes, not process control",
        "common_mistake_avoided": "Building more complex instruction interfaces instead of enabling natural task description",
        "when_to_use": "When users struggle with prescriptive interfaces and want to describe what they want, not how to do it"
      },
      "tags": [
        "code-generation",
        "agents",
        "UX",
        "developer-tools"
      ]
    },
    {
      "id": "spotify-feedback-loops-2025",
      "title": "Spotify: From Autonomous Agents to Constrained Verification Loops",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Spotify"
      ],
      "year": 2025,
      "initial_problem": "Ensuring AI coding agents produce reliable, production-ready code changes at scale",
      "initial_assumptions": [
        "LLMs can handle verification independently through their reasoning capabilities",
        "Passing CI is sufficient validation for agent-generated PRs",
        "More agent autonomy leads to better results"
      ],
      "why_it_fails": [
        "Agents produced non-functional PRs that passed CI but broke functionality",
        "Agents attempted scope creep beyond the original prompt",
        "Changes were hard to spot in reviews and could break functionality in production",
        "Without external guardrails, agents often produce code that simply doesn't work"
      ],
      "first_principle_insight": "Predictability requires constraining agent autonomy, not expanding it. Agents need external guardrails and verification loops - the fundamental realization is that more autonomy without verification leads to worse outcomes.",
      "how_to_reframe": {
        "old_atomic_unit": "Autonomous agent with full reasoning responsibility",
        "new_atomic_unit": "Constrained agent with external verification loops and scope enforcement"
      },
      "reframe": {
        "new_atomic_unit": "Agent bounded by verification loops, LLM judges, and sandboxing",
        "new_problem_type": "Autonomous execution \u2192 Verified execution with guardrails",
        "new_objective": "Produce predictable, verified code changes through external constraints rather than relying on agent self-verification",
        "architectural_changes": [
          {
            "change": "Verification loops",
            "description": "Independent verifiers (Maven, formatting, testing) that activate automatically based on codebase contents"
          },
          {
            "change": "LLM judge layer",
            "description": "Secondary LLM evaluates diffs against original prompts to catch scope creep"
          },
          {
            "change": "Sandboxing",
            "description": "Limited agent access to codebase, file tools, and verifiers only"
          },
          {
            "change": "Stop hooks",
            "description": "Pre-PR verification ensures failures prevent merging"
          }
        ],
        "results": "Out of thousands of sessions, the judge vetoes approximately 25% of proposed changes, with agents successfully course-correcting roughly 50% of those vetoed cases"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Spotify Engineering Blog (2025): Background Coding Agents: Predictable Results Through Strong Feedback Loops (Part 3)",
          "url": "https://engineering.atspotify.com/2025/12/feedback-loops-background-coding-agents-part-3",
          "note": "Third part of three-part series focusing on verification and guardrails"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - Building verification signals that catch failures before production",
        "common_mistake_avoided": "Assuming more capable models need less verification",
        "when_to_use": "When deploying agents to production where reliability matters more than capability"
      },
      "tags": [
        "code-generation",
        "agents",
        "verification",
        "guardrails",
        "reliability"
      ]
    },
    {
      "id": "shopify-sidekick-jit-instructions-2025",
      "title": "Shopify: From Monolithic Prompts to Just-In-Time Instructions",
      "category": "Architectural Pivot - Prompt Engineering",
      "companies_involved": [
        "Shopify"
      ],
      "year": 2025,
      "initial_problem": "Scaling Shopify Sidekick as the number of tools grew from 0-20 to 50+",
      "initial_assumptions": [
        "All tool guidance should be crammed into the system prompt upfront",
        "Comprehensive instructions prevent errors",
        "More detailed prompts lead to better performance"
      ],
      "why_it_fails": [
        "Death by a Thousand Instructions - special cases and edge case handling became unmaintainable",
        "System became increasingly difficult to reason about with unclear boundaries",
        "Conflicting guidance slowed performance",
        "Cache efficiency suffered from monolithic prompts"
      ],
      "first_principle_insight": "Instead of anticipating every scenario upfront, deliver instructions dynamically. Craft the perfect context for the LLM for every single situation - not a token less, not a token more.",
      "how_to_reframe": {
        "old_atomic_unit": "Comprehensive upfront system prompt with all possible instructions",
        "new_atomic_unit": "Just-in-time instruction delivery relevant to the current context only"
      },
      "reframe": {
        "new_atomic_unit": "Contextual, on-demand instructions returned alongside tool data",
        "new_problem_type": "Static prompt engineering \u2192 Dynamic context assembly",
        "new_objective": "Deliver minimal, relevant instructions exactly when needed rather than loading everything statically",
        "architectural_changes": [
          {
            "change": "JIT Instructions",
            "description": "Return relevant instructions alongside tool data exactly when needed"
          },
          {
            "change": "Localized guidance",
            "description": "Instructions appear only when relevant to current context"
          },
          {
            "change": "Cache efficiency",
            "description": "Dynamic adjustments without breaking cache"
          },
          {
            "change": "Modularity",
            "description": "Different instructions per context/beta flags"
          }
        ],
        "results": "System became more maintainable while performance improved across all metrics"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Shopify Engineering Blog (2025): Building production-ready agentic systems: Lessons from Shopify Sidekick",
          "url": "https://shopify.engineering/building-production-ready-agentic-systems",
          "note": "Detailed lessons from scaling Sidekick to 50+ tools"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Trading upfront completeness for runtime efficiency",
        "common_mistake_avoided": "Adding more instructions to handle edge cases instead of rethinking delivery",
        "when_to_use": "When system prompts grow unwieldy and maintainability suffers"
      },
      "tags": [
        "agents",
        "prompt-engineering",
        "scalability",
        "e-commerce"
      ]
    },
    {
      "id": "shopify-taxonomy-agents-2025",
      "title": "Shopify: From Manual Curation to Multi-Agent Taxonomy Evolution",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Shopify"
      ],
      "year": 2025,
      "initial_problem": "Maintaining and evolving product taxonomy across diverse verticals at Shopify scale",
      "initial_assumptions": [
        "Taxonomy requires manual curation by domain experts",
        "Experts analyze gaps, propose changes, and implement through careful review",
        "The process should be reactive, triggered when merchants encounter problems"
      ],
      "why_it_fails": [
        "Traditional expertise couldn't scale across diverse product verticals",
        "Commerce never stands still - new categories emerged faster than humans could adapt",
        "Inconsistencies accumulated organically across the taxonomy",
        "Reactive process missed opportunities to proactively improve merchant/customer experiences"
      ],
      "first_principle_insight": "Rather than replacing human expertise, AI agents can augment it. Different analysis types - structural consistency checks and real product data examination - can be intelligently combined to discover improvements neither approach would identify alone.",
      "how_to_reframe": {
        "old_atomic_unit": "Manual expert analysis of taxonomy gaps (reactive, one-at-a-time)",
        "new_atomic_unit": "Multi-agent parallel analysis combining structural and product-driven insights (proactive, continuous)"
      },
      "reframe": {
        "new_atomic_unit": "Multi-agent system with specialized analysis roles",
        "new_problem_type": "Manual curation \u2192 Continuous AI-augmented evolution",
        "new_objective": "Enable comprehensive, proactive taxonomy improvement while freeing experts for strategic decisions",
        "architectural_changes": [
          {
            "change": "Structural analysis agents",
            "description": "Agents checking logical consistency across taxonomy"
          },
          {
            "change": "Product-driven analysis agents",
            "description": "Agents examining real merchant behavior and product data"
          },
          {
            "change": "Intelligent synthesis",
            "description": "Conflict resolution when different analysis types disagree"
          },
          {
            "change": "Equivalence detection",
            "description": "Understanding attribute-based category relationships"
          },
          {
            "change": "Domain-specific AI judges",
            "description": "Automated quality assurance for proposed changes"
          }
        ],
        "results": "Taxonomy experts might analyze a few categories per day while the system evaluates hundreds in parallel, transforming years of work into weeks for specific taxonomy branches"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Shopify Engineering Blog (2025): Beyond classification: How AI agents are evolving Shopify's product taxonomy at scale",
          "url": "https://shopify.engineering/product-taxonomy-at-scale",
          "note": "Detailed description of multi-agent taxonomy evolution system"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Recognizing that 'taxonomy maintenance' decomposes into multiple specialized analysis types",
        "common_mistake_avoided": "Trying to replace experts with AI instead of augmenting their capabilities",
        "when_to_use": "When expert-driven processes can't scale to match the rate of change in the domain"
      },
      "tags": [
        "agents",
        "multi-agent",
        "taxonomy",
        "e-commerce",
        "augmentation"
      ]
    },
    {
      "id": "linkedin-multi-agent-orchestration-2025",
      "title": "LinkedIn: From Monolithic Agents to Multi-Agent Orchestration",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2025,
      "initial_problem": "Building AI agents to handle complex, multi-step tasks at LinkedIn scale",
      "initial_assumptions": [
        "Build monolithic agent applications to handle complex tasks",
        "Each agent should be a self-contained application",
        "Need to build custom orchestration infrastructure for agent coordination"
      ],
      "why_it_fails": [
        "Complex tasks require coordination across multiple specialized capabilities",
        "Monolithic agents become unwieldy as functionality grows",
        "Building custom orchestration duplicates existing infrastructure capabilities"
      ],
      "first_principle_insight": "For most use cases, the agent is not a single monolithic application, but rather a facade over multiple agentic applications. Rather than inventing new orchestration, leverage existing messaging infrastructure which already has the necessary distributed systems properties.",
      "how_to_reframe": {
        "old_atomic_unit": "Monolithic agent application handling all task aspects",
        "new_atomic_unit": "Orchestrated multi-agent system leveraging existing messaging infrastructure"
      },
      "reframe": {
        "new_atomic_unit": "Modular agents coordinated through production messaging platform",
        "new_problem_type": "Monolithic agent \u2192 Distributed agent composition",
        "new_objective": "Compose agents as a platform capability using battle-tested infrastructure",
        "architectural_changes": [
          {
            "change": "Reuse existing infrastructure",
            "description": "Production messaging system provides FIFO delivery, message history, resilience, multi-region scaling"
          },
          {
            "change": "Adapter layer",
            "description": "Libraries abstracting messaging-to-gRPC conversions through central agent lifecycle service"
          },
          {
            "change": "Consistent agent definition",
            "description": "gRPC service schemas with platform-specific proto3 annotations registered in skill registry"
          }
        ],
        "results": "Parallel task execution improving performance on long-running workflows; fault isolation preventing failures in one agent from crashing the whole system; developer productivity through familiar abstractions"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2025): The LinkedIn Generative AI Application Tech Stack: Extending to Build AI Agents",
          "url": "https://www.linkedin.com/blog/engineering/generative-ai/the-linkedin-generative-ai-application-tech-stack-extending-to-build-ai-agents",
          "note": "Technical details on LinkedIn's multi-agent orchestration approach"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Recognizing existing infrastructure as an alternative to custom orchestration",
        "common_mistake_avoided": "Building custom orchestration when existing infrastructure already solves the problem",
        "when_to_use": "When building multi-agent systems at scale with existing distributed systems infrastructure"
      },
      "tags": [
        "agents",
        "multi-agent",
        "orchestration",
        "infrastructure",
        "distributed-systems"
      ]
    },
    {
      "id": "linkedin-hiring-assistant-plan-execute-2025",
      "title": "LinkedIn: From ReAct to Plan-and-Execute for Enterprise Agents",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "LinkedIn"
      ],
      "year": 2025,
      "initial_problem": "Building a reliable hiring assistant agent that works at LinkedIn scale (1.2+ billion profiles)",
      "initial_assumptions": [
        "ReAct-style architecture where LLM directly handles all reasoning and tool use is sufficient",
        "Iterative LLM reasoning can handle complex multi-step recruiting tasks",
        "More capable models will solve reliability issues"
      ],
      "why_it_fails": [
        "Instruction-following reliability: LLMs may not follow instructions and plans reliably",
        "Hallucinations: fabricated or irrelevant outputs risk damaging trust",
        "Intelligence vs. latency tradeoffs: richer reasoning often comes at unacceptable performance costs",
        "At enterprise scale, iterative reasoning becomes unreliable and costly"
      ],
      "first_principle_insight": "For enterprise recruiting at scale, relying solely on iterative LLM reasoning is unreliable and costly. Complex, multi-step tasks need explicit upfront structure through separation of planning and execution phases.",
      "how_to_reframe": {
        "old_atomic_unit": "Single ReAct loop handling both reasoning and execution",
        "new_atomic_unit": "Separated Planner (high-level reasoning) and Executor (step-by-step tool interaction)"
      },
      "reframe": {
        "new_atomic_unit": "Plan-and-Execute architecture with separated reasoning phases",
        "new_problem_type": "Iterative reasoning \u2192 Structured planning + execution",
        "new_objective": "Prioritize reliability and scale over flexibility through upfront planning",
        "architectural_changes": [
          {
            "change": "Planner component",
            "description": "Performs high-level reasoning to produce structured, task-specific plans"
          },
          {
            "change": "Executor component",
            "description": "Runs plans step-by-step using ReAct-style tool interaction"
          },
          {
            "change": "Differentiated LLMs",
            "description": "Different models per task based on cost/capability tradeoffs"
          }
        ],
        "results": "Fewer LLM calls through upfront planning; improved task completion rates on complex workflows; cost efficiency through differentiated LLMs; better handling of predictable recruiting workflows"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "LinkedIn Engineering Blog (2025): Building the agentic future of recruiting: how we engineered LinkedIn's Hiring Assistant",
          "url": "https://www.linkedin.com/blog/engineering/ai/how-we-engineered-linkedins-hiring-assistant",
          "note": "Detailed engineering post on Hiring Assistant architecture"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Trading flexibility for reliability at scale",
        "common_mistake_avoided": "Assuming ReAct is sufficient for all agent use cases",
        "when_to_use": "When building agents for complex but predictable workflows where reliability matters more than flexibility"
      },
      "tags": [
        "agents",
        "recruiting",
        "plan-execute",
        "reliability",
        "enterprise"
      ]
    },
    {
      "id": "harvey-agentic-search-2025",
      "title": "Harvey: From Static RAG to Iterative Agentic Search",
      "category": "Architectural Pivot - RAG Reframe",
      "companies_involved": [
        "Harvey AI"
      ],
      "year": 2025,
      "initial_problem": "Enabling AI to perform complex legal research requiring multiple information sources and synthesis",
      "initial_assumptions": [
        "Traditional RAG approach: embed query, retrieve relevant chunks, generate answer",
        "One-shot retrieval is sufficient for legal research",
        "The system should execute static lookups based on the initial query"
      ],
      "why_it_fails": [
        "Complex legal research requires multiple information-gathering rounds and synthesis across sources",
        "System couldn't adapt strategy mid-query",
        "Implicit questions like 'What's our exposure on termination provisions?' require broader coverage than 1-2 sources",
        "Static lookups miss the iterative nature of legal research"
      ],
      "first_principle_insight": "Legal research is inherently iterative - lawyers refine queries based on discoveries, check multiple jurisdictions, cross-reference statutes. The system needed to reason about the research process itself rather than execute static lookups.",
      "how_to_reframe": {
        "old_atomic_unit": "Single query \u2192 retrieve \u2192 generate answer",
        "new_atomic_unit": "Iterative research loop: plan \u2192 select tools \u2192 retrieve \u2192 synthesize \u2192 check completeness \u2192 iterate"
      },
      "reframe": {
        "new_atomic_unit": "Agentic research workflow with iterative refinement",
        "new_problem_type": "Static RAG \u2192 Iterative agentic research",
        "new_objective": "Enable dynamic, multi-round research that adapts based on discoveries",
        "architectural_changes": [
          {
            "change": "ReAct-inspired workflow",
            "description": "Agent plans, selects tools, retrieves, synthesizes, checks completeness, iterates"
          },
          {
            "change": "Privacy-preserving evaluation",
            "description": "In-house legal experts (ALRs/SBDLs) evaluate instead of using customer data"
          },
          {
            "change": "Centralized evaluation infrastructure",
            "description": "OpenTelemetry traces and LangSmith for monitoring"
          }
        ],
        "results": "Tool selection precision improved from near zero to 0.8-0.9; complex queries now scale to 3-10 retrieval operations based on demands (vs. 1-2 previously); system maintains current knowledge across 150+ legal sources with citation accuracy"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Harvey AI Blog (2025): How Agentic Search Unlocks Legal Research Intelligence",
          "url": "https://www.harvey.ai/blog/how-agentic-search-unlocks-legal-research-intelligence",
          "note": "Technical post on Harvey's agentic search architecture"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Recognizing that legal research is iterative, not single-shot",
        "common_mistake_avoided": "Treating complex research as a single retrieval operation",
        "when_to_use": "When the domain requires iterative exploration and synthesis across multiple sources"
      },
      "tags": [
        "RAG",
        "agents",
        "legal",
        "search",
        "iteration"
      ]
    },
    {
      "id": "harvey-tool-bundles-2025",
      "title": "Harvey: From Centralized Control to Distributed Tool Bundles",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Harvey AI"
      ],
      "year": 2025,
      "initial_problem": "Scaling agent development as more teams want to contribute capabilities to Harvey's Assistant",
      "initial_assumptions": [
        "Features routed through centralized design decisions (Need to draft? Use Draft mode)",
        "Core team controls all integrations",
        "New capabilities from other teams limited to retrieval knowledge sources"
      ],
      "why_it_fails": [
        "Hit a UX, engineering, and collaboration wall",
        "People didn't discover features; integrating multiple retrieval calls became complex",
        "New integrations had no clear launch path",
        "One team became a bottleneck for all feature development",
        "Multiple engineers modifying shared system prompt created conflicts"
      ],
      "first_principle_insight": "When shifting to agents, you're no longer merging unit-testable code - you're merging English. Multiple engineers modifying a shared system prompt creates conflicts with no clean separation of concerns. The solution is distributed ownership with centralized quality control.",
      "how_to_reframe": {
        "old_atomic_unit": "Centralized system prompt controlled by core team",
        "new_atomic_unit": "Tool Bundles with embedded instructions owned by distributed teams"
      },
      "reframe": {
        "new_atomic_unit": "Encapsulated Tool Bundles with embedded instructions and eval gates",
        "new_problem_type": "Centralized control \u2192 Distributed contribution with quality gates",
        "new_objective": "Enable teams to contribute independently while maintaining system integrity",
        "architectural_changes": [
          {
            "change": "No custom orchestration",
            "description": "Pure agent framework via OpenAI Agent SDK"
          },
          {
            "change": "Tool Bundles",
            "description": "Encapsulated capabilities with embedded instructions, allowing distributed ownership"
          },
          {
            "change": "Eval gates",
            "description": "Leave-one-out validation preventing regressions"
          }
        ],
        "results": "Feature development scaled from one team to four, with emergent feature combinations and centralized quality control"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Harvey AI Blog (2025): 3 Principles That Helped us Scale Agent Development",
          "url": "https://www.harvey.ai/blog/principles-that-helped-us-scale-agent-development",
          "note": "Lessons on scaling agent development at Harvey"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Trading centralized control for distributed scalability",
        "common_mistake_avoided": "Keeping all agent development centralized as the team grows",
        "when_to_use": "When multiple teams need to contribute to an agent system without becoming bottlenecked"
      },
      "tags": [
        "agents",
        "scaling",
        "team-structure",
        "tool-bundles"
      ]
    },
    {
      "id": "uber-semantic-search-2025",
      "title": "Uber: From Lexical Matching to Semantic Search",
      "category": "Architectural Pivot - Search Reframe",
      "companies_involved": [
        "Uber"
      ],
      "year": 2025,
      "initial_problem": "Enabling effective search across Uber Eats' diverse food and restaurant catalog",
      "initial_assumptions": [
        "Lexical matching (exact string-to-document matching) is sufficient for search",
        "Queries precisely match stored text",
        "Keyword-based retrieval handles user intent"
      ],
      "why_it_fails": [
        "Couldn't handle synonyms ('soda' vs 'soft drink')",
        "Typos broke search ('mozzarela')",
        "Shorthand not understood ('gf pizza' for gluten-free)",
        "Language mixing failed ('pan' meaning bread in Spanish)",
        "Users don't search how content is indexed"
      ],
      "first_principle_insight": "Semantic search shifts from matching words to matching meaning. It encodes queries and documents into vectors in the same space, so semantically similar things are close - even without keyword overlap.",
      "how_to_reframe": {
        "old_atomic_unit": "Query string matched against document keywords",
        "new_atomic_unit": "Query embedding matched against document embeddings in semantic vector space"
      },
      "reframe": {
        "new_atomic_unit": "Semantic vectors in shared embedding space",
        "new_problem_type": "Lexical matching \u2192 Semantic similarity",
        "new_objective": "Match user intent to content meaning regardless of exact wording",
        "architectural_changes": [
          {
            "change": "Two-tower model",
            "description": "Qwen LLM backbone for both query and document encoding"
          },
          {
            "change": "Matryoshka Representation Learning (MRL)",
            "description": "Variable embedding dimensions (128-1536) for flexibility"
          },
          {
            "change": "Quantization strategies",
            "description": "Int7 scalar quantization vs float32 to balance cost/quality"
          },
          {
            "change": "Blue/green deployment",
            "description": "Index column level deployment for safe rollouts"
          }
        ],
        "results": "34% latency reduction with shard-level tuning; 50% storage cost reduction via MRL dimension reduction; recall maintained above 0.95 despite aggressive optimization"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog (2025): Evolution and Scale of Uber's Delivery Search Platform",
          "url": "https://www.uber.com/en-GB/blog/evolution-and-scale-of-ubers-delivery-search-platform/",
          "note": "Technical deep-dive on Uber Eats search evolution"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Recognizing that 'matching' can mean keyword or meaning",
        "common_mistake_avoided": "Assuming users search using the exact terms in your content",
        "when_to_use": "When keyword search fails due to vocabulary mismatch, typos, or multilingual content"
      },
      "tags": [
        "search",
        "embeddings",
        "semantic-search",
        "food-delivery"
      ]
    },
    {
      "id": "grab-custom-vision-llm-2025",
      "title": "Grab: From LoRA Fine-tuning to Custom Vision LLM Architecture",
      "category": "Architectural Pivot - Multi-Model Architecture",
      "companies_involved": [
        "Grab"
      ],
      "year": 2025,
      "initial_problem": "Processing Southeast Asian documents with non-Latin scripts (Thai, Vietnamese) and dense layouts",
      "initial_assumptions": [
        "Fine-tuning existing open-source Vision LLMs (like Qwen2-VL) with LoRA would suffice",
        "Lightweight parameter updates can adapt models to new languages",
        "The language decoder is the bottleneck for non-Latin scripts"
      ],
      "why_it_fails": [
        "LoRA fine-tuning achieved high accuracy for Indonesian documents but struggled with Thai and Vietnamese",
        "Dense, unstructured layouts remained problematic",
        "Open-source models had extensive multilingual LLM decoder pre-training but lacked visual text in SEA languages during vision encoder training"
      ],
      "first_principle_insight": "The vision component - not the language model - was the bottleneck. Open-source models had multilingual corpus coverage for the LLM decoder but lacked visual text in SEA languages during vision encoder and joint training.",
      "how_to_reframe": {
        "old_atomic_unit": "Pre-trained Vision LLM + LoRA adapter layers",
        "new_atomic_unit": "Custom Vision LLM with SEA-language-specific vision encoder training"
      },
      "reframe": {
        "new_atomic_unit": "Purpose-built 1B parameter Vision LLM with language-specific visual training",
        "new_problem_type": "Fine-tuning \u2192 Custom architecture with targeted training",
        "new_objective": "Build a smaller, faster model with vision encoder specifically trained on SEA language visual text",
        "architectural_changes": [
          {
            "change": "Vision encoder from Qwen2-VL 2B",
            "description": "Extracted capable vision component from larger model"
          },
          {
            "change": "Compact language decoder from Qwen2.5 0.5B",
            "description": "Efficient decoder for inference speed"
          },
          {
            "change": "Custom projector layer",
            "description": "New connection between vision and language components"
          },
          {
            "change": "Four-stage training pipeline",
            "description": "Specialized training emphasizing language-specific visual training"
          }
        ],
        "results": "Custom 1B model achieved performance comparable to larger 2B model with only 3pp accuracy gap, while latency far outperforms the 2B model and external APIs"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Grab Engineering Blog (2025): How we built a custom vision LLM to improve document processing at Grab",
          "url": "https://engineering.grab.com/custom-vision-llm-at-grab",
          "note": "Technical details on Grab's custom Vision LLM"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Identifying the actual bottleneck (vision encoder, not language decoder)",
        "common_mistake_avoided": "Assuming fine-tuning is always sufficient for domain adaptation",
        "when_to_use": "When fine-tuning hits a ceiling and you've identified a specific architectural component as the bottleneck"
      },
      "tags": [
        "vision-llm",
        "document-processing",
        "multilingual",
        "custom-model"
      ]
    },
    {
      "id": "salesforce-agentforce-goal-oriented-2025",
      "title": "Salesforce: From Prescriptive Rules to Goal-Oriented Agents",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Salesforce"
      ],
      "year": 2025,
      "initial_problem": "Deploying AI agents at enterprise scale for customer service, sales, and internal use",
      "initial_assumptions": [
        "Treat AI agents like traditional chatbots with rigid, prescriptive rules",
        "Comprehensive blocklists and restrictions prevent errors",
        "Detailed instructions for every scenario ensure correct behavior"
      ],
      "why_it_fails": [
        "Overly restrictive guardrails backfired - agent refused to discuss legitimate Microsoft Teams integration because 'Microsoft' was on competitor blocklist",
        "SDR agent responded 'I don't know' 30% of the time when asked for lead details",
        "Customer support interactions felt transactional rather than empathetic",
        "Prescriptive rules couldn't anticipate every valid scenario"
      ],
      "first_principle_insight": "We had been treating our AI agent like an old-school chatbot with overly prescriptive directions, when what we really needed to do was give the agent a goal and let it determine how to deliver on it. Tell agents what to achieve, not how to achieve it.",
      "how_to_reframe": {
        "old_atomic_unit": "Prescriptive rule set defining exact behaviors and restrictions",
        "new_atomic_unit": "Goal-oriented directive letting the LLM determine how to achieve outcomes"
      },
      "reframe": {
        "new_atomic_unit": "Goal-oriented agent with principle-based guidelines",
        "new_problem_type": "Rule-based chatbot \u2192 Goal-oriented agent",
        "new_objective": "Let agents achieve goals using LLM capabilities rather than constraining to predefined behaviors",
        "architectural_changes": [
          {
            "change": "Principle-based instructions",
            "description": "Replaced rigid blocklists with guidelines like 'act in customer's best interest'"
          },
          {
            "change": "Goal-oriented directives",
            "description": "Shifted from prescriptive rules to outcome-focused instructions"
          },
          {
            "change": "Continuous iteration loops",
            "description": "Built feedback loops based on real user interactions"
          }
        ],
        "results": "SDR agent reduced 'I don't know' responses from 30% to under 10%; service agent handled 1.5+ million support requests with majority resolved autonomously; 86% employee adoption in Slack; 99% global workforce using internal agents"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Salesforce Blog (2025): From Pilot to Playbook: What We Learned from Our First Year Using Agentforce",
          "url": "https://www.salesforce.com/news/stories/first-year-agentforce-customer-zero/",
          "note": "Lessons from Salesforce's first year deploying Agentforce internally"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) - Defining agent success as achieving goals, not following rules",
        "common_mistake_avoided": "Over-constraining agents with prescriptive rules that prevent legitimate use cases",
        "when_to_use": "When agents fail on edge cases because rules can't anticipate every scenario"
      },
      "tags": [
        "agents",
        "enterprise",
        "goal-oriented",
        "customer-service"
      ]
    },
    {
      "id": "uber-agentic-rag-2025",
      "title": "Uber: From Simple RAG to Agentic-RAG for Security Policies",
      "category": "Architectural Pivot - RAG Reframe",
      "companies_involved": [
        "Uber"
      ],
      "year": 2025,
      "initial_problem": "Building an internal copilot (Testing Genie) to answer security and privacy policy questions accurately",
      "initial_assumptions": [
        "Traditional RAG (retrieve semantically relevant chunks, pass to LLM) is sufficient for policy questions",
        "Semantic similarity effectively captures document relevance",
        "Two-step retrieve-then-generate works for domain-specific queries"
      ],
      "why_it_fails": [
        "Testing against 100+ security/privacy queries revealed incomplete, inaccurate responses",
        "Simple semantic similarity couldn't distinguish subtle policy variations across documents",
        "Documents had subtle distinctions in data retention, classification, and geography-specific protocols that vector search missed"
      ],
      "first_principle_insight": "Document chunks often have subtle distinctions\u2014not only within the same policy document but also across multiple documents. Simple vector search couldn't capture these contextual nuances required for high-stakes domains like security.",
      "how_to_reframe": {
        "old_atomic_unit": "Query \u2192 retrieve similar chunks \u2192 generate answer",
        "new_atomic_unit": "Query \u2192 pre-retrieval agents (optimize query, identify sources) \u2192 enhanced retrieval \u2192 post-processing \u2192 structured answer"
      },
      "reframe": {
        "new_atomic_unit": "Agentic RAG pipeline with pre/post-retrieval agents",
        "new_problem_type": "Simple RAG \u2192 Multi-agent RAG with query optimization",
        "new_objective": "Capture subtle policy distinctions through query refinement, source narrowing, and enhanced document processing",
        "architectural_changes": [
          {
            "change": "Pre-retrieval agents",
            "description": "Query Optimizer refines ambiguous queries; Source Identifier narrows document scope"
          },
          {
            "change": "Enhanced document processing",
            "description": "Custom Google Docs loader with LLM-powered enrichment, table-aware chunking, metadata enrichment (summaries, FAQs, keywords)"
          },
          {
            "change": "Hybrid retrieval",
            "description": "Vector search combined with BM25 using enriched metadata"
          },
          {
            "change": "Post-processing agent",
            "description": "De-duplicates and structures retrieved context before generation"
          }
        ],
        "results": "27% relative increase in acceptable answers; 60% relative reduction in incorrect advice; enabled broader deployment across security/privacy help channels"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog (2025): Enhanced Agentic-RAG: What If Chatbots Could Deliver Near-Human Precision?",
          "url": "https://www.uber.com/en-GB/blog/enhanced-agentic-rag/",
          "note": "Technical deep-dive on Uber's agentic RAG architecture"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Recognizing that retrieval has multiple sub-problems (query understanding, source selection, chunk relevance)",
        "common_mistake_avoided": "Assuming simple semantic search is sufficient for nuanced domain content",
        "when_to_use": "When RAG fails on subtle distinctions or domain-specific nuances"
      },
      "tags": [
        "RAG",
        "agents",
        "security",
        "policy",
        "enterprise"
      ]
    },
    {
      "id": "harvey-enterprise-rag-privacy-2025",
      "title": "Harvey: Data Privacy as Primary RAG Architecture Constraint",
      "category": "Architectural Pivot - Privacy Architecture",
      "companies_involved": [
        "Harvey AI"
      ],
      "year": 2025,
      "initial_problem": "Selecting a vector database for enterprise-grade RAG serving legal clients with sensitive data",
      "initial_assumptions": [
        "Vector databases should be selected based on standard technical criteria (performance, accuracy, scalability)",
        "Data privacy is a compliance checkbox, not an architectural constraint",
        "Performance metrics are the primary selection criteria"
      ],
      "why_it_fails": [
        "Enterprise RAG deployment revealed that data privacy and customer control were non-negotiable constraints",
        "Conventional database selection frameworks didn't address data sovereignty requirements",
        "Centralized storage models conflicted with customer security requirements"
      ],
      "first_principle_insight": "Enterprise RAG success requires treating customer data sovereignty as a primary architectural constraint, not a compliance checkbox. Vector database choice became fundamentally about architectural control\u2014enabling data to live in customers' own secure storage.",
      "how_to_reframe": {
        "old_atomic_unit": "Vector database evaluated on performance metrics",
        "new_atomic_unit": "Vector database evaluated on data ownership architecture + performance"
      },
      "reframe": {
        "new_atomic_unit": "Decentralized data ownership as peer requirement to performance",
        "new_problem_type": "Performance optimization \u2192 Privacy-first architecture with performance",
        "new_objective": "Enable vector database hosting within private cloud environments with customer-controlled storage",
        "architectural_changes": [
          {
            "change": "Decentralized data ownership",
            "description": "Prioritizing databases where data can live in various cloud buckets"
          },
          {
            "change": "Private cloud hosting",
            "description": "Vector database hosted within customer's own secure environment"
          },
          {
            "change": "Embeddings + source data separation",
            "description": "Both stored in customer-controlled storage"
          }
        ],
        "results": "Selected LanceDB Enterprise for strength across latency, accuracy, ingestion throughput, scalability, data privacy, and hosting\u2014positioning data privacy as peer requirement to performance"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Harvey AI Blog (2025): Enterprise-Grade RAG Systems",
          "url": "https://www.harvey.ai/blog/enterprise-grade-rag-systems",
          "note": "Harvey's approach to enterprise RAG architecture decisions"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Elevating privacy from constraint to primary design dimension",
        "common_mistake_avoided": "Treating data privacy as an afterthought rather than architectural driver",
        "when_to_use": "When building RAG for enterprise clients with data sovereignty requirements"
      },
      "tags": [
        "RAG",
        "privacy",
        "enterprise",
        "legal",
        "architecture"
      ]
    },
    {
      "id": "grab-user-foundation-model-2025",
      "title": "Grab: From Task-Specific Features to User Foundation Models",
      "category": "Architectural Pivot - Transfer Learning Paradigm",
      "companies_involved": [
        "Grab"
      ],
      "year": 2025,
      "initial_problem": "Building user understanding across Grab's superapp (transport, food, payments) for recommendations, fraud detection, and churn prediction",
      "initial_assumptions": [
        "Manually engineered features specific to individual tasks are sufficient",
        "Each downstream application requires separate feature engineering",
        "Task-specific optimization produces the best results"
      ],
      "why_it_fails": [
        "Features were task-specific and didn't generalize across use cases",
        "Couldn't effectively capture sequential user behavior",
        "Missed cross-vertical signals (e.g., transport patterns informing food recommendations)",
        "Existing general-purpose LLMs lacked domain understanding of Grab's ecosystem",
        "Hundreds to thousands of manually engineered features were siloed within teams"
      ],
      "first_principle_insight": "A single-task approach is fundamentally misaligned with the needs of a superapp. A unified model could capture complex and diverse interactions between users, merchants, and drivers across all verticals rather than optimizing for one objective.",
      "how_to_reframe": {
        "old_atomic_unit": "Task-specific feature set engineered per downstream application",
        "new_atomic_unit": "Unified user embedding capturing cross-vertical behavior patterns"
      },
      "reframe": {
        "new_atomic_unit": "Pre-trained user foundation model with dual embeddings",
        "new_problem_type": "Task-specific supervised learning \u2192 Unsupervised pre-training with transfer",
        "new_objective": "Learn universal user representations that transfer across all downstream tasks",
        "architectural_changes": [
          {
            "change": "Unified tokenization",
            "description": "key:value pairs handling both tabular and sequential data"
          },
          {
            "change": "Modality-specific adapters",
            "description": "For text, IDs, locations, and numbers"
          },
          {
            "change": "Unsupervised pre-training",
            "description": "Masked language modeling and dual next-action prediction across all services"
          },
          {
            "change": "Dual embeddings",
            "description": "Capturing both long-term identity and short-term intent"
          }
        ],
        "results": "Pre-trained embeddings now power multiple downstream systems (ad optimization, fraud detection, churn prediction) without task-specific retraining, accelerating development across the organization"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Grab Engineering Blog (2025): User foundation models for Grab",
          "url": "https://engineering.grab.com/user-foundation-models-for-grab",
          "note": "Technical details on Grab's user foundation model architecture"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Recognizing foundation models as alternative to task-specific ML",
        "common_mistake_avoided": "Building siloed task-specific models when cross-task patterns exist",
        "when_to_use": "When multiple downstream tasks share underlying user/entity patterns"
      },
      "tags": [
        "foundation-models",
        "user-modeling",
        "superapp",
        "transfer-learning"
      ]
    },
    {
      "id": "meta-gem-foundation-ads-2025",
      "title": "Meta: From Isolated Verticals to Unified Ads Foundation Model (GEM)",
      "category": "Architectural Pivot - Multi-Model Architecture",
      "companies_involved": [
        "Meta"
      ],
      "year": 2025,
      "initial_problem": "Scaling ads recommendation across Meta's surfaces (Facebook, Instagram) while capturing cross-platform insights",
      "initial_assumptions": [
        "Different Meta surfaces can be treated either in isolation or identically",
        "Individual vertical models operating separately is sufficient",
        "Cross-platform learning is not critical for ad performance"
      ],
      "why_it_fails": [
        "Separate models couldn't leverage learnings across platforms",
        "Treating all surfaces identically ignored their unique characteristics",
        "Sparse conversion signals and diverse data types made scaling difficult",
        "Fragmented approach limited overall performance"
      ],
      "first_principle_insight": "A single large foundation model trained across the entire ecosystem could learn universal patterns while maintaining surface-specific optimization\u2014creating 'the central brain' of ads recommendation.",
      "how_to_reframe": {
        "old_atomic_unit": "Independent vertical models per surface",
        "new_atomic_unit": "Unified foundation model with multi-domain learning"
      },
      "reframe": {
        "new_atomic_unit": "GEM (Generative Ads Model) - LLM-scale foundation model for ads",
        "new_problem_type": "Isolated vertical optimization \u2192 Cross-surface foundation learning",
        "new_objective": "Learn universal ad recommendation patterns while maintaining surface-specific optimization",
        "architectural_changes": [
          {
            "change": "Multi-Domain Learning",
            "description": "Domain-specific optimization within unified architecture for cross-surface learning"
          },
          {
            "change": "Novel attention mechanisms",
            "description": "For both sequence and non-sequence features"
          },
          {
            "change": "Hierarchical knowledge transfer",
            "description": "Foundation model distills knowledge to downstream models"
          }
        ],
        "results": "5% increase in Instagram ad conversions; 3% increase in Facebook Feed conversions; 4x efficiency improvement in performance gains per compute unit; 2x effectiveness over standard knowledge distillation techniques"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Meta Engineering Blog (2025): Meta's Generative Ads Model (GEM): The Central Brain Accelerating Ads Recommendation AI Innovation",
          "url": "https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/",
          "note": "Technical details on Meta's GEM foundation model for ads"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Recognizing that 'ads recommendation' spans multiple surfaces with shared patterns",
        "common_mistake_avoided": "Treating related problems as independent when they share underlying structure",
        "when_to_use": "When multiple related ML tasks could benefit from shared foundation learning"
      },
      "tags": [
        "foundation-models",
        "ads",
        "recommendation",
        "multi-task"
      ]
    },
    {
      "id": "harvey-hybrid-evaluation-2025",
      "title": "Harvey: From Manual Expert Review to Hybrid Evaluation Systems",
      "category": "Architectural Pivot - Evaluation Reframe",
      "companies_involved": [
        "Harvey AI"
      ],
      "year": 2025,
      "initial_problem": "Scaling AI evaluation for legal applications where accuracy is critical",
      "initial_assumptions": [
        "Domain experts should manually review AI outputs for quality",
        "High-quality evaluation requires human expert judgment",
        "Manual review is the gold standard for legal AI evaluation"
      ],
      "why_it_fails": [
        "Data Scarcity: Expert capacity couldn't match the volume of test cases needed",
        "Feedback Latency: Discrete batch reviews slowed iteration cycles",
        "Fragmented Expertise: Different jurisdictions required specialized reviewers",
        "Regression Risks: Improvements in one area went unmonitored elsewhere"
      ],
      "first_principle_insight": "While expert-led reviews offer a high level of rigor, they face key limitations that demand complementary automated systems. The solution is to extend human feedback with continuous, data-driven evaluation rather than replace expertise entirely.",
      "how_to_reframe": {
        "old_atomic_unit": "Manual expert review of AI outputs",
        "new_atomic_unit": "Hybrid system combining automated pipelines with expert-informed auto-grading"
      },
      "reframe": {
        "new_atomic_unit": "Three-pillar hybrid evaluation system",
        "new_problem_type": "Manual evaluation \u2192 Automated + human hybrid",
        "new_objective": "Combine human rigor with automated scale for trustworthy AI in high-stakes work",
        "architectural_changes": [
          {
            "change": "Automated evaluation pipelines",
            "description": "Nightly canary tests before production deployment"
          },
          {
            "change": "Legal expertise-informed auto-grading",
            "description": "Achieving 95%+ accuracy on citation verification"
          },
          {
            "change": "Dedicated data infrastructure",
            "description": "Versioning, access controls, and immutable evaluation baselines"
          }
        ],
        "results": "GPT-4.1 vs GPT-4o comparison showed mean ratings improved 10% (5.10\u21925.63) with statistically significant median gains (5\u21926 on 7-point scale), enabling confident model stack decisions"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Harvey AI Blog (2025): Scaling AI Evaluation Through Expertise",
          "url": "https://www.harvey.ai/blog/scaling-ai-evaluation-through-expertise",
          "note": "Harvey's approach to scaling evaluation for legal AI"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - Building scalable evaluation infrastructure",
        "common_mistake_avoided": "Relying solely on manual expert review when automated signals can scale",
        "when_to_use": "When expert evaluation is gold standard but doesn't scale"
      },
      "tags": [
        "evaluation",
        "legal",
        "hybrid",
        "automation",
        "quality"
      ]
    },
    {
      "id": "salesforce-hero-agents-2025",
      "title": "Salesforce: From Agent Sprawl to Focused Hero Agents",
      "category": "Pragmatic Pivot - Infrastructure First",
      "companies_involved": [
        "Salesforce"
      ],
      "year": 2025,
      "initial_problem": "Deploying AI agents across the enterprise to automate business processes",
      "initial_assumptions": [
        "Building hundreds of agents simultaneously maximizes value",
        "Broad coverage across all departments accelerates transformation",
        "More agents equals more automation"
      ],
      "why_it_fails": [
        "Building hundreds of agents led to duplication, lack of adoption, and blurry results",
        "Rapid scaling without proof points created confusion rather than impact",
        "No clear ROI demonstration for individual agents"
      ],
      "first_principle_insight": "You have to earn the right to scale fast. Success requires demonstrating clear business value in specific domains before expanding\u2014focused impact beats enterprise-wide transformation.",
      "how_to_reframe": {
        "old_atomic_unit": "Many agents deployed horizontally across organization",
        "new_atomic_unit": "Hero agents with measurable ROI in targeted vertical areas"
      },
      "reframe": {
        "new_atomic_unit": "Focused hero agents with proven business impact",
        "new_problem_type": "Horizontal agent sprawl \u2192 Vertical depth with ROI proof",
        "new_objective": "Demonstrate clear value in specific domains before scaling",
        "architectural_changes": [
          {
            "change": "Vertical focus",
            "description": "Developed hero agents with measurable ROI in targeted areas (Data 360, Salesforce Help, Sales Engagement)"
          },
          {
            "change": "Continuous instrumentation",
            "description": "Built monitoring dashboards measuring speed, relevancy, user satisfaction, adoption"
          },
          {
            "change": "Unified data layer",
            "description": "Data 360 as foundational infrastructure ensuring agents access clean, governed data"
          }
        ],
        "results": "2+ million autonomous conversations via Agentforce Service; 75% of previously untouched leads receiving personalized outreach; Salesforce Help handling 2.2 million conversations; $100 million annualized cost savings from automated support"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Salesforce Blog (2025): 5 Lessons We Learned Building the World's First Agentic Enterprise",
          "url": "https://www.salesforce.com/blog/5-lessons-salesforce-agentic-enterprise/",
          "note": "Lessons from Salesforce's agentic enterprise transformation"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Trading breadth for depth to prove value",
        "common_mistake_avoided": "Scaling agents before proving ROI in focused areas",
        "when_to_use": "When deploying agents enterprise-wide without clear proof points"
      },
      "tags": [
        "agents",
        "enterprise",
        "ROI",
        "scaling",
        "strategy"
      ]
    },
    {
      "id": "nubank-joint-fusion-embeddings-2025",
      "title": "Nubank: From Late Fusion to Joint Fusion for Transaction Models",
      "category": "Architectural Pivot - Data Representation (Classification)",
      "companies_involved": [
        "Nubank"
      ],
      "year": 2025,
      "initial_problem": "Combining transaction embeddings with tabular features for fraud detection and user modeling",
      "initial_assumptions": [
        "Late fusion (combining pre-trained embeddings with tabular features using GBTs) is sufficient",
        "Embeddings can be optimized independently from tabular data",
        "Gradient Boosted Trees effectively combine different feature types"
      ],
      "why_it_fails": [
        "Late fusion is suboptimal because finetuned embeddings are learned separately from tabular features",
        "GBTs lack differentiability, preventing simultaneous optimization of both components",
        "Separate optimization misses mutual learning opportunities"
      ],
      "first_principle_insight": "Joint training could enable mutual learning: allowing the transformer to capture information absent in tabular features while both systems optimize together. This required moving from non-differentiable GBTs to differentiable deep neural networks.",
      "how_to_reframe": {
        "old_atomic_unit": "Pre-trained embeddings + tabular features \u2192 GBT (late fusion)",
        "new_atomic_unit": "Embeddings + tabular features \u2192 joint end-to-end training (joint fusion)"
      },
      "reframe": {
        "new_atomic_unit": "DCNv2 with joint embedding optimization",
        "new_problem_type": "Late fusion \u2192 Joint fusion with end-to-end training",
        "new_objective": "Enable mutual learning between transformer embeddings and tabular features",
        "architectural_changes": [
          {
            "change": "DCNv2 with numerical embeddings",
            "description": "Replaced GBTs with differentiable deep neural network for tabular processing"
          },
          {
            "change": "Low-dimensional projection",
            "description": "Project tabular embeddings to shared space"
          },
          {
            "change": "Concatenation and normalization",
            "description": "Combined with transformer embeddings with regularization (dropout, weight decay)"
          },
          {
            "change": "End-to-end MLP",
            "description": "Final predictions through jointly trained MLP"
          }
        ],
        "results": "Meaningful AUC improvements over baseline LightGBM models without adding new data sources\u2014purely through improved feature learning during end-to-end training"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Nubank Engineering Blog (2025): Fine-tuning transaction user models",
          "url": "https://building.nubank.com/fine-tuning-transaction-user-models/",
          "note": "Technical details on Nubank's joint fusion approach"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Recognizing joint training as alternative to late fusion",
        "common_mistake_avoided": "Assuming separate optimization of components is sufficient",
        "when_to_use": "When combining embeddings with tabular features and late fusion underperforms"
      },
      "tags": [
        "embeddings",
        "fusion",
        "fintech",
        "fraud-detection",
        "deep-learning"
      ]
    },
    {
      "id": "nubank-ai-search-deflection-2025",
      "title": "Nubank: From Manual Search to AI-Powered Self-Service",
      "category": "Architectural Pivot - Search Reframe",
      "companies_involved": [
        "Nubank"
      ],
      "year": 2025,
      "initial_problem": "9,000+ employees spending significant time searching for information across decentralized Confluence documentation",
      "initial_assumptions": [
        "Employees should navigate documentation to find information",
        "Support tickets are acceptable for information requests",
        "Information fragmentation is a challenge to manage"
      ],
      "why_it_fails": [
        "Employees spent significant time navigating dozens of pages",
        "Support tickets consumed resources and time (8 hours average)",
        "Traditional navigation couldn't scale with growing documentation"
      ],
      "first_principle_insight": "Rather than viewing information fragmentation as merely a challenge, reframe it as an opportunity: how could we streamline access to information at scale, while still empowering teams to own their content? Shift from push (employees hunting) to pull (AI delivers answers).",
      "how_to_reframe": {
        "old_atomic_unit": "Employee navigates documentation or opens support ticket",
        "new_atomic_unit": "AI delivers answer through familiar Slack interface"
      },
      "reframe": {
        "new_atomic_unit": "Two-stage RAG with department routing",
        "new_problem_type": "Manual search \u2192 AI-powered self-service",
        "new_objective": "Instant answers through familiar interface while maintaining content ownership",
        "architectural_changes": [
          {
            "change": "Dynamic Few-Shot Classification",
            "description": "Route queries to correct departments"
          },
          {
            "change": "LLM-powered answer generation",
            "description": "Using only relevant departmental docs"
          },
          {
            "change": "Department isolation",
            "description": "Prevents mixing answers from different domain documentation"
          },
          {
            "change": "Slack integration",
            "description": "Familiar interface for all employees"
          }
        ],
        "results": "96% ticket deflection on internal domains; 280K user messages in 6 months; 9 seconds for answers (vs 30 minutes manual search or 8 hours via tickets); 80% positive feedback on answer quality"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Nubank Engineering Blog (2025): AI Solution for Search",
          "url": "https://building.nubank.com/ai-solution-for-search/",
          "note": "Nubank's internal AI search assistant implementation"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) - Reframing from 'better search' to 'instant answers'",
        "common_mistake_avoided": "Improving search navigation instead of eliminating the need to search",
        "when_to_use": "When employees waste time searching fragmented documentation"
      },
      "tags": [
        "RAG",
        "search",
        "internal-tools",
        "productivity",
        "fintech"
      ]
    },
    {
      "id": "slack-security-multi-persona-agents-2025",
      "title": "Slack: From Monolithic Prompt to Multi-Persona Security Agents",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Slack"
      ],
      "year": 2025,
      "initial_problem": "Automating security investigations that require complex multi-step analysis",
      "initial_assumptions": [
        "A single comprehensive prompt with multiple sections can guide investigation",
        "Prompt engineering can achieve fine-grained control over agent behavior",
        "One model call with elaborate prompting is sufficient"
      ],
      "why_it_fails": [
        "Performance was highly inconsistent despite prompt refinements",
        "System would sometimes quickly jump to convenient or spurious conclusions without questioning methods",
        "Prompts are just guidelines - not effective for fine-grained control",
        "Mixed success from emphasizing verification and questioning assumptions"
      ],
      "first_principle_insight": "Complex investigation processes need structural control, not just better guidance. The solution is to shift from treating investigation as a single task to decomposing it into orchestrated sequential tasks with different personas.",
      "how_to_reframe": {
        "old_atomic_unit": "Single 300-word prompt with five sections guiding one model call",
        "new_atomic_unit": "Orchestrated multi-persona agents with separate invocations per task"
      },
      "reframe": {
        "new_atomic_unit": "Multi-persona agent system with phased investigation",
        "new_problem_type": "Single elaborate prompt \u2192 Decomposed multi-agent orchestration",
        "new_objective": "Achieve consistent performance through structural control rather than prompt refinement",
        "architectural_changes": [
          {
            "change": "Separate model invocations",
            "description": "Each defined task gets structured outputs"
          },
          {
            "change": "Multi-persona design",
            "description": "Director (orchestrator), Experts (domain-specific), Critic (quality reviewer)"
          },
          {
            "change": "Investigation phases",
            "description": "Discovery, Director Decision, Trace, Conclude"
          },
          {
            "change": "Knowledge pyramid",
            "description": "Low-cost models for data gathering, high-cost models for synthesis"
          }
        ],
        "results": "Consistent performance; unprompted discoveries (identifying credential exposures experts missed); better observability through dashboard for real-time investigation supervision"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Slack Engineering Blog (2025): Streamlining Security Investigations with Agents",
          "url": "https://slack.engineering/streamlining-security-investigations-with-agents/",
          "note": "Slack's multi-persona agent approach to security investigations"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Breaking investigation into distinct phases and personas",
        "common_mistake_avoided": "Trying to solve complex tasks with elaborate single prompts",
        "when_to_use": "When single prompts produce inconsistent results on complex multi-step tasks"
      },
      "tags": [
        "agents",
        "security",
        "multi-persona",
        "orchestration",
        "investigation"
      ]
    },
    {
      "id": "meta-llm-mutation-testing-2025",
      "title": "Meta: From Test Assessment to LLM-Powered Test Generation",
      "category": "Architectural Pivot - Code Generation",
      "companies_involved": [
        "Meta"
      ],
      "year": 2025,
      "initial_problem": "Ensuring compliance and test quality for privacy-related code at Meta's scale",
      "initial_assumptions": [
        "Mutation testing is primarily a way of assessing test quality",
        "Traditional mutation testing approaches are the only option despite known scalability issues",
        "Human engineers must construct compliance tests manually"
      ],
      "why_it_fails": [
        "Traditional mutation testing wasn't scalable",
        "Generated unrealistic mutants that wasted resources",
        "Produced equivalent mutants that couldn't be killed",
        "Required massive computational overhead",
        "Could overstretch testing efforts by focusing on low-impact faults"
      ],
      "first_principle_insight": "By leveraging generative AI, mutation testing can be reversed: instead of assessing whether tests catch mutants, generate relevant tests from domain-specific mutants. Combine LLMs with mutation testing rather than replacing traditional testing.",
      "how_to_reframe": {
        "old_atomic_unit": "Mutation testing as test quality assessment",
        "new_atomic_unit": "Mutation testing as test generation engine powered by LLMs"
      },
      "reframe": {
        "new_atomic_unit": "LLM-powered test generation from domain-specific mutants",
        "new_problem_type": "Test assessment \u2192 Test generation",
        "new_objective": "Generate compliance tests automatically while shifting burden from test construction to test evaluation",
        "architectural_changes": [
          {
            "change": "LLM-generated mutants",
            "description": "Generate fewer, highly-relevant mutants from plain-text specifications"
          },
          {
            "change": "Auto-generated tests",
            "description": "Tests guaranteed to catch the generated mutants"
          },
          {
            "change": "LLM equivalence detector",
            "description": "Filter out unkillable mutants automatically"
          }
        ],
        "results": "Engineers accepted 73% of generated tests during Oct-Dec 2024 trials, with 36% rated as directly privacy-relevant"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Meta Engineering Blog (2025): LLMs Are the Key to Mutation Testing and Better Compliance",
          "url": "https://engineering.fb.com/2025/09/30/security/llms-are-the-key-to-mutation-testing-and-better-compliance/",
          "note": "Meta's Automated Compliance Hardening tool"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Reversing the purpose of mutation testing",
        "common_mistake_avoided": "Using AI to optimize existing process instead of reimagining it",
        "when_to_use": "When an assessment tool could be inverted into a generation tool"
      },
      "tags": [
        "testing",
        "compliance",
        "code-generation",
        "mutation-testing"
      ]
    },
    {
      "id": "shopify-rankflow-dsl-2025",
      "title": "Shopify: Bridging ML Flexibility and C++ Performance with DSL",
      "category": "Architectural Pivot - Infrastructure First",
      "companies_involved": [
        "Shopify"
      ],
      "year": 2025,
      "initial_problem": "Building world-class product search that allows rapid ML experimentation without sacrificing latency",
      "initial_assumptions": [
        "Must choose between C++ performance and Python flexibility",
        "Pure C++ systems lack flexibility for rapid ML iteration",
        "Python/Java solutions introduce unacceptable latency overhead"
      ],
      "why_it_fails": [
        "Traditional approaches force strict tradeoffs between speed and flexibility",
        "Pure C++ system is slow to adapt to new ideas",
        "Python/Java solutions introduced unacceptable latency overhead and memory inefficiency at Shopify's query volume"
      ],
      "first_principle_insight": "Rather than accept the either/or choice, create a domain-specific language matching Python's simplicity while compiling to C++ performance. Abstraction can bridge the gap.",
      "how_to_reframe": {
        "old_atomic_unit": "Choose between Python flexibility OR C++ performance",
        "new_atomic_unit": "DSL that provides Python-like syntax compiling to C++ execution"
      },
      "reframe": {
        "new_atomic_unit": "RankFlow DSL with TurboDSL execution engine",
        "new_problem_type": "Binary choice \u2192 Abstraction bridge",
        "new_objective": "Enable data scientists to write Python-like code that compiles to optimized C++",
        "architectural_changes": [
          {
            "change": "RankFlow DSL",
            "description": "Domain-specific language matching Python's simplicity"
          },
          {
            "change": "TurboDSL execution engine",
            "description": "Compiles DSL to optimized C++ automatically"
          },
          {
            "change": "Automatic performance analysis",
            "description": "Every PR gets statistical significance testing with \u00b12% tolerance"
          }
        ],
        "results": "48% speedup in ranking feature computation; eliminated deployment complexity and version skew; enables shipping ML innovations without sacrificing millisecond latency"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Shopify Engineering Blog (2025): Building world-class product search at Shopify",
          "url": "https://shopify.engineering/world-class-product-search",
          "note": "Technical details on RankFlow and TurboDSL"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Finding a third option when presented with binary choice",
        "common_mistake_avoided": "Accepting false dichotomies in system design",
        "when_to_use": "When facing performance vs flexibility tradeoffs in ML systems"
      },
      "tags": [
        "search",
        "DSL",
        "performance",
        "ML-infrastructure"
      ]
    },
    {
      "id": "wayfair-hybrid-agent-workflow-2025",
      "title": "Wayfair: From Pure Agents to Hybrid Workflow with Embedded Agency",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Wayfair"
      ],
      "year": 2025,
      "initial_problem": "Automating supplier ticket management with AI agents",
      "initial_assumptions": [
        "Fully agentic approach with specialized agents coordinated by supervisor would work best",
        "Agent architecture is simple to design and easy to extend",
        "Agents can reliably coordinate through a supervisor"
      ],
      "why_it_fails": [
        "Agents ignored information passed to them",
        "Supervisor made unnecessary repeated calls",
        "One agent used its judgment to incorrectly decide it did not need to call the JIRA API",
        "Communication breakdowns reduced reliability and increased costs through excess LLM calls"
      ],
      "first_principle_insight": "Neither pure agentic nor purely deterministic workflows are optimal. Fully agentic systems lack discipline; purely deterministic workflows can't handle edge cases. The solution is a hybrid: deterministic workflow with embedded agency for specific tasks requiring judgment.",
      "how_to_reframe": {
        "old_atomic_unit": "Specialized agents coordinated by supervisor agent",
        "new_atomic_unit": "Deterministic workflow with embedded ReAct agent for specific tasks"
      },
      "reframe": {
        "new_atomic_unit": "Hybrid workflow with embedded agency",
        "new_problem_type": "Pure agent orchestration \u2192 Deterministic workflow + targeted agency",
        "new_objective": "Maintain strict operational sequencing while allowing agent flexibility for tasks requiring judgment",
        "architectural_changes": [
          {
            "change": "Deterministic workflow orchestration",
            "description": "Overall process controlled by deterministic logic"
          },
          {
            "change": "Embedded ReAct agent",
            "description": "Specifically for supplier ID lookup with BigQuery access and retry strategies"
          },
          {
            "change": "Task-specific agency",
            "description": "Agent can try different querying strategies for edge cases like typos"
          }
        ],
        "results": "93% question type accuracy (vs 75% human baseline); 98% language identification accuracy; 88% supplier ID accuracy; reduced processing time and costs"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Wayfair Tech Blog (2025): Automating Supplier Ticket Management with LLM Agents: Lessons from the Field",
          "url": "https://www.aboutwayfair.com/careers/tech-blog/automating-supplier-ticket-management-with-llm-agents-lessons-from-the-field",
          "note": "Lessons from Wayfair's hybrid agent architecture"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Finding middle ground between fully agentic and deterministic",
        "common_mistake_avoided": "Going fully agentic when only specific tasks need agent flexibility",
        "when_to_use": "When pure agent architectures show coordination failures"
      },
      "tags": [
        "agents",
        "workflow",
        "hybrid",
        "e-commerce",
        "automation"
      ]
    },
    {
      "id": "wayfair-llm-style-labeling-2025",
      "title": "Wayfair: From Manual Annotation to LLM-Powered Style Labeling",
      "category": "Architectural Pivot - GenAI for Data Enrichment",
      "companies_involved": [
        "Wayfair"
      ],
      "year": 2025,
      "initial_problem": "Labeling style compatibility across tens of millions of SKUs for personalized recommendations",
      "initial_assumptions": [
        "Manual human annotation is required for accurate style labels",
        "Complex approaches like RLHF or DPO are needed for LLM-based labeling",
        "Style compatibility requires extensive human expertise"
      ],
      "why_it_fails": [
        "Human annotation is slow and costly",
        "Manual approach couldn't scale to tens of millions of SKUs",
        "Existing recommendations relied on click signals that missed subjective style compatibility"
      ],
      "first_principle_insight": "Prompt engineering could replace labor-intensive annotation. Anchor LLM reasoning in concrete design principles (shape, material, color harmony, scale) rather than generic descriptions. Few-shot examples from real catalog scenarios outperform lengthy rule lists.",
      "how_to_reframe": {
        "old_atomic_unit": "Human expert annotating style compatibility",
        "new_atomic_unit": "Multimodal LLM processing product images + metadata with design principles"
      },
      "reframe": {
        "new_atomic_unit": "LLM-powered style labeling with concrete design principles",
        "new_problem_type": "Manual annotation \u2192 Automated multimodal labeling",
        "new_objective": "Generate style compatibility labels at scale through prompt engineering anchored in design principles",
        "architectural_changes": [
          {
            "change": "Multimodal LLM (Gemini 2.5 Pro)",
            "description": "Processing product images and metadata"
          },
          {
            "change": "Design principle anchoring",
            "description": "Concrete principles (shape, material, color harmony, scale) guide reasoning"
          },
          {
            "change": "Few-shot examples",
            "description": "Real catalog scenarios outperform lengthy rule lists"
          },
          {
            "change": "Scalable batch pipeline",
            "description": "Google Cloud infrastructure for automated annotation"
          }
        ],
        "results": "11% improvement in annotation accuracy through iterative prompt refinement; style-aware personalization at scale; labels generated orders of magnitude faster than manual work"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Wayfair Tech Blog (2025): Teaching Wayfair's Catalog to 'See' Style: An LLM-Powered Style Compatibility Labeling Pipeline",
          "url": "https://www.aboutwayfair.com/careers/tech-blog/teaching-wayfairs-catalog-to-see-style-an-llm-powered-style-compatibility-labeling-pipeline-on-google-cloud",
          "note": "Technical details on Wayfair's style labeling pipeline"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Prompt engineering as alternative to complex fine-tuning",
        "common_mistake_avoided": "Assuming complex approaches (RLHF/DPO) are needed when prompt engineering suffices",
        "when_to_use": "When manual labeling bottlenecks can be replaced by LLM + domain principles"
      },
      "tags": [
        "labeling",
        "multimodal",
        "style",
        "e-commerce",
        "prompt-engineering"
      ]
    },
    {
      "id": "target-llm-accessory-recommendations-2025",
      "title": "Target: From Manual Attribute Weighting to LLM-Automated Discovery",
      "category": "Architectural Pivot - GenAI for Data Enrichment",
      "companies_involved": [
        "Target"
      ],
      "year": 2025,
      "initial_problem": "Identifying which product attributes matter most for pairing accessories with core items across massive catalog",
      "initial_assumptions": [
        "Human experts must evaluate attribute importance across each category",
        "Manual attribute weighting is necessary for quality recommendations",
        "Simple attribute matching is sufficient for accessory pairing"
      ],
      "why_it_fails": [
        "Manual expert evaluation of all attributes across each category was impractical at scale",
        "Simple attribute matching missed stylistic and aesthetic coherence",
        "Cross-category recommendations required extensive merchant curation"
      ],
      "first_principle_insight": "LLMs are quite good at using concepts like color harmony and stylistic coherence beyond simple attribute matching. Automate attribute importance discovery while preserving human merchant insight for curation.",
      "how_to_reframe": {
        "old_atomic_unit": "Human experts manually defining attribute weights",
        "new_atomic_unit": "LLM analyzing product data and assigning weights automatically"
      },
      "reframe": {
        "new_atomic_unit": "LLM-automated attribute discovery with aesthetic matching",
        "new_problem_type": "Manual attribute weighting \u2192 Automated discovery with human curation",
        "new_objective": "Leverage AI to automate attribute discovery while preserving human merchant insight",
        "architectural_changes": [
          {
            "change": "Attribute importance automation",
            "description": "LLMs analyze product data and assign weights automatically"
          },
          {
            "change": "Aesthetic matching",
            "description": "LLM-based scoring for visual/stylistic coherence beyond attribute matching"
          },
          {
            "change": "Type-level scaling",
            "description": "Algorithm works on item types rather than individual items"
          },
          {
            "change": "Human-in-loop curation",
            "description": "Merchants curate co-purchase lists for cross-category recommendations"
          }
        ],
        "results": "11% increase in interaction rate; 12% increase in display-to-conversion rates; 9%+ growth in attributable demand; full production rollout April 2025"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Target Tech Blog (2025): Improving Accessory Recommendations with LLMs at Target",
          "url": "https://tech.target.com/blog/accessory-recommendations-with-llms",
          "note": "Target's LLM-powered accessory recommendation system"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) - Reframing from 'define attributes' to 'discover attributes'",
        "common_mistake_avoided": "Trying to manually scale what can be automated",
        "when_to_use": "When expert knowledge bottlenecks can be augmented by LLM pattern recognition"
      },
      "tags": [
        "recommendations",
        "retail",
        "LLM",
        "attribute-discovery"
      ]
    },
    {
      "id": "meta-instagram-diversity-ranking-2025",
      "title": "Meta: From Engagement Optimization to Diversity-Aware Notification Ranking",
      "category": "Optimization Pivot",
      "companies_involved": [
        "Meta"
      ],
      "year": 2025,
      "initial_problem": "Ranking Instagram notifications to maximize user engagement",
      "initial_assumptions": [
        "Maximizing engagement metrics like CTR is the right objective",
        "ML models should prioritize content users previously interacted with most",
        "More relevant notifications equals better user experience"
      ],
      "why_it_fails": [
        "Engagement-first approach created uniformity trap",
        "System caused overexposure to same creators or product types",
        "Overlooked valuable and diverse experiences",
        "Users received repetitive notifications that felt spammy",
        "Led users to disable notifications entirely - opposite of desired outcome"
      ],
      "first_principle_insight": "Personalization and diversity aren't opposing forces - they're complementary. Relevant notifications must also be varied to maintain long-term user satisfaction. Short-term engagement optimization undermines long-term engagement.",
      "how_to_reframe": {
        "old_atomic_unit": "Notification ranked by predicted engagement (CTR)",
        "new_atomic_unit": "Notification ranked by engagement \u00d7 diversity penalty factors"
      },
      "reframe": {
        "new_atomic_unit": "Diversity-aware ranking with multiplicative penalties",
        "new_problem_type": "Engagement optimization \u2192 Engagement + diversity co-optimization",
        "new_objective": "Balance relevance with variation to maintain long-term user satisfaction",
        "architectural_changes": [
          {
            "change": "Diversity-aware framework layer",
            "description": "Layered on top of engagement model rather than replacing it"
          },
          {
            "change": "Multiplicative penalty factors",
            "description": "Penalize candidates similar to recently sent notifications"
          },
          {
            "change": "Multi-dimensional diversity",
            "description": "Penalties across author, product type, and content dimensions"
          }
        ],
        "results": "Significantly reduced daily notification volume while improving CTR; users got fewer, more diverse, and more engaging notifications"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Meta Engineering Blog (2025): A New Ranking Framework for Better Notification Quality on Instagram",
          "url": "https://engineering.fb.com/2025/09/02/ml-applications/a-new-ranking-framework-for-better-notification-quality-on-instagram/",
          "note": "Instagram's diversity-aware notification ranking"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) - Recognizing that short-term metric optimization undermines long-term goals",
        "common_mistake_avoided": "Optimizing for immediate engagement at expense of long-term satisfaction",
        "when_to_use": "When engagement optimization leads to repetitive or spammy experiences"
      },
      "tags": [
        "ranking",
        "notifications",
        "diversity",
        "engagement",
        "social"
      ]
    },
    {
      "id": "uber-ureview-precision-2025",
      "title": "Uber: From Volume to Precision in AI Code Review",
      "category": "Pragmatic Pivot",
      "companies_involved": [
        "Uber"
      ],
      "year": 2025,
      "initial_problem": "Building an AI-powered code review system that developers trust and use",
      "initial_assumptions": [
        "Catching more issues is better",
        "Comprehensive coverage maximizes value",
        "Volume of suggestions demonstrates capability"
      ],
      "why_it_fails": [
        "Simple standalone prompts resulted in many false-positive comments",
        "High-volume suggestions caused developers to dismiss AI feedback as noise",
        "Classic 'cry wolf' problem where quantity destroyed utility",
        "Engineers learned to ignore AI suggestions entirely"
      ],
      "first_principle_insight": "Comment quality matters far more than quantity. Developers quickly lose confidence in a tool that generates low-quality or irrelevant suggestions. Trustworthiness through precision beats comprehensiveness through volume.",
      "how_to_reframe": {
        "old_atomic_unit": "Maximize issues detected",
        "new_atomic_unit": "Maximize useful issues addressed"
      },
      "reframe": {
        "new_atomic_unit": "Precision-first code review with multi-stage filtering",
        "new_problem_type": "Maximize detection \u2192 Maximize trust through precision",
        "new_objective": "Build developer confidence through high-precision, actionable suggestions",
        "architectural_changes": [
          {
            "change": "Confidence scoring",
            "description": "Filter low-confidence suggestions"
          },
          {
            "change": "Semantic deduplication",
            "description": "Remove redundant comments"
          },
          {
            "change": "Category-based suppression",
            "description": "Disable comment types based on developer feedback patterns"
          },
          {
            "change": "Selective comment types",
            "description": "Only surface categories developers find valuable"
          }
        ],
        "results": "75% of comments marked useful (vs 51% for human reviewers); 65% of posted comments actually addressed by engineers"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog (2025): uReview: Scalable, Trustworthy GenAI for Code Review at Uber",
          "url": "https://www.uber.com/en-GB/blog/ureview/",
          "note": "Uber's precision-first approach to AI code review"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) - Reframing success metric from coverage to trust",
        "common_mistake_avoided": "Optimizing for detection volume when precision drives adoption",
        "when_to_use": "When AI tool adoption is blocked by low signal-to-noise ratio"
      },
      "tags": [
        "code-review",
        "precision",
        "developer-tools",
        "trust"
      ]
    },
    {
      "id": "uber-perfinsights-validation-2025",
      "title": "Uber: From Single-Shot LLM to Validated Detection Pipeline",
      "category": "Architectural Pivot - Agent Architecture",
      "companies_involved": [
        "Uber"
      ],
      "year": 2025,
      "initial_problem": "Detecting performance optimization opportunities in Go code at scale",
      "initial_assumptions": [
        "Single-shot LLM-based antipattern detection is sufficient",
        "Better prompting will improve detection quality",
        "Trusting one model's judgment is acceptable"
      ],
      "why_it_fails": [
        "Single-shot LLM detection produced inconsistent and unreliable results",
        "Responses varied between runs",
        "Included hallucinations and often generated non-runnable code",
        "Initial false positive rate exceeded 80%"
      ],
      "first_principle_insight": "Precision and developer trust require validation layers, not just smarter prompting. Ensemble approaches and domain-specific rule validators dramatically reduce hallucinations. Shift from 'better AI' to 'AI + validation pipeline'.",
      "how_to_reframe": {
        "old_atomic_unit": "Single LLM call detecting performance issues",
        "new_atomic_unit": "Multi-model validation pipeline with domain-specific rules"
      },
      "reframe": {
        "new_atomic_unit": "LLM juries + rule validators + profiling-first filtering",
        "new_problem_type": "Single-shot detection \u2192 Validated detection through redundancy",
        "new_objective": "Build trustworthy detection through verification rather than hoping for perfect detection",
        "architectural_changes": [
          {
            "change": "LLM juries",
            "description": "Multiple independent models validating each detection"
          },
          {
            "change": "LLMCheck framework",
            "description": "Domain-specific rule-based validators catching false positives"
          },
          {
            "change": "Profiling-first filtering",
            "description": "CPU/memory profiles narrow analysis to top 30 functions before AI processing"
          }
        ],
        "results": "False positives dropped from over 80% to low teens; detection time reduced from days to hours; generated hundreds of merged diffs into production; ~93% engineering time savings per optimization cycle"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog (2025): PerfInsights: Detecting Performance Optimization Opportunities in Go Code using Generative AI",
          "url": "https://www.uber.com/en-GB/blog/perfinsights/",
          "note": "Uber's validated approach to AI-powered performance detection"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - Building verification that catches AI failures",
        "common_mistake_avoided": "Trying to improve single model instead of building validation layers",
        "when_to_use": "When single-shot LLM produces inconsistent results requiring trust"
      },
      "tags": [
        "performance",
        "validation",
        "LLM-juries",
        "code-analysis"
      ]
    },
    {
      "id": "hubspot-prediction-platform-2025",
      "title": "HubSpot: From Isolated Pipelines to Object-Agnostic Prediction Platform",
      "category": "Architectural Pivot - Infrastructure First",
      "companies_involved": [
        "HubSpot"
      ],
      "year": 2025,
      "initial_problem": "Scoring millions of CRM objects daily across different prediction use cases",
      "initial_assumptions": [
        "Each predictive use case needs its own isolated inference pipeline",
        "Separate implementations are necessary for contact scoring, deal scoring, etc.",
        "Teams should build their own scalability solutions"
      ],
      "why_it_fails": [
        "Fragmented approach meant re-solving scalability challenges already addressed by other teams",
        "Created redundant engineering work",
        "Inconsistent solutions for handling high CRUD volume and latency requirements"
      ],
      "first_principle_insight": "A consistent data structure behind CRM and similar inference pipeline across use cases reveals an opportunity for abstraction rather than duplication. Build once, serve all scoring use cases.",
      "how_to_reframe": {
        "old_atomic_unit": "Isolated inference pipeline per use case",
        "new_atomic_unit": "Object-agnostic platform serving all scoring use cases"
      },
      "reframe": {
        "new_atomic_unit": "Prediction Engine as standardized ML inference platform",
        "new_problem_type": "Duplicate pipelines \u2192 Shared platform abstraction",
        "new_objective": "Standardize ML inference at scale through shared, modular components",
        "architectural_changes": [
          {
            "change": "Object-agnostic platform",
            "description": "Single platform serving all CRM object types"
          },
          {
            "change": "Modular components",
            "description": "Update listeners, debouncers, scoring workers, feedback managers"
          },
          {
            "change": "Delta thresholding",
            "description": "Only update when scores change meaningfully"
          },
          {
            "change": "Explanation-splitting",
            "description": "Separate heavy explanation computation from scoring"
          }
        ],
        "results": "78% reduction in upstream service load; 44% reduction in contact scoring request volume; 57% faster offline inferencing; 22% fewer CRM updates through delta thresholding"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "HubSpot Product Blog (2025): Behind HubSpot AI: How Does Prediction Engine Score Millions of CRM Objects Daily?",
          "url": "https://product.hubspot.com/blog/behind-hubspot-ai-how-does-prediction-engine-score-millions-of-crm-objects-daily",
          "note": "HubSpot's platform approach to ML inference"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Recognizing shared patterns across seemingly different use cases",
        "common_mistake_avoided": "Building separate solutions when shared infrastructure would serve all",
        "when_to_use": "When multiple teams solve similar ML infrastructure problems independently"
      },
      "tags": [
        "platform",
        "inference",
        "CRM",
        "infrastructure",
        "scalability"
      ]
    },
    {
      "id": "vinted-dense-retrieval-vespa-2025",
      "title": "Vinted: From Application-Layer Workarounds to Vespa-Native Dense Retrieval",
      "category": "Architectural Pivot - Search Reframe",
      "companies_involved": [
        "Vinted"
      ],
      "year": 2025,
      "initial_problem": "Integrating dense retrieval (semantic search) with lexical search at scale for fashion marketplace",
      "initial_assumptions": [
        "Dense retrieval is a filler for low-recall scenarios",
        "Application-layer logic can manage hybrid search complexity",
        "Controlling retrieval behavior from application code is acceptable"
      ],
      "why_it_fails": [
        "Full integration across all search sessions produced inconsistent results",
        "Query like 'zx750' with filters returned ~7,000 near-neighbor matches alongside ~300 lexical ones",
        "Exact vs approximate nearest-neighbor switching introduced unpredictable behavior",
        "Filters sometimes increased total results rather than narrowing them",
        "Application-layer workarounds would make codebase toxic"
      ],
      "first_principle_insight": "Pushing complexity into the search engine itself is preferable to application-layer workarounds. Master Vespa's ranking system rather than build hacky orchestration logic. Two requests introducing application complexity is unattractive.",
      "how_to_reframe": {
        "old_atomic_unit": "Application-layer control over dense vs lexical retrieval",
        "new_atomic_unit": "Vespa-native ranking with reciprocal rank fusion"
      },
      "reframe": {
        "new_atomic_unit": "Search engine-native hybrid retrieval with sophisticated ranking",
        "new_problem_type": "Application orchestration \u2192 Engine-native solution",
        "new_objective": "Consistent hybrid retrieval behavior through engine capabilities rather than application logic",
        "architectural_changes": [
          {
            "change": "Global-phase reranking",
            "description": "Reciprocal rank fusion to limit neighbor matches"
          },
          {
            "change": "Boolean constant fields",
            "description": "Identify match-type provenance"
          },
          {
            "change": "Rank-score-drop-limit filtering",
            "description": "Surgically remove dense-retrieval-only matches"
          },
          {
            "change": "Custom JVM optimization",
            "description": "GraalVM + ZGC to handle increased reranking overhead"
          }
        ],
        "results": "<0.02% error rate on billion-scale dataset; eliminated timeouts through retry strategies; tight latency budgets maintained with consistency guarantees"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Vinted Engineering Blog (2025): Dense Retrieval",
          "url": "https://vinted.engineering/2025/11/18/dense-retrieval/",
          "note": "Vinted's journey to Vespa-native dense retrieval"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Choosing engine complexity over application complexity",
        "common_mistake_avoided": "Building application-layer workarounds instead of mastering platform capabilities",
        "when_to_use": "When hybrid search produces inconsistent behavior from application-layer orchestration"
      },
      "tags": [
        "search",
        "dense-retrieval",
        "Vespa",
        "hybrid-search",
        "e-commerce"
      ]
    },
    {
      "id": "zalando-postmortem-llm-pipeline-2025",
      "title": "Zalando: From Manual Postmortem Analysis to Multi-Stage LLM Pipeline",
      "category": "Architectural Pivot - GenAI as Data Infrastructure",
      "companies_involved": [
        "Zalando"
      ],
      "year": 2025,
      "initial_problem": "Analyzing hundreds of postmortem documents to identify patterns and inform infrastructure investments",
      "initial_assumptions": [
        "Human-centric postmortem analysis is the gold standard",
        "Large context window LLMs can process entire document sets",
        "Single powerful model with massive context is sufficient"
      ],
      "why_it_fails": [
        "Individual reviewers spent 15-20 minutes per document creating cognitive bottleneck",
        "Strategic questions like 'Why datastores fail most frequently at scale?' became impossible to answer quickly",
        "Initial LLM attempts (NotebookLM, 3B-12B models) produced up to 40% hallucination rate",
        "Lost-in-the-middle effect: large context windows overlooked mid-document details",
        "Surface attribution errors: models made causal claims based on keyword proximity alone"
      ],
      "first_principle_insight": "Specialization and constraint reduce hallucination better than scale. A multi-stage LLM pipeline with discrete stages proves more effective and reliable than single high-end LLMs with large context windows.",
      "how_to_reframe": {
        "old_atomic_unit": "Human reviewer analyzing full postmortem (15-20 min each)",
        "new_atomic_unit": "Multi-stage pipeline: summarize \u2192 classify \u2192 analyze \u2192 aggregate patterns"
      },
      "reframe": {
        "new_atomic_unit": "Map-fold architecture with four discrete LLM stages",
        "new_problem_type": "Manual analysis \u2192 Staged LLM pipeline with human validation",
        "new_objective": "Surface hidden patterns through constrained, specialized stages rather than single powerful model",
        "architectural_changes": [
          {
            "change": "Summarization stage",
            "description": "Extract 5 key dimensions with strict constraints"
          },
          {
            "change": "Classification stage",
            "description": "Identify direct technology connections only"
          },
          {
            "change": "Analyzer stage",
            "description": "Produce 3-5 sentence digests grounded in source data"
          },
          {
            "change": "Patterns stage",
            "description": "Aggregate findings into actionable themes"
          },
          {
            "change": "Human curation at each stage",
            "description": "Validate outputs before proceeding"
          }
        ],
        "results": "Processing time from days to under 24 hours for annual analysis; hidden patterns surfaced (e.g., 80% ElastiCache CPU utilization causing latency); directly enabled infrastructure investments preventing 25% of subsequent datastore incidents"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Zalando Engineering Blog (2025): Dead Ends or Data Goldmines? Investment Insights from Two Years of AI-Powered Postmortem Analysis",
          "url": "https://engineering.zalando.com/posts/2025/09/dead-ends-or-data-goldmines-ai-powered-postmortem-analysis.html",
          "note": "Zalando's multi-stage LLM approach to postmortem analysis"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Breaking analysis into discrete, constrained stages",
        "common_mistake_avoided": "Assuming larger context windows solve analysis problems",
        "when_to_use": "When single-model approaches produce hallucinations on complex document analysis"
      },
      "tags": [
        "postmortem",
        "LLM-pipeline",
        "analysis",
        "infrastructure",
        "reliability"
      ]
    },
    {
      "id": "etsy-llm-attribute-extraction-2025",
      "title": "Etsy: From Traditional ML to LLM-Based Attribute Extraction",
      "category": "Architectural Pivot - GenAI for Data Enrichment",
      "companies_involved": [
        "Etsy"
      ],
      "year": 2025,
      "initial_problem": "Extracting structured product attributes from 100M+ unique handmade items where sellers mostly provide unstructured data (descriptions, photos, titles) and no global SKUs exist",
      "initial_assumptions": [
        "Supervised product attribute extraction models can capture the diversity of attributes",
        "Sequence tagging approaches can scale to multiple attributes",
        "Transformer-based QA models (AVEQA, MAVEQA) can generalize with enough training data",
        "Human annotation is necessary for ground truth evaluation"
      ],
      "why_it_fails": [
        "Supervised models had limited efficacy - couldn't enumerate all possible attributes and values",
        "Many attributes were so sparse that traditional classification struggled with the long tail",
        "Sequence tagging had difficulty scaling to multiple attributes",
        "Transformer QA models required large amounts of application-specific training data",
        "Human annotation was error-prone (annotators made mistakes on thousands of listings), time-consuming, and expensive"
      ],
      "first_principle_insight": "Foundational LLMs have vast general knowledge from pre-training, can process large context windows quickly and affordably, and can follow instructions given a small number of examples. This eliminates the need to enumerate all attributes or collect application-specific training data.",
      "how_to_reframe": {
        "old_atomic_unit": "Supervised model trained on enumerated attributes with application-specific data",
        "new_atomic_unit": "Foundational LLM with context engineering (listing data + few-shot examples + taxonomy + business rules)"
      },
      "reframe": {
        "new_atomic_unit": "LLM-based attribute extraction with context engineering",
        "new_problem_type": "Application-specific ML training \u2192 Foundation model with prompt/context engineering",
        "new_objective": "Extract attributes at scale using LLM's pre-trained knowledge with minimal application-specific training",
        "architectural_changes": [
          {
            "change": "Context engineering pipeline",
            "description": "Seller-provided listing data, few-shot examples from domain experts, business logic from taxonomy, category-specific rules"
          },
          {
            "change": "Silver label generation",
            "description": "High-performance LLMs generate ground truth labels instead of human annotation, with human-in-the-loop review"
          },
          {
            "change": "Scalable inference",
            "description": "JSON context \u2192 parallel prompts \u2192 LiteLLM routing across regions \u2192 Pydantic validation"
          },
          {
            "change": "Performance monitoring",
            "description": "Automated evaluation against ground truth before production runs, with alerting on deviation"
          }
        ],
        "results": "Attribute coverage increased from 31% to 91% in target categories; search filter engagement increased; post-click conversion rate increased; enabled color swatches on search results from LLM-inferred attributes"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Etsy Code as Craft Blog (2025): Understanding Etsy's Vast Inventory with LLMs",
          "url": "https://www.etsy.com/codeascraft/understanding-etsyas-vast-inventory-with-llms",
          "note": "By Vipul Setty, Noa Bendit-Shtull - October 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Foundation models as alternative to application-specific training",
        "common_mistake_avoided": "Trying to enumerate and train on all possible attributes instead of leveraging pre-trained knowledge",
        "when_to_use": "When attribute extraction requires handling long-tail, sparse categories that traditional ML can't capture"
      },
      "tags": [
        "attribute-extraction",
        "e-commerce",
        "LLM",
        "context-engineering",
        "marketplace"
      ]
    },
    {
      "id": "etsy-llm-buyer-profiles-2025",
      "title": "Etsy: Scaling LLM-Based Buyer Profiles for Personalization",
      "category": "Pragmatic Pivot - Infrastructure First",
      "companies_involved": [
        "Etsy"
      ],
      "year": 2025,
      "initial_problem": "Understanding nuanced interests of 90M buyers (styles, preferences, shopping missions) to personalize discovery across 100M+ listings",
      "initial_assumptions": [
        "Traditional search and recommendation systems can capture nuanced buyer interests",
        "LLM-based profile generation can run with full historical data (2 years)",
        "API endpoints are sufficient for retrieving user activity data",
        "Standard batch processing is adequate for 90M users"
      ],
      "why_it_fails": [
        "Traditional systems fall short of capturing nuanced interests like specific styles and aesthetic preferences",
        "Initial LLM approach would take 21 days for 10M users - prohibitively slow",
        "Initial cost was prohibitively expensive for large-scale personalization",
        "API endpoints created bottlenecks for efficient data retrieval"
      ],
      "first_principle_insight": "LLM-based personalization is feasible at scale, but requires aggressive optimization: reduce input tokens, shift data sources, parallelize processing, and use smaller models with optimized prompts. The 94% cost reduction came not from better models but from engineering the pipeline.",
      "how_to_reframe": {
        "old_atomic_unit": "Full historical user data processed sequentially via APIs",
        "new_atomic_unit": "Optimized 9-month window processed in parallel via BigQuery with smaller, prompt-optimized models"
      },
      "reframe": {
        "new_atomic_unit": "Scalable LLM pipeline with aggressive token and cost optimization",
        "new_problem_type": "Naive LLM usage \u2192 Optimized LLM infrastructure at scale",
        "new_objective": "Generate structured buyer profiles economically at 90M user scale",
        "architectural_changes": [
          {
            "change": "Data source optimization",
            "description": "Shifted from API endpoints to BigQuery tables clustered and partitioned for efficient querying"
          },
          {
            "change": "Token reduction",
            "description": "Reduced from 2 years to 9 months of session data, lighter prompt corrections"
          },
          {
            "change": "Parallel processing",
            "description": "Managed concurrency to avoid rate limits with increased batch sizes"
          },
          {
            "change": "Model cost optimization",
            "description": "Adjusted prompts for high quality results with smaller, cheaper models"
          },
          {
            "change": "Airflow orchestration",
            "description": "Batching and staggering tasks by user_id for parallelization"
          }
        ],
        "results": "Profile generation time reduced from 21 days to 3 days for 10M users; cost reduced by 94% per million users; enabled query rewriting ('cool posters' \u2192 'cool posters + hippie|boho|vintage|nature') and personalized refinement pills"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Etsy Code as Craft Blog (2025): Building Etsy Buyer Profiles with LLMs",
          "url": "https://www.etsy.com/codeascraft/building-etsy-buyer-profiles-with-llms",
          "note": "By Isobel Scott - September 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Trading data completeness for scalability and cost",
        "common_mistake_avoided": "Assuming LLM capabilities scale without infrastructure optimization",
        "when_to_use": "When LLM-based features are promising in prototype but prohibitively expensive at scale"
      },
      "tags": [
        "personalization",
        "buyer-profiles",
        "LLM",
        "scalability",
        "cost-optimization",
        "e-commerce"
      ]
    },
    {
      "id": "doordash-multi-agent-ecosystem-2025",
      "title": "DoorDash: From Single Agents to Collaborative AI Ecosystem",
      "category": "Architectural Pivot - Multi-Agent Systems",
      "companies_involved": [
        "DoorDash"
      ],
      "year": 2025,
      "initial_problem": "Knowledge at DoorDash is vast and distributed across experimentation platforms, metrics hubs, dashboards, wikis, and Slack. Answering complex business questions required significant context-switching and hours of work.",
      "initial_assumptions": [
        "Self-service tools can empower users to explore data independently",
        "Single agents with powerful LLMs can handle complex, long-horizon tasks",
        "More capable models will solve reasoning limitations"
      ],
      "why_it_fails": [
        "Self-service tools assume users know which data sources to query and have technical skills",
        "Single agents suffer from 'context pollution' - as they perform more steps, context window fills with intermediate thoughts",
        "Context pollution degrades reasoning, increases token costs, and limits ability to handle long-running tasks",
        "No single architecture is optimal for all task types"
      ],
      "first_principle_insight": "The solution isn't more powerful single agents, but collaborative agent ecosystems. Different architectural patterns (workflows, agents, deep agents, swarms) suit different problem types. Build a portfolio of capabilities - use deterministic workflows for certified high-stakes tasks, single agents for ad-hoc exploration, deep agents for complex analysis, and swarms for dynamic collaboration.",
      "how_to_reframe": {
        "old_atomic_unit": "Single agent handling all reasoning with growing context",
        "new_atomic_unit": "Portfolio of specialized agent architectures matched to task complexity and certainty"
      },
      "reframe": {
        "new_atomic_unit": "Hierarchical/collaborative agents with specialized roles and shared memory",
        "new_problem_type": "Single agent with all tools \u2192 Multi-agent ecosystem with architectural diversity",
        "new_objective": "Match task type to optimal agent architecture while enabling agent-to-agent collaboration",
        "architectural_changes": [
          {
            "change": "Evolutionary architecture spectrum",
            "description": "Four levels: Workflows (deterministic, auditable), Agents (ReAct loop, adaptive), Deep Agents (hierarchical delegation, persistent workspace), Swarms (decentralized, emergent)"
          },
          {
            "change": "Multistage search engine",
            "description": "RRF combining BM25 keyword search with dense semantic search plus re-ranker for fast context retrieval"
          },
          {
            "change": "Schema-aware SQL generation",
            "description": "DescribeTable tool with pre-cached example values for each column to improve filtering accuracy"
          },
          {
            "change": "Zero-Data Statistical Query Validation",
            "description": "EXPLAIN-based checks plus statistical metadata validation without exposing sensitive data"
          },
          {
            "change": "LLM-as-judge evaluation",
            "description": "Automated framework measuring faithfulness and contextual relevance using DeepEval"
          },
          {
            "change": "Context quarantine",
            "description": "Passing only final artifacts between agents rather than full conversational history"
          }
        ],
        "results": "Business leaders and operations managers can ask complex questions and receive trustworthy answers in seconds instead of hours; unified cognitive layer over DoorDash's data and operations"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering Blog (2025): Beyond Single Agents: How DoorDash is building a collaborative AI ecosystem",
          "url": "https://careersatdoordash.com/blog/beyond-single-agents-doordash-building-collaborative-ai-ecosystem/",
          "note": "By Aydar Akhmetzyanov, Harsha Reddy, Jash Radia, Gurudev Jagdale, Sahal Sadique, Lokesh Sharma - November 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Matching architectural patterns to task requirements",
        "common_mistake_avoided": "Assuming more powerful single agents will solve all problems instead of building architectural diversity",
        "when_to_use": "When enterprise knowledge is distributed and tasks vary from routine reporting to exploratory analysis"
      },
      "tags": [
        "multi-agent",
        "agent-architecture",
        "enterprise-ai",
        "data-platform",
        "workflows",
        "LangGraph"
      ]
    },
    {
      "id": "doordash-llm-grocery-cold-start-2025",
      "title": "DoorDash: LLM-Powered Cold Start via Cross-Vertical Preference Transfer",
      "category": "Architectural Pivot - Efficient LLM at Scale",
      "companies_involved": [
        "DoorDash"
      ],
      "year": 2025,
      "initial_problem": "Cold start problem for grocery recommendations - how to recommend useful items to consumers who are new to grocery delivery but have restaurant order history on DoorDash",
      "initial_assumptions": [
        "Feed entire order history plus full grocery taxonomy to LLM for each user",
        "LLMs can handle large context windows with consistent quality",
        "Per-user LLM inference is the natural approach for personalization"
      ],
      "why_it_fails": [
        "Context bloat leads to degraded output quality and increased hallucinations",
        "At 200M+ active customers, per-user LLM inference would cost seven figures per run",
        "Throughput and cost make frequent signal refresh operationally unfeasible"
      ],
      "first_principle_insight": "Compress user signal before engaging LLMs. Instead of per-user LLM calls, represent orders as tagsets, then map unique tagsets to grocery taxonomies once (offline). With tens of thousands of unique tagsets vs 200M+ users, this achieves 10,000x cost reduction while maintaining personalization quality.",
      "how_to_reframe": {
        "old_atomic_unit": "Per-user LLM inference with full history and taxonomy",
        "new_atomic_unit": "Offline tagset-to-taxonomy mapping reused across all users"
      },
      "reframe": {
        "new_atomic_unit": "Compressed tagset signals with precomputed LLM mappings",
        "new_problem_type": "Per-user LLM personalization \u2192 Shared offline inference with personalized scoring",
        "new_objective": "Bootstrap grocery preferences from restaurant history at scale with sustainable costs",
        "architectural_changes": [
          {
            "change": "Signal compression via tagsets",
            "description": "Represent orders using dish, dietary, and cuisine tags aggregated by recency and frequency"
          },
          {
            "change": "LLM-assisted tag cleaning",
            "description": "Filter contradictory tags (chicken wings + vegetarian), overly generic tags, and canonicalize synonyms"
          },
          {
            "change": "RAG-narrowed mapping",
            "description": "K-NN retrieval narrows ~200 taxonomy candidates before LLM mapping, reducing hallucinations"
          },
          {
            "change": "Offline tagset-to-taxonomy mapping",
            "description": "Weekly batch job maps unique tagsets to taxonomies with relevance scores 1-5"
          },
          {
            "change": "Personalized scoring model",
            "description": "Recency (exponential decay) \u00d7 frequency scoring to rank taxonomies per user"
          },
          {
            "change": "LLM-as-judge evaluation",
            "description": "Quantitative judge-vs-model metrics (MAE, quadratic weighted kappa, nDCG@3) for rapid prompt iteration"
          }
        ],
        "results": "10,000x cost reduction compared to naive approach; statistically significant improvements to order penetration for convenience and grocery; personalized recommendations like 'hot pot soup base, potstickers, burritos' surfaced from restaurant history"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering Blog (2025): Using LLMs to infer grocery preferences from DoorDash restaurant orders",
          "url": "https://careersatdoordash.com/blog/doordash-llms-for-grocery-preferences-from-restaurant-orders/",
          "note": "By Yucong Ji, Raghav Saboo, Vivek Paharia, Isa Lyubimova - September 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Identifying that the atomic unit should be tagset, not user",
        "common_mistake_avoided": "Scaling LLM personalization by making per-user calls instead of finding shared structures",
        "when_to_use": "When LLM-based personalization faces prohibitive per-user inference costs at scale"
      },
      "tags": [
        "cold-start",
        "personalization",
        "LLM",
        "RAG",
        "cost-optimization",
        "e-commerce",
        "cross-vertical"
      ]
    },
    {
      "id": "flipkart-unified-ranking-ads-organic-2025",
      "title": "Flipkart: Unified Ranking for Ads and Organic Recommendations",
      "category": "Architectural Pivot - Unified Ranking",
      "companies_involved": [
        "Flipkart"
      ],
      "year": 2025,
      "initial_problem": "Placement of sponsored ads alongside organic recommendations in product page widgets. Fixed slot approach (ads pinned at positions 2 and 5) was safe but sub-optimal - couldn't adapt to context.",
      "initial_assumptions": [
        "Pinning ads at fixed positions is the safest approach",
        "Organic and sponsored items should be ranked by separate pipelines",
        "The two ranked lists can be stitched together at the end"
      ],
      "why_it_fails": [
        "Can't adapt to context - if top 5 relevant products are all sponsored, or no ads are relevant",
        "Separate pipelines lead to calibration mismatch - organic biased toward popular items, sponsored toward cold-start items",
        "Feature parity gap between two independently evolved pipelines",
        "Fallback risk during system failures could flood page entirely with ads"
      ],
      "first_principle_insight": "Sponsored and organic items should compete on a level playing field using a single model. The key insight is that you can train one model on combined data if you give it the tools to understand context: a simple 'isAds' binary flag plus cross-features (e.g., product_popularity_X_isAds) let the model learn different weights for the same feature depending on content type.",
      "how_to_reframe": {
        "old_atomic_unit": "Separate ranking pipelines merged by fixed slot pinning",
        "new_atomic_unit": "Single unified ranker with content-type-aware cross-features"
      },
      "reframe": {
        "new_atomic_unit": "Unified ranking model that learns content-type-specific feature weights",
        "new_problem_type": "Separate Ad/Organic pipelines \u2192 Single model with calibration features",
        "new_objective": "Optimize both relevance and revenue through fair competition between all content types",
        "architectural_changes": [
          {
            "change": "isAds calibration feature",
            "description": "Binary flag allows model to learn inherent CTR/CVR differences between content types"
          },
          {
            "change": "Cross-features with isAds",
            "description": "27 key features crossed with isAds flag to learn separate weights for sponsored vs organic contexts"
          },
          {
            "change": "Content-specific objective functions",
            "description": "Organic score = \u03b1\u00d7pCTR + \u03b2\u00d7pCVR; Sponsored adds cost components for ad bid value"
          },
          {
            "change": "Circuit breaker fallback",
            "description": "Auto-revert to pinned positions if organic ranker failure rate exceeds threshold"
          },
          {
            "change": "Unified pair stats",
            "description": "Compute directional pair statistics for both organic and ads impressions, plus TOTAL aggregates"
          }
        ],
        "results": "1.36% increase in organic orders (statistically significant); 3.4% increase in Ad revenue (statistically significant); launched to 100% of Flipkart traffic"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Flipkart Tech Blog (2025): The Science of Unified Ranking: Integrating Ads and Organic Recommendations",
          "url": "https://blog.flipkart.tech/the-science-of-unified-ranking-integrating-ads-and-organic-recommendations-8cc24113ef21",
          "note": "By Amar Kumar - November 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Questioning the assumption that ads and organic need separate pipelines",
        "common_mistake_avoided": "Treating sponsored and organic content as fundamentally incompatible for unified ranking",
        "when_to_use": "When you have multiple content types competing for the same slots and fixed allocation leaves value on the table"
      },
      "tags": [
        "unified-ranking",
        "ads",
        "recommendations",
        "e-commerce",
        "feature-engineering",
        "calibration"
      ]
    },
    {
      "id": "whatnot-new-user-unified-ml-2025",
      "title": "Whatnot: Killing Heuristics for Unified ML Cold-Start Recommendations",
      "category": "Pragmatic Pivot - Unified Architecture",
      "companies_involved": [
        "Whatnot"
      ],
      "year": 2025,
      "initial_problem": "Cold-start problem for new users on For You Feed - users have no interaction history, so recommendations relied on ad-hoc heuristics instead of ML",
      "initial_assumptions": [
        "New users are a 'separate species' requiring different ranking logic",
        "Heuristic lookalike models are good enough for new users",
        "ML models require historical user interaction features to work",
        "A specialized new-user model would outperform a global model"
      ],
      "why_it_fails": [
        "Complex hand-engineered lookalike logic that was hard to understand and maintain",
        "Completely separate feed generation path (retrieval, enrichment, filtering, sorting) from regular users",
        "Bespoke data pipelines and datasets requiring dedicated maintenance and siloed expertise",
        "New users deserve ML-quality ranking, not 'good enough' heuristics"
      ],
      "first_principle_insight": "One global model trained on all users outperforms a model trained only on new users. GBDT models can fit decision rules to small 'pockets' of skewed data. Instead of historical features, use request-time features (onboarding selections) with proper training-production parity via Change Data Capture (CDC).",
      "how_to_reframe": {
        "old_atomic_unit": "Separate heuristic-based ranking pipeline for new users",
        "new_atomic_unit": "Single unified ML model serving all users with request-time features"
      },
      "reframe": {
        "new_atomic_unit": "Global ranking model with request-time features for cold-start users",
        "new_problem_type": "Heuristic lookalike + separate pipelines \u2192 Unified ML with online inference",
        "new_objective": "Provide ML-quality recommendations to new users while reducing architectural complexity",
        "architectural_changes": [
          {
            "change": "Global model over specialized model",
            "description": "Single GBDT model trained on all users outperformed new-user-only model; patterns learned from mature users transfer to new users"
          },
          {
            "change": "Request-time features",
            "description": "Onboarding selections passed directly from client to model at inference time, not stored in feature store"
          },
          {
            "change": "CDC for training parity",
            "description": "Change Data Capture ensures request-time features in training match production with point-in-time correctness"
          },
          {
            "change": "Online inference for all users",
            "description": "Generate predictions 'on the fly' without requiring users in offline warehouse"
          },
          {
            "change": "Eliminated separate feed generation path",
            "description": "New users now follow same retrieval, enrichment, filtering, and ranking as regular users"
          }
        ],
        "results": "10% increase in For You Feed engagement from new users (statistically significant); eliminated technical debt, bespoke pipelines, and siloed knowledge; future ML improvements now benefit all users"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Whatnot Engineering Blog (2025): The New User Dilemma: Why We Killed Our Heuristics (& What We Built Instead)",
          "url": "https://medium.com/whatnot-engineering/the-new-user-dilemma-why-we-killed-our-heuristics-what-we-built-instead-0d7a834fda5f",
          "note": "By Miguel Fernandez-Montes Cuberta, David Kawashima, Ford Bohrmann - September 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Global model vs specialized model; unified vs separate architectures",
        "common_mistake_avoided": "Assuming new users need fundamentally different systems instead of testing unified approaches",
        "when_to_use": "When you have separate heuristic or rule-based systems for edge cases that could potentially be unified with ML"
      },
      "tags": [
        "cold-start",
        "recommendations",
        "unified-architecture",
        "GBDT",
        "online-inference",
        "marketplace"
      ]
    },
    {
      "id": "duolingo-codex-feature-flag-agent-2025",
      "title": "Duolingo: Simplifying Agent Architecture with Codex CLI",
      "category": "Pragmatic Pivot - Simplified Agent Architecture",
      "companies_involved": [
        "Duolingo"
      ],
      "year": 2025,
      "initial_problem": "Automating simple engineering tasks like removing feature flags - needed to build reusable patterns for code-operating agents",
      "initial_assumptions": [
        "Need complex toolchains like LangChain or fast-agent with GitHub MCP",
        "Need multiple prompts run in loops (3 prompts)",
        "Custom internal tools (Baymax) needed to operate on local code",
        "1-2 weeks development time is expected"
      ],
      "why_it_fails": [
        "Complex architecture with multiple prompts and custom tooling",
        "GitHub MCP unnecessary for tasks not operating on PRs or code history",
        "LangChain and fast-agent versions were still incomplete after 1-2 weeks"
      ],
      "first_principle_insight": "Simple tools that 'just work' beat complex orchestration. Codex CLI with full-auto mode worked immediately with a single merged prompt. For local code operations: clone repo, run agent, create PR - no GitHub MCP needed. Temporal provides the retry logic needed for non-deterministic agent behavior.",
      "how_to_reframe": {
        "old_atomic_unit": "Complex multi-prompt chains with custom tools and MCP integrations",
        "new_atomic_unit": "Single prompt + Codex CLI subprocess + Temporal workflow orchestration"
      },
      "reframe": {
        "new_atomic_unit": "Codex CLI in full-auto mode with single prompt",
        "new_problem_type": "Complex agent orchestration \u2192 Simple CLI subprocess with workflow retry logic",
        "new_objective": "Build reusable patterns for code-operating agents with minimal complexity",
        "architectural_changes": [
          {
            "change": "Codex CLI over custom toolchains",
            "description": "Dropped LangChain/fast-agent, merged 3 prompts into 1, Codex CLI just worked immediately"
          },
          {
            "change": "Temporal for workflow orchestration",
            "description": "Provides retry logic for non-deterministic agents, easy local testing, and activity isolation"
          },
          {
            "change": "Local code over GitHub MCP",
            "description": "Clone to temp directory, operate locally, create PR via standard GitHub - no MCP needed"
          },
          {
            "change": "Full-auto sandboxed execution",
            "description": "codex exec --dangerously-bypass-approvals-and-sandbox on isolated ECS instance"
          },
          {
            "change": "Gateway dispatcher pattern",
            "description": "Platform Self-Service UI kicks off gateway workflow which dispatches to specific worker"
          }
        ],
        "results": "Prototype in ~1 day, production in ~1 week; immediately successful where LangChain/fast-agent took 1-2 weeks without completion; established reusable pattern for future agents"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Duolingo Engineering Blog (2025): Building an AI agent to remove feature flags",
          "url": "https://blog.duolingo.com/buildingaiagents/",
          "note": "By Jesse Welch - December 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Simpler tools that 'just work' vs complex orchestration frameworks",
        "common_mistake_avoided": "Over-engineering agent architecture with multiple prompts and custom tooling when simpler solutions exist",
        "when_to_use": "When building code-operating agents and tempted to use complex frameworks - try the simplest tool first"
      },
      "tags": [
        "ai-agents",
        "developer-tools",
        "codex",
        "temporal",
        "feature-flags",
        "automation"
      ]
    },
    {
      "id": "pinterest-user-journeys-2025",
      "title": "Pinterest: From Interest-Based to Journey-Aware Recommendations",
      "category": "Architectural Pivot - Temporal Understanding",
      "companies_involved": [
        "Pinterest"
      ],
      "year": 2025,
      "initial_problem": "Pinterest needed to move beyond understanding immediate interests to comprehend underlying long-term user goals for their inspiration-to-realization platform, but had no large training datasets for this new product",
      "initial_assumptions": [
        "Predefined journey taxonomy with fixed set of journeys could work",
        "Interest-based notifications are sufficient for engagement",
        "Large training datasets are required for new ML products",
        "Need to build many new components for journey inference"
      ],
      "why_it_fails": [
        "Predefined taxonomy risks overlapping with existing systems, requires significant maintenance, slow to adapt to trends",
        "Interest-based notifications miss the underlying intent and context",
        "No large training data exists for journey understanding"
      ],
      "first_principle_insight": "Define a journey as the intersection of interest, intent, and context at a specific point in time. Use dynamic keyword extraction from user activities (search, pins, boards), cluster with pretrained embeddings, and start with few hundred human-annotated examples. LLM judgments correlate with human assessments for scalable evaluation.",
      "how_to_reframe": {
        "old_atomic_unit": "User interest (static category like 'home decor')",
        "new_atomic_unit": "User journey (interest + intent + context, e.g., 'kitchen renovation' in 'ready to buy' stage)"
      },
      "reframe": {
        "new_atomic_unit": "Dynamic user journey as keyword cluster with stage prediction",
        "new_problem_type": "Interest classification \u2192 Journey extraction with temporal lifecycle",
        "new_objective": "Understand long-term goals to assist users in achieving them, not just recommending content",
        "architectural_changes": [
          {
            "change": "Dynamic keyword extraction over taxonomy",
            "description": "Extract keywords from search history, pin interactions, and boards; cluster with pretrained SearchSage embeddings"
          },
          {
            "change": "Journey stage prediction",
            "description": "Classify journeys as situational vs evergreen, ongoing vs ended based on engagement patterns"
          },
          {
            "change": "Journey ranking and diversification",
            "description": "Point-wise ranking model with penalty for similar top-ranked journeys"
          },
          {
            "change": "LLM for evaluation and expansion",
            "description": "LLM-as-judge for 5-level relevance scoring; LLM generates related journeys stored in KV store"
          },
          {
            "change": "Streaming incremental inference",
            "description": "Full inference on algorithm change, daily incremental for active users"
          },
          {
            "change": "Lean philosophy",
            "description": "Minimize new components, start with few hundred examples, leverage pretrained models, make extensible"
          }
        ],
        "results": "88% higher email click rate vs interest-based notifications; 32% higher push open rate; 23% increase in positive feedback rate"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2025): Identify User Journeys at Pinterest",
          "url": "https://medium.com/pinterest-engineering/identify-user-journeys-at-pinterest-b517f6275b42",
          "note": "By Lin Zhu, Jaewon Yang, Ravi Kiran Holur Vijay - October 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Redefining the atomic unit from static interest to dynamic journey",
        "common_mistake_avoided": "Building fixed taxonomies when dynamic extraction is more flexible and adaptable",
        "when_to_use": "When current recommendations capture what users like but miss why they're engaging and what they're trying to accomplish"
      },
      "tags": [
        "user-journeys",
        "personalization",
        "clustering",
        "recommendations",
        "LLM-evaluation",
        "notifications"
      ]
    },
    {
      "id": "uber-delivery-search-semantic-2025",
      "title": "Uber: Evolution from Lexical to Semantic Delivery Search",
      "category": "Architectural Pivot - Search Reframe",
      "companies_involved": [
        "Uber"
      ],
      "year": 2025,
      "initial_problem": "Uber's delivery search relied on lexical matching that couldn't handle real-world query complexity - synonyms, typos, shorthand, and language mixing produced poor results",
      "initial_assumptions": [
        "String-matching based search is sufficient for product discovery",
        "Exact keyword matches are the primary way users search",
        "Separate models needed for different verticals and markets"
      ],
      "why_it_fails": [
        "Users searching 'soda' wouldn't find 'soft drink'",
        "Multilingual queries produced irrelevant results",
        "Lexical matching misses semantic intent entirely"
      ],
      "first_principle_insight": "Semantic search shifts from matching words to matching meaning. Encode queries and documents into vectors in the same space so semantically similar things are close - even without keyword overlap. One LLM fine-tuned on proprietary data can support all verticals and markets.",
      "how_to_reframe": {
        "old_atomic_unit": "Keyword string matching",
        "new_atomic_unit": "Semantic vector similarity in shared embedding space"
      },
      "reframe": {
        "new_atomic_unit": "Two-tower neural network with LLM embeddings",
        "new_problem_type": "Lexical matching \u2192 Semantic search with unified embeddings",
        "new_objective": "Match meaning, not words, across all verticals with single model",
        "architectural_changes": [
          {
            "change": "Two-tower neural network",
            "description": "Query encoder and document encoder trained jointly, with Qwen LLMs as backbone"
          },
          {
            "change": "Unified cross-vertical model",
            "description": "One model supports all Uber Eats verticals and markets instead of separate models"
          },
          {
            "change": "Batch/online split",
            "description": "Documents embedded offline in batch, queries embedded in real-time"
          },
          {
            "change": "Matryoshka Representation Learning",
            "description": "Flexible embedding dimensions for latency/quality tradeoffs"
          },
          {
            "change": "Quantization and dimension reduction",
            "description": "Reduced storage costs ~50% without meaningful accuracy loss"
          }
        ],
        "results": "34% latency reduction; 17% CPU savings; ~50% storage cost reduction; semantic understanding across synonyms, typos, and languages"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Uber Engineering Blog (2025): Evolution and Scale of Uber's Delivery Search Platform",
          "url": "https://www.uber.com/en-GB/blog/evolution-and-scale-of-ubers-delivery-search-platform/",
          "note": "2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Semantic vectors as alternative to lexical matching",
        "common_mistake_avoided": "Building separate search models per vertical instead of unified semantic approach",
        "when_to_use": "When lexical search fails on synonyms, typos, or cross-language queries"
      },
      "tags": [
        "search",
        "semantic-search",
        "embeddings",
        "two-tower",
        "LLM",
        "e-commerce"
      ]
    },
    {
      "id": "digits-process-daemons-2025",
      "title": "Digits: From Agent Frameworks to 'Process Daemons'",
      "category": "Pragmatic Pivot - Simplified Agent Architecture",
      "companies_involved": [
        "Digits"
      ],
      "year": 2025,
      "initial_problem": "Digits needed to deploy AI agents for accounting automation (vendor data enrichment, client onboarding) and adopted popular frameworks like LangChain and CrewAI",
      "initial_assumptions": [
        "Established agent frameworks are production-ready",
        "More abstraction layers help development",
        "Agents need complex orchestration"
      ],
      "why_it_fails": [
        "Frameworks contained 'too many dependencies' and excessive complexity",
        "Not suitable for production environments",
        "Marketing hype obscured fundamental simplicity"
      ],
      "first_principle_insight": "An agent is surprisingly simple at its core - just ~100 lines combining an objective, LLM, tools, and response mechanism. The term 'agent' implies autonomy; 'Process Daemon' better describes background processes that execute tasks with oversight and observability.",
      "how_to_reframe": {
        "old_atomic_unit": "Framework-orchestrated autonomous agents",
        "new_atomic_unit": "Custom agent loops as 'Process Daemons' with oversight"
      },
      "reframe": {
        "new_atomic_unit": "Minimal custom agent loop (~100 lines) with governance",
        "new_problem_type": "Framework-heavy agents \u2192 Lightweight process daemons",
        "new_objective": "Execute accounting tasks with transparency, control, and observability",
        "architectural_changes": [
          {
            "change": "Custom agent loops",
            "description": "Built own loops instead of relying on LangChain/CrewAI frameworks"
          },
          {
            "change": "Go reflection for tools",
            "description": "Leveraged existing APIs using Go reflection for dynamic schema generation"
          },
          {
            "change": "OpenTelemetry observability",
            "description": "Used open standard instead of vendor-specific solutions"
          },
          {
            "change": "Separate LLM guardrails",
            "description": "Deployed guardrails using separate LLMs for response validation"
          },
          {
            "change": "Terminology shift",
            "description": "Rebranded 'agents' as 'Process Daemons' to emphasize oversight and observability"
          }
        ],
        "results": "Multiple production agents handling complex accounting workflows; avoided vendor lock-in; comprehensive monitoring and security; governance controls"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Digits Blog (2025): Agents in production: Lessons shared at MLOps World 2025",
          "url": "https://digits.com/blog/mlops-world-2025-slides/",
          "note": "MLOps World 2025 presentation"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Questioning assumptions about agent complexity",
        "common_mistake_avoided": "Adopting complex frameworks when simple custom loops suffice",
        "when_to_use": "When framework complexity exceeds the actual requirements of agent tasks"
      },
      "tags": [
        "ai-agents",
        "frameworks",
        "simplification",
        "observability",
        "accounting",
        "production"
      ]
    },
    {
      "id": "bayezian-clinical-trial-agents-2025",
      "title": "Bayezian: Multi-Agent Memory Failures in Clinical Trials",
      "category": "Cautionary Tale - Agent Memory Architecture",
      "companies_involved": [
        "Bayezian Limited"
      ],
      "year": 2025,
      "initial_problem": "Automating detection of protocol deviations in clinical trials - manual review through spreadsheets was time-consuming and inconsistent",
      "initial_assumptions": [
        "Multi-agent systems with specialized agents can coordinate on structured tasks",
        "Agents can reliably transfer context between handoffs",
        "System can act as intelligent triage layer without replacing human reviewers"
      ],
      "why_it_fails": [
        "Information broke down between agent handoffs",
        "Detection by one agent wouldn't reliably transfer to the next",
        "System missed contextual details like visit windows",
        "Agent flagged missing Day 14 lab without recalling Day 13 test fulfilled requirement",
        "Memory gaps between steps created cascading errors"
      ],
      "first_principle_insight": "Autonomy without proper memory architecture doesn't equal understanding. Agents don't understand flexibility unless it's made explicit. The absence of a test on the exact date triggered a response - it couldn't infer that nearby dates fulfilled the requirement.",
      "how_to_reframe": {
        "old_atomic_unit": "Independent specialized agents with implicit handoffs",
        "new_atomic_unit": "Agents with structured memory snapshots and explicit handoff signals"
      },
      "reframe": {
        "new_atomic_unit": "Memory-aware multi-agent system with explicit context transfer",
        "new_problem_type": "Autonomous multi-agent \u2192 Memory-structured multi-agent with explicit handoffs",
        "new_objective": "Reduce manual review while ensuring contextual understanding across agent transitions",
        "architectural_changes": [
          {
            "change": "Structured memory snapshots",
            "description": "Capture protocol rules and context at each stage"
          },
          {
            "change": "Independent reasoning generation",
            "description": "Move beyond rigid templates to allow flexible reasoning"
          },
          {
            "change": "Explicit handoff signals",
            "description": "Mark context explicitly for downstream agents"
          },
          {
            "change": "Information flow tracking",
            "description": "Monitor how context transfers between steps"
          }
        ],
        "results": "Useful performance on structured examples; reduced manual review time; improved consistency; still required human validation for edge cases; unable to infer flexibly or fill contextual gaps humans find obvious"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "AI Hub (2025): Deploying agentic AI: what worked, what broke, and what we learned",
          "url": "https://aihub.org/2025/09/15/deploying-agentic-ai-what-worked-what-broke-and-what-we-learned/",
          "note": "Bayezian Limited case study - September 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - Memory failures as signals that framing is wrong",
        "common_mistake_avoided": "Assuming agents can infer context that wasn't explicitly provided",
        "when_to_use": "When designing multi-agent systems - cautionary tale about memory architecture"
      },
      "tags": [
        "multi-agent",
        "memory",
        "healthcare",
        "clinical-trials",
        "cautionary-tale",
        "handoffs"
      ]
    },
    {
      "id": "fuzzylabs-sre-agent-fastmcp-2025",
      "title": "FuzzyLabs: Custom MCP Client for Autonomous SRE Agent",
      "category": "Pragmatic Pivot - Infrastructure Control",
      "companies_involved": [
        "FuzzyLabs"
      ],
      "year": 2025,
      "initial_problem": "Building an autonomous agent for Site Reliability Engineering to automate incident diagnosis",
      "initial_assumptions": [
        "Existing AI applications (Claude Desktop, Cursor) would suffice for MCP client functionality",
        "Third-party MCP implementations are adequate for agentic workflows"
      ],
      "why_it_fails": [
        "Anthropic would gate-keep token usage making full agentic workflows difficult",
        "Applications required manual approval of tool calls, eliminating true autonomy",
        "Couldn't control costs or implement security constraints"
      ],
      "first_principle_insight": "To build truly autonomous agents, you need to understand and control the MCP client layer directly rather than depending on third-party implementations. Control enables cost optimization, security, and full autonomy.",
      "how_to_reframe": {
        "old_atomic_unit": "Third-party MCP client (Claude Desktop, Cursor)",
        "new_atomic_unit": "Custom MCP client with full control over tool execution"
      },
      "reframe": {
        "new_atomic_unit": "Custom FastMCP client with cost, security, and autonomy controls",
        "new_problem_type": "Third-party MCP dependency \u2192 Custom MCP infrastructure",
        "new_objective": "Autonomous incident diagnosis with cost control and security",
        "architectural_changes": [
          {
            "change": "Custom FastMCP client",
            "description": "Built own MCP client instead of relying on Claude Desktop/Cursor"
          },
          {
            "change": "Tool caching",
            "description": "Cache tool results to reduce redundant LLM calls"
          },
          {
            "change": "Tool filtering",
            "description": "Security and efficiency controls on which tools agents can access"
          },
          {
            "change": "Enforced agent timeout",
            "description": "5-minute maximum execution time to prevent runaway agents"
          },
          {
            "change": "Stop conditioning",
            "description": "Halt reasoning after task completion to prevent unnecessary token usage"
          }
        ],
        "results": "83% cost reduction per diagnosis from tool caching alone; successfully diagnosed simulated cart-service crash by analyzing logs, identifying problematic file, fetching source code, posting recommendations to Slack"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "FuzzyLabs Blog (2025): How We Built Our SRE Agent using FastMCP",
          "url": "https://www.fuzzylabs.ai/blog-post/how-we-built-our-sre-agent-using-fastmcp",
          "note": "2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Trading convenience of third-party tools for control and cost efficiency",
        "common_mistake_avoided": "Relying on third-party MCP implementations that limit autonomy and cost control",
        "when_to_use": "When building production agents and third-party tools gate-keep critical functionality"
      },
      "tags": [
        "ai-agents",
        "MCP",
        "SRE",
        "cost-optimization",
        "infrastructure",
        "autonomy"
      ]
    },
    {
      "id": "grab-graph-networks-fraud-2025",
      "title": "Grab: From Individual Signals to Graph-Based Fraud Detection",
      "category": "Relational Pivot - Individual to Graph",
      "companies_involved": [
        "Grab"
      ],
      "year": 2025,
      "initial_problem": "Escalating fraud from professional syndicates using device farms and GPS spoofing - fraudsters camouflaged themselves as normal users",
      "initial_assumptions": [
        "Individual-level fraud signals are sufficient for detection",
        "Examining isolated user behavior can identify bad actors",
        "Rule-based systems can catch sophisticated fraud"
      ],
      "why_it_fails": [
        "Traditional rule-based detection couldn't identify coordinated fraudsters",
        "Analyzing isolated user behavior was insufficient against organized crime networks",
        "Individual signals miss syndicate-level coordination patterns"
      ],
      "first_principle_insight": "When entities are viewed as a macro graph network, we uncover patterns otherwise unseen. By analyzing all users sharing devices or IP addresses together, you can identify syndicate structures rather than isolated bad actors.",
      "how_to_reframe": {
        "old_atomic_unit": "Individual user behavior signals",
        "new_atomic_unit": "Graph network of users connected by shared devices, IPs, and behaviors"
      },
      "reframe": {
        "new_atomic_unit": "Graph-based fraud detection with GNN models",
        "new_problem_type": "Individual fraud detection \u2192 Network-level syndicate detection",
        "new_objective": "Identify fraud networks and syndicates, not just individual bad actors",
        "architectural_changes": [
          {
            "change": "Graph Database Platform",
            "description": "1+ billion nodes enabling real-time visualization and network-based feature engineering"
          },
          {
            "change": "Graph Neural Networks (GNNs)",
            "description": "Exploit structural correlations to detect fraud patterns like money laundering networks"
          },
          {
            "change": "10+ feature dimensions",
            "description": "Risk specialists gained real-time visibility across multiple network features"
          },
          {
            "change": "Sustainable foundation",
            "description": "GNN models became foundation to combat many different kinds of fraud"
          }
        ],
        "results": "Real-time visibility for risk specialists across 10+ feature dimensions; sustainable foundation for multiple fraud types; ability to detect coordinated syndicate activity invisible to individual-level analysis"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Grab Engineering Blog (2025): Graph Networks for Fraud Detection",
          "url": "https://engineering.grab.com/graph-networks",
          "note": "2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Changing atomic unit from individual to network",
        "common_mistake_avoided": "Treating fraud detection as individual classification when it's a network problem",
        "when_to_use": "When facing coordinated adversarial behavior that individual-level detection misses"
      },
      "tags": [
        "fraud-detection",
        "graph-neural-networks",
        "GNN",
        "security",
        "ride-sharing",
        "relational"
      ]
    },
    {
      "id": "pinterest-mcp-observability-2025",
      "title": "Pinterest: MCP-Based Autonomous Observability Without Infrastructure Overhaul",
      "category": "Pragmatic Pivot - AI Bridge Over Legacy Infrastructure",
      "companies_involved": [
        "Pinterest"
      ],
      "year": 2025,
      "initial_problem": "Pinterest's observability infrastructure predates OpenTelemetry - logs, traces, and metrics exist in separate silos with no correlation. Engineers must jump between multiple interfaces when root-causing issues, with steep learning curves for each unique tool.",
      "initial_assumptions": [
        "Need to overturn entire o11y infrastructure to incorporate OTel standards",
        "Complete infrastructure overhaul required to unify observability data",
        "Individual pillars must be connected at the data layer for holistic analysis"
      ],
      "why_it_fails": [
        "Not feasible to overturn entire infrastructure for a mature, large-scale system",
        "On-call engineers lose valuable time jumping between interfaces and applying filters",
        "Steep learning curve for tools unique to each pillar extends time loss for new engineers",
        "Advanced ML analysis that holistically understands system health becomes non-trivial"
      ],
      "first_principle_insight": "Use AI agents with Model Context Protocol (MCP) to bridge the gaps WITHOUT mandating complete infrastructure overhaul. The agent can connect dots across silos, find correlations, and build hypotheses from patterns - even without a thread connecting the data together at the infrastructure level.",
      "how_to_reframe": {
        "old_atomic_unit": "Infrastructure-level data unification (requires OTel overhaul)",
        "new_atomic_unit": "AI agent with MCP tools that queries all silos and synthesizes context on demand"
      },
      "reframe": {
        "new_atomic_unit": "MCP server as unified interface to fragmented observability data",
        "new_problem_type": "Infrastructure unification \u2192 Agent-based context synthesis",
        "new_objective": "Enable holistic observability analysis without rebuilding infrastructure",
        "architectural_changes": [
          {
            "change": "Centralized MCP Server",
            "description": "Provides agents tools for metrics, logs, traces, change events, alerts, dependency graphs, and post-mortem documents"
          },
          {
            "change": "Link generation over raw data",
            "description": "Agent generates filtered dashboard links instead of parsing raw data - conserves context window"
          },
          {
            "change": "Specific tool documentation",
            "description": "Guide agents to query small time periods and iterate, preventing context overflow"
          },
          {
            "change": "LLM summarization within server",
            "description": "Additional LLM summarizes data before returning to main agent, conserving context space"
          },
          {
            "change": "Fine-grained context control",
            "description": "O11y team controls what data/abilities agents get - maintains safety and privacy"
          },
          {
            "change": "Tricorder Agent",
            "description": "Engineer provides alert link, agent gathers relevant info and hypothesizes cause with suggestions"
          }
        ],
        "results": "Engineers no longer jump between interfaces; agent autonomously explores dependency graphs for upstream/downstream issues; reduced MTTR; hub for company-wide agentic projects (won first place at hackathon)"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2025): Autonomous Observability at Pinterest (Part 1 of 2)",
          "url": "https://medium.com/pinterest-engineering/autonomous-observability-at-pinterest-part-1-of-2-eb0adae830ba",
          "note": "By Marcel Mateos Salles, Jorge Chavez, Khashayar Kamran, Andres Almeida, Peter Kim, Ajay Jha - December 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - AI agents as alternative to infrastructure overhaul",
        "common_mistake_avoided": "Assuming legacy infrastructure must be replaced to achieve modern observability capabilities",
        "when_to_use": "When facing fragmented legacy systems where complete overhaul is not feasible but unified analysis is needed"
      },
      "tags": [
        "observability",
        "MCP",
        "ai-agents",
        "legacy-systems",
        "infrastructure",
        "OpenTelemetry"
      ]
    },
    {
      "id": "pinterest-pinner-surveys-quality-2025",
      "title": "Pinterest: From Engagement-Only to Survey-Informed Content Quality",
      "category": "Architectural Pivot - Metric Reframe",
      "companies_involved": [
        "Pinterest"
      ],
      "year": 2025,
      "initial_problem": "Optimizing recommendations purely for engagement tends to promote low-quality clickbait with limited long-term engagement. Not all engagement is good - can mislead system into promoting harmful content.",
      "initial_assumptions": [
        "Engagement signals are sufficient proxy for content quality",
        "More clicks/interactions = better content",
        "Guidelines on 'high quality' can be constructed without understanding user perception"
      ],
      "why_it_fails": [
        "Engagement-only optimization promotes clickbait",
        "Can promote low-quality or even harmful content",
        "Misses subjective notion of what users actually consider quality",
        "Short-term engagement doesn't equal fulfilling long-term engagement"
      ],
      "first_principle_insight": "Ask users directly what they consider quality through surveys, then train ML models on their responses. Surveys de-bias the system by ensuring the engagement we reward comes from high-quality content. An effective approach is pairwise ranking (which is better?) rather than absolute scoring.",
      "how_to_reframe": {
        "old_atomic_unit": "Engagement signals (clicks, repins, saves) as quality proxy",
        "new_atomic_unit": "User survey responses as ground truth for quality, ML model as scalable proxy"
      },
      "reframe": {
        "new_atomic_unit": "Survey-trained visual quality model as ranking feature",
        "new_problem_type": "Engagement optimization \u2192 Survey-informed quality optimization",
        "new_objective": "Deliver content that makes users feel good and drives fulfilling long-term engagement",
        "architectural_changes": [
          {
            "change": "In-app survey collection",
            "description": "5k Pins rated 1-5 for visual appeal, 10+ responses per image to reduce subjectivity"
          },
          {
            "change": "Pairwise ranking formulation",
            "description": "Predict which image is better rather than absolute scores - expands 5k images to ~2.5M training pairs"
          },
          {
            "change": "Variable margin loss",
            "description": "Higher margin penalty for images with more certain ratings, lower for high variance"
          },
          {
            "change": "Simple neural network",
            "description": "92k parameter fully-connected network prevents overfitting, enables cheap inference at scale"
          },
          {
            "change": "L1-grouped training",
            "description": "Only compare images within same interest vertical to focus on visual quality, not semantic differences"
          },
          {
            "change": "Integration into all surfaces",
            "description": "Model scores used as ranking feature in Homefeed, Search, and Related Pins"
          }
        ],
        "results": "Significant reductions in 'low quality' sessions; increases in 'successful' sessions; increased repins of organic content; increased long click-throughs on shopping content; win/win for Pinners and business across all 3 major surfaces"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2025): Improving Quality of Recommended Content through Pinner Surveys",
          "url": "https://medium.com/pinterest-engineering/improving-quality-of-recommended-content-through-pinner-surveys-eebca8a52652",
          "note": "By Rudraksh Kapil, Michal Giemza, Devan Srinivasan, Leif Sigerson, Stephanie Chen, Wendy Matheny, Jianjin Dong, Qinglong Zeng - December 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 1 (Outcome) - Redefining success metric from engagement to user-perceived quality",
        "common_mistake_avoided": "Optimizing purely for engagement which promotes clickbait and harmful content",
        "when_to_use": "When engagement metrics are leading to low-quality content promotion or short-term thinking"
      },
      "tags": [
        "recommendations",
        "content-quality",
        "surveys",
        "pairwise-ranking",
        "engagement",
        "user-feedback"
      ]
    },
    {
      "id": "pinterest-llm-search-relevance-2025",
      "title": "Pinterest: LLM Teacher-Student Distillation for Search Relevance",
      "category": "Architectural Pivot - Knowledge Distillation",
      "companies_involved": [
        "Pinterest"
      ],
      "year": 2025,
      "initial_problem": "Search relevance model needed improvement to ensure content is genuinely pertinent to user queries, not just relying on past engagement. Cross-encoder LLMs are accurate but too slow/expensive for real-time serving.",
      "initial_assumptions": [
        "In-house embeddings (SearchSAGE) are sufficient for relevance",
        "Human-labeled data from US queries will generalize globally",
        "Large LLMs can be directly served for real-time search"
      ],
      "why_it_fails": [
        "SearchSAGE baseline alone has lower accuracy (19.7% below Llama-3-8B)",
        "Cross-encoder LLMs have prohibitive latency and cost for real-time search",
        "Human-labeled data limited to US, doesn't cover global languages",
        "Limited human annotation doesn't capture seasonal concepts"
      ],
      "first_principle_insight": "Use LLM as teacher to generate labels at massive scale (billions of rows daily), then distill into lightweight student model for serving. Semi-supervised learning on unlabeled engagement data generalizes from US human labels to unseen languages globally. Enriched text features (BLIP captions, high-engagement queries, board titles) significantly boost prediction.",
      "how_to_reframe": {
        "old_atomic_unit": "Human-labeled relevance data + in-house embeddings",
        "new_atomic_unit": "LLM-generated labels at scale + lightweight distilled student model"
      },
      "reframe": {
        "new_atomic_unit": "Teacher-student distillation with semi-supervised global label generation",
        "new_problem_type": "Limited human labels + slow LLM \u2192 Scalable distillation + fast student",
        "new_objective": "Scale relevance training globally without human annotation in every language",
        "architectural_changes": [
          {
            "change": "Cross-encoder LLM teacher",
            "description": "Llama-3-8B fine-tuned on human labels, predicts 5-scale relevance"
          },
          {
            "change": "Enriched Pin text features",
            "description": "Titles, BLIP synthetic captions, high-engagement queries, board titles, link metadata"
          },
          {
            "change": "Knowledge distillation",
            "description": "LLM teacher labels billions of daily logged query-Pin pairs, student learns from soft labels"
          },
          {
            "change": "Lightweight student model",
            "description": "Feed-forward network on query/Pin embeddings and interaction features for real-time serving"
          },
          {
            "change": "Semi-supervised multilingual learning",
            "description": "Multilingual LLM generalizes US human labels to unseen languages and countries"
          }
        ],
        "results": "Llama-3-8B outperforms BERT-base by 12.5%, baseline by 19.7% in accuracy; +2.18% nDCG@20 search feed relevance; >1.5% search fulfillment rate increase globally; generalizes to unseen languages without local annotation"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Pinterest Engineering Blog (2025): Improving Pinterest Search Relevance Using Large Language Models",
          "url": "https://medium.com/pinterest-engineering/improving-pinterest-search-relevance-using-large-language-models-4cd938d4e892",
          "note": "By Han Wang, Mukuntha Narayanan, Onur Gungor, Jinfeng Rao - April 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Trading LLM accuracy at inference for distillation + fast student",
        "common_mistake_avoided": "Trying to serve large LLMs directly instead of distilling to lightweight models",
        "when_to_use": "When LLM quality is needed but serving constraints require fast, cheap inference"
      },
      "tags": [
        "search",
        "knowledge-distillation",
        "LLM",
        "semi-supervised",
        "multilingual",
        "relevance"
      ]
    },
    {
      "id": "airtable-omni-qa-assistant-2025",
      "title": "Airtable: Building Production-Ready Q&A with Layered Context and Replanning",
      "category": "Architectural Pivot - Agent Reliability",
      "companies_involved": [
        "Airtable"
      ],
      "year": 2025,
      "initial_problem": "Building a reliable Q&A agent (Omni) that can research complex databases with large schemas and thousands of records. LLMs tend to jump to conclusions, compound mistakes, or hallucinate - a system right only half the time is unusable.",
      "initial_assumptions": [
        "Provide full schema to LLM for context",
        "LLM can directly process large amounts of data within context window",
        "Simple RAG is sufficient for retrieval",
        "Direct answers without explicit step-by-step reasoning"
      ],
      "why_it_fails": [
        "Large schemas alone consume most of context window",
        "Real-world schemas are noisy with deprecated/empty columns and ambiguous fields",
        "LLMs jump to conclusions prematurely and compound initial mistakes",
        "Unique IDs in citations are not token efficient (17-char ID = up to 15 tokens)",
        "LLM can overlook tables or columns in filtering step"
      ],
      "first_principle_insight": "Break down exploration into layered steps: high-level schema first, detailed schema on active view, then query. Plan and explicitly replan upon new data discovery. Use hybrid search with correction mechanism fallback. Encode IDs for token efficiency. A system right only half the time is unusable - every technique must target reliability.",
      "how_to_reframe": {
        "old_atomic_unit": "Full schema dump + single-shot RAG query",
        "new_atomic_unit": "Layered contextual exploration + planning/replanning + hybrid search with fallback"
      },
      "reframe": {
        "new_atomic_unit": "Multi-step agent with contextual schema exploration, replanning, and correction mechanisms",
        "new_problem_type": "Single-shot Q&A \u2192 Iterative exploration with fault tolerance",
        "new_objective": "Production-ready assistant users can trust, not just demo-quality responses",
        "architectural_changes": [
          {
            "change": "Two-step schema exploration",
            "description": "Tool to understand schema (high-level first), separate tool to query data - pares down context to most useful info"
          },
          {
            "change": "Planning and replanning",
            "description": "Chain-of-thought planning step plus replanning upon discovery of new data; handles backtracking scenarios"
          },
          {
            "change": "Hybrid search with fallback",
            "description": "Keyword + semantic search combined; if no meaningful data found, retry on wider scope for fault tolerance"
          },
          {
            "change": "Token-efficient citations",
            "description": "Encode database IDs into contextually relevant representations (17 chars \u2192 3 tokens); checksum for collision avoidance"
          },
          {
            "change": "Inline citation tags",
            "description": "Citations in natural flow of conversation for verification and hallucination reduction"
          },
          {
            "change": "Dual evaluation system",
            "description": "Curated eval suite with deterministic + LLM-as-judge scorers; live feedback from production"
          }
        ],
        "results": "30% latency improvement from token-efficient IDs; 15% cost savings; production-ready assistant launched June 2025; model-agnostic evaluation enables cross-model comparison"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Airtable Engineering Blog (2025): How we built a high quality Q&A assistant",
          "url": "https://medium.com/airtable-eng/how-we-built-a-high-quality-q-a-assistant-738ae9efeb7a",
          "note": "By Ruoni Wang - August 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - Using evaluation suite to detect and fix failure modes",
        "common_mistake_avoided": "Treating agent reliability as nice-to-have instead of core requirement; dumping full context instead of layered exploration",
        "when_to_use": "When building Q&A agents over complex databases where hallucination or partial answers are unacceptable"
      },
      "tags": [
        "qa-assistant",
        "LLM",
        "RAG",
        "agent-reliability",
        "planning",
        "evaluation"
      ]
    },
    {
      "id": "openai-contract-data-agent-2025",
      "title": "OpenAI: Scaling Contract Review with AI Agents (Humans in the Loop)",
      "category": "Architectural Pivot - Expert Augmentation",
      "companies_involved": [
        "OpenAI"
      ],
      "year": 2025,
      "initial_problem": "Every enterprise deal has a signed contract with dates, billing terms, renewal clauses. Manual process of reading line by line and retyping into spreadsheets broke when volume went from hundreds to 1000+ contracts per month in <6 months.",
      "initial_assumptions": [
        "Manual process can scale with more people",
        "Linear headcount growth matches contract volume growth",
        "Read contracts \u2192 retype to spreadsheet is sustainable"
      ],
      "why_it_fails": [
        "Only hired one new person while volume more than doubled",
        "Manual approach broke under hypergrowth",
        "Process wasn't going to scale - would need linear headcount growth"
      ],
      "first_principle_insight": "Take the repetition out of contract review, keep experts firmly in control. Don't dump everything into context - pull only what's relevant, reason against it, show work. Shift human role from manual entry to judgment. 'Manual work already done, not decisions replaced.'",
      "how_to_reframe": {
        "old_atomic_unit": "Human reads contract \u2192 manual entry into spreadsheet",
        "new_atomic_unit": "Agent parses overnight with reasoning \u2192 human reviews structured output with annotations"
      },
      "reframe": {
        "new_atomic_unit": "Ingest-Inference-Review pipeline with retrieval-augmented reasoning",
        "new_problem_type": "Manual data entry \u2192 Agent-assisted expert review",
        "new_objective": "Scale contract processing without linear headcount growth while maintaining expert control",
        "architectural_changes": [
          {
            "change": "Unified ingestion pipeline",
            "description": "PDFs, scanned copies, phone photos with handwritten edits all flow into one pipeline"
          },
          {
            "change": "Retrieval-augmented prompting",
            "description": "Don't dump 1000 pages into context; pull only relevant sections, reason against them, show work"
          },
          {
            "change": "Structured output with reasoning",
            "description": "Parse contracts into structured data with annotations explaining why terms are non-standard, citing references"
          },
          {
            "change": "Expert review workflow",
            "description": "Agent highlights unusual terms, humans confirm ASC 606 classification - experts drive outcome"
          },
          {
            "change": "Continuous feedback loop",
            "description": "Each cycle of human feedback sharpens the agent, making every review faster and more accurate"
          },
          {
            "change": "Queryable data warehouse output",
            "description": "Tabular output enables downstream data analysis"
          }
        ],
        "results": "Reviews cut in half, ready overnight; thousands of contracts processed without expanding headcount in lockstep; non-standard terms flagged with reasoning and references; architecture now supports procurement, compliance, and month-end close"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "OpenAI Blog (2025): Turning contracts into searchable data at OpenAI",
          "url": "https://openai.com/index/openai-contract-data-agent/",
          "note": "By Wei An Lee and Siddharth Jain - 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Separating repetitive work from expert judgment",
        "common_mistake_avoided": "Trying to replace expert judgment instead of augmenting it; scaling headcount linearly with volume",
        "when_to_use": "When expert work involves significant repetitive parsing that doesn't scale, but judgment must remain with humans"
      },
      "tags": [
        "document-processing",
        "contracts",
        "finance",
        "human-in-the-loop",
        "RAG",
        "enterprise"
      ]
    },
    {
      "id": "coupang-llm-journey-2024",
      "title": "Coupang: LLM Patterns for Multilingual E-commerce (Labels, Embeddings, Categorization)",
      "category": "Architectural Pivot - LLM Application Patterns",
      "companies_involved": [
        "Coupang"
      ],
      "year": 2024,
      "initial_problem": "Coupang operates in South Korea and Taiwan where training data is small in Korean/Mandarin. Global sellers create challenges in product understanding across languages. Human labels are costly for multilingual content.",
      "initial_assumptions": [
        "Need single ML model per category for categorization",
        "Human labels are required for training data",
        "Separate embeddings for image and text data",
        "Traditional ML techniques can handle multilingual diversity"
      ],
      "why_it_fails": [
        "Single model per category created operational burden managing multiple models",
        "Human labels challenging and costly for multilingual content (Korean, Mandarin, English)",
        "Unified multi-class models yielded noisy predictions for tail categories",
        "Separate image/text embeddings inferior to joint modeling"
      ],
      "first_principle_insight": "LLMs solve multiple problems: (1) Generate weak labels at scale rivaling human quality - solves label scarcity; (2) Joint image+text modeling yields superior embeddings; (3) Single LLM-powered categorizer replaces multiple category-specific models. Use in-context learning for rapid prototyping, distill to smaller models for production.",
      "how_to_reframe": {
        "old_atomic_unit": "Category-specific models + human labels + separate embeddings",
        "new_atomic_unit": "Single LLM with weak label generation + joint embeddings + distillation to production"
      },
      "reframe": {
        "new_atomic_unit": "LLM as unified solution for labels, embeddings, and categorization",
        "new_problem_type": "Multiple specialized models \u2192 Single LLM with multiple application patterns",
        "new_objective": "Leverage LLMs to overcome multilingual label scarcity and model proliferation",
        "architectural_changes": [
          {
            "change": "LLM weak label generation",
            "description": "Generate labels at scale rivaling human quality; especially valuable for under-resourced languages and new segments"
          },
          {
            "change": "Joint vision-language modeling",
            "description": "CLIP-style joint image+text embeddings outperform separate embeddings for ad retrieval and similarity"
          },
          {
            "change": "Single LLM categorizer",
            "description": "Replaced multiple category-specific models with one LLM-powered categorizer with gains across most categories"
          },
          {
            "change": "In-context learning for prototyping",
            "description": "Fast iteration with prompts, no training needed; most popular pattern internally"
          },
          {
            "change": "Distillation to production",
            "description": "Train large source LLM \u2192 distill smaller model for real-time inference"
          },
          {
            "change": "Hybrid multi-region GPU infrastructure",
            "description": "Cloud + on-prem across Asia-Pacific & US to mitigate GPU shortage; match GPU to workload (A100 for training, A10G for testing)"
          },
          {
            "change": "Ray + vLLM batch inference",
            "description": "~20x throughput improvement; handles heterogeneous CPU preprocessing + GPU inference"
          }
        ],
        "results": "Improved production ML systems across search, catalog, operations, ads; weak labels overcome label scarcity in under-resourced languages; single categorizer with precision gains across categories; nearline inference supports diverse downstream applications with smaller resource footprint"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Coupang Engineering Blog (2024): Accelerating Coupang's AI Journey with LLMs",
          "url": "https://medium.com/coupang-engineering/accelerating-coupangs-ai-journey-with-llms-2817d55004d3",
          "note": "By ML Platform Team - October 2024"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - LLMs as alternative to specialized models and human labeling",
        "common_mistake_avoided": "Building separate models per category instead of unified LLM approach; waiting for human labels instead of generating weak labels",
        "when_to_use": "When facing multilingual/low-resource language challenges, label scarcity, or operational burden from model proliferation"
      },
      "tags": [
        "LLM",
        "multilingual",
        "weak-labels",
        "distillation",
        "categorization",
        "e-commerce",
        "embeddings"
      ]
    },
    {
      "id": "doordash-chatbot-ugc-knowledge-2025",
      "title": "DoorDash: Auto-Generating Knowledge Base from Escalated Chats",
      "category": "Architectural Pivot - Knowledge Base Automation",
      "companies_involved": [
        "DoorDash"
      ],
      "year": 2025,
      "initial_problem": "Support chatbot handles huge volume but as marketplace grows, new policies, product changes, and long-tail edge cases demand fresh answers. Manually maintaining knowledge base cannot scale - too resource-intensive and time-consuming.",
      "initial_assumptions": [
        "Knowledge base must be manually written and maintained",
        "Content specialists must identify gaps by reviewing transcripts",
        "KB articles are written from scratch based on policy documents"
      ],
      "why_it_fails": [
        "Manual KB maintenance doesn't scale with marketplace complexity",
        "Takes weeks to identify gaps and draft new articles",
        "Content specialists spend time on transcript review instead of refinement",
        "Long-tail edge cases remain unaddressed"
      ],
      "first_principle_insight": "Escalated chats are signals of KB gaps; agent resolutions are user-generated content that already contains the answer. Cluster escalated transcripts to find highest-ROI gaps, use LLM to classify (actionable vs informational) and draft articles from agent resolutions. Humans review and refine instead of writing from scratch.",
      "how_to_reframe": {
        "old_atomic_unit": "Manual identification of KB gaps \u2192 manual article writing",
        "new_atomic_unit": "Clustering identifies gaps automatically \u2192 LLM drafts from agent resolutions \u2192 humans review"
      },
      "reframe": {
        "new_atomic_unit": "Escalation-driven, LLM-drafted knowledge base with human review",
        "new_problem_type": "Manual KB maintenance \u2192 Automated gap detection and draft generation",
        "new_objective": "Scale KB content creation at machine speed while humans focus on refinement",
        "architectural_changes": [
          {
            "change": "Semantic clustering of escalated chats",
            "description": "Embed chat summaries, cluster by cosine similarity (threshold 0.70-0.90), rank by frequency/severity to prioritize gaps"
          },
          {
            "change": "LLM smart classifier",
            "description": "Classifies clusters as actionable (workflow recipes) vs informational (KB article candidates)"
          },
          {
            "change": "LLM draft generation from UGC",
            "description": "Ingests issue summary + agent resolutions to produce polished KB draft in minutes instead of weeks"
          },
          {
            "change": "Human review queue",
            "description": "Specialists check policy references, tone, edge cases; flag nuances for conditional branching"
          },
          {
            "change": "Issue-only embedding for retrieval",
            "description": "Embed only 'user issue' portion for RAG, not entire KB entry - increases precision"
          },
          {
            "change": "Prompt/model consistency",
            "description": "Same summarization approach in production chatbot and KB generation ensures retrieval accuracy"
          }
        ],
        "results": "Escalation rate dropped from 78% to 43% on high-traffic clusters; ~75% of KB retrievals now contain only UGC content; articles drafted in minutes instead of weeks; specialists focus on refinement not transcript review"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering Blog (2025): A scalable LLM approach to enhancing chatbot knowledge with user-generated content",
          "url": "https://careersatdoordash.com/blog/doordash-llm-chatbot-knowledge-with-ugc/",
          "note": "By Tony Luo, Zhe Jia, Gisselle Xie - August 2025"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Recognizing that agent resolutions ARE the content, just unstructured",
        "common_mistake_avoided": "Writing KB articles from scratch when agent resolutions already contain the answers",
        "when_to_use": "When support escalations signal KB gaps and agent resolutions can be mined as user-generated content"
      },
      "tags": [
        "chatbot",
        "knowledge-base",
        "clustering",
        "LLM",
        "RAG",
        "user-generated-content",
        "support"
      ]
    },
    {
      "id": "doordash-llm-behavioral-silos-2025",
      "title": "DoorDash: LLMs as Semantic Bridge Across Behavioral Silos",
      "category": "Architectural Pivot - Cross-Vertical Personalization",
      "companies_involved": [
        "DoorDash"
      ],
      "year": 2025,
      "initial_problem": "Multi-vertical marketplaces have 'behavioral silos' - customers have deep history in some categories (restaurants with compact menus, dense data) but are effectively cold-start in others (grocery with 100k+ SKUs where behavior spreads thinly).",
      "initial_assumptions": [
        "Standard recommenders work with available interaction data per SKU",
        "Each vertical should be modeled separately",
        "Need to wait for user history to accumulate in each vertical",
        "Popularity baselines are acceptable for sparse catalog data"
      ],
      "why_it_fails": [
        "Signal quality varies wildly by category - restaurants have dense clean data, grocery is sparse",
        "Same customer is well-understood in restaurants but cold-start in grocery",
        "Popularity baselines overexpose head products, pushing aside relevant long-tail items",
        "Waiting for history means poor personalization in new verticals"
      ],
      "first_principle_insight": "Consumer behavior across verticals contains hidden patterns (cuisine preferences, dietary patterns, price anchors) that can be abstracted into cross-domain semantic features. LLMs act as a 'semantic bridge' - translating noisy activity in one vertical into high-fidelity, generalizable representations for another. E.g., Indian food orders \u2192 interest in Indian grocery items.",
      "how_to_reframe": {
        "old_atomic_unit": "Per-vertical interaction history with cold-start in new categories",
        "new_atomic_unit": "LLM-derived cross-vertical affinity features that transfer knowledge across silos"
      },
      "reframe": {
        "new_atomic_unit": "Hierarchical RAG pipeline generating taxonomy-aligned affinity features",
        "new_problem_type": "Per-vertical cold start \u2192 Cross-vertical semantic feature transfer",
        "new_objective": "Personalize from day one in new verticals by reusing signals from data-rich categories",
        "architectural_changes": [
          {
            "change": "Hierarchical RAG (H-RAG)",
            "description": "Translate orders/search into 4-level product taxonomy (L1-L4); predict broad categories first to constrain deeper search space"
          },
          {
            "change": "Prompt engineering for reliability",
            "description": "Chronological ordering, rich taxonomy context, temperature=0.1, confidence threshold \u22650.80 to filter spurious associations"
          },
          {
            "change": "GPT-4o-mini for cost efficiency",
            "description": "Similar quality to GPT-4o at lower cost; cached static prompts, dynamic user history only"
          },
          {
            "change": "Just-in-time feature materialization",
            "description": "Recompute affinities only when user performs new action - 80% cost reduction total"
          },
          {
            "change": "Multi-task ranker integration",
            "description": "LLM-derived sparse features concatenated with engagement features; mean pooling for variable-length embeddings"
          },
          {
            "change": "Dual evaluation",
            "description": "Human raters + LLM-as-judge on 3-point personalization scale; search features scored higher than order features (explicit vs implicit intent)"
          }
        ],
        "results": "Consistent offline and online gains; improved relevance especially for cold-start scenarios; practical inference costs via prompt design, caching, and small language models; RecSys 2025 paper"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "DoorDash Engineering Blog (2025): Mind the Gap: Using LLMs to Bridge Behavioral Silos in Multi-Vertical Recommendations",
          "url": "https://careersatdoordash.com/blog/doordash-llms-bridge-behavioral-silos-in-multi-vertical-recommendations/",
          "note": "By Nimesh Sinha, Raghav Saboo, Sudeep Das, Martin Wang - December 2025; RecSys 2025 paper"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - LLMs as semantic bridge instead of waiting for per-vertical history",
        "common_mistake_avoided": "Treating verticals as independent instead of leveraging cross-vertical semantic patterns",
        "when_to_use": "When expanding into new verticals with sparse data but existing rich behavioral signals in other categories"
      },
      "tags": [
        "recommendations",
        "cold-start",
        "LLM",
        "cross-vertical",
        "RAG",
        "personalization",
        "multi-vertical"
      ]
    },
    {
      "id": "bloom-176b-init-parameter-reframe",
      "title": "BLOOM 176B: Init Parameter First Principles Reframe",
      "category": "Diagnostic Pivot - Training Stability",
      "companies_involved": [
        "BigScience",
        "Hugging Face"
      ],
      "year": 2022,
      "initial_problem": "Training a 176B parameter multilingual language model with stable convergence across the full training run.",
      "initial_assumptions": [
        "Megatron-LM default init-method-std of 0.02 is a reasonable starting point",
        "Hyperparameters that work at smaller scales will transfer to larger models",
        "Training instability is primarily a learning rate or optimizer issue"
      ],
      "why_it_fails": [
        "Default init std of 0.02 was 'way too big' for 176B scale model",
        "Caused numerical instability in early training at 104B checkpoint",
        "Query * Key matrix multiplication output would blow up in fp16 mixed precision",
        "Inherited defaults without deriving from first principles"
      ],
      "first_principle_insight": "Parameter initialization must scale with model dimensions. The correct init std should be derived from the hidden dimension using sqrt(1/(NHIDDEN*3)), not inherited from smaller model defaults. For NHIDDEN=14336, this gives 0.00482 instead of the default 0.02 \u2014 a 4x reduction that prevents numerical blow-ups.",
      "how_to_reframe": {
        "old_atomic_unit": "Use framework defaults that worked at smaller scale",
        "new_atomic_unit": "Derive initialization from first principles based on model architecture"
      },
      "reframe": {
        "new_atomic_unit": "Architecture-derived initialization parameters",
        "new_problem_type": "Default hyperparameter tuning \u2192 First principles derivation",
        "new_objective": "Stable training from step 1 by matching initialization to model scale",
        "architectural_changes": [
          {
            "change": "Derived init std from hidden dimension",
            "description": "Applied formula sqrt(1/(NHIDDEN*3)) = 0.00482 for NHIDDEN=14336 instead of default 0.02"
          },
          {
            "change": "Scaled attention normalization",
            "description": "Moved normalization factor inward before Q*K multiplication: scale Q and K by 1.0/sqrt(norm_factor) separately to prevent fp16 overflow"
          }
        ],
        "results": "Stable training of BLOOM-176B to completion. This was described as 'one of the crucial fixes' that enabled the full training run. The team successfully trained the largest open multilingual model at the time."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Stas Bekman, ML Engineering Open Book - Training Instabilities",
          "url": "https://github.com/stas00/ml-engineering/blob/master/training/instabilities/README.md",
          "note": "Detailed documentation of BLOOM training challenges and solutions"
        },
        {
          "type": "primary",
          "citation": "BigScience Workshop (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
          "url": "https://arxiv.org/abs/2211.05100",
          "note": "Original BLOOM paper"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Diagnosis) - Questioning inherited defaults, deriving from first principles",
        "common_mistake_avoided": "Blindly using framework defaults without considering scale",
        "when_to_use": "When scaling models and experiencing training instability \u2014 question whether hyperparameters were derived or inherited"
      },
      "tags": [
        "LLM-training",
        "initialization",
        "first-principles",
        "numerical-stability",
        "large-scale"
      ]
    },
    {
      "id": "ml-infra-bottleneck-shift-reframe",
      "title": "ML Infrastructure: The Bottleneck Shift from Compute to Data Movement",
      "category": "Architectural Pivot - Infrastructure First",
      "companies_involved": [
        "BigScience",
        "Hugging Face"
      ],
      "year": 2024,
      "initial_problem": "ML teams invest heavily in GPU compute but achieve only ~50% Model FLOPS Utilization (MFU) on multi-node setups despite expensive hardware.",
      "initial_assumptions": [
        "Faster GPUs = faster training",
        "Invest 80% of budget in compute, 20% in infrastructure",
        "Peak TFLOPS is the metric that matters",
        "Network and storage are commodity concerns"
      ],
      "why_it_fails": [
        "GPUs sit idle waiting for data \u2014 'The bottleneck is in moving bits, not compute'",
        "Advertised peak TFLOPS vs actual MFU gap is where projects die",
        "The 'furnace' (GPU) is starved of fuel (data) due to insufficient network/storage bandwidth",
        "Teams optimize the wrong constraint"
      ],
      "first_principle_insight": "Modern ML infrastructure is fundamentally constrained by data movement, not compute. The locomotive analogy: investing in a bigger engine while starving it of fuel wastes the investment. Teams should invest equally in network/storage as in accelerators, not 80/20 toward compute.",
      "how_to_reframe": {
        "old_atomic_unit": "GPU TFLOPS as the optimization target",
        "new_atomic_unit": "End-to-end data throughput as the optimization target"
      },
      "reframe": {
        "new_atomic_unit": "Balanced infrastructure investment across compute, network, and storage",
        "new_problem_type": "Compute optimization \u2192 System throughput optimization",
        "new_objective": "Maximize actual MFU by removing data movement bottlenecks",
        "architectural_changes": [
          {
            "change": "Equal infrastructure investment",
            "description": "Allocate budget equally across accelerators, network, and storage instead of 80/20 toward compute"
          },
          {
            "change": "MFU as primary metric",
            "description": "Track Model FLOPS Utilization instead of peak TFLOPS \u2014 this reveals actual vs theoretical performance"
          },
          {
            "change": "Network-first design",
            "description": "Design cluster topology around data movement patterns, not just GPU count"
          }
        ],
        "results": "Teams that rebalanced infrastructure investment saw MFU improvements from ~50% toward 70%+. The insight: 'The gap between marketing specs and achievable throughput is where projects die.'"
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Stas Bekman, ML Engineering Open Book - AI Battlefield Engineering",
          "url": "https://github.com/stas00/ml-engineering/blob/master/insights/ai-battlefield.md",
          "note": "Production insights from BLOOM-176B and IDEFICS-80B training"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Identifying the real atomic unit (throughput, not compute)",
        "common_mistake_avoided": "Optimizing GPU specs when the real constraint is IO bandwidth",
        "when_to_use": "When GPU utilization is low despite expensive hardware \u2014 look at data movement"
      },
      "tags": [
        "infrastructure",
        "GPU-utilization",
        "bottleneck-analysis",
        "large-scale-training",
        "MFU"
      ]
    },
    {
      "id": "palm-bad-batch-pragmatic-pivot",
      "title": "PaLM: The 'Bad Batch' Pragmatic Pivot",
      "category": "Pragmatic Pivot - Route Around",
      "companies_involved": [
        "Google"
      ],
      "year": 2022,
      "initial_problem": "During PaLM training, the team observed 'dozens of loss spikes at highly irregular intervals' that disrupted training stability.",
      "initial_assumptions": [
        "Loss spikes have diagnosable root causes",
        "With enough investigation, we can identify and fix the source",
        "Training instability requires understanding before mitigation"
      ],
      "why_it_fails": [
        "Root cause was 'unidentified combination of data batch and model parameter state'",
        "Spikes were irreproducible \u2014 couldn't recreate the conditions",
        "Debugging would consume months without guarantee of solution",
        "Perfect understanding isn't always achievable or necessary"
      ],
      "first_principle_insight": "Sometimes the right pivot is 'accept and route around' rather than 'understand and fix.' When a problem is irreproducible and the workaround is cheap, pragmatism beats perfectionism. The PaLM team implemented batch skipping: restart from checkpoint and skip the problematic batch. Not a root cause fix, but an effective mitigation.",
      "how_to_reframe": {
        "old_atomic_unit": "Diagnose and fix the root cause of every failure",
        "new_atomic_unit": "Implement cheap workarounds for irreproducible failures"
      },
      "reframe": {
        "new_atomic_unit": "Pragmatic mitigation over perfect diagnosis",
        "new_problem_type": "Root cause analysis \u2192 Cost-effective workaround",
        "new_objective": "Keep training running with minimal disruption",
        "architectural_changes": [
          {
            "change": "Checkpoint-based recovery",
            "description": "Restart from last good checkpoint when loss spike detected"
          },
          {
            "change": "Batch skipping",
            "description": "Skip the problematic data batch and continue training"
          },
          {
            "change": "Spike detection automation",
            "description": "Automated monitoring to detect and recover from spikes without manual intervention"
          }
        ],
        "results": "Successfully trained PaLM-540B to completion despite dozens of unexplained loss spikes. The pragmatic approach allowed training to continue while the root cause remained unknown."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Stas Bekman, ML Engineering Open Book - Training Instabilities",
          "url": "https://github.com/stas00/ml-engineering/blob/master/training/instabilities/README.md",
          "note": "Documents PaLM loss spike phenomenon and mitigation"
        },
        {
          "type": "primary",
          "citation": "Chowdhery et al. (2022). PaLM: Scaling Language Modeling with Pathways",
          "url": "https://arxiv.org/abs/2204.02311",
          "note": "Original PaLM paper"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Pivot) - Architectural courage to route around instead of debug endlessly",
        "common_mistake_avoided": "Spending months debugging irreproducible issues when a workaround exists",
        "when_to_use": "When facing irreproducible failures and the cost of workaround is lower than diagnosis"
      },
      "tags": [
        "LLM-training",
        "pragmatic-pivot",
        "loss-spikes",
        "architectural-courage",
        "workaround"
      ]
    },
    {
      "id": "bloom-memory-leak-pragmatic-pivot",
      "title": "BLOOM/IDEFICS: Memory Leak Pragmatic Pivot",
      "category": "Pragmatic Pivot - Ship Over Perfect",
      "companies_involved": [
        "BigScience",
        "Hugging Face"
      ],
      "year": 2023,
      "initial_problem": "During IDEFICS-80B training, the team discovered 'some tiny CPU memory leak that would often take multiple days to lead to running out of CPU memory.'",
      "initial_assumptions": [
        "Bugs should be fixed before production training",
        "Memory leaks require debugging and patching",
        "Training should not proceed with known issues"
      ],
      "why_it_fails": [
        "Debugging the leak would delay training significantly",
        "The leak was slow enough that workaround was viable",
        "Perfect code isn't required if monitoring catches problems early",
        "Time-to-training-completion was the real constraint"
      ],
      "first_principle_insight": "Documented workarounds outweigh extended debugging if they unblock progress. The team implemented memory monitoring with automatic graceful exits: check memory thresholds periodically, voluntarily exit and restart before OOM. Imperfect solution that ships beats perfect solution that doesn't.",
      "how_to_reframe": {
        "old_atomic_unit": "Fix all bugs before training",
        "new_atomic_unit": "Monitor and mitigate bugs that don't block progress"
      },
      "reframe": {
        "new_atomic_unit": "Graceful degradation with monitoring",
        "new_problem_type": "Bug fixing \u2192 Operational mitigation",
        "new_objective": "Complete training on schedule despite known imperfections",
        "architectural_changes": [
          {
            "change": "Memory threshold monitoring",
            "description": "Check CPU memory usage at regular intervals during training"
          },
          {
            "change": "Graceful exit protocol",
            "description": "When memory exceeds threshold, save checkpoint and exit cleanly before OOM"
          },
          {
            "change": "Automatic job resubmission",
            "description": "SLURM job arrays automatically restart training from checkpoint after graceful exit"
          }
        ],
        "results": "Successfully trained IDEFICS-80B to completion without fixing the underlying memory leak. The monitoring overhead was negligible compared to the debugging time saved."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Stas Bekman, ML Engineering Open Book - Fault Tolerance",
          "url": "https://github.com/stas00/ml-engineering/blob/master/training/fault-tolerance/README.md",
          "note": "Documents IDEFICS training challenges and mitigations"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Pivot) - 'Good enough' threshold \u2014 when workaround is cheaper than fix",
        "common_mistake_avoided": "Delaying training to achieve perfect code when imperfect + monitored is sufficient",
        "when_to_use": "When facing slow-moving bugs where monitoring can catch problems before they're catastrophic"
      },
      "tags": [
        "LLM-training",
        "pragmatic-pivot",
        "fault-tolerance",
        "monitoring",
        "good-enough"
      ]
    },
    {
      "id": "bloom-hardware-failure-planning",
      "title": "BLOOM: Planning for 10% Hardware Failure Rate",
      "category": "Pragmatic Pivot - Infrastructure Control",
      "companies_involved": [
        "BigScience",
        "Hugging Face"
      ],
      "year": 2022,
      "initial_problem": "Large-scale distributed training across hundreds of GPUs assumes hardware reliability, but new accelerators have 'as large as 10% failure rate early on.'",
      "initial_assumptions": [
        "Hardware works; plan for success",
        "Dynamic node allocation is more efficient than fixed allocation",
        "Failures are rare exceptions to handle reactively",
        "Training infrastructure is a solved problem"
      ],
      "why_it_fails": [
        "10% failure rate on new accelerators is normal, not exceptional",
        "Dynamic allocation exposes you to poorly-validated hardware between users",
        "Reactive failure handling causes scrambling during crises",
        "You can't see signals you don't measure"
      ],
      "first_principle_insight": "Prevention beats recovery. For large-scale training, assume hardware will fail and design for it: use fixed node allocations (validated hardware), maintain 5-10% spare capacity, implement multi-layered watchdog systems. The easiest way to avoid losing training time is to prevent certain types of problems from happening.",
      "how_to_reframe": {
        "old_atomic_unit": "Plan for success, handle failures reactively",
        "new_atomic_unit": "Plan for failure, prevent problems proactively"
      },
      "reframe": {
        "new_atomic_unit": "Failure-aware infrastructure design",
        "new_problem_type": "Reactive failure handling \u2192 Proactive failure prevention",
        "new_objective": "Minimize training interruptions through prevention and fast recovery",
        "architectural_changes": [
          {
            "change": "Fixed node allocation",
            "description": "Use dedicated validated hardware instead of dynamic pools with unknown failure history"
          },
          {
            "change": "Spare capacity buffer",
            "description": "Maintain 5-10% spare hardware to enable automatic failover without scrambling"
          },
          {
            "change": "Multi-layered watchdog system",
            "description": "Job status monitoring, disk space alerts (usage + inodes), application-level memory monitoring"
          },
          {
            "change": "Automatic node draining",
            "description": "Failed hardware automatically isolated from pool; jobs automatically resume on healthy nodes"
          }
        ],
        "results": "BLOOM-176B training completed successfully despite multiple hardware failures. The proactive approach meant failures were handled automatically rather than requiring manual intervention."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Stas Bekman, ML Engineering Open Book - Fault Tolerance",
          "url": "https://github.com/stas00/ml-engineering/blob/master/training/fault-tolerance/README.md",
          "note": "Documents BLOOM fault tolerance strategies"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - Define monitoring and stop criteria before you build",
        "common_mistake_avoided": "Assuming hardware reliability and scrambling when failures occur",
        "when_to_use": "Any large-scale distributed training \u2014 assume 5-10% failure rate and design accordingly"
      },
      "tags": [
        "infrastructure",
        "fault-tolerance",
        "monitoring",
        "hardware-failures",
        "large-scale-training"
      ]
    },
    {
      "id": "gene-perturbation-baseline-beats-foundation-2025",
      "title": "Gene Perturbation Prediction: Linear Baselines Beat Foundation Models",
      "category": "Cautionary Tale - Complexity Without Improvement",
      "companies_involved": [
        "Academic Research"
      ],
      "initial_problem": "Predict how genetic perturbations (alterations in gene expression or function) affect the transcriptome (gene expression profile of cells)",
      "initial_assumptions": [
        "Foundation models trained on massive biological data will capture complex gene interactions",
        "Deep learning architectures can learn non-linear relationships in genetic data",
        "More sophisticated models will outperform simple baselines"
      ],
      "why_it_fails": [
        "Deep learning models failed to consistently outperform simple mean prediction or linear models",
        "For double perturbations, prediction error was HIGHER in deep learning models than additive baselines",
        "Foundation models showed negligible benefits despite massive computational investment"
      ],
      "first_principle_insight": "Always benchmark against simple baselines before investing in complexity. The gap between simple and complex may be smaller than assumed  or non-existent.",
      "how_to_reframe": {
        "old_atomic_unit": "Complex learned representation of gene interactions",
        "new_atomic_unit": "Linear combination of individual gene effects"
      },
      "reframe": {
        "new_atomic_unit": "Simple additive model of perturbation effects",
        "new_problem_type": "Linear regression baseline",
        "new_objective": "Establish whether complexity earns its place with measurable improvement",
        "architectural_changes": [
          {
            "change": "Baseline-first evaluation",
            "description": "Compare 5 foundation models and 2 deep learning models against deliberately simple baselines (mean prediction, linear model)"
          }
        ],
        "results": "None of the deep learning models consistently outperformed the baselines. Simple linear models matched or exceeded foundation model performance."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Nature Methods (2025): Deep-learning-based gene perturbation effect prediction does not yet outperform simple linear baselines",
          "url": "https://www.nature.com/articles/s41592-025-02772-6",
          "note": "Rigorous benchmark study comparing foundation models to simple baselines"
        },
        {
          "type": "secondary",
          "citation": "Phys.org (2025): Complex deep learning models are no better at understanding genetic perturbation than simple baseline ones",
          "url": "https://phys.org/news/2025-08-complex-deep-genetic-perturbation-simple.html",
          "note": "Summary of findings"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Complexity must earn its place with measurable improvement",
        "common_mistake_avoided": "Assuming foundation models will outperform baselines without rigorous comparison",
        "when_to_use": "Before any deep learning investment, establish what a simple baseline achieves"
      },
      "tags": [
        "baseline-comparison",
        "foundation-models",
        "cautionary-tale",
        "simplicity-principle",
        "biology",
        "deep-learning"
      ]
    },
    {
      "id": "klarna-chatbot-llm-reversal-2024-2025",
      "title": "Klarna Chatbot: LLM Hype to Human Reinvestment",
      "category": "Cautionary Tale - LLM Over-Deployment",
      "companies_involved": [
        "Klarna",
        "OpenAI"
      ],
      "initial_problem": "Handle high volume of customer service inquiries efficiently while reducing costs",
      "initial_assumptions": [
        "LLM-powered chatbot can replace human agents at scale",
        "AI assistant can handle two-thirds of customer service chats",
        "Cost savings of $40M annually justify the investment",
        "Quality will be maintained or improved with AI"
      ],
      "why_it_fails": [
        "Quality degradation: CEO admitted 'cost unfortunately seems to have been a too predominant evaluation factor'",
        "Critics noted much of this automation could have been achieved with traditional rules-based approaches pre-LLM",
        "The Pragmatic Engineer: 'Klarna could have easily automated away L1 support that answers basic questions ages ago, without an LLM'",
        "Over-optimization for cost led to lower quality customer experience"
      ],
      "first_principle_insight": "Just because you CAN use an LLM doesn't mean you SHOULD. Many customer service tasks are better served by rules, templates, and human agents. Mature companies built effective L1 automation before LLMs existed.",
      "how_to_reframe": {
        "old_atomic_unit": "LLM-generated response to any customer query",
        "new_atomic_unit": "Tiered system: rules for simple queries, humans for complex ones"
      },
      "reframe": {
        "new_atomic_unit": "Hybrid system with human reinvestment",
        "new_problem_type": "Tiered automation with quality gates",
        "new_objective": "Optimize for quality AND efficiency, not just cost reduction",
        "architectural_changes": [
          {
            "change": "2024: LLM deployment",
            "description": "OpenAI-powered chatbot handling 2/3 of customer service, claimed $40M savings"
          },
          {
            "change": "2025: Reversal",
            "description": "Redirected investment back into human agents due to quality concerns"
          }
        ],
        "results": "Klarna reversed course in 2025, reinvesting in human agents. CEO acknowledged quality suffered when cost was the dominant factor."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "The Pragmatic Engineer (2024): Klarna's AI chatbot: how revolutionary is it, really?",
          "url": "https://blog.pragmaticengineer.com/klarnas-ai-chatbot/",
          "note": "Critical analysis arguing rules could have achieved similar results"
        },
        {
          "type": "secondary",
          "citation": "PolyAI (2025): What companies can learn from Klarna's AI chatbot reversal",
          "url": "https://poly.ai/blog/klarna-ai-customer-service-lessons/",
          "note": "Analysis of the reversal decision"
        },
        {
          "type": "secondary",
          "citation": "OpenAI Case Study (2024): Klarna's AI assistant does the work of 700 full-time agents",
          "url": "https://openai.com/index/klarna/",
          "note": "Original announcement of the LLM deployment"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Always consider non-LLM options; Step 5 (Signals) - Quality metrics matter, not just cost",
        "common_mistake_avoided": "Deploying LLMs for tasks that rules + templates + humans handle better",
        "when_to_use": "When evaluating LLM deployment for customer service or L1 support tasks"
      },
      "tags": [
        "llm-deployment",
        "customer-service",
        "cautionary-tale",
        "cost-vs-quality",
        "chatbot",
        "genai"
      ]
    },
    {
      "id": "speech-recognition-custom-evals-reframe-2025",
      "title": "EdTech Speech Recognition: Custom Evals Reveal Hardware, Not Model Problem",
      "category": "Diagnostic Pivot - Right Layer of the Stack",
      "companies_involved": [
        "Education Technology (unnamed)"
      ],
      "initial_problem": "Speech recognition app for elementary students was failing, but the team had no idea where it was actually failing or which sounds were consistently misclassified",
      "initial_assumptions": [
        "Generic model benchmarks showing improvements would translate to better production performance",
        "Upgrading to a newer model with better benchmarks would improve their specific use case",
        "Performance issues were model-related"
      ],
      "why_it_fails": [
        "After upgrading based on benchmark claims, results were 'all over the place  better on some phonemes, worse on others'",
        "Generic benchmarks measure breadth across many tasks, not depth on one specific task",
        "The actual problem wasn't at the model layer at all"
      ],
      "first_principle_insight": "Custom evals help you diagnose the right layer. Generic benchmarks measure breadth across many tasks. Your application needs depth on one specific task. What looks like a model problem may be a hardware problem.",
      "how_to_reframe": {
        "old_atomic_unit": "Model performance on generic benchmarks",
        "new_atomic_unit": "Model performance on domain-specific samples from actual production environment"
      },
      "reframe": {
        "new_atomic_unit": "Domain-specific evaluation with real classroom audio samples",
        "new_problem_type": "Layer diagnosis (input/hardware vs model)",
        "new_objective": "Identify which layer of the stack is causing failures before attempting fixes",
        "architectural_changes": [
          {
            "change": "Built custom eval suite",
            "description": "Created evaluations using 500+ real classroom audio samples instead of generic benchmarks"
          },
          {
            "change": "Layer-specific diagnosis",
            "description": "Discovered plosive consonants were being filtered by noise-reducing microphones  a hardware issue, not a model issue"
          },
          {
            "change": "Right intervention",
            "description": "Identified that plosive issue required hardware upgrades, not model changes"
          }
        ],
        "results": "Avoided costly migration with unmeasured consequences. Identified root cause as hardware (microphones filtering plosives), not model. Reduced iteration cycles from days to hours. Deployed with measurable confidence in quality thresholds."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Randy Olson (2025): Why Custom Evals Matter for Production LLMs",
          "url": "https://www.randalolson.com/2025/12/22/why-custom-evals-matter-for-production-llms/",
          "note": "Detailed walkthrough of building custom evals for speech recognition"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - Define domain-specific evals before trusting generic benchmarks",
        "common_mistake_avoided": "Trusting generic benchmarks for domain-specific applications; blaming the model when the problem is at a different layer",
        "when_to_use": "When model performance doesn't match benchmark claims; when you need to diagnose which layer of the stack is failing"
      },
      "tags": [
        "custom-evals",
        "speech-recognition",
        "diagnostic",
        "layer-diagnosis",
        "hardware-vs-model",
        "genai",
        "production"
      ]
    },
    {
      "id": "dead-salmons-interpretability-validation-2024",
      "title": "Dead Salmons of AI Interpretability: When Explanations Explain Nothing",
      "category": "Cautionary Tale - Interpretability Validation",
      "year": "2024",
      "companies_involved": ["Research (Mloux et al.)"],
      "initial_problem": "Interpretability methods are widely used to explain model behavior, but how do we know if these explanations are genuine insights or statistical artifacts?",
      "initial_assumptions": [
        "Interpretability methods reveal how models actually reason",
        "Plausible-looking explanations indicate meaningful insights",
        "If an explanation method produces coherent outputs, it's working correctly"
      ],
      "why_it_fails": [
        "Many interpretability methods produce seemingly meaningful explanations for random or untrained models",
        "Without baseline validation, researchers can't distinguish signal from noise",
        "The field has published findings that may be statistical artifacts, not genuine insights"
      ],
      "first_principle_insight": "Interpretability methods need sanity checks against null baselines. If a method produces 'explanations' for random models, it's not explaining anything  it's finding patterns in noise. This is the AI equivalent of neuroscience's 'dead salmon' problem, where researchers found apparent brain activation in a deceased fish due to uncorrected multiple comparisons.",
      "how_to_reframe": {
        "old_atomic_unit": "Does the explanation look plausible?",
        "new_atomic_unit": "Does the explanation differ meaningfully from a null baseline (random/untrained model)?"
      },
      "reframe": {
        "new_atomic_unit": "Explanation validated against null baseline",
        "new_problem_type": "Validation before interpretation",
        "new_objective": "Only trust explanations that pass sanity checks against random/untrained baselines",
        "architectural_changes": [
          {
            "change": "Null Baseline Testing",
            "description": "Always test interpretability methods against null baselines. Three sanity checks to run routinely: (1) Same architecture, random weights  does the method find 'explanations' in untrained noise? (2) Same data, shuffled labels  does training on meaningless targets produce similar explanations? (3) Frozen model, randomized probe targets  does the probe find patterns that aren't there?"
          },
          {
            "change": "Statistical Rigor",
            "description": "Apply proper multiple comparison corrections. The dead salmon in neuroscience showed 'brain activation' only because statistical corrections weren't applied."
          },
          {
            "change": "Explanation Contrast",
            "description": "Evaluate explanations by how much they differ between meaningful conditions (trained vs untrained, correct vs incorrect predictions), not by how plausible they appear."
          }
        ],
        "results": "The paper demonstrates that numerous interpretability approaches yield plausible outputs regardless of model quality, calling for more rigorous validation standards before trusting explanatory findings."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Mloux et al. (2024): The Dead Salmons of AI Interpretability",
          "url": "https://arxiv.org/abs/2512.18792",
          "note": "Research paper examining false positive rates in interpretability methods"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 5 (Signals) - How will we know if we're wrong? Validate interpretability tools against null baselines before trusting outputs.",
        "common_mistake_avoided": "Trusting interpretability outputs at face value without baseline validation",
        "when_to_use": "When using any interpretability or explainability method; when evaluating published interpretability research",
        "cross_reference": "Extends the Sanity Check principle from Lesson 4 (Part 2) to interpretability methods. The same null baseline tests that validate models also validate the tools we use to explain them."
      },
      "tags": [
        "interpretability",
        "validation",
        "cautionary-tale",
        "signals",
        "false-positives",
        "sanity-checks",
        "research-methods"
      ]
    },
    {
      "id": "anthropic-swarm-migrations-2025",
      "title": "Anthropic Internal: Swarm Pattern Delivers 10x Speedup on Large-Scale Migrations",
      "category": "Architectural Pivot - Agent Architecture",
      "year": "2025",
      "companies_involved": ["Anthropic"],
      "initial_problem": "Engineering teams spending excessive time on large-scale code migrations (framework upgrades, lint rule rollouts, API changes across hundreds of files)",
      "initial_assumptions": [
        "Sequential processing is the natural approach for code changes",
        "A single agent can handle migration tasks",
        "Parallelization adds too much coordination complexity"
      ],
      "why_it_fails": [
        "Sequential processing of hundreds of files takes days or weeks",
        "Single agents get stuck, lose context, or make inconsistent changes",
        "Manual migrations are tedious and error-prone"
      ],
      "first_principle_insight": "Large-scale migrations are embarrassingly parallel  each file change is independent. The atomic unit isn't 'the migration' but 'the file change.' A swarm of subagents processing files concurrently can deliver 10x+ speedup.",
      "how_to_reframe": {
        "old_atomic_unit": "The migration as a single task",
        "new_atomic_unit": "Each file change as an independent, parallelizable subtask"
      },
      "reframe": {
        "new_atomic_unit": "Independent file-level change",
        "new_problem_type": "Map-reduce over parallelizable chunks",
        "new_objective": "Maximize throughput via concurrent subagent execution",
        "architectural_changes": [
          {
            "change": "Swarm Architecture",
            "description": "Main orchestrator creates migration plan (enumerate all items), breaks into atomic chunks, spawns 10+ subagents working in parallel, then consolidates and verifies results"
          },
          {
            "change": "Atomic Task Design",
            "description": "Each subtask must be independent  no shared state, no ordering dependencies between files"
          },
          {
            "change": "Prerequisites",
            "description": "Requires clear specifications and good test coverage to validate parallel changes"
          }
        ],
        "results": "Internal Anthropic users spending $1,000+/month on migrations report 10x+ speedup vs sequential approaches. Pattern now common for framework upgrades, lint rollouts, and API migrations."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Bali, N. (2026): The Agentic AI Handbook",
          "url": "https://www.nibzard.com/agentic-handbook",
          "note": "Documents swarm pattern with Anthropic internal usage data"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 2 (Deconstruction) - Finding the atomic unit that enables parallelization",
        "common_mistake_avoided": "Treating inherently parallel tasks as sequential",
        "when_to_use": "Large-scale migrations, batch processing, any task with hundreds of independent subtasks"
      },
      "tags": [
        "agents",
        "swarm",
        "parallelization",
        "migrations",
        "code-generation",
        "architecture"
      ]
    },
    {
      "id": "skill-library-token-optimization-2025",
      "title": "Skill Library Evolution: 91% Token Reduction Through Progressive Tool Discovery",
      "category": "Pragmatic Pivot - Agent Architecture",
      "year": "2025",
      "companies_involved": ["Various (pattern documented in Agentic Patterns)"],
      "initial_problem": "Agents with large tool sets consume excessive tokens listing all available tools in every context, increasing cost and latency",
      "initial_assumptions": [
        "Agents need access to all tools at all times",
        "More tools = more capable agent",
        "Token cost of tool descriptions is acceptable overhead"
      ],
      "why_it_fails": [
        "26 tools at 17,000 tokens per request adds significant cost",
        "Most tasks only need 2-4 tools",
        "Token overhead scales with every request, not just complex ones"
      ],
      "first_principle_insight": "Tool discovery should be progressive, not exhaustive. Match tool availability to task requirements. The atomic unit isn't 'all capabilities' but 'capabilities needed for this specific task.'",
      "how_to_reframe": {
        "old_atomic_unit": "Full tool catalog in every context",
        "new_atomic_unit": "Dynamically selected tools based on task classification"
      },
      "reframe": {
        "new_atomic_unit": "Task-specific tool subset",
        "new_problem_type": "Tool routing/selection before execution",
        "new_objective": "Minimize token overhead while maintaining capability",
        "architectural_changes": [
          {
            "change": "Progressive Disclosure",
            "description": "Classify incoming task, select relevant tool subset, only include those tools in context"
          },
          {
            "change": "Skill Library",
            "description": "Persist working code as reusable capabilities. Agent discovers and reuses in future sessions."
          },
          {
            "change": "Tool Metadata",
            "description": "Tag tools with task types they support for efficient routing"
          }
        ],
        "results": "91% token reduction achieved  from 26 tools at 17,000 tokens to 4 selected tools at 1,500 tokens per request."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Agentic Patterns Repository (2025): Progressive Tool Discovery",
          "url": "https://agentic-patterns.com/",
          "note": "Documents token optimization patterns for agent systems"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 4 (Trade-offs) - Token cost vs capability breadth",
        "common_mistake_avoided": "Loading all tools regardless of task requirements",
        "when_to_use": "Any agent system with 10+ tools; cost-sensitive deployments"
      },
      "tags": [
        "agents",
        "optimization",
        "tokens",
        "cost-reduction",
        "tool-use",
        "pragmatic-pivot"
      ]
    },
    {
      "id": "agent-rft-trajectory-learning-2025",
      "title": "Agent Reinforcement Fine-Tuning: 50-72% Improvement from 100-1000 Trajectories",
      "category": "Architectural Pivot - Agent Architecture",
      "year": "2025",
      "companies_involved": ["Various (research pattern)"],
      "initial_problem": "Agent performance plateaus; prompting and tool design improvements yield diminishing returns",
      "initial_assumptions": [
        "Fine-tuning requires massive datasets",
        "Agent behavior is too complex to learn from examples",
        "Prompting is sufficient for agent optimization"
      ],
      "why_it_fails": [
        "Traditional RL requires millions of samples",
        "Prompting can't capture complex multi-step strategies",
        "Generic models don't learn organization-specific workflows"
      ],
      "first_principle_insight": "Train on agent workflows, not just input-output pairs. Successful execution trajectories (the full sequence of tool calls, reasoning, and results) contain learnable strategies. The atomic unit isn't 'correct answer' but 'successful trajectory.'",
      "how_to_reframe": {
        "old_atomic_unit": "Input-output training pairs",
        "new_atomic_unit": "Complete successful execution trajectories including tool interactions"
      },
      "reframe": {
        "new_atomic_unit": "Successful agent trajectory",
        "new_problem_type": "End-to-end training on actual tool interactions",
        "new_objective": "Learn strategies for tool use, not just responses",
        "architectural_changes": [
          {
            "change": "Trajectory Collection",
            "description": "Log successful agent execution trajectories including all tool calls, intermediate reasoning, and final results"
          },
          {
            "change": "Agent RFT Training",
            "description": "Fine-tune model on these complete workflows rather than isolated examples"
          },
          {
            "change": "Continuous Learning",
            "description": "As agents succeed in production, add trajectories to training set for iterative improvement"
          }
        ],
        "results": "50-72% performance improvements achieved from just 100-1,000 successful trajectories  far more sample-efficient than traditional RL approaches requiring millions of samples."
      },
      "sources": [
        {
          "type": "primary",
          "citation": "Bali, N. (2026): The Agentic AI Handbook - Agent RFT Pattern",
          "url": "https://www.nibzard.com/agentic-handbook",
          "note": "Documents Agent RFT results and methodology"
        }
      ],
      "teaching_notes": {
        "loop_step_illustrated": "Step 3 (Alternatives) - Fine-tuning as an alternative when prompting plateaus",
        "common_mistake_avoided": "Assuming fine-tuning requires massive data; ignoring trajectory-level learning",
        "when_to_use": "When agent performance plateaus; when you have 100+ successful execution logs"
      },
      "tags": [
        "agents",
        "fine-tuning",
        "reinforcement-learning",
        "trajectories",
        "optimization",
        "learning"
      ]
    }
  ]
}